{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPYJUV9Jc9j"
      },
      "source": [
        "# Question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "way3SxOsJc9l"
      },
      "source": [
        "Question answering tasks return an answer given a question. If you've ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then you've used a question answering model before. There are two common types of question answering tasks:\n",
        "\n",
        "- Extractive: extract the answer from the given context.\n",
        "- Abstractive: generate an answer from the context that correctly answers the question.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question answering.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [LXMERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lxmert), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [NystrÃ¶mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [Splinter](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/splinter), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
        "\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate\n",
        "```\n",
        "\n",
        "We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2XGBChJc9m"
      },
      "source": [
        "## Load SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CySLoVz8Jc9m"
      },
      "source": [
        "Start by loading a smaller subset of the SQuAD dataset from the ðŸ¤— Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4zPoF5kJc9m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\",streaming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8N5xSwVJc9n"
      },
      "source": [
        "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_enxbNAJc9n"
      },
      "outputs": [],
      "source": [
        "squad = squad.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHApoP-Jc9n"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PVBMY8ZNJc9n",
        "outputId": "69fc988d-d8e0-4760-8a03-6030722c2d59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56cc62406d243a140015ef7c',\n",
              " 'title': 'IPod',\n",
              " 'context': 'The name iPod was proposed by Vinnie Chieco, a freelance copywriter, who (with others) was called by Apple to figure out how to introduce the new player to the public. After Chieco saw a prototype, he thought of the movie 2001: A Space Odyssey and the phrase \"Open the pod bay door, Hal!\", which refers to the white EVA Pods of the Discovery One spaceship. Chieco saw an analogy to the relationship between the spaceship and the smaller independent pods in the relationship between a personal computer and the music player. Apple researched the trademark and found that it was already in use. Joseph N. Grasso of New Jersey had originally listed an \"iPod\" trademark with the U.S. Patent and Trademark Office (USPTO) in July 2000 for Internet kiosks. The first iPod kiosks had been demonstrated to the public in New Jersey in March 1998, and commercial use began in January 2000, but had apparently been discontinued by 2001. The trademark was registered by the USPTO in November 2003, and Grasso assigned it to Apple Computer, Inc. in 2005.',\n",
              " 'question': 'In what year was Apple given rights to the iPod name?',\n",
              " 'answers': {'text': ['2005'], 'answer_start': [1035]}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9m9-CCJc9n"
      },
      "source": [
        "There are several important fields here:\n",
        "\n",
        "- `answers`: the starting location of the answer token and the answer text.\n",
        "- `context`: background information from which the model needs to extract the answer.\n",
        "- `question`: the question a model should answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwSgm7VJc9o"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF3e_rh1Jc9o"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to process the `question` and `context` fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ClzKBVqJc9o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iqA9RnQJc9o"
      },
      "source": [
        "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
        "\n",
        "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
        "2. Next, map the start and end positions of the answer to the original `context` by setting\n",
        "   `return_offset_mapping=True`.\n",
        "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) method to\n",
        "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
        "\n",
        "Here is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gTOrbxUWJc9o"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGMl2sDJc9p"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once. Remove any columns you don't need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EoY7PK4sJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00<00:00, 5790.42 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5629.46 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMgkX807Jc9p"
      },
      "source": [
        "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in ðŸ¤— Transformers, the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) does not apply any additional preprocessing such as padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q4mzMRuuJc9p"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb5kV6LCJc9p"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"from unsloth import FastLanguageModel , is_bfloat16_supported\n",
        "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "model , tokenizer= FastLanguageModel.from_pretrained(model_name ,\n",
        "\n",
        "                                                      load_in_4bit=True,\n",
        "                                                      \n",
        "                                                      max_seq_length=max_seq_length,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      \n",
        "                                                      )\n",
        "\n",
        "model = FastLanguageModel.get_peft_model( model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfDX8OBJc9p"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Csc0G1vmJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No ROCm runtime is found, using ROCM_HOME='/usr'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 3070. Max memory: 7.765 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.27.dev792. FA = True.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,943,040 || all params: 7,289,966,592 || trainable%: 0.5754\n",
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MistralForCausalLM(\n",
            "      (model): MistralModel(\n",
            "        (embed_tokens): Embedding(32768, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x MistralDecoderLayer(\n",
            "            (self_attn): MistralSdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): MistralMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): MistralRMSNorm()\n",
            "            (post_attention_layernorm): MistralRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): MistralRMSNorm()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel , is_bfloat16_supporte\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer,BitsAndBytesConfig\n",
        "from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\n",
        "print(torch.cuda.is_available())\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n",
        "\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    \n",
        "\n",
        ")\n",
        "#\"albert/albert-base-v2\"\n",
        "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "model , tokenizer= FastLanguageModel.from_pretrained(model_name ,\n",
        "                                                      #attn_implementation=\"flash_attention_2\",\n",
        "                                                      #quantization_config=config,\n",
        "                                                      #device_map='auto',\n",
        "                                                      load_in_4bit=True,\n",
        "                                                      \n",
        "                                                      max_seq_length=max_seq_length,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      \n",
        "                                                      )\n",
        "#model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=False)\n",
        "#loraconfig = LoraConfig(r=16,target_modules=['query','key','value'],task_type='QUESTION_ANS')\n",
        "#model = FastLanguageModel.get_peft_model(model=model,lora_alpha= loraconfig)\n",
        "model = FastLanguageModel.get_peft_model( model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    \n",
        "\n",
        ")\n",
        "model.print_trainable_parameters()\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight\n",
            "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight\n"
          ]
        }
      ],
      "source": [
        "for n,p in model.named_parameters():\n",
        "    if p.requires_grad==True:\n",
        "        print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRHUxiR_Jc9p"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VpWeLFttJc9q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 4,000 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 8 | Total steps = 500\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise']\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#import torch._dynamo\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#torch._dynamo.config.suppress_errors = True\u001b[39;00m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_squad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m             )\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:348\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/trainer.py:3282\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 3282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3284\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3285\u001b[0m         )\n\u001b[1;32m   3286\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3287\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
          ]
        }
      ],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "#['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise']\n",
        "#import torch._dynamo\n",
        "#torch._dynamo.config.suppress_errors = True\n",
        " \n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    args=TrainingArguments(\n",
        "            output_dir=\"my_awesome_qa_model\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            learning_rate=1e-4,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=1,\n",
        "            weight_decay=0.01,\n",
        "            #bf16 =True, #amper series\n",
        "            tf32=True,\n",
        "            fp16 = False,#not is_bfloat16_supported(),\n",
        "            bf16 =True, #is_bfloat16_supported(),\n",
        "            optim='adamw_hf',\n",
        "            #weight_decay=0.01,\n",
        "            #dataloader_pin_memory=True,\n",
        "            #dataloader_num_workers=0,\n",
        "            #torch_compile=True,  #seems not good error :(\n",
        "            \n",
        "            #push_to_hub=True,\n",
        "            \n",
        "            )\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"my_awesome_qa_model\")\n",
        "tokenizer.save_pretrained(\"my_awesome_qa_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRSp4sdgJc9q"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScr8NIhJc9q"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdYllCwdJc9q"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRXM4rCmJc9q"
      },
      "source": [
        "Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.\n",
        "\n",
        "If have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) chapter from the ðŸ¤— Hugging Face Course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AWXIEhJc9r"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58smYqZJc9r"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Come up with a question and some context you'd like the model to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZgS3YinJc9r"
      },
      "outputs": [],
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bColCDK3Jc9r"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for question answering with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s4QdRuuJc9r",
        "outputId": "67b311da-e4b7-4996-8ddc-14f283c03aa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.006293352693319321,\n",
              " 'start': 79,\n",
              " 'end': 92,\n",
              " 'answer': 'languages and'}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lJo7j-VJc9r"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMQHxqnxJc9w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0'), 'token_type_ids': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0'), 'attention_mask': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0')}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "inputs['input_ids']=inputs['input_ids'].cuda()\n",
        "inputs['token_type_ids']=inputs['input_ids'].cuda()\n",
        "inputs['attention_mask']=inputs['input_ids'].cuda()\n",
        "inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbdwXWhJc9x"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDfvfZvmJc9x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQpzG1hZJc9x"
      },
      "source": [
        "Get the highest probability from the model output for the start and end positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pyrLCnKJc9x"
      },
      "outputs": [],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjCRwwjZJc9x"
      },
      "source": [
        "Decode the predicted tokens to get the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJNQfEXMJc9y",
        "outputId": "df50798f-2eea-4407-a834-d97b44d302ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS]'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/unslothai/unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a comparison table of the listed optimizers based on various criteria such as usage, precision, hardware support, and typical applications. Note that specific details such as supported hardware (GPU, TPU, NPU) and implementation differences can influence performance and suitability for different training scenarios.\n",
        "\n",
        "| Optimizer Name                 | Implementation      | Precision        | Hardware Support            | Key Features                                   | Typical Applications                       |\n",
        "|--------------------------------|---------------------|------------------|-----------------------------|------------------------------------------------|--------------------------------------------|\n",
        "| adamw_hf                       | Hugging Face        | 32-bit           | CPU, GPU                    | Decoupled weight decay, Transformer training  | NLP, Transformers                          |\n",
        "| adamw_torch                    | PyTorch             | 32-bit           | CPU, GPU                    | Decoupled weight decay, flexible             | General deep learning                      |\n",
        "| adamw_torch_fused              | PyTorch             | 32-bit           | CPU, GPU                    | Fused operations for efficiency              | General deep learning                      |\n",
        "| adamw_torch_xla                | PyTorch             | 32-bit           | TPU                         | TPU optimized                                 | High-performance training on TPUs          |\n",
        "| adamw_torch_npu_fused          | PyTorch             | 32-bit           | NPU                         | Fused operations for NPUs                    | Training on NPU hardware                   |\n",
        "| adamw_apex_fused               | NVIDIA Apex         | 32-bit, mixed    | GPU                         | Fused operations, mixed precision            | High-performance training on GPUs          |\n",
        "| adafactor                      | TensorFlow, HF      | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| adamw_anyprecision             | Custom              | Variable         | CPU, GPU                    | Flexible precision handling                  | Custom precision requirements              |\n",
        "| sgd                            | Standard            | 32-bit           | CPU, GPU                    | Simple, effective                            | Classic machine learning, simple models    |\n",
        "| adagrad                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | Sparse data                                |\n",
        "| adamw_bnb_8bit                 | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| adamw_8bit                     | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Large models with memory constraints       |\n",
        "| lion_8bit                      | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Memory constrained environments            |\n",
        "| lion_32bit                     | Custom              | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| paged_adamw_32bit              | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_adamw_8bit               | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| paged_lion_32bit               | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_lion_8bit                | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| rmsprop                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | RNNs, general deep learning                |\n",
        "| rmsprop_bnb                    | BitsAndBytes        | 32-bit           | CPU, GPU                    | BitsAndBytes optimization                    | Memory efficient training                  |\n",
        "| rmsprop_bnb_8bit               | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| rmsprop_bnb_32bit              | BitsAndBytes        | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| galore_adamw                   | Galore              | 32-bit           | CPU, GPU                    | Enhanced AdamW                               | Advanced training scenarios                |\n",
        "| galore_adamw_8bit              | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Memory constrained environments            |\n",
        "| galore_adafactor               | Galore              | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| galore_adamw_layerwise         | Galore              | 32-bit           | CPU, GPU                    | Layerwise optimization                       | Advanced training scenarios                |\n",
        "| galore_adamw_8bit_layerwise    | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, layerwise optimization     | Memory constrained environments            |\n",
        "| galore_adafactor_layerwise     | Galore              | 32-bit, mixed    | CPU, GPU                    | Layerwise optimization, memory efficient     | NLP, large models                          |\n",
        "\n",
        "### Notes:\n",
        "- **Precision**: Indicates whether the optimizer supports standard 32-bit precision or has options for mixed/8-bit precision for memory efficiency.\n",
        "- **Hardware Support**: Identifies the primary hardware the optimizer is designed to run on efficiently, e.g., CPU, GPU, TPU, NPU.\n",
        "- **Key Features**: Highlights unique aspects or enhancements that distinguish each optimizer.\n",
        "- **Typical Applications**: Common use cases or scenarios where the optimizer is particularly effective.\n",
        "\n",
        "Choosing the right optimizer depends on your specific training needs, hardware availability, and whether you need to manage large models or datasets within memory constraints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence speed of an optimizer depends on various factors such as the type of model, the dataset, the specific problem being solved, and the tuning of hyperparameters. However, here are some general insights into the convergence speed of the listed optimizers:\n",
        "\n",
        "1. **AdamW variants**:\n",
        "    - `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, `adamw_torch_xla`, `adamw_torch_npu_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adamw_bnb_8bit`, `adamw_8bit`, `galore_adamw`, `galore_adamw_8bit`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`:\n",
        "      - AdamW is known for fast convergence due to its adaptive learning rate and decoupled weight decay. The fused versions (`fused`, `apex_fused`, `torch_fused`) can provide additional speedup due to more efficient computations.\n",
        "      - `adamw_xla` and `adamw_npu_fused` are optimized for specific hardware (TPU and NPU, respectively), which can lead to faster convergence on those platforms.\n",
        "\n",
        "2. **Adafactor**:\n",
        "    - `adafactor`, `galore_adafactor`, `galore_adafactor_layerwise`:\n",
        "      - Adafactor is memory-efficient and suitable for training very large models. It can converge quickly in large-scale NLP tasks, particularly when memory constraints are an issue.\n",
        "\n",
        "3. **Lion**:\n",
        "    - `lion_8bit`, `lion_32bit`, `paged_lion_32bit`, `paged_lion_8bit`:\n",
        "      - Lion optimizers are less commonly used but can offer faster convergence in some scenarios due to their specific optimization strategies.\n",
        "\n",
        "4. **SGD**:\n",
        "    - `sgd`:\n",
        "      - SGD with momentum can converge quickly in some scenarios but generally requires more careful tuning of learning rates and momentum parameters. It may not converge as fast as AdamW in many deep learning tasks.\n",
        "\n",
        "5. **Adagrad**:\n",
        "    - `adagrad`:\n",
        "      - Adagrad adapts the learning rate based on the historical gradient, which can be beneficial for sparse data but may lead to slower convergence in dense data scenarios.\n",
        "\n",
        "6. **Paged AdamW**:\n",
        "    - `paged_adamw_32bit`, `paged_adamw_8bit`:\n",
        "      - These optimizers are designed to handle large datasets with better memory management. Convergence speed can be good, especially for large-scale training.\n",
        "\n",
        "7. **RMSprop**:\n",
        "    - `rmsprop`, `rmsprop_bnb`, `rmsprop_bnb_8bit`, `rmsprop_bnb_32bit`:\n",
        "      - RMSprop is designed for fast convergence in non-stationary settings. It can converge faster than SGD in many cases.\n",
        "\n",
        "8. **Galore Optimizers**:\n",
        "    - `galore_adamw`, `galore_adamw_8bit`, `galore_adafactor`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`, `galore_adafactor_layerwise`:\n",
        "      - These optimizers are designed for advanced training scenarios and can offer fast convergence, especially when specific memory constraints or layer-wise optimizations are needed.\n",
        "\n",
        "### General Recommendations:\n",
        "- For most standard deep learning tasks, **AdamW variants** are likely to offer the fastest convergence due to their adaptive learning rates and weight decay.\n",
        "- For large-scale NLP tasks, **Adafactor** can be very efficient and fast.\n",
        "- For training on specific hardware (TPU, NPU), use the optimizers optimized for those platforms like `adamw_torch_xla` or `adamw_torch_npu_fused`.\n",
        "- For memory-constrained environments, **8-bit variants** and **Paged optimizers** can offer good convergence speed while managing memory efficiently.\n",
        "- If using large datasets, **paged variants** of AdamW or Lion can be particularly effective.\n",
        "\n",
        "Ultimately, the best way to determine which optimizer converges the fastest for your specific use case is to experiment with a few of them on your dataset and model. Hyperparameter tuning (such as learning rate adjustments) also plays a significant role in convergence speed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
