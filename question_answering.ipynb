{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPYJUV9Jc9j"
      },
      "source": [
        "# Question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "way3SxOsJc9l"
      },
      "source": [
        "Question answering tasks return an answer given a question. If you've ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then you've used a question answering model before. There are two common types of question answering tasks:\n",
        "\n",
        "- Extractive: extract the answer from the given context.\n",
        "- Abstractive: generate an answer from the context that correctly answers the question.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question answering.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [LXMERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lxmert), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [NystrÃ¶mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [Splinter](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/splinter), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
        "\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate\n",
        "```\n",
        "\n",
        "We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2XGBChJc9m"
      },
      "source": [
        "## Load SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CySLoVz8Jc9m"
      },
      "source": [
        "Start by loading a smaller subset of the SQuAD dataset from the ðŸ¤— Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4zPoF5kJc9m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.5M/14.5M [00:00<00:00, 33.9MB/s]\n",
            "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.82M/1.82M [00:00<00:00, 7.24MB/s]\n",
            "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87599/87599 [00:01<00:00, 83901.86 examples/s] \n",
            "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [00:00<00:00, 72123.00 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "cache_dir = '/proj/ciptmp/ix05ogym/.cache/'\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\",streaming=False,cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8N5xSwVJc9n"
      },
      "source": [
        "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_enxbNAJc9n"
      },
      "outputs": [],
      "source": [
        "squad = squad.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHApoP-Jc9n"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PVBMY8ZNJc9n",
        "outputId": "69fc988d-d8e0-4760-8a03-6030722c2d59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56d2581659d6e41400145edc',\n",
              " 'title': 'To_Kill_a_Mockingbird',\n",
              " 'context': 'Absent mothers and abusive fathers are another theme in the novel. Scout and Jem\\'s mother died before Scout could remember her, Mayella\\'s mother is dead, and Mrs. Radley is silent about Boo\\'s confinement to the house. Apart from Atticus, the fathers described are abusers. Bob Ewell, it is hinted, molested his daughter, and Mr. Radley imprisons his son in his house until Boo is remembered only as a phantom. Bob Ewell and Mr. Radley represent a form of masculinity that Atticus does not, and the novel suggests that such men as well as the traditionally feminine hypocrites at the Missionary Society can lead society astray. Atticus stands apart as a unique model of masculinity; as one scholar explains: \"It is the job of real men who embody the traditional masculine qualities of heroic individualism, bravery, and an unshrinking knowledge of and dedication to social justice and morality, to set the society straight.\"',\n",
              " 'question': 'Who was the only non-abusive father mentioned?',\n",
              " 'answers': {'text': ['Atticus'], 'answer_start': [229]}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9m9-CCJc9n"
      },
      "source": [
        "There are several important fields here:\n",
        "\n",
        "- `answers`: the starting location of the answer token and the answer text.\n",
        "- `context`: background information from which the model needs to extract the answer.\n",
        "- `question`: the question a model should answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwSgm7VJc9o"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF3e_rh1Jc9o"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to process the `question` and `context` fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8ClzKBVqJc9o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\",cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iqA9RnQJc9o"
      },
      "source": [
        "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
        "\n",
        "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
        "2. Next, map the start and end positions of the answer to the original `context` by setting\n",
        "   `return_offset_mapping=True`.\n",
        "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) method to\n",
        "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
        "\n",
        "Here is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gTOrbxUWJc9o"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGMl2sDJc9p"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once. Remove any columns you don't need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EoY7PK4sJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:00<00:00, 5392.55 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5351.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMgkX807Jc9p"
      },
      "source": [
        "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in ðŸ¤— Transformers, the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) does not apply any additional preprocessing such as padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q4mzMRuuJc9p"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb5kV6LCJc9p"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from unsloth import FastLanguageModel , is_bfloat16_supported\\nmodel_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\\nmax_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\\n\\nmodel , tokenizer= FastLanguageModel.from_pretrained(model_name ,\\n\\n                                                      load_in_4bit=True,\\n                                                      \\n                                                      max_seq_length=max_seq_length,\\n                                                      cache_dir=\\'/proj/ciptmp/ix05ogym/.cache/\\',\\n                                                      \\n                                                      )\\n\\nmodel = FastLanguageModel.get_peft_model( model,\\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\\n    lora_alpha = 16,\\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n\\n)'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from unsloth import FastLanguageModel , is_bfloat16_supported\n",
        "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n",
        "\n",
        "model , tokenizer= FastLanguageModel.from_pretrained(model_name ,\n",
        "\n",
        "                                                      load_in_4bit=True,\n",
        "                                                      \n",
        "                                                      max_seq_length=max_seq_length,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      \n",
        "                                                      )\n",
        "\n",
        "model = FastLanguageModel.get_peft_model( model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfDX8OBJc9p"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Csc0G1vmJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No ROCm runtime is found, using ROCM_HOME='/usr'\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AlbertForQuestionAnswering(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear4bit(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
            "              (activation): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n",
            "trainable params: 99,842 || all params: 11,194,372 || trainable%: 0.8919\n",
            "11829256\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#from unsloth import FastLanguageModel , is_bfloat16_supported\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer,BitsAndBytesConfig\n",
        "from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\n",
        "print(torch.cuda.is_available())\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "#\"albert/albert-base-v2\"\n",
        "model_name = \"albert/albert-base-v2\"#\"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name ,\n",
        "                                                      #attn_implementation=\"flash_attention_2\",\n",
        "                                                      quantization_config=config,\n",
        "                                                      #low_cpu_mem_usage=True,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      #device_map=\"auto\",\n",
        "                                                      \n",
        "                                                      )\n",
        "print(model)\n",
        "\n",
        "#model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=False)\n",
        "loraconfig = LoraConfig(r=16,target_modules=['query','key','value','dense'],task_type='QUESTION_ANS')\n",
        "model = get_peft_model(model, loraconfig)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "print(model.get_memory_footprint())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.lora_B.default.weight\n",
            "base_model.model.qa_outputs.modules_to_save.default.weight\n",
            "base_model.model.qa_outputs.modules_to_save.default.bias\n"
          ]
        }
      ],
      "source": [
        "for n,p in model.named_parameters():\n",
        "    if p.requires_grad==True:\n",
        "        print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRHUxiR_Jc9p"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VpWeLFttJc9q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 01:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.204600</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "NaN or Inf found in input tensor.\n",
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "NaN or Inf found in input tensor.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=3.204638671875, metrics={'train_runtime': 102.1162, 'train_samples_per_second': 39.171, 'train_steps_per_second': 4.896, 'total_flos': 67171553280000.0, 'train_loss': 3.204638671875, 'epoch': 1.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "#['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise']\n",
        "#import torch._dynamo\n",
        "#torch._dynamo.config.suppress_errors = True\n",
        " \n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    args=TrainingArguments(\n",
        "            output_dir=\"my_awesome_qa_model\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            learning_rate=1e-4,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=1,\n",
        "            weight_decay=0.01,\n",
        "            #bf16 =True, #amper series\n",
        "            tf32=True,\n",
        "            fp16 = False,#not is_bfloat16_supported(),\n",
        "            bf16 =True, #is_bfloat16_supported(),\n",
        "            optim='adamw_hf',\n",
        "            #weight_decay=0.01,\n",
        "            #dataloader_pin_memory=True,\n",
        "            #dataloader_num_workers=0,\n",
        "            #torch_compile=True,  #seems not good error :(\n",
        "            \n",
        "            #push_to_hub=True,\n",
        "            report_to='tensorboard'\n",
        "            \n",
        "            )\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('my_awesome_qa_model/tokenizer_config.json',\n",
              " 'my_awesome_qa_model/special_tokens_map.json',\n",
              " 'my_awesome_qa_model/spiece.model',\n",
              " 'my_awesome_qa_model/added_tokens.json',\n",
              " 'my_awesome_qa_model/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"my_awesome_qa_model\")\n",
        "tokenizer.save_pretrained(\"my_awesome_qa_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRSp4sdgJc9q"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScr8NIhJc9q"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdYllCwdJc9q"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRXM4rCmJc9q"
      },
      "source": [
        "Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.\n",
        "\n",
        "If have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) chapter from the ðŸ¤— Hugging Face Course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AWXIEhJc9r"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58smYqZJc9r"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Come up with a question and some context you'd like the model to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3ZgS3YinJc9r"
      },
      "outputs": [],
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bColCDK3Jc9r"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for question answering with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8s4QdRuuJc9r",
        "outputId": "67b311da-e4b7-4996-8ddc-14f283c03aa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.01029176265001297,\n",
              " 'start': 89,\n",
              " 'end': 107,\n",
              " 'answer': 'and 13 programming'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lJo7j-VJc9r"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CMQHxqnxJc9w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0'), 'token_type_ids': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0'), 'attention_mask': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0')}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "inputs['input_ids']=inputs['input_ids'].cuda()\n",
        "inputs['token_type_ids']=inputs['input_ids'].cuda()\n",
        "inputs['attention_mask']=inputs['input_ids'].cuda()\n",
        "inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbdwXWhJc9x"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lDfvfZvmJc9x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQpzG1hZJc9x"
      },
      "source": [
        "Get the highest probability from the model output for the start and end positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0pyrLCnKJc9x"
      },
      "outputs": [],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjCRwwjZJc9x"
      },
      "source": [
        "Decode the predicted tokens to get the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nJNQfEXMJc9y",
        "outputId": "df50798f-2eea-4407-a834-d97b44d302ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS]'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/unslothai/unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a comparison table of the listed optimizers based on various criteria such as usage, precision, hardware support, and typical applications. Note that specific details such as supported hardware (GPU, TPU, NPU) and implementation differences can influence performance and suitability for different training scenarios.\n",
        "\n",
        "| Optimizer Name                 | Implementation      | Precision        | Hardware Support            | Key Features                                   | Typical Applications                       |\n",
        "|--------------------------------|---------------------|------------------|-----------------------------|------------------------------------------------|--------------------------------------------|\n",
        "| adamw_hf                       | Hugging Face        | 32-bit           | CPU, GPU                    | Decoupled weight decay, Transformer training  | NLP, Transformers                          |\n",
        "| adamw_torch                    | PyTorch             | 32-bit           | CPU, GPU                    | Decoupled weight decay, flexible             | General deep learning                      |\n",
        "| adamw_torch_fused              | PyTorch             | 32-bit           | CPU, GPU                    | Fused operations for efficiency              | General deep learning                      |\n",
        "| adamw_torch_xla                | PyTorch             | 32-bit           | TPU                         | TPU optimized                                 | High-performance training on TPUs          |\n",
        "| adamw_torch_npu_fused          | PyTorch             | 32-bit           | NPU                         | Fused operations for NPUs                    | Training on NPU hardware                   |\n",
        "| adamw_apex_fused               | NVIDIA Apex         | 32-bit, mixed    | GPU                         | Fused operations, mixed precision            | High-performance training on GPUs          |\n",
        "| adafactor                      | TensorFlow, HF      | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| adamw_anyprecision             | Custom              | Variable         | CPU, GPU                    | Flexible precision handling                  | Custom precision requirements              |\n",
        "| sgd                            | Standard            | 32-bit           | CPU, GPU                    | Simple, effective                            | Classic machine learning, simple models    |\n",
        "| adagrad                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | Sparse data                                |\n",
        "| adamw_bnb_8bit                 | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| adamw_8bit                     | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Large models with memory constraints       |\n",
        "| lion_8bit                      | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Memory constrained environments            |\n",
        "| lion_32bit                     | Custom              | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| paged_adamw_32bit              | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_adamw_8bit               | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| paged_lion_32bit               | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_lion_8bit                | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| rmsprop                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | RNNs, general deep learning                |\n",
        "| rmsprop_bnb                    | BitsAndBytes        | 32-bit           | CPU, GPU                    | BitsAndBytes optimization                    | Memory efficient training                  |\n",
        "| rmsprop_bnb_8bit               | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| rmsprop_bnb_32bit              | BitsAndBytes        | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| galore_adamw                   | Galore              | 32-bit           | CPU, GPU                    | Enhanced AdamW                               | Advanced training scenarios                |\n",
        "| galore_adamw_8bit              | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Memory constrained environments            |\n",
        "| galore_adafactor               | Galore              | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| galore_adamw_layerwise         | Galore              | 32-bit           | CPU, GPU                    | Layerwise optimization                       | Advanced training scenarios                |\n",
        "| galore_adamw_8bit_layerwise    | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, layerwise optimization     | Memory constrained environments            |\n",
        "| galore_adafactor_layerwise     | Galore              | 32-bit, mixed    | CPU, GPU                    | Layerwise optimization, memory efficient     | NLP, large models                          |\n",
        "\n",
        "### Notes:\n",
        "- **Precision**: Indicates whether the optimizer supports standard 32-bit precision or has options for mixed/8-bit precision for memory efficiency.\n",
        "- **Hardware Support**: Identifies the primary hardware the optimizer is designed to run on efficiently, e.g., CPU, GPU, TPU, NPU.\n",
        "- **Key Features**: Highlights unique aspects or enhancements that distinguish each optimizer.\n",
        "- **Typical Applications**: Common use cases or scenarios where the optimizer is particularly effective.\n",
        "\n",
        "Choosing the right optimizer depends on your specific training needs, hardware availability, and whether you need to manage large models or datasets within memory constraints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence speed of an optimizer depends on various factors such as the type of model, the dataset, the specific problem being solved, and the tuning of hyperparameters. However, here are some general insights into the convergence speed of the listed optimizers:\n",
        "\n",
        "1. **AdamW variants**:\n",
        "    - `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, `adamw_torch_xla`, `adamw_torch_npu_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adamw_bnb_8bit`, `adamw_8bit`, `galore_adamw`, `galore_adamw_8bit`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`:\n",
        "      - AdamW is known for fast convergence due to its adaptive learning rate and decoupled weight decay. The fused versions (`fused`, `apex_fused`, `torch_fused`) can provide additional speedup due to more efficient computations.\n",
        "      - `adamw_xla` and `adamw_npu_fused` are optimized for specific hardware (TPU and NPU, respectively), which can lead to faster convergence on those platforms.\n",
        "\n",
        "2. **Adafactor**:\n",
        "    - `adafactor`, `galore_adafactor`, `galore_adafactor_layerwise`:\n",
        "      - Adafactor is memory-efficient and suitable for training very large models. It can converge quickly in large-scale NLP tasks, particularly when memory constraints are an issue.\n",
        "\n",
        "3. **Lion**:\n",
        "    - `lion_8bit`, `lion_32bit`, `paged_lion_32bit`, `paged_lion_8bit`:\n",
        "      - Lion optimizers are less commonly used but can offer faster convergence in some scenarios due to their specific optimization strategies.\n",
        "\n",
        "4. **SGD**:\n",
        "    - `sgd`:\n",
        "      - SGD with momentum can converge quickly in some scenarios but generally requires more careful tuning of learning rates and momentum parameters. It may not converge as fast as AdamW in many deep learning tasks.\n",
        "\n",
        "5. **Adagrad**:\n",
        "    - `adagrad`:\n",
        "      - Adagrad adapts the learning rate based on the historical gradient, which can be beneficial for sparse data but may lead to slower convergence in dense data scenarios.\n",
        "\n",
        "6. **Paged AdamW**:\n",
        "    - `paged_adamw_32bit`, `paged_adamw_8bit`:\n",
        "      - These optimizers are designed to handle large datasets with better memory management. Convergence speed can be good, especially for large-scale training.\n",
        "\n",
        "7. **RMSprop**:\n",
        "    - `rmsprop`, `rmsprop_bnb`, `rmsprop_bnb_8bit`, `rmsprop_bnb_32bit`:\n",
        "      - RMSprop is designed for fast convergence in non-stationary settings. It can converge faster than SGD in many cases.\n",
        "\n",
        "8. **Galore Optimizers**:\n",
        "    - `galore_adamw`, `galore_adamw_8bit`, `galore_adafactor`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`, `galore_adafactor_layerwise`:\n",
        "      - These optimizers are designed for advanced training scenarios and can offer fast convergence, especially when specific memory constraints or layer-wise optimizations are needed.\n",
        "\n",
        "### General Recommendations:\n",
        "- For most standard deep learning tasks, **AdamW variants** are likely to offer the fastest convergence due to their adaptive learning rates and weight decay.\n",
        "- For large-scale NLP tasks, **Adafactor** can be very efficient and fast.\n",
        "- For training on specific hardware (TPU, NPU), use the optimizers optimized for those platforms like `adamw_torch_xla` or `adamw_torch_npu_fused`.\n",
        "- For memory-constrained environments, **8-bit variants** and **Paged optimizers** can offer good convergence speed while managing memory efficiently.\n",
        "- If using large datasets, **paged variants** of AdamW or Lion can be particularly effective.\n",
        "\n",
        "Ultimately, the best way to determine which optimizer converges the fastest for your specific use case is to experiment with a few of them on your dataset and model. Hyperparameter tuning (such as learning rate adjustments) also plays a significant role in convergence speed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
