{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBjn-QC-y5W"
      },
      "source": [
        "NOTE: You might need to restart the session after the pip install block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No ROCm runtime is found, using ROCM_HOME='/usr'\n",
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "772d15f2e45542f0bdccb8bb2506c3d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import T5ForConditionalGeneration, AutoModel,AutoTokenizer,AutoModelForMaskedLM , Trainer,TrainingArguments,\\\n",
        "BitsAndBytesConfig,pipeline,default_data_collator,DataCollatorWithPadding,DataCollatorForLanguageModeling\n",
        "from transformers.utils import move_cache\n",
        "from chat_template_utils import get_json_schema\n",
        "from llama_cpp import Llama\n",
        "from peft import *\n",
        "import datasets\n",
        "import torchmetrics\n",
        "import torch\n",
        "from hqq.engine.hf import HQQModelForCausalLM\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from huggingface_hub import snapshot_download\n",
        "#import deepspeed\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from var_dump import var_dump\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "cache_dir='/var/tmp/.cache/' #'/proj/ciptmp/ix05ogym/.cache/'\n",
        "output_dir = cache_dir+'outputs/'\n",
        "\n",
        "move_cache(cache_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /var/tmp/.cache/models--PrunaAI--Mistral-7B-v0.3-GGUF-smashed/snapshots/f92b9b698d97195a46282d82b2125d2bd2e65287/./Mistral-7B-v0.3.Q4_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = tmpwaj2eqkf\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  217 tensors\n",
            "llama_model_loader: - type q5_K:    8 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 1027\n",
            "llm_load_vocab: token to piece cache size = 0.1731 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32768\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Small\n",
            "llm_load_print_meta: model params     = 7.25 B\n",
            "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
            "llm_load_print_meta: general.name     = tmpwaj2eqkf\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 781 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3952.02 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.13 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '32768', 'general.name': 'tmpwaj2eqkf', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '14', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "model = Llama.from_pretrained(\n",
        "    repo_id=\"PrunaAI/Mistral-7B-v0.3-GGUF-smashed\",\n",
        "    filename=\"*Q4_K_S.gguf\",\n",
        "    cache_dir=cache_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /home/cip/ce/ix05ogym/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login('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')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>▁[INST]▁use▁this▁HTML▁format:▁<TAG_NAME▁label=\"LABEL_NAME\"▁type=\"TYPE_NAME\"▁>▁<0x0A><0x0A>▁<input▁type=\"email\">▁▁[/INST]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:1054: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 <input type=\"password\">\n",
            "\n",
            "  <input type=\"text\">\n",
            "\n",
            "  <input type=\"url\">\n",
            "\n",
            "  <input type=\"date\">\n",
            "\n",
            "  <input type=\"number\">\n",
            "\n",
            "  <input type=\"}\n",
            "2 use this HTML format: <TAG_NAME label=\"LABEL_NAME\" type=\"TYPE_NAME\" >  <input type=\"password\">   The default password is '123456'. Please change it when you login for\"}\n",
            "3 ## What is a form?\n",
            "\n",
            "A form is an element of a web page that allows users to enter information. You can create forms with HTML, but it's easier to use the Form Builder. The Form Builder gives you a\"}\n",
            "4 class=\"form-control\" id=\"exampleInputEmail1\" aria-describedby=\"emailHelp\" placeholder=\"Enter email\">\n",
            "\n",
            "<select class=\"custom-select\" id=\"inlineFormCustomSelectPref\">\n",
            "  <option selected\"}\n",
            "5 input type=\"tel\">   <textarea>\n",
            "\n",
            "# How to use the SMS Notification Service\n",
            "\n",
            "This article explains how to use the SMS notification service. This service is included in our premium plan and above. We will\"}\n",
            "6 label=\"Email Address\"  type=\"text\">  </form>\n",
            "\n",
            "## The Wave\n",
            "\n",
            "The Wave is a unique, patented technology that consists of a series of large-scale, wave-shaped panels made from recycled\"}\n",
            "7 email@example.com  </input>\n",
            "\n",
            "## What is a tag?\n",
            "\n",
            "Tags are the basic building blocks of your website. They allow you to add content and functionality to your pages. You can think of a tag as being\"}\n",
            "8 use this HTML format: <TAG_NAME label=\"LABEL_NAME\" type=\"TYPE_NAME\" >  <input type=\"text\">\n",
            "\n",
            "## About Us\n",
            "\n",
            "The Center for Applied Research and Educational Technology (CARET\"}\n",
            "9 use this HTML format: <TAG_NAME label=\"LABEL_NAME\" type=\"TYPE_NAME\" >  <textarea>\n",
            "\n",
            "# About Us\n",
            "\n",
            "## What is E-Waste?\n",
            "\n",
            "Electronic Waste\"}\n",
            "10 use this HTML format: <TAG_NAME label=\"LABEL_NAME\" type=\"TYPE_NAME\" >  <input type=\"number\">   The following example shows how to create a number field. <input type=\"number\">   The\"}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[335], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     44\u001b[0m     output \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:1674\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1611\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1612\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1639\u001b[0m \n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:1607\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1605\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1607\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:1132\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1130\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1131\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1132\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:740\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    742\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    743\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    744\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    759\u001b[0m         )\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/llama.py:578\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    574\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    576\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    577\u001b[0m )\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
            "File \u001b[0;32m/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/llama_cpp/_internals.py:344\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import (\n",
        "    UserMessage,SystemMessage\n",
        ")\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "from mistral_common.protocol.instruct.tool_calls import (\n",
        "    Function,\n",
        "    Tool,\n",
        "    \n",
        ")\n",
        "\n",
        "def sum(a:int,b:int):\n",
        "    \"\"\"\n",
        "    this function calculate the summation of two intigers.\n",
        "    Args:\n",
        "        a: the first number \n",
        "        b: the second number\n",
        "    \"\"\"\n",
        "    \n",
        "    return a+b\n",
        "\n",
        "mtok = MistralTokenizer.from_model('open-mistral-7b')\n",
        "sum_schema = get_json_schema(sum)\n",
        "#system_content = \"\"\"you are a web navigator. should generate your answer in short json with following format: { \"element_type\" : \"YOUR_RESPONSE\" }\"\"\"# you should choose tools between [\"clicking\",\"typing\"]. you do not generate anything more.\"\"\"\n",
        "system_content =\"\"\"use this HTML format: <TAG_NAME label=\"LABEL_NAME\" type=\"TYPE_NAME\" > \"\"\"\n",
        "encoded = mtok.encode_chat_completion(ChatCompletionRequest(model='open-mistral-7b',\n",
        "                                                  messages=[SystemMessage(content=system_content),\n",
        "                                                                          \n",
        "                                                            UserMessage(content=\"\"\" <input type=\"email\"> \"\"\")                \n",
        "                                                            ]\n",
        "                                                  ,tools=[Tool(function= Function(**sum_schema['function'])\n",
        "                                                      )]\n",
        "                                                  )\n",
        "                            )\n",
        "\n",
        "\n",
        "print(encoded.text)\n",
        "i=0\n",
        "model.verbose = False\n",
        "while True:\n",
        "    t = model(encoded.text,max_tokens=50,stop='}',temperature=0.5)\n",
        "    i+=1\n",
        "    \n",
        "    output = t['choices'][0]['text']\n",
        "    try:\n",
        "    \n",
        "        output = output.strip()\n",
        "        if output[-1]!= '\"':\n",
        "            output+='\"'\n",
        "        output+='}'\n",
        "        print(i,output)\n",
        "        start = output.find('{')\n",
        "        end = output.find('}',start)\n",
        "        output = output[start:end+1]\n",
        "        awnser = json.loads(output)['answer']\n",
        "        if awnser!=\"YOUR_RESPONSE\":\n",
        "            break\n",
        "        \n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sum_schema = get_json_schema(sum)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"\",cache_dir=cache_dir)\\nchat_1 = { \"role\":\"system\",\"content\":\"you are a chatbot and should generate your anwser in a short json format.\" }\\nchat_2 = { \"role\":\"user\",\"content\":\"hello how are you?\" }\\ntokenizer.apply_chat_template(conversation=[chat_1,chat_2],\\n                              tools = [],\\n                              documents =[],\\n                              )\\n'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "\n",
        "#https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "#https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.apply_chat_template.add_generation_prompt\n",
        "#https://github.com/mistralai/mistral-common\n",
        "\n",
        "\"\"\"sum_schema = get_json_schema(sum)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"\",cache_dir=cache_dir)\n",
        "chat_1 = { \"role\":\"system\",\"content\":\"you are a chatbot and should generate your anwser in a short json format.\" }\n",
        "chat_2 = { \"role\":\"user\",\"content\":\"hello how are you?\" }\n",
        "tokenizer.apply_chat_template(conversation=[chat_1,chat_2],\n",
        "                              tools = [],\n",
        "                              documents =[],\n",
        "                              )\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'llama-2'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get_peft_model(model,LoraConfig(target_modules=))\n",
        "model.create_chat_completion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AutoTokenizer.from_pretrained()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiefB_6WJqYI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
        "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5mUr765MGiy"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "c0c593c03a964270b665b5290c42a60a",
            "7d4a41c7aff442c091170a7a4fd3721d",
            "fbe05f93dd7a4b6aa49461692739e532",
            "6dc1d08dc7f74bd59d3c4e77375c723c",
            "e1724ac6b74842beb699d1e31387411a",
            "00ab964e29c24259b9ae6bddb768c8e4",
            "9d82c6578dfa4e0c85d9d9305819128f",
            "2d71f2e009f843fb93ec067c74bcdb87",
            "4c4c0ad7e00945f6a26bcdbc0cfe2581",
            "1ebb8055cdf344b19f51adee9ed01f6f",
            "3b0707c98f854b86acbd93a9a148783b",
            "89cea55de3504cf7ae7a1e8fe930d673",
            "3043c17bcd5a4e55bbebe2816e25362b",
            "b368e5890343426da3919e88a3256b12",
            "f26d725b385140ae8072889811cddbcd",
            "3966ec3d84944e61955abdef9d6820e8",
            "addb7a0362f34adf86e50d2fa6cc91a0",
            "89a354426bcd43069ef2a56f7327e56a",
            "f0b4a085e56a4686bc3f80f8b2298450",
            "1d777cc0a6274a029d7c26e39da2c5c1",
            "c029588ae4544d6b956d0e655220999d",
            "e2522fc182a34c988930c8560cb529b5",
            "70fc89f5dd4144969670897c8b01d1d5",
            "30f788d5023a4f689351769d8a219589",
            "94a31bd1bba248269f703e24775821c9",
            "55367c0c7c1f4512ac91604f12d98690",
            "558c62d8a69c48d4903e1b4f125f30b5",
            "bfbb8e459dbf4a34b66506a7178a9b20",
            "050da96dd3f54880830b6834bedefa2c",
            "214bb06e05914deead7c551f20e105d1",
            "41855b940a5240b594e802a78c97708d",
            "ae9d5773829147878b2f258e6b0f79ae",
            "32c27701e7594f1daca7727e45d08714",
            "ce8672e29d58439e977ad782dc5333f2",
            "ef42c8327c56443888deed9ab86f0140",
            "700f1b111b634c33bd651d3a07e4e536",
            "f8a4204d074c45d0be046f20a27b0b02",
            "b31cf5c90ce64ca68487a636a08f6e0f",
            "d0f01c1becd04a5eb7705b7707d2486d",
            "c174d2b2990e4bc98dff602a01aedac1",
            "c16c894c3e8e4366b3de74b5ff4bae63",
            "e65cb0bbaa534c96a5ece677d384d581",
            "29b2703622534a2eb5d187dac9a0df64",
            "3614543c66b04299bb5396a798bde520",
            "3c79248334874a33b057ac4d45c28487",
            "79c7d9c3d5de4afc9235abd70c0282bd",
            "e7a62ffa0a994529bdbfbe04107d97bd",
            "839e7ffb6d184854a4ef390f63c0623a",
            "7cec2fcf521c406384ac6dd3d761545e",
            "6ce89c7b2d7347519e0a3c3e425307f0",
            "ac84907197e840adbafdde5e285d6797",
            "abada74e7d7948b4af8278cc0f1fc4e7",
            "7919dc82c268477c995799f6eb7c8812",
            "01a48b2fabc047289055ece61c2bfc25",
            "5e247cc4bb0043efabf84c04e8d6bc71",
            "7c73db98415d449daecc9737c5ece6bd",
            "d89246246fb14a3592b81977709054f3",
            "eb8a4693e73346888afc78a4e4334d48",
            "e241ac7a9a654f259bff4fad4ef4d864",
            "30e3e29da081419399eafc5ba36d866e",
            "9afd5e6246fa4840a40946e51c68fe92",
            "36f3b88bf91c41bcaeeac907742a2231",
            "d7154de6309c47f3bb862576095990f1",
            "dfce68787500438c99ddaf9b9d22c8dd",
            "d6e32b0bb9d04262aac6aafa1f1c2d9b",
            "87ec3c72ae264d13bf511dde37c789eb",
            "252e3794be324e4cb9012d9c7436a354",
            "8b473539efeb4761a4099387c9ca621c",
            "f6e994f7c650479093637515b91944ad",
            "ef375417bc9b489baaf2de842eb1e202",
            "7d0f49848a814151b9a37ade0a4b6230",
            "825bf8b8cb7d40b9a1358b0ec6498da0",
            "bac0d0b407164f03809b906e2ced4f46",
            "a91b52da1d0e46acaddf39e56cfde272",
            "e7a30d8214b643b095cd96b4891a626f",
            "50d89ecf829c48a7a440edbd8c9da646",
            "b240ee301d3e4d1bbf25c9b1f08de0a3",
            "b50de3e5cdf347e5aab60f094a68bb8c",
            "72400b38fdf444dd828883137a582c92",
            "23f43f5413574003a1c9da6c64f66094",
            "82a4b522c9374f3f99c37f2a7e51e806",
            "d0d281d0de3a4ccbbcf338fa643fec7c",
            "7d347d14055047bc8e3b65c778ac13ee",
            "48857749e5384cfca0dde016aa629d0a",
            "9ff0e9893aef468cbbe64be6fe11d6ab",
            "cbfa0381090644d8814d28d2eddde4b7",
            "cdfead5806ec44ff9a5cb3f69a8e0a81",
            "57c02ceef2ac4a1ba9c9f6ae95335e5d",
            "581c3e7160f640829916dd31e6a7f8c7",
            "a87ba0ef836b4f09b173d776f4403f36",
            "eee3332b37194897a89d720281a1d723",
            "cd80870815c6404082529225d66464b3",
            "bb5446bdb5d5405a99e882fe039ca507",
            "9abc0b659b45401f91a4ac7eaec55d0c",
            "a4e4665c267d41adb19fd02660ebd2a8",
            "c0b7ca02f5414b2c836fcc017f18232c",
            "fdd5b782e63d45a1bae6206c809d6f64",
            "dbd418a47d0e4ea3910c35c294e861f4",
            "ee73e11a610048bb85278d10574f0fdd",
            "6cbcdf9df15444b8a7eb14cc68c4d92f",
            "c9fcec87f9da4af0bfd493580f52de4b",
            "cb25ed10b9fa47be8a5c72a3949512f3",
            "c7ab54127fb9408c9cd00cf69e157197",
            "2ce0b7f13c2d450997b9116c1f6b32a3",
            "d32f1cc602234d77b480f65a865c78ba",
            "fd707a87d9f941b3881b8367bd2fbc8d",
            "214bdd2a8637420c888455de15c4feec",
            "d299fbe545184c46a18bb729ee1f8cfd",
            "51f9fdd0c9964533bd010a47eaa00668",
            "d797eb12ae4c41ffb9bcd3c65f916901",
            "41c72237f77f43228a54f554dad18932",
            "7e2e776a81d44309b67b84f5b83e8ef0",
            "c1e964e22e7848019438cbe7cd5d4f17",
            "2fb9dd87d34648e19c2b7fc40ea74b9d",
            "861082a1d5d047eb9f8dda13d98ac7f1",
            "93d7ee1016e942af97aa28b588cab39b",
            "05f8a128cefc4d39b63cc048816e624a",
            "6f8c6472cf1c4833aee4f8c71193a550",
            "ad54e26275154750b22775a874c0ee30",
            "9002872bca43436cb041057c88b419c2",
            "e636de4725bf4491ae6b03f6315fdf80",
            "178c1cc88b864ca39357c56fca000fbe",
            "285337b26b1c40159d2d63d325d17f39",
            "41b74bdb1c0049e08e913471d084eda4",
            "9cabf8b8f0884916bc52bafba59b9356",
            "6cc24940d70a4d34b8b4ad13242923ab",
            "374147daabd84ff6ac961a5a6afbc200",
            "6bd22e97eb494bafb178dfc57d5bf397",
            "ba44f27f57ff4a73934d6f335aa79357",
            "00beeb2b5d024ebfb823b273ef13ef22",
            "00db3c1cc2e14fdf994cdb0f56868608",
            "d16e0c5d6c0f46e6848a4c168c63fa61"
          ]
        },
        "id": "WUt3vhKm0-L8",
        "outputId": "4b7d245d-ca82-466f-db95-db0384e65e66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0c593c03a964270b665b5290c42a60a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89cea55de3504cf7ae7a1e8fe930d673",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70fc89f5dd4144969670897c8b01d1d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce8672e29d58439e977ad782dc5333f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c79248334874a33b057ac4d45c28487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c73db98415d449daecc9737c5ece6bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "252e3794be324e4cb9012d9c7436a354",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b50de3e5cdf347e5aab60f094a68bb8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "581c3e7160f640829916dd31e6a7f8c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cbcdf9df15444b8a7eb14cc68c4d92f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41c72237f77f43228a54f554dad18932",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "178c1cc88b864ca39357c56fca000fbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8gI1imENgcx"
      },
      "outputs": [],
      "source": [
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from typing import Optional, List, Mapping, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVpApK370u8K"
      },
      "outputs": [],
      "source": [
        "class CustomLLMMistral(LLM):\n",
        "    model: MistralForCausalLM\n",
        "    tokenizer: LlamaTokenizerFast\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
        "\n",
        "        messages = [\n",
        "         {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "        model_inputs = encodeds.to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
        "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        if stop is not None:\n",
        "          for word in stop:\n",
        "            output = output.split(word)[0].strip()\n",
        "\n",
        "        # Mistral 7B sometimes fails to properly close the Markdown Snippets.\n",
        "        # If they are not correctly closed, Langchain will struggle to parse the output.\n",
        "        while not output.endswith(\"```\"):\n",
        "          output += \"`\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjF6EWaH1HI7"
      },
      "outputs": [],
      "source": [
        "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlv8MGdnSgJ_",
        "outputId": "790881d1-2ddb-4ef0-e5b6-58c45a26249a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=9725e095c69b2d5158e7f3c68b2376f85e3b1320565a9aa13904432354219623\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-fsPHA4hfBX"
      },
      "source": [
        "### Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GADmKfbcaKqV"
      },
      "outputs": [],
      "source": [
        "import numexpr as ne\n",
        "from langchain.tools import WikipediaQueryRun, BaseTool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain.agents import Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGACkKM9hNGO"
      },
      "source": [
        "We limit the number of results to 1 and the maximum number of characters to 2500. This limitation is imposed because, although Mistral 7B can support prompts of up to 32000 tokens, using a free Colab account would not provide sufficient memory for overly large inputs. But feel free to experiment changing the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ggXzXRBOpN"
      },
      "outputs": [],
      "source": [
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=2500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQD_kxGoMheY",
        "outputId": "10938f58-a998-4bb2-b41e-7fde19521fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Page: Deep learning\n",
            "Summary: Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function.\n"
          ]
        }
      ],
      "source": [
        "print(wikipedia.run(\"Deep Learning\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qXMWtcPaNMO"
      },
      "outputs": [],
      "source": [
        "wikipedia_tool = Tool(\n",
        "    name=\"wikipedia\",\n",
        "    description=\"Never search for more than one concept at a single step. If you need to compare two concepts, search for each one individually. Syntax: string with a simple concept\",\n",
        "    func=wikipedia.run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwOI2ZNKTroH"
      },
      "outputs": [],
      "source": [
        "class Calculator(BaseTool):\n",
        "    name = \"calculator\"\n",
        "    description = \"Use this tool for math operations. It requires numexpr syntax. Use it always you need to solve any math operation. Be sure syntax is correct.\"\n",
        "\n",
        "    def _run(self, expression: str):\n",
        "      try:\n",
        "        return ne.evaluate(expression).item()\n",
        "      except Exception:\n",
        "        return \"This is not a numexpr valid syntax. Try a different syntax.\"\n",
        "\n",
        "    def _arun(self, radius: int):\n",
        "        raise NotImplementedError(\"This tool does not support async\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MD_ARaGVg8i",
        "outputId": "633ea369-f341-48ec-a43c-5e5a46fa294b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculator_tool = Calculator()\n",
        "calculator_tool.run(\"2+3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB9hV6Eialyq"
      },
      "outputs": [],
      "source": [
        "tools = [wikipedia_tool, calculator_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieNdbNj9hakQ"
      },
      "source": [
        "### Customizing the Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmHtlAqctPxy"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kI0wCpDMheZ"
      },
      "outputs": [],
      "source": [
        "system=\"\"\"\n",
        "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
        "The json structure should contain the following keys:\n",
        "thought -> your thoughts\n",
        "action -> name of a tool\n",
        "action_input -> parameters to send to the tool\n",
        "\n",
        "These are the tools you can use: {tool_names}.\n",
        "\n",
        "These are the tools descriptions:\n",
        "\n",
        "{tools}\n",
        "\n",
        "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
        "If there is not enough information, keep trying.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwGo3V3JMheZ"
      },
      "outputs": [],
      "source": [
        "human=\"\"\"\n",
        "Add the word \"STOP\" after each markdown snippet. Example:\n",
        "\n",
        "```json\n",
        "{{\"thought\": \"<your thoughts>\",\n",
        " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
        " \"action_input\": \"<tool parameters or the final output\"}}\n",
        "```\n",
        "STOP\n",
        "\n",
        "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
        "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
        "Remember to add STOP after each snippet.\n",
        "\n",
        "These were the previous steps given to solve this query and the information you already gathered:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRtZ21B5dkPI"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", human),\n",
        "        MessagesPlaceholder(\"agent_scratchpad\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWR8f8i4hkte"
      },
      "source": [
        "### Building the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3BemxPxRKKd"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27QqSFwLbEjx"
      },
      "outputs": [],
      "source": [
        "agent = create_json_chat_agent(\n",
        "    tools = tools,\n",
        "    llm = llm,\n",
        "    prompt = prompt,\n",
        "    stop_sequence = [\"STOP\"],\n",
        "    template_tool_response = \"{observation}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtnzYtaAMheZ"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLf8fkFhQbC"
      },
      "outputs": [],
      "source": [
        "#agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6DkeQtdhojI"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW03b22pUC5T",
        "outputId": "55592eb4-322d-4e4c-beab-3eb5af9ff634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"This is a mathematical problem that can be solved using the calculator tool.\",\n",
            " \"action\": \"calculator\",\n",
            " \"action_input\": \"23 + 17\"}```\u001b[0m\u001b[33;1m\u001b[1;3m40\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The calculation was correct, the result of 23 plus 17 is 40.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"40\"}```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'How much is 23 plus 17?', 'output': '40'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"How much is 23 plus 17?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "warEhFbHfMY4",
        "outputId": "e9b1a617-a0a0-4290-affb-9ff79583431c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The capital of France is a concept that can be found on Wikipedia.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"capital of France\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Paris\n",
            "Summary: Paris is the capital and most populous city of France. With an official estimated population of 2,102,650 residents as of 1 January 2023 in an area of more than 105 km2 (41 sq mi), Paris is the fourth-most populated city in the European Union and the 30th most densely populated city in the world in 2022. Since the 17th century, Paris has been one of the world's major centres of finance, diplomacy, commerce, culture, fashion, and gastronomy. For its leading role in the arts and sciences, as well as its early and extensive system of street lighting, in the 19th century, it became known as the City of Light.The City of Paris is the centre of the Île-de-France region, or Paris Region, with an official estimated population of 12,271,794 inhabitants on 1 January 2023, or about 19% of the population of France. The Paris Region had a GDP of €765 billion (US$1.064 trillion, PPP) in 2021, the highest in the European Union. According to the Economist Intelligence Unit Worldwide Cost of Living Survey, in 2022, Paris was the city with the ninth-highest cost of living in the world.Paris is a major railway, highway, and air-transport hub served by two international airports: Charles de Gaulle Airport (the third-busiest airport in Europe) and Orly Airport. Opened in 1900, the city's subway system, the Paris Métro, serves 5.23 million passengers daily; it is the second-busiest metro system in Europe after the Moscow Metro. Gare du Nord is the 24th-busiest railway station in the world and the busiest outside Japan, with 262 million passengers in 2015. Paris has one of the most sustainable transportation systems and is one of the only two cities in the world that received the Sustainable Transport Award twice.Paris is especially known for its museums and architectural landmarks: the Louvre received 8.9. million visitors in 2023, on track for keeping its position as the most-visited art museum in the world. The Musée d'Orsay, Musée Marmottan Monet and Musée de l'Or\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The capital city of France, as per the obtained information, is Paris.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Paris\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the capital of France?', 'output': 'Paris'}"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"What is the capital of France?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u39Bg86r_BN",
        "outputId": "f4b6bf7a-4ff6-42f8-ba0a-8623126207ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The inventor of the radio is not a simple concept. I will search for more information using the 'wikipedia' tool.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"radio inventor\"}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Invention of radio\n",
            "Summary: The invention of radio communication was preceded by many decades of establishing theoretical underpinnings, discovery and experimental investigation of radio waves, and engineering and technical developments related to their transmission and detection. These developments allowed Guglielmo Marconi to turn radio waves into a wireless communication system.\n",
            "The idea that the wires needed for electrical telegraph could be eliminated, creating a wireless telegraph, had been around for a while before the establishment of radio-based communication. Inventors attempted to build systems based on electric conduction, electromagnetic induction, or on other theoretical ideas. Several inventors/experimenters came across the phenomenon of radio waves before its existence was proven; it was written off as electromagnetic induction at the time.\n",
            "The discovery of electromagnetic waves, including radio waves, by Heinrich Rudolf Hertz in the 1880s came after theoretical development on the connection between electricity and magnetism that started in the early 1800s. This work culminated in a theory of electromagnetic radiation developed by James Clerk Maxwell by 1873, which Hertz demonstrated experimentally. Hertz considered electromagnetic waves to be of little practical value. Other experimenters, such as Oliver Lodge and Jagadish Chandra Bose, explored the physical properties of electromagnetic waves, and they developed electric devices and methods to improve the transmission and detection of electromagnetic waves. But they did not apparently see the value in developing a communication system based on electromagnetic waves.\n",
            "In the mid-1890s, building on techniques physicists were using to study electromagnetic waves, Guglielmo Marconi developed the first apparatus for long-distance radio communication. On 23 December 1900, the Canadian inventor Reginald A. Fessenden became the first person to send audio (wireless telephony) by means of electromagnetic waves, successfully transmitting over a distance of about a mile (1.6 kilometers,) and six years later on Christmas Eve 1906 he became the first person to make a public wireless broadcast.By 1910, these various wireless systems had come to be called \"radio\".\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information gathered from the previous steps, it appears that Guglielmo Marconi was instrumental in the development and implementation of the first long-distance radio communication system. However, it's important to confirm this information and clarify his exact role in the invention of radio. I will search for more information using the 'wikipedia' tool.\n",
            "\n",
            "```json\n",
            "{\"thought\": \"Guglielmo Marconi's role in the invention of radio needs further clarification. I will search for more information using the 'wikipedia' tool.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Guglielmo Marconi\"}\n",
            "```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Guglielmo Marconi\n",
            "Summary: Guglielmo Giovanni Maria Marconi, 1st Marquis of Marconi  (Italian: [ɡuʎˈʎɛlmo marˈkoːni]; 25 April 1874 – 20 July 1937) was an Italian inventor and electrical engineer, known for his creation of a practical radio wave–based wireless telegraph system. This led to Marconi being credited as the inventor of radio, and he shared the 1909 Nobel Prize in Physics with Karl Ferdinand Braun \"in recognition of their contributions to the development of wireless telegraphy\".Marconi was also an entrepreneur, businessman, and founder of The Wireless Telegraph & Signal Company in the United Kingdom in 1897 (which became the Marconi Company). In 1929, Marconi was ennobled as a Marchese (marquis) by King Victor Emmanuel III of Italy, and, in 1931, he set up Vatican Radio for Pope Pius XI.\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"Guglielmo Marconi is confirmed to be the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate. I will provide this information as the next step.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Guglielmo Marconi is the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate.\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Who was the inventor of the Radio?',\n",
              " 'output': 'Guglielmo Marconi is the inventor of the practical radio wave–based wireless telegraph system and a Nobel laureate.'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"Who was the inventor of the Radio?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veMiVzCfgESZ",
        "outputId": "31166682-c10d-494f-b6c6-7c6a79417ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"The population of Madrid is the information needed to find its double. I will use the wikipedia tool to find the population of Madrid.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Population of Madrid\"\n",
            "}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Madrid\n",
            "Summary: Madrid ( mə-DRID, Spanish: [maˈðɾið] ) is the capital and most populous city of Spain. The city has almost 3.4 million inhabitants and a metropolitan area population of approximately 7 million. It is the second-largest city in the European Union (EU), and its monocentric metropolitan area is the second-largest in the EU. The municipality covers 604.3 km2 (233.3 sq mi) geographical area. Madrid lies on the River Manzanares in the central part of the Iberian Peninsula at about 650 meters above mean sea level. The capital city of both Spain and the surrounding autonomous community of Madrid (since 1983), it is also the political, economic, and cultural centre of the country. The climate of Madrid features hot summers and cool winters.\n",
            "The Madrid urban agglomeration has the second-largest GDP in the European Union and its influence in politics, education, entertainment, environment, media, fashion, science, culture, and the arts all contribute to its status as one of the world's major global cities. Due to its economic output, high standard of living, and market size, Madrid is considered the major financial centre and the leading economic hub of the Iberian Peninsula and of Southern Europe. The metropolitan area hosts major Spanish companies such as Telefónica, Iberia, BBVA and FCC. It concentrates the bulk of banking operations in the country and it is the Spanish-speaking city generating the largest amount of webpages. For innovation, Madrid is ranked 19th in the world and 7th in Europe from 500 cities, in the 2022–2023 annual analysts Innovation Cities Index, published by 2ThinkNow.Madrid houses the headquarters of the UN's World Tourism Organization (UNWTO), the Ibero-American General Secretariat (SEGIB), the Organization of Ibero-American States (OEI), and the Public Interest Oversight Board (PIOB). It also hosts major international regulators and promoters of the Spanish language: the Standing Committee of the Association of Spanish Language\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I have the population of Madrid from the previous step. I will calculate the double of the population using the calculator tool.\",\n",
            " \"action\": \"calculator\",\n",
            " \"action_input\": \"2 * 3.4e6\"\n",
            "}\n",
            "```\u001b[0m\u001b[33;1m\u001b[1;3m6800000.0\u001b[0m\u001b[32;1m\u001b[1;3mAI: ```json\n",
            "{\"thought\": \"The double of the population of Madrid is 6,800,000.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"6,800,000\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the double of the population of Madrid?',\n",
              " 'output': '6,800,000'}"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"What is the double of the population of Madrid?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1I2_LXMOUQ4"
      },
      "source": [
        "The following example usually fails due to limits in the model's reasoning capacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SC-la0D7gZ0",
        "outputId": "c86d0571-1f87-444c-eaa1-f788de104709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I need to find the birth years of Tom Hanks and Kevin Costner to determine who is older.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Tom Hanks\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Tom Hanks\n",
            "Summary: Thomas Jeffrey Hanks (born July 9, 1956) is an American actor and filmmaker. Known for both his comedic and dramatic roles, he is one of the most popular and recognizable film stars worldwide, and is regarded as an American cultural icon. Hanks' films have grossed more than $4.9 billion in North America and more than $9.96 billion worldwide, making him the fourth-highest-grossing actor in North America. He has received numerous honors including the AFI Life Achievement Award in 2002, the Kennedy Center Honor in 2014, the Presidential Medal of Freedom and the French Legion of Honor both in 2016, as well as the Golden Globe Cecil B. DeMille Award in 2020.Hanks made his breakthrough with leading roles in a series of comedy films which received positive media attention, such as Splash (1984), The Money Pit (1986), Big (1988), and A League of Their Own (1992). He won two consecutive Academy Awards for Best Actor for starring as a gay lawyer suffering from AIDS in Philadelphia (1993) and the title character in Forrest Gump (1994). Hanks collaborated with film director Steven Spielberg on five films: Saving Private Ryan (1998), Catch Me If You Can (2002), The Terminal (2004), Bridge of Spies (2015), and The Post (2017), as well as the WW II miniseries Band of Brothers (2001), The Pacific (2010), and Masters of the Air (2024). With the former he launched his career as a director, producer, and screenwriter. He has also frequently collaborated with film directors Ron Howard, Nora Ephron, and Robert Zemeckis.\n",
            "Hanks' other films include the romantic comedies Sleepless in Seattle (1993) and You've Got Mail (1998); the dramas Apollo 13 (1995), The Green Mile (1999), Cast Away (2000), Road to Perdition (2002) and Cloud Atlas (2012); and the biographical dramas Charlie Wilson's War (2007), Captain Phillips (2013), Saving Mr. Banks (2013), Sully (2016), A Beautiful Day in the Neighborhood (2019), News of the World (2020) and Elvis (2022). He has also appear\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
            "{\"thought\": \"I have the birth year of Tom Hanks which is 1956. I need the birth year of Kevin Costner to compare and find out who is older.\",\n",
            " \"action\": \"wikipedia\",\n",
            " \"action_input\": \"Kevin Costner\"}```\u001b[0m\u001b[36;1m\u001b[1;3mPage: Kevin Costner\n",
            "Summary: Kevin Michael Costner (born January 18, 1955) is an American actor, producer, and director. He has received various accolades, including two Academy Awards, three Golden Globe Awards, and a Primetime Emmy Award.\n",
            "He rose to prominence starring in such films as The Untouchables (1987), Bull Durham (1988), Field of Dreams (1989), JFK (1991), Robin Hood: Prince of Thieves (1991), The Bodyguard (1992), A Perfect World (1993), and Wyatt Earp (1994). During this time, Costner directed and starred in the western epic Dances with Wolves (1990), for which he won two Academy Awards for Best Picture and Best Director. He then starred in and co-produced Waterworld (1995) and directed The Postman (1997) and Open Range (2003).Costner's other notable films include Silverado (1985) No Way Out (1987), Tin Cup (1996), Message in a Bottle (1999), For Love of the Game (1999), Thirteen Days (2000), Mr. Brooks (2007), Swing Vote (2008), The Company Men (2010), 3 Days to Kill (2014), Draft Day (2014), Black or White (2014), McFarland, USA (2015), and The Highwaymen (2019). He has also played supporting parts in such films as The Upside of Anger (2005), Man of Steel (2013), Jack Ryan: Shadow Recruit (2014), Hidden Figures (2016), Molly's Game (2017), and Let Him Go (2020).\n",
            "On television, Costner portrayed Devil Anse Hatfield in the miniseries Hatfields & McCoys (2012), winning the Primetime Emmy Award for Outstanding Lead Actor in a Limited or Anthology Series or Movie. Since 2018, he has starred as John Dutton on the Paramount Network original drama series Yellowstone for which he received a Screen Actors Guild Award nomination and a Golden Globe award.\n",
            "\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mAI: ```json\n",
            "{\"thought\": \"I have the birth year of Tom Hanks which is 1956 and the birth year of Kevin Costner which is 1955. I can now compare the two to determine who is older.\",\n",
            " \"action\": \"Final Answer\",\n",
            " \"action_input\": \"Tom Hanks is older than Kevin Costner.\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Who is older, Tom Hanks or Kevin Costner?',\n",
              " 'output': 'Tom Hanks is older than Kevin Costner.'}"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"Who is older, Tom Hanks or Kevin Costner?\"})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
