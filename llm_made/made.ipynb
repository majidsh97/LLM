{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/yannickrud/made-project.git /home/cip/ce/ix05ogym/Majid/LLM/llm_made/data//0/\n  stderr: 'fatal: destination path '/home/cip/ce/ix05ogym/Majid/LLM/llm_made/data//0' already exists and is not an empty directory.\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m url_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/yannickrud/made-project.git\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/tosiful/made-tim.git\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(url_list):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mgit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m repo_path_list \u001b[38;5;241m=\u001b[39m [data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_path)]\n",
      "File \u001b[0;32m/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/site-packages/git/repo/base.py:1328\u001b[0m, in \u001b[0;36mRepo.clone_from\u001b[0;34m(cls, url, to_path, progress, env, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     git\u001b[38;5;241m.\u001b[39mupdate_environment(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv)\n\u001b[0;32m-> 1328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGitCmdObjectDB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsafe_protocols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsafe_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/site-packages/git/repo/base.py:1237\u001b[0m, in \u001b[0;36mRepo._clone\u001b[0;34m(cls, git, url, path, odb_default_type, progress, multi_options, allow_unsafe_protocols, allow_unsafe_options, **kwargs)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     cmdline \u001b[38;5;241m=\u001b[39m remove_password_if_present(cmdline)\n\u001b[1;32m   1236\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCmd(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms unused stdout: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cmdline, stdout)\n\u001b[0;32m-> 1237\u001b[0m     \u001b[43mfinalize_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;66;03m# our git command could have a different working dir than our actual\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;66;03m# environment, hence we prepend its working dir if required\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misabs(path):\n",
      "File \u001b[0;32m/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/site-packages/git/util.py:437\u001b[0m, in \u001b[0;36mfinalize_process\u001b[0;34m(proc, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for the process (clone, fetch, pull or push) and handle its errors accordingly\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# TODO: No close proc-streams??\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/site-packages/git/cmd.py:602\u001b[0m, in \u001b[0;36mGit.AutoInterrupt.wait\u001b[0;34m(self, stderr)\u001b[0m\n\u001b[1;32m    600\u001b[0m     errstr \u001b[38;5;241m=\u001b[39m read_all_from_possibly_closed_stream(p_stderr)\n\u001b[1;32m    601\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoInterrupt wait stderr: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (errstr,))\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs), status, errstr)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status\n",
      "\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v -- https://github.com/yannickrud/made-project.git /home/cip/ce/ix05ogym/Majid/LLM/llm_made/data//0/\n  stderr: 'fatal: destination path '/home/cip/ce/ix05ogym/Majid/LLM/llm_made/data//0' already exists and is not an empty directory.\n'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import git\n",
    "data_path = '/home/cip/ce/ix05ogym/Majid/LLM/llm_made/data/'\n",
    "url_list = ['https://github.com/yannickrud/made-project.git',\n",
    "            'https://github.com/tosiful/made-tim.git']\n",
    "for i,url in enumerate(url_list):\n",
    "    \n",
    "    git.Repo.clone_from(url,data_path+f'{i}/')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path_list = [data_path + f'{p}/' for p in os.listdir(data_path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##\n",
      "Pipline code:\n",
      "import requests\n",
      "from lxml import html\n",
      "from zipfile import ZipFile\n",
      "import pandas as pd\n",
      "from io import BytesIO, StringIO\n",
      "from tqdm import tqdm\n",
      "import re\n",
      "import os\n",
      "\n",
      "from sqlalchemy import create_engine\n",
      "\n",
      "def get_data_files(url: str) -> tuple[pd.DataFrame, list[bytes]]:\n",
      "\tzip_files = []\n",
      "\tr = requests.get(url)\n",
      "\n",
      "\twebpage = html.fromstring(r.content)\n",
      "\tlinks = webpage.xpath('//a/@href')\n",
      "\tfor link in tqdm(links):\n",
      "\n",
      "\t\tif link.endswith('.txt'):\n",
      "\t\t\tmeta_data = extract_meta_data_from_link(url, link)\n",
      "\t\t\tcontinue\n",
      "\n",
      "\t\tif not link.endswith('.zip'):\n",
      "\t\t\tcontinue\n",
      "\n",
      "\t\tresponse = requests.get(url + link)\n",
      "\t\tzip_files.append(response.content)\n",
      "\n",
      "\treturn meta_data, zip_files\n",
      "\n",
      "\n",
      "\n",
      "def extract_meta_data_from_link(url, link) -> pd.DataFrame:\n",
      "\tresponse = requests.get(url + link)\n",
      "\tlines = []\n",
      "\tfor i, line in enumerate(StringIO(response.text)):\n",
      "\t\tsplit1 = line.split(maxsplit=6)\n",
      "\t\tif i == 0:\n",
      "\t\t\tsplit2 = split1[-1].split()\n",
      "\t\telif i == 1:\n",
      "\t\t\tcontinue\n",
      "\t\telse:\n",
      "\t\t\tsplit2 = split1[6].strip()\n",
      "\t\t\tsplit2 = re.split('\\s{2,}', split2)\n",
      "\n",
      "\t\tsplit1 = split1[:-1] + split2\n",
      "\t\tlines.append(split1)\n",
      "\n",
      "\tdf = pd.DataFrame(lines)\n",
      "\tdf.columns = df.iloc[0]\n",
      "\tdf = df[1:]\n",
      "\tdf.reset_index(drop=True, inplace=True)\n",
      "\treturn df\n",
      "\n",
      "\n",
      "def extract_data_from_zip(zip_files: list[bytes]) -> pd.DataFrame:\n",
      "\t\"\"\"\n",
      "\tExtracts data from a list of zip files and returns it as a pandas DataFrame.\n",
      "\n",
      "\t:param zip_files: A list of bytes representing the content of the zip files.\n",
      "\t:type zip_files: list[bytes]\n",
      "\t:return: A pandas DataFrame containing the extracted data.\n",
      "\t:rtype: pd.DataFrame\n",
      "\t\"\"\"\n",
      "\tdf_list = []\n",
      "\tfor content in zip_files:\n",
      "\t\twith ZipFile(BytesIO(content)) as zip:\n",
      "\t\t\tnames = zip.namelist()\n",
      "\n",
      "\t\t\tfor name in names:\n",
      "\t\t\t\tif not name.startswith('produkt_'):\n",
      "\t\t\t\t\tcontinue\n",
      "\n",
      "\t\t\t\twith zip.open(name) as f:\n",
      "\t\t\t\t\tdf = pd.read_csv(f, sep=';')\n",
      "\t\t\t\t\tdf_list.append(df)\n",
      "\n",
      "\tdf = pd.concat(df_list, ignore_index=True)\n",
      "\treturn df\n",
      "\n",
      "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
      "\t# convert start and end date of measurement to datetime\n",
      "\ttime_related_columns = ['MESS_DATUM_BEGINN', 'MESS_DATUM_ENDE']\n",
      "\tdf[time_related_columns] = df[time_related_columns].apply(pd.to_datetime, format=\"%Y%m%d\")\n",
      "\n",
      "\tdf.drop(columns='eor', inplace=True)\n",
      "\treturn df\n",
      "\n",
      "def transform_meta_data(df):\n",
      "\tnumeric_cols = ['Stations_id', 'Stationshoehe', 'geoBreite', 'geoLaenge']\n",
      "\tdf[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)\n",
      "\t# von_datum, bis_datum\n",
      "\ttime_related_columns = ['von_datum', 'bis_datum']\n",
      "\tdf[time_related_columns] = df[time_related_columns].apply(pd.to_datetime, format=\"%Y%m%d\")\n",
      "\treturn df\n",
      "\n",
      "def run_pipeline(base_url):\n",
      "\textract_meta_data_from_link(base_url, \"wetter_jahreswerte_Beschreibung_Stationen.txt\")\n",
      "\t# extract the data and convert to a pandas dataframe\n",
      "\tmeta_data, zip_files = get_data_files(base_url)\n",
      "\t\n",
      "\tdf = extract_data_from_zip(zip_files)\n",
      "\n",
      "\tmeta_data = transform_meta_data(meta_data)\n",
      "\tdf = transform_data(df)\n",
      "\n",
      "\t# create database\n",
      "\tdata_base_path = './data/'\n",
      "\tos.makedirs(data_base_path, exist_ok=True)\n",
      "\n",
      "\tengine = create_engine(f'sqlite:///data/project.sqlite', echo=False)\n",
      "\t# writes the meta data to table description and extracted data to table 'weather_phenomena'\n",
      "\tmeta_data.to_sql('description', engine, if_exists='replace', index=False)\n",
      "\tdf.to_sql('weather_phenomena', engine, if_exists='replace', index=False)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "\tbase_url = 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/annual/weather_phenomena/historical/'\n",
      "\trun_pipeline(base_url)\n",
      "\n",
      "\texit(0)\n",
      "\n",
      "\n",
      "\n",
      "##\n",
      "Report:\n",
      "data-reportJune 6, 20241 Data reportYannick Rudolf, 227221561.1 QuestionHow have the number and types of weather phenomena in Germany changed over time?1.2 Data sourcesTo answer the question, the historical data sets of 998 different stations of the German Weather Service are loaded. The data of each station is provided in a structured text file. The text files contain comma separated values with “;” as delimiter.The station specific data sets contain information on the number of different weather phenomena on an annual basis. The weather phenomena considered areParameterParameter descriptionData typeJA_GEWITTERNumber of days with thunderstorms per year JA_GLATTEIS Number of days with black ice per year JA_GRAUPEL Number of days with sleet per year Number of days with hail per year JA_HAGEL JA_NEBEL Number of days with fog per year JA_STURM_6 Number of days with Storm > 6 on the Beaufort scale per year JA_STURM_8 Number of days with Storm > 8 on the Beaufort scale per year JA_TAUNumber of days with Dew per yearNUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBERThe meta data for the stations is summarized in an extra data set. The meta data is provided as a semi-structured table within a text file. The meta data contains information about the id, the name, the location, and the duration for which the respective station has recorded data.Source: Deutscher Wetterdienst1.2.1 Data qualitySince the German Weather Service is a state institution and the data is the state of natural events in the past, it can be assumed that the data is correct. Although the data might be correct it is not complete as there are missing values for several years on different weather stations. The 998 weather stations provide 34 years of data on average where inbetween 2.3 years are missing on1average. As the weather stations are distributed all over germany the data is representative for the historical weather phenomena in germany.1.2.2 LicenseThe data used in this project is provided on the open data portal, Climate Data Center (CDC) through the German Weather Service. - Terms of use - LicenseThe data is under the Creative Commons BY 4.0 “CC BY 4.0” License. This means the data is free to share and adapt for any purpose under the following constraints - apropriate credit to the authors - indicate if changes were made - provide a link to the license - You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.To fulfill the obligations of the license the authors of the data get mentioned in the README.md of the project, in this report, and the final report.1.3 Data pipelineThe whole pipeline is in python and is structured as follows. Getting the data -> Processing the data to two dataframes (meta and usage data) -> Transform data -> Store data1.3.1 Getting the dataThe pipeline starts with a GET request to the provided url. Then with xpath and the lxml library all links from the reponse are extracted. The pipeline checks for every link if it is a zip-archive or a text file. The zip-archive indicates it is data from a weather station. The text file is meta data. Files with other suﬀixes are currently not handled. Each file is requested. The data is not written to files but the responses to the http requests are packed into io streams using the io package.1.3.2 Processing the dataFor the meta data each line is processed iteratively within a string stream. The values of each line are split into a list of strings. As the data only is semi-structured there is not always a clear seperation between the values. The header row is an exception. The header is split by whitespaces. The first 6 values are numerical values which do not include any whitespaces. Therefore, these values are split by whitespaces. The last two values are strings that can contain whitespaces (e.g. Arolsen-Neu Berich). These values are seperated with a regular expression that checks for 2 or more occurences of whitespaces and then splits the values.Each list of strings is then stored in an additional list. The additional list is then converted to a pandas dataframe. The first row is set as column names.For each station the data is processed within a Bytes stream. Therefore, the ZipFile package is used to open the zip archive. The file names within the zip archive are extracted. Each name is checked if it starts with ‘produkt_’. This prefix indicates the actual data of the station. The other files within the zip archive are meta data which overlap with the previously described meta data and is therefore ignored. The actual data is then read with pandas with “;” as separator. The dataframe is stored in a list of dataframes.When the stations data is processed the list of dataframes gets concatenated to a big dataframe as it is more eﬀicent than appending the dataframes one by one.21.3.3 Transforming the dataFor the meta data the columns “Stations_id”, “Stationshoehe”, “geoBreite”, “geoLaenge” are con- verted to numerical columns. The columns “von_datum”, “bis_datum” are converted to datetime columns.For the stations data only the columns “MESS_DATUM_BEGINN”, “MESS_DATUM_ENDE” are converted to datetime.The date columns are converted to improve readability and comperability. The “Stations_id” is converted due to comperability reasons to the staitons data column “STATIONS_ID”. The other numerical columns are converted due to common sense.1.4 ResultsThe meta data und data are stored in a SQLite database called project.sqlite. The data is written to the sql table with sqlalchemy and pandas. Within the database are two tables. “descrip- tion” contains the meta data of the weather stations. The stations data is stored in the table “weather_phenomena”.1.5 LimitationsOne big limitation with these data sets is that they are not static. It is historical data that still gets updated annually which will make the results of the final report obsolete next year. Yet the pipeline will still be able to update the database. It would have been an option to design the pipeline the way that it stores the data files locally. Not doing this was a conscious decision, as this pipeline should store as little data as necessary persistently and therefore only store the final database.3\n",
      "##\n",
      "\n",
      "You are given a code and report. You need to find some good datasets related to America's economy, society, .... and merge all the data based on some criteria. then write an ETL data pipeline for that. then analysis the created dataset and answer an important question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Define the path to your PDF file\n",
    "pdf_file_path = \"your_file.pdf\"\n",
    "\n",
    "# Extract content from the PDF\n",
    "\n",
    "# Print the extracted text\n",
    "\n",
    "\n",
    "def get_data(p):\n",
    "        p += 'project/'\n",
    "        code_path =  p + 'pipeline.py'\n",
    "        data_report_path = p + 'data-report.pdf'\n",
    "        analysis_report_path =  p + 'analysis-report.pdf'\n",
    "        code = ''\n",
    "        with open(code_path,'r') as file :\n",
    "                code  = file.read()\n",
    "                \n",
    "        elements = partition_pdf(filename=data_report_path)\n",
    "        data_report = ''\n",
    "        for element in elements:\n",
    "            data_report +=element.text\n",
    "            \n",
    "        elements = partition_pdf(filename=analysis_report_path)\n",
    "        analysis_report = ''\n",
    "        for element in elements:\n",
    "           analysis_report +=element.text\n",
    "            \n",
    "        return code,data_report,analysis_report\n",
    "            \n",
    "prompt=\"\"\"##\n",
    "Pipline code:\n",
    "{code}\n",
    "##\n",
    "Report:\n",
    "{report}\n",
    "##\n",
    "\n",
    "You are given a code and report. You need to find some good datasets related to America's economy, society, .... and merge all the data based on some criteria. then write an ETL data pipeline for that. then analysis the created dataset and answer an important question.\n",
    "\"\"\"\n",
    "for p in repo_path_list:            \n",
    "    code,data_report,analysis_report = get_data(p)  \n",
    "    fp = prompt.format(code=code,report= data_report)\n",
    "    print(fp)\n",
    "    break\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
