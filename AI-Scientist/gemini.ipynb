{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key='AIzaSyDcsi9U5RgrnT3BG34Q0SMbbIvBc5kyFG0')\n",
    "safe = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]\n",
    "system = \"You are an ambitious AI researcher who is looking to publish a paper that will contribute significantly to the field.\",\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\",system_instruction=system,safety_settings=safe,generation_config=genai.types.GenerationConfig(\n",
    "        # Only one candidate for now.\n",
    "        candidate_count=1,\n",
    "        #stop_sequences=[\"x\"],\n",
    "        #max_output_tokens=20,\n",
    "        #temperature=1.0,\n",
    "    ),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_json_between_markers(llm_output):\n",
    "    json_start_marker = \"```json\"\n",
    "    json_end_marker = \"```\"\n",
    "\n",
    "    # Find the start and end indices of the JSON string\n",
    "    start_index = llm_output.find(json_start_marker)\n",
    "    if start_index != -1:\n",
    "        start_index += len(json_start_marker)  # Move past the marker\n",
    "        end_index = llm_output.find(json_end_marker, start_index)\n",
    "    else:\n",
    "        return None  # JSON markers not found\n",
    "\n",
    "    if end_index == -1:\n",
    "        return None  # End marker not found\n",
    "\n",
    "    # Extract the JSON string\n",
    "    json_string = llm_output[start_index:end_index].strip()\n",
    "    try:\n",
    "        parsed_json = json.loads(json_string)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ideas_string = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"Name\": \"adaptive block size\",\n",
    "    \"Title\": \"Adaptive Block Size: Dynamic Context Window Adjustment for Efficient Training\",\n",
    "    \"Experiment\": \"Modify the model to dynamically adjust its block size during training, starting with a smaller block size and gradually increasing it. This could potentially lead to faster initial training and better long-range dependency learning.\",\n",
    "    \"Interestingness\": 6,\n",
    "    \"Feasibility\": 4,\n",
    "    \"Novelty\": 4\n",
    "  },\n",
    "  {\n",
    "    \"Name\": \"layerwise learning rates\",\n",
    "    \"Title\": \"Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models\",\n",
    "    \"Experiment\": \"Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.\",\n",
    "    \"Interestingness\": 4,\n",
    "    \"Feasibility\": 6,\n",
    "    \"Novelty\": 2\n",
    "  }\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "idea_first_prompt = \"\"\"{task_description}\n",
    "\n",
    "Here are the ideas that you have already generated:\n",
    "\n",
    "'''\n",
    "{prev_ideas_string}\n",
    "'''\n",
    "\n",
    "Come up with the next impactful and creative idea for research experiments.\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "THOUGHT:\n",
    "<THOUGHT>\n",
    "\n",
    "NEW IDEA JSON:\n",
    "```json\n",
    "<JSON>\n",
    "```\n",
    "\n",
    "In <THOUGHT>, first briefly discuss your intuitions and motivations for the idea. Detail your high-level plan, necessary design choices and ideal outcomes of the experiments. Justify how the idea is different from the existing ones.\n",
    "\n",
    "In <JSON>, provide the new idea in JSON format with the following fields:\n",
    "- \"Name\": Search keywords used for searching similar ideas.\n",
    "- \"Title\": A title for the idea, will be used for the report writing.\n",
    "- \"Experiment\": An outline of the implementation. E.g. which functions need to be added or modified, how results will be obtained, ...\n",
    "- \"abstract\": An outline of idea, why it is useful. what problems it solves, ....\n",
    "- \"Interestingness\": A rating from 1 to 10 (lowest to highest).\n",
    "- \"Feasibility\": A rating from 1 to 10 (lowest to highest).\n",
    "- \"Novelty\": A rating from 1 to 10 (lowest to highest).\n",
    "\n",
    "Be cautious and realistic on your ratings.\n",
    "This JSON will be automatically parsed, so ensure the format is precise.\n",
    "\"\"\"\n",
    "#You will have {num_reflections} rounds to iterate on the idea, but do not need to use them all.\n",
    "\n",
    "task_description = \"You are given the following file to work with, which trains multiple small language models on multiple datasets of text at the character level.\"\n",
    "\n",
    "msg = idea_first_prompt.format(\n",
    "                    task_description = task_description,\n",
    "                    #code=code,\n",
    "                    prev_ideas_string=prev_ideas_string,\n",
    "                    #num_reflections=num_reflections,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_from_llm(user=\"\",system=\"\"):\n",
    "    p = [ \n",
    "        #{\"role\": \"user\",\"parts\": \"System prompt: \"+ system},\n",
    "        {\"role\": \"user\", \"parts\":  user}\n",
    "        ]\n",
    "    response = model.generate_content(p)\n",
    "    return response.text\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def search_for_papers(query, result_limit=10):\n",
    "    if not query:\n",
    "        return None\n",
    "    rsp = requests.get(\n",
    "        \"https://api.semanticscholar.org/graph/v1/paper/search\",\n",
    "        #headers={\"X-API-KEY\": S2_API_KEY},\n",
    "        params={\n",
    "            \"query\": query,\n",
    "            \"limit\": result_limit,\n",
    "            \"fields\": \"title,authors,venue,year,abstract,citationStyles,citationCount\",\n",
    "        },\n",
    "    )\n",
    "    print(f\"Response Status Code: {rsp.status_code}\")\n",
    "    print(\n",
    "        f\"Response Content: {rsp.text[:500]}\"\n",
    "    )  # Print the first 500 characters of the response content\n",
    "    rsp.raise_for_status()\n",
    "    results = rsp.json()\n",
    "    total = results[\"total\"]\n",
    "    time.sleep(1.0)\n",
    "    if not total:\n",
    "        return None\n",
    "\n",
    "    papers = results[\"data\"]\n",
    "    return papers\n",
    "\n",
    "\n",
    "\n",
    "def search_for_papers(query, result_limit=10):\n",
    "    if not query:\n",
    "        return None\n",
    "    rsp = requests.get(\n",
    "        \"https://dblp.org/search/publ/api\",\n",
    "        params={\n",
    "            \"q\": query,\n",
    "            \"format\": \"json\",\n",
    "            \"h\":result_limit,\n",
    "        },\n",
    "    )\n",
    "    print(f\"Response Status Code: {rsp.status_code}\")\n",
    "    \n",
    "    rsp.raise_for_status()\n",
    "    results = rsp.json()\n",
    "\n",
    "    try:\n",
    "        papers = results['result']['hits']['hit']\n",
    "    except : \n",
    "        papers = []\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "I'm intrigued by the idea of exploring how different character-level encodings might affect the performance and generalization capabilities of the model. It's commonly accepted that tokenization greatly impacts language models, and while character-level encoding seems like a natural choice, it might be possible to design more effective representations.  I want to investigate if alternative encoding schemes can lead to more efficient training and improved performance, especially in scenarios where the dataset contains a high number of unique characters or specific domain-specific vocabulary.\n",
      "\n",
      "The experiment will focus on comparing the performance of models trained with different character-level encoding strategies. I'll introduce novel encoding schemes, like using character bigrams or trigrams or even incorporating a \"meta-character\" that represents rare or unknown characters. Each encoding will be evaluated based on its impact on training time, model size, and performance on downstream tasks.  I believe this approach can potentially unlock new avenues for character-level language modeling, offering a more robust and flexible approach to handling complex text data. \n",
      "\n",
      "NEW IDEA JSON:\n",
      "```json\n",
      "{\n",
      "  \"Name\": \"character encoding variations\",\n",
      "  \"Title\": \"Beyond Basic Characters: Exploring Novel Encoding Strategies for Character-level Language Models\",\n",
      "  \"Experiment\": \"Implement different character-level encoding schemes (e.g., character bigrams, trigrams, meta-character) and train separate models for each.  Evaluate their performance on various downstream tasks, analyzing the impact of encoding complexity on model size, training time, and accuracy.\",\n",
      "  \"abstract\": \"This research explores the impact of character-level encoding schemes on the performance and efficiency of small language models. By introducing novel encoding strategies, such as character bigrams, trigrams, and a 'meta-character' for rare characters, we aim to investigate whether these variations can lead to improved model efficiency and performance.  The study evaluates the trade-offs between encoding complexity and model size, training time, and accuracy on various downstream tasks.\",\n",
      "  \"Interestingness\": 8,\n",
      "  \"Feasibility\": 7,\n",
      "  \"Novelty\": 6\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompt = idea_first_prompt.format(\n",
    "                    task_description = task_description,\n",
    "                    prev_ideas_string=prev_ideas_string,\n",
    "                )\n",
    "idea_string = get_response_from_llm(user_prompt)        \n",
    "print(idea_string)\n",
    "idea_json = extract_json_between_markers(idea_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_prompt = '''You have this idea:\n",
    "\"\"\"\n",
    "{idea}\n",
    "{abstract}\n",
    "\"\"\"\n",
    "\n",
    "The last searched queries are:\n",
    "\"\"\"\n",
    "{last_query}\n",
    "\"\"\"\n",
    "\n",
    "These ideas are the search results which means they are already explored:\n",
    "\"\"\"\n",
    "{last_query_results}\n",
    "\"\"\"\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "THOUGHT:\n",
    "<THOUGHT>\n",
    "\n",
    "RESPONSE:\n",
    "```json\n",
    "<JSON>\n",
    "```\n",
    "\n",
    "In <THOUGHT>, first briefly reason over the novelty of the idea based on provided explored ideas. if there is similarity between ideas provide the exact title that idea.\n",
    "If you have made your decision, add \"Decision made: novel.\" or \"Decision made: not novel.\" to your thoughts and JSON. Be harsh in your decision.\n",
    "\n",
    "In <JSON>, respond in JSON format with ONLY the following field:\n",
    "- \"Novelty\" : true or false based on your reasoning.\n",
    "- \"Query\": two small words to search the literature. The query must be different from all the last searched queries.\n",
    "\n",
    "This JSON will be automatically parsed, so ensure the format is precise.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Basic Characters: Exploring Novel Encoding Strategies for Character-level Language Models\n",
      "0\n",
      "character encoding variations\n",
      "Response Status Code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "While the idea of exploring character-level encoding schemes is interesting, it appears to be a well-explored area. The specific variations mentioned, such as character bigrams and trigrams, are commonly used techniques in character-level language modeling.  The idea of a 'meta-character' for rare characters could be a slight novelty, but it's likely that similar approaches for handling rare characters have been investigated before.  \n",
      "\n",
      "Decision made: not novel. \n",
      "\n",
      "RESPONSE:\n",
      "```json\n",
      "{\n",
      "\"Novelty\": false,\n",
      "\"Query\": \"rare character handling\"\n",
      "}\n",
      "``` \n",
      "\n",
      "---------------------------------------------------\n",
      "1\n",
      "rare character handling\n",
      "Response Status Code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "The idea is not entirely novel. Character bigrams and trigrams are common techniques used in character-level language models, and handling rare characters is a well-studied problem. The \"meta-character\" concept for rare characters could be considered an interesting approach, but it might be viewed as a slight variation on existing methods for rare character handling.  Decision made: not novel.\n",
      "\n",
      "RESPONSE:\n",
      "```json\n",
      "{\n",
      "\"Novelty\": false,\n",
      "\"Query\": \"rare character compression\"\n",
      "}\n",
      "``` \n",
      "\n",
      "---------------------------------------------------\n",
      "2\n",
      "rare character compression\n",
      "Response Status Code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "The idea seems interesting, but the search queries reveal a lack of novelty. The concept of exploring different character-level encoding schemes has already been investigated, as seen in the \"character encoding variations\" search query. The \"rare character handling\" and \"rare character compression\" queries further suggest that using bigrams, trigrams, and meta-characters to handle rare characters is not a new concept.  \n",
      "\n",
      "Decision made: not novel. \n",
      "\n",
      "RESPONSE:\n",
      "```json\n",
      "{\n",
      "\"Novelty\": false,\n",
      "\"Query\": \"encoding impact\"\n",
      "}\n",
      "``` \n",
      "\n",
      "---------------------------------------------------\n",
      "3\n",
      "encoding impact\n",
      "Response Status Code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Impact of a Computer System and the Encoding Staff Organization on the Encoding Stays and on Health Institution Financial Production in France.',\n",
       " 'Transparent Risks: The Impact of the Specificity and Visual Encoding of Uncertainty on Decision Making.',\n",
       " 'The impact of video encoding parameters on QoE of simulated FPV drone control.',\n",
       " 'FAW: Flag aligned word-based encoding for in-place integers compression.',\n",
       " 'BERLib: A Basic Encoding Rules implementation.',\n",
       " 'A Comparative Study on the Impact of Categorical Encoding on Black Box Model Interpretability.',\n",
       " 'An Empirical Study on the Impact of Positional Encoding in Transformer-Based Monaural Speech Enhancement.',\n",
       " 'Impact of Input Encoding and ADC Resolution on Matrix-Vector Multiplication Accuracy.',\n",
       " 'An Empirical Study on the Impact of Positional Encoding in Transformer-based Monaural Speech Enhancement.',\n",
       " 'Dissociating the Impact of Memorability on Electrophysiological Correlates of Memory Encoding Success.',\n",
       " 'Impact of Feature Encoding on Malware Classification Explainability.',\n",
       " 'Evaluating the Impact of Perceptually Uniform Encoding on the Performance of Feature Descriptors for HDR Image Retrieval.',\n",
       " 'The Impact of Positional Encoding on Length Generalization in Transformers.',\n",
       " 'Impact of Event Encoding and Dissimilarity Measures on Traffic Crash Characterization Based on Sequence of Events.',\n",
       " 'The Impact of Positional Encoding on Length Generalization in Transformers.',\n",
       " 'Impact of Feature Encoding on Malware Classification Explainability.',\n",
       " 'Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy.',\n",
       " 'A normative model of peripersonal space encoding as performing impact prediction.',\n",
       " 'On the impact of linkage learning, gene-pool optimal mixing, and non-redundant encoding on permutation optimization.',\n",
       " 'Encoding Impact of Network Modification on Controllability via Edge Centrality Matrix.',\n",
       " 'Impact of Categorical Variable Encodings on Bayesian Optimization Performance.',\n",
       " 'Impacts of aging on suprasegmental and segmental encoding of vocally-expressed confidence in Wuxi dialect.',\n",
       " 'An Impact of the Encoding Bitrate on the Quality of Streamed Video Presented on Screens of Different Resolutions.',\n",
       " 'Unsupervised Anomaly Detection for Auditing Data and Impact of Categorical Encodings.',\n",
       " 'The Impact of Encoding and Transport for Massive Real-time IoT Data on Edge Resource Consumption.',\n",
       " 'Development of an Encoding Method on a Co-Simulation Platform for Mitigating the Impact of Unreliable Communication.',\n",
       " 'Inductance Impact on Digital Encoding Performance of 850-nm Multimode VCSELs for 50-Gbps NRZ-OOK Data Link.',\n",
       " 'The Impact of Positional Encodings on Multilingual Compression.',\n",
       " 'Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation.',\n",
       " 'Impact of Categorical Variables Encoding on Property Mass Valuation.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "The provided search results focus on various aspects of encoding, but none directly address the specific combination of character bigrams, trigrams, and a 'meta-character' for rare characters within the context of character-level language models. While the idea of character-level encoding and rare character handling is explored, the proposed combination and its impact on model efficiency and performance are not covered.  \n",
      "\n",
      "Decision made: novel.\n",
      "\n",
      "RESPONSE:\n",
      "```json\n",
      "{\n",
      "\"Novelty\": true,\n",
      "\"Query\": \"character bigrams model\" \n",
      "}\n",
      "``` \n",
      "\n",
      "---------------------------------------------------\n",
      "4\n",
      "character bigrams model\n",
      "Response Status Code: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Japanese language model based on bigrams and its application to on-line character recognition.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THOUGHT:\n",
      "The idea of using character bigrams, trigrams, and a \"meta-character\" for rare characters has been explored before. The provided search result \"Japanese language model based on bigrams and its application to on-line character recognition\" is a clear indication that this approach has been investigated in the past.  Therefore, the core idea of encoding variations is not novel. However, the specific focus on small language models and the analysis of trade-offs between encoding complexity and model performance could potentially offer a unique angle. \n",
      "\n",
      "Decision made: not novel. \n",
      "\n",
      "RESPONSE:\n",
      "```json\n",
      "{\n",
      "\"Novelty\": false,\n",
      "\"Query\": \"encoding trade-offs\"\n",
      "}\n",
      "``` \n",
      "\n",
      "NOT NEW!!!!!\n"
     ]
    }
   ],
   "source": [
    "#search\n",
    "kw =[]\n",
    "kw.append( idea_json['Name'] )\n",
    "print(idea_json['Title'])\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    print(kw[-1])\n",
    "    search_result = search_for_papers(kw[-1],30)\n",
    "    titles = [r['info']['title'] for r in search_result]\n",
    "    display(titles)\n",
    "\n",
    "    np = novelty_prompt.format(idea = idea_json['Title'],last_query_results=titles,last_query = kw,abstract=idea_json['abstract'])\n",
    "    r = get_response_from_llm(np)\n",
    "    j = extract_json_between_markers(r)\n",
    "    kw.append( j['Query'] )\n",
    "    print(r)\n",
    "    \n",
    "    if j['Novelty']==False and search_result!=[]:\n",
    "        print('NOT NEW!!!!!')\n",
    "        break\n",
    "    \n",
    "    print('---------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Novelty': False,\n",
       " 'Query': 'Character-level attention dynamics small language models'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
