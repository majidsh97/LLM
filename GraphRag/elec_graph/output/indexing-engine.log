15:45:59,653 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
15:45:59,655 graphrag.index.cli INFO Starting pipeline run for: 20241016-154559, dryrun=False
15:45:59,655 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4-turbo-preview",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
15:45:59,658 graphrag.index.create_pipeline_config INFO skipping workflows 
15:45:59,658 graphrag.index.run.run INFO Running pipeline
15:45:59,658 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
15:45:59,661 graphrag.index.input.load_input INFO loading input from root_dir=input
15:45:59,661 graphrag.index.input.load_input INFO using file storage for input
15:45:59,663 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
17:15:58,255 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
17:15:58,257 graphrag.index.cli INFO Starting pipeline run for: 20241016-171558, dryrun=False
17:15:58,257 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": {
            "file": "emy.py"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:15:58,259 graphrag.index.create_pipeline_config INFO skipping workflows 
17:15:58,259 graphrag.index.run.run INFO Running pipeline
17:15:58,260 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
17:15:58,261 graphrag.index.input.load_input INFO loading input from root_dir=input
17:15:58,261 graphrag.index.input.load_input INFO using file storage for input
17:15:58,262 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
17:15:58,263 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
17:15:58,265 graphrag.index.input.text INFO Found 1 files, loading 1
17:15:58,266 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
17:15:58,266 graphrag.index.run.run INFO Final # of rows loaded: 1
17:15:58,374 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:15:58,376 datashaper.workflow.workflow INFO executing verb orderby
17:15:58,378 datashaper.workflow.workflow INFO executing verb zip
17:15:58,379 datashaper.workflow.workflow INFO executing verb aggregate_override
17:15:58,382 datashaper.workflow.workflow INFO executing verb chunk
17:16:00,427 datashaper.workflow.workflow INFO executing verb select
17:16:00,429 datashaper.workflow.workflow INFO executing verb unroll
17:16:00,480 datashaper.workflow.workflow INFO executing verb rename
17:16:00,485 datashaper.workflow.workflow INFO executing verb genid
17:16:00,489 datashaper.workflow.workflow INFO executing verb unzip
17:16:00,494 datashaper.workflow.workflow INFO executing verb copy
17:16:00,496 datashaper.workflow.workflow INFO executing verb filter
17:16:00,507 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:16:00,677 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
17:16:00,677 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:16:00,706 datashaper.workflow.workflow INFO executing verb entity_extract
17:16:00,735 datashaper.workflow.workflow INFO executing verb snapshot
17:16:00,742 datashaper.workflow.workflow INFO executing verb merge_graphs
17:16:00,746 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:16:00,751 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
17:16:00,875 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
17:16:00,876 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
17:16:00,887 datashaper.workflow.workflow INFO executing verb summarize_descriptions
17:16:00,905 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:16:00,943 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
17:16:00,943 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
17:16:00,948 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:16:00,954 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
17:16:01,70 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
17:16:01,70 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
17:16:01,81 datashaper.workflow.workflow INFO executing verb cluster_graph
17:16:01,114 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:16:01,120 datashaper.workflow.workflow INFO executing verb embed_graph
17:16:01,140 root INFO Starting preprocessing of transition probabilities on graph with 3 nodes and 2 edges
17:16:01,140 root INFO Starting at time 1729091761.140867
17:16:01,140 root INFO Beginning preprocessing of transition probabilities for 3 vertices
17:16:01,140 root INFO Completed 1 / 3 vertices
17:16:01,140 root INFO Completed 2 / 3 vertices
17:16:01,141 root INFO Completed 3 / 3 vertices
17:16:01,145 root INFO Completed preprocessing of transition probabilities for vertices
17:16:01,145 root INFO Beginning preprocessing of transition probabilities for 2 edges
17:16:01,145 root INFO Completed 1 / 2 edges
17:16:01,145 root INFO Completed 2 / 2 edges
17:16:01,146 root INFO Completed preprocessing of transition probabilities for edges
17:16:01,146 root INFO Simulating walks on graph at time 1729091761.1464953
17:16:01,146 root INFO Walk iteration: 1/10
17:16:01,147 root INFO Walk iteration: 2/10
17:16:01,148 root INFO Walk iteration: 3/10
17:16:01,149 root INFO Walk iteration: 4/10
17:16:01,150 root INFO Walk iteration: 5/10
17:16:01,155 root INFO Walk iteration: 6/10
17:16:01,156 root INFO Walk iteration: 7/10
17:16:01,157 root INFO Walk iteration: 8/10
17:16:01,158 root INFO Walk iteration: 9/10
17:16:01,159 root INFO Walk iteration: 10/10
17:16:01,160 root INFO Learning embeddings at time 1729091761.1600971
17:16:01,161 gensim.models.word2vec INFO collecting all words and their counts
17:16:01,161 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
17:16:01,161 gensim.models.word2vec INFO collected 3 word types from a corpus of 560 raw words and 30 sentences
17:16:01,162 gensim.models.word2vec INFO Creating a fresh vocabulary
17:16:01,162 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 3 unique words (100.00% of original 3, drops 0)', 'datetime': '2024-10-16T17:16:01.162254', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:16:01,162 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 560 word corpus (100.00% of original 560, drops 0)', 'datetime': '2024-10-16T17:16:01.162722', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:16:01,163 gensim.models.word2vec INFO deleting the raw counts dictionary of 3 items
17:16:01,164 gensim.models.word2vec INFO sample=0.001 downsamples 3 most-common words
17:16:01,164 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 31.61631398030046 word corpus (5.6%% of prior 560)', 'datetime': '2024-10-16T17:16:01.164411', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:16:01,165 gensim.models.word2vec INFO estimated required memory for 3 words and 1536 dimensions: 38364 bytes
17:16:01,165 gensim.models.word2vec INFO resetting layer weights
17:16:01,166 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-16T17:16:01.166220', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
17:16:01,166 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 3 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-16T17:16:01.166475', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:16:01,174 gensim.models.word2vec INFO EPOCH 0: training on 560 raw words (34 effective words) took 0.0s, 22393 effective words/s
17:16:01,179 gensim.models.word2vec INFO EPOCH 1: training on 560 raw words (25 effective words) took 0.0s, 24076 effective words/s
17:16:01,182 gensim.models.word2vec INFO EPOCH 2: training on 560 raw words (39 effective words) took 0.0s, 41250 effective words/s
17:16:01,190 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1680 raw words (98 effective words) took 0.0s, 4227 effective words/s', 'datetime': '2024-10-16T17:16:01.190440', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:16:01,191 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=3, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-16T17:16:01.191304', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
17:16:01,192 root INFO Completed. Ending time is 1729091761.1920285 Elapsed time is -0.05116152763366699
17:16:01,209 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:16:01,218 datashaper.workflow.workflow INFO executing verb select
17:16:01,222 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
17:16:01,348 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:16:01,348 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:16:01,360 datashaper.workflow.workflow INFO executing verb unpack_graph
17:16:01,365 datashaper.workflow.workflow INFO executing verb rename
17:16:01,370 datashaper.workflow.workflow INFO executing verb select
17:16:01,375 datashaper.workflow.workflow INFO executing verb dedupe
17:16:01,380 datashaper.workflow.workflow INFO executing verb rename
17:16:01,386 datashaper.workflow.workflow INFO executing verb filter
17:16:01,398 datashaper.workflow.workflow INFO executing verb text_split
17:16:01,403 datashaper.workflow.workflow INFO executing verb drop
17:16:01,408 datashaper.workflow.workflow INFO executing verb merge
17:16:01,414 datashaper.workflow.workflow INFO executing verb text_embed
17:16:01,417 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:16:01,454 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
17:16:01,454 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
17:16:01,454 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
17:16:01,527 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:02,770 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:05,131 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:10,16 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:18,92 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:28,113 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['COMPANY_A:Company_A is a test company', 'COMPANY_B:Company_B owns Company_A and also shares an address with Company_A', 'PERSON_C:Person_C is director of Company_A']}
17:16:56,155 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
17:16:56,156 graphrag.index.cli INFO Starting pipeline run for: 20241016-171656, dryrun=False
17:16:56,156 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:16:56,159 graphrag.index.create_pipeline_config INFO skipping workflows 
17:16:56,159 graphrag.index.run.run INFO Running pipeline
17:16:56,160 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
17:16:56,162 graphrag.index.input.load_input INFO loading input from root_dir=input
17:16:56,162 graphrag.index.input.load_input INFO using file storage for input
17:16:56,162 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
17:16:56,163 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
17:16:56,165 graphrag.index.input.text INFO Found 1 files, loading 1
17:16:56,165 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
17:16:56,166 graphrag.index.run.run INFO Final # of rows loaded: 1
17:16:56,279 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:16:56,282 datashaper.workflow.workflow INFO executing verb orderby
17:16:56,283 datashaper.workflow.workflow INFO executing verb zip
17:16:56,285 datashaper.workflow.workflow INFO executing verb aggregate_override
17:16:56,288 datashaper.workflow.workflow INFO executing verb chunk
17:16:56,377 datashaper.workflow.workflow INFO executing verb select
17:16:56,379 datashaper.workflow.workflow INFO executing verb unroll
17:16:56,382 datashaper.workflow.workflow INFO executing verb rename
17:16:56,384 datashaper.workflow.workflow INFO executing verb genid
17:16:56,386 datashaper.workflow.workflow INFO executing verb unzip
17:16:56,389 datashaper.workflow.workflow INFO executing verb copy
17:16:56,391 datashaper.workflow.workflow INFO executing verb filter
17:16:56,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:16:56,524 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
17:16:56,524 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:16:56,535 datashaper.workflow.workflow INFO executing verb entity_extract
17:16:56,538 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:16:56,576 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
17:16:56,576 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
17:16:58,832 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:16:58,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.25820343894884. input_tokens=1844, output_tokens=260
17:17:00,477 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:17:00,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6380444939713925. input_tokens=34, output_tokens=380
17:17:00,506 datashaper.workflow.workflow INFO executing verb snapshot
17:17:00,512 datashaper.workflow.workflow INFO executing verb merge_graphs
17:17:00,516 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:17:00,521 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
17:17:00,637 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
17:17:00,637 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
17:17:00,648 datashaper.workflow.workflow INFO executing verb summarize_descriptions
17:17:00,656 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:17:00,660 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
17:17:00,775 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
17:17:00,776 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
17:17:00,787 datashaper.workflow.workflow INFO executing verb cluster_graph
17:17:00,797 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:17:00,803 datashaper.workflow.workflow INFO executing verb embed_graph
17:17:00,806 root INFO Starting preprocessing of transition probabilities on graph with 4 nodes and 5 edges
17:17:00,806 root INFO Starting at time 1729091820.8067698
17:17:00,806 root INFO Beginning preprocessing of transition probabilities for 4 vertices
17:17:00,806 root INFO Completed 1 / 4 vertices
17:17:00,806 root INFO Completed 2 / 4 vertices
17:17:00,806 root INFO Completed 3 / 4 vertices
17:17:00,806 root INFO Completed 4 / 4 vertices
17:17:00,806 root INFO Completed preprocessing of transition probabilities for vertices
17:17:00,807 root INFO Beginning preprocessing of transition probabilities for 5 edges
17:17:00,807 root INFO Completed 1 / 5 edges
17:17:00,808 root INFO Completed 2 / 5 edges
17:17:00,808 root INFO Completed 3 / 5 edges
17:17:00,808 root INFO Completed 4 / 5 edges
17:17:00,808 root INFO Completed 5 / 5 edges
17:17:00,808 root INFO Completed preprocessing of transition probabilities for edges
17:17:00,809 root INFO Simulating walks on graph at time 1729091820.8096118
17:17:00,809 root INFO Walk iteration: 1/10
17:17:00,810 root INFO Walk iteration: 2/10
17:17:00,810 root INFO Walk iteration: 3/10
17:17:00,811 root INFO Walk iteration: 4/10
17:17:00,811 root INFO Walk iteration: 5/10
17:17:00,812 root INFO Walk iteration: 6/10
17:17:00,813 root INFO Walk iteration: 7/10
17:17:00,813 root INFO Walk iteration: 8/10
17:17:00,814 root INFO Walk iteration: 9/10
17:17:00,814 root INFO Walk iteration: 10/10
17:17:00,815 root INFO Learning embeddings at time 1729091820.815058
17:17:00,815 gensim.models.word2vec INFO collecting all words and their counts
17:17:00,815 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
17:17:00,817 gensim.models.word2vec INFO collected 4 word types from a corpus of 720 raw words and 40 sentences
17:17:00,817 gensim.models.word2vec INFO Creating a fresh vocabulary
17:17:00,817 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 4 unique words (100.00% of original 4, drops 0)', 'datetime': '2024-10-16T17:17:00.817727', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:17:00,818 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 720 word corpus (100.00% of original 720, drops 0)', 'datetime': '2024-10-16T17:17:00.818264', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:17:00,818 gensim.models.word2vec INFO deleting the raw counts dictionary of 4 items
17:17:00,818 gensim.models.word2vec INFO sample=0.001 downsamples 4 most-common words
17:17:00,819 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 47.360583452959204 word corpus (6.6%% of prior 720)', 'datetime': '2024-10-16T17:17:00.819464', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:17:00,819 gensim.models.word2vec INFO estimated required memory for 4 words and 1536 dimensions: 51152 bytes
17:17:00,820 gensim.models.word2vec INFO resetting layer weights
17:17:00,820 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-16T17:17:00.820530', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
17:17:00,821 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-16T17:17:00.821048', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:17:00,823 gensim.models.word2vec INFO EPOCH 0: training on 720 raw words (52 effective words) took 0.0s, 180045 effective words/s
17:17:00,824 gensim.models.word2vec INFO EPOCH 1: training on 720 raw words (57 effective words) took 0.0s, 193431 effective words/s
17:17:00,825 gensim.models.word2vec INFO EPOCH 2: training on 720 raw words (49 effective words) took 0.0s, 218258 effective words/s
17:17:00,825 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2160 raw words (158 effective words) took 0.0s, 40945 effective words/s', 'datetime': '2024-10-16T17:17:00.825415', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:17:00,827 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-16T17:17:00.827543', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
17:17:00,828 root INFO Completed. Ending time is 1729091820.828221 Elapsed time is -0.021451234817504883
17:17:00,833 datashaper.workflow.workflow INFO executing verb snapshot_rows
17:17:00,840 datashaper.workflow.workflow INFO executing verb select
17:17:00,843 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
17:17:00,982 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:17:00,982 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:17:00,995 datashaper.workflow.workflow INFO executing verb unpack_graph
17:17:01,0 datashaper.workflow.workflow INFO executing verb rename
17:17:01,4 datashaper.workflow.workflow INFO executing verb select
17:17:01,9 datashaper.workflow.workflow INFO executing verb dedupe
17:17:01,14 datashaper.workflow.workflow INFO executing verb rename
17:17:01,18 datashaper.workflow.workflow INFO executing verb filter
17:17:01,30 datashaper.workflow.workflow INFO executing verb text_split
17:17:01,35 datashaper.workflow.workflow INFO executing verb drop
17:17:01,40 datashaper.workflow.workflow INFO executing verb merge
17:17:01,46 datashaper.workflow.workflow INFO executing verb text_embed
17:17:01,48 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:17:01,85 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
17:17:01,85 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
17:17:01,85 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
17:17:02,269 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
17:17:02,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2106606590095907. input_tokens=100, output_tokens=0
17:17:02,313 datashaper.workflow.workflow INFO executing verb drop
17:17:02,319 datashaper.workflow.workflow INFO executing verb filter
17:17:02,330 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:17:02,458 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:17:02,459 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:17:02,475 datashaper.workflow.workflow INFO executing verb layout_graph
17:17:04,880 datashaper.workflow.workflow INFO executing verb unpack_graph
17:17:04,887 datashaper.workflow.workflow INFO executing verb unpack_graph
17:17:04,894 datashaper.workflow.workflow INFO executing verb filter
17:17:04,909 datashaper.workflow.workflow INFO executing verb drop
17:17:04,915 datashaper.workflow.workflow INFO executing verb select
17:17:04,922 datashaper.workflow.workflow INFO executing verb snapshot
17:17:04,931 datashaper.workflow.workflow INFO executing verb rename
17:17:04,938 datashaper.workflow.workflow INFO executing verb convert
17:17:04,962 datashaper.workflow.workflow INFO executing verb join
17:17:05,26 datashaper.workflow.workflow INFO executing verb rename
17:17:05,30 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:17:05,174 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:17:05,175 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:17:05,194 datashaper.workflow.workflow INFO executing verb create_final_communities
17:17:05,209 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:17:05,348 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
17:17:05,350 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:17:05,355 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:17:05,372 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
17:17:05,381 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
17:17:05,389 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:17:05,530 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
17:17:05,530 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:17:05,535 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:17:05,537 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:17:05,555 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
17:17:05,571 datashaper.workflow.workflow INFO executing verb select
17:17:05,575 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:17:05,711 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
17:17:05,712 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:17:05,718 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:17:05,736 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
17:17:05,745 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
17:17:05,754 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
17:17:05,764 datashaper.workflow.workflow INFO executing verb prepare_community_reports
17:17:05,765 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 5
17:17:05,803 datashaper.workflow.workflow INFO executing verb create_community_reports
17:17:09,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:17:09,203 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3728058820124716. input_tokens=2125, output_tokens=511
17:17:09,238 datashaper.workflow.workflow INFO executing verb window
17:17:09,242 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:17:09,383 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
17:17:09,383 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:17:09,415 datashaper.workflow.workflow INFO executing verb unroll
17:17:09,425 datashaper.workflow.workflow INFO executing verb select
17:17:09,434 datashaper.workflow.workflow INFO executing verb rename
17:17:09,444 datashaper.workflow.workflow INFO executing verb join
17:17:09,455 datashaper.workflow.workflow INFO executing verb aggregate_override
17:17:09,466 datashaper.workflow.workflow INFO executing verb join
17:17:09,478 datashaper.workflow.workflow INFO executing verb rename
17:17:09,487 datashaper.workflow.workflow INFO executing verb convert
17:17:09,491 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
17:17:09,631 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
17:17:09,631 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
17:17:09,656 datashaper.workflow.workflow INFO executing verb rename
17:17:09,659 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:17:09,699 graphrag.index.cli INFO All workflows completed successfully.
20:13:41,34 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:13:41,39 graphrag.index.cli INFO Starting pipeline run for: 20241016-201341, dryrun=False
20:13:41,40 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
20:13:41,46 graphrag.index.create_pipeline_config INFO skipping workflows 
20:13:41,47 graphrag.index.run.run INFO Running pipeline
20:13:41,47 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:13:41,50 graphrag.index.input.load_input INFO loading input from root_dir=input
20:13:41,50 graphrag.index.input.load_input INFO using file storage for input
20:13:41,53 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
20:13:41,54 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
20:13:41,58 graphrag.index.input.text INFO Found 1 files, loading 1
20:13:41,62 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
20:13:41,62 graphrag.index.run.run INFO Final # of rows loaded: 1
20:13:41,170 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:13:41,173 datashaper.workflow.workflow INFO executing verb orderby
20:13:41,178 datashaper.workflow.workflow INFO executing verb zip
20:13:41,180 datashaper.workflow.workflow INFO executing verb aggregate_override
20:13:41,190 datashaper.workflow.workflow INFO executing verb chunk
20:13:43,151 datashaper.workflow.workflow INFO executing verb select
20:13:43,153 datashaper.workflow.workflow INFO executing verb unroll
20:13:43,160 datashaper.workflow.workflow INFO executing verb rename
20:13:43,162 datashaper.workflow.workflow INFO executing verb genid
20:13:43,164 datashaper.workflow.workflow INFO executing verb unzip
20:13:43,167 datashaper.workflow.workflow INFO executing verb copy
20:13:43,169 datashaper.workflow.workflow INFO executing verb filter
20:13:43,181 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:13:43,344 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
20:13:43,345 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:13:43,381 datashaper.workflow.workflow INFO executing verb entity_extract
20:13:43,385 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:13:43,423 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
20:13:43,423 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:13:43,432 datashaper.workflow.workflow INFO executing verb snapshot
20:13:43,437 datashaper.workflow.workflow INFO executing verb merge_graphs
20:13:43,441 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:13:43,445 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
20:13:43,559 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
20:13:43,559 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
20:13:43,570 datashaper.workflow.workflow INFO executing verb summarize_descriptions
20:13:43,581 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:13:43,585 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
20:13:43,704 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
20:13:43,705 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
20:13:43,715 datashaper.workflow.workflow INFO executing verb cluster_graph
20:13:43,729 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:13:43,736 datashaper.workflow.workflow INFO executing verb embed_graph
20:13:43,742 root INFO Starting preprocessing of transition probabilities on graph with 4 nodes and 5 edges
20:13:43,742 root INFO Starting at time 1729102423.7424846
20:13:43,742 root INFO Beginning preprocessing of transition probabilities for 4 vertices
20:13:43,742 root INFO Completed 1 / 4 vertices
20:13:43,743 root INFO Completed 2 / 4 vertices
20:13:43,743 root INFO Completed 3 / 4 vertices
20:13:43,744 root INFO Completed 4 / 4 vertices
20:13:43,745 root INFO Completed preprocessing of transition probabilities for vertices
20:13:43,745 root INFO Beginning preprocessing of transition probabilities for 5 edges
20:13:43,746 root INFO Completed 1 / 5 edges
20:13:43,747 root INFO Completed 2 / 5 edges
20:13:43,748 root INFO Completed 3 / 5 edges
20:13:43,748 root INFO Completed 4 / 5 edges
20:13:43,749 root INFO Completed 5 / 5 edges
20:13:43,750 root INFO Completed preprocessing of transition probabilities for edges
20:13:43,750 root INFO Simulating walks on graph at time 1729102423.7508068
20:13:43,751 root INFO Walk iteration: 1/10
20:13:43,752 root INFO Walk iteration: 2/10
20:13:43,752 root INFO Walk iteration: 3/10
20:13:43,753 root INFO Walk iteration: 4/10
20:13:43,754 root INFO Walk iteration: 5/10
20:13:43,754 root INFO Walk iteration: 6/10
20:13:43,755 root INFO Walk iteration: 7/10
20:13:43,756 root INFO Walk iteration: 8/10
20:13:43,756 root INFO Walk iteration: 9/10
20:13:43,757 root INFO Walk iteration: 10/10
20:13:43,758 root INFO Learning embeddings at time 1729102423.7580886
20:13:43,759 gensim.models.word2vec INFO collecting all words and their counts
20:13:43,759 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:13:43,759 gensim.models.word2vec INFO collected 4 word types from a corpus of 720 raw words and 40 sentences
20:13:43,759 gensim.models.word2vec INFO Creating a fresh vocabulary
20:13:43,759 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 4 unique words (100.00% of original 4, drops 0)', 'datetime': '2024-10-16T20:13:43.759937', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:13:43,760 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 720 word corpus (100.00% of original 720, drops 0)', 'datetime': '2024-10-16T20:13:43.760502', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:13:43,761 gensim.models.word2vec INFO deleting the raw counts dictionary of 4 items
20:13:43,761 gensim.models.word2vec INFO sample=0.001 downsamples 4 most-common words
20:13:43,761 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 47.360583452959204 word corpus (6.6%% of prior 720)', 'datetime': '2024-10-16T20:13:43.761906', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:13:43,762 gensim.models.word2vec INFO estimated required memory for 4 words and 1536 dimensions: 51152 bytes
20:13:43,763 gensim.models.word2vec INFO resetting layer weights
20:13:43,763 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-16T20:13:43.763750', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:13:43,763 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-16T20:13:43.763963', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:13:43,770 gensim.models.word2vec INFO EPOCH 0: training on 720 raw words (52 effective words) took 0.0s, 22195 effective words/s
20:13:43,774 gensim.models.word2vec INFO EPOCH 1: training on 720 raw words (57 effective words) took 0.0s, 54370 effective words/s
20:13:43,777 gensim.models.word2vec INFO EPOCH 2: training on 720 raw words (49 effective words) took 0.0s, 57544 effective words/s
20:13:43,777 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2160 raw words (158 effective words) took 0.0s, 12291 effective words/s', 'datetime': '2024-10-16T20:13:43.777649', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:13:43,778 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-16T20:13:43.778501', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:13:43,779 root INFO Completed. Ending time is 1729102423.7792628 Elapsed time is -0.03677821159362793
20:13:43,794 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:13:43,803 datashaper.workflow.workflow INFO executing verb select
20:13:43,806 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
20:13:43,931 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
20:13:43,931 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:13:43,944 datashaper.workflow.workflow INFO executing verb unpack_graph
20:13:43,949 datashaper.workflow.workflow INFO executing verb rename
20:13:43,954 datashaper.workflow.workflow INFO executing verb select
20:13:43,958 datashaper.workflow.workflow INFO executing verb dedupe
20:13:43,963 datashaper.workflow.workflow INFO executing verb rename
20:13:43,968 datashaper.workflow.workflow INFO executing verb filter
20:13:43,980 datashaper.workflow.workflow INFO executing verb text_split
20:13:43,985 datashaper.workflow.workflow INFO executing verb drop
20:13:43,991 datashaper.workflow.workflow INFO executing verb merge
20:13:43,997 datashaper.workflow.workflow INFO executing verb text_embed
20:13:43,999 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:13:44,36 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
20:13:44,36 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
20:13:44,36 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
20:13:44,46 datashaper.workflow.workflow INFO executing verb drop
20:13:44,52 datashaper.workflow.workflow INFO executing verb filter
20:13:44,62 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
20:13:44,193 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
20:13:44,194 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:13:44,209 datashaper.workflow.workflow INFO executing verb layout_graph
20:13:46,677 datashaper.workflow.workflow INFO executing verb unpack_graph
20:13:46,684 datashaper.workflow.workflow INFO executing verb unpack_graph
20:13:46,691 datashaper.workflow.workflow INFO executing verb drop
20:13:46,697 datashaper.workflow.workflow INFO executing verb filter
20:13:46,712 datashaper.workflow.workflow INFO executing verb select
20:13:46,719 datashaper.workflow.workflow INFO executing verb snapshot
20:13:46,728 datashaper.workflow.workflow INFO executing verb rename
20:13:46,735 datashaper.workflow.workflow INFO executing verb convert
20:13:46,751 datashaper.workflow.workflow INFO executing verb join
20:13:46,766 datashaper.workflow.workflow INFO executing verb rename
20:13:46,769 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
20:13:46,915 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
20:13:46,915 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:13:46,934 datashaper.workflow.workflow INFO executing verb create_final_communities
20:13:46,949 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
20:13:47,86 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
20:13:47,92 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:13:47,99 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:13:47,116 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
20:13:47,124 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
20:13:47,130 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
20:13:47,265 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
20:13:47,266 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
20:13:47,270 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:13:47,273 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:13:47,291 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
20:13:47,306 datashaper.workflow.workflow INFO executing verb select
20:13:47,309 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
20:13:47,445 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
20:13:47,445 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:13:47,449 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:13:47,468 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
20:13:47,477 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
20:13:47,487 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
20:13:47,498 datashaper.workflow.workflow INFO executing verb prepare_community_reports
20:13:47,498 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 5
20:13:47,518 datashaper.workflow.workflow INFO executing verb create_community_reports
20:13:51,57 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:13:51,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5396421890000056. input_tokens=2126, output_tokens=516
20:13:51,94 datashaper.workflow.workflow INFO executing verb window
20:13:51,98 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
20:13:51,246 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
20:13:51,250 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
20:13:51,271 datashaper.workflow.workflow INFO executing verb unroll
20:13:51,281 datashaper.workflow.workflow INFO executing verb select
20:13:51,291 datashaper.workflow.workflow INFO executing verb rename
20:13:51,300 datashaper.workflow.workflow INFO executing verb join
20:13:51,312 datashaper.workflow.workflow INFO executing verb aggregate_override
20:13:51,322 datashaper.workflow.workflow INFO executing verb join
20:13:51,334 datashaper.workflow.workflow INFO executing verb rename
20:13:51,344 datashaper.workflow.workflow INFO executing verb convert
20:13:51,348 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
20:13:51,486 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
20:13:51,487 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
20:13:51,512 datashaper.workflow.workflow INFO executing verb rename
20:13:51,515 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
20:13:51,555 graphrag.index.cli INFO All workflows completed successfully.
20:17:42,105 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:17:42,106 graphrag.index.cli INFO Starting pipeline run for: 20241016-201742, dryrun=False
20:17:42,106 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
20:17:42,109 graphrag.index.create_pipeline_config INFO skipping workflows 
20:17:42,109 graphrag.index.run.run INFO Running pipeline
20:17:42,109 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:17:42,110 graphrag.index.input.load_input INFO loading input from root_dir=input
20:17:42,110 graphrag.index.input.load_input INFO using file storage for input
20:17:42,113 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
20:17:42,114 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
20:17:42,116 graphrag.index.input.text INFO Found 1 files, loading 1
20:17:42,118 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
20:17:42,118 graphrag.index.run.run INFO Final # of rows loaded: 1
20:17:42,228 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:17:42,230 datashaper.workflow.workflow INFO executing verb orderby
20:17:42,232 datashaper.workflow.workflow INFO executing verb zip
20:17:42,233 datashaper.workflow.workflow INFO executing verb aggregate_override
20:17:42,237 datashaper.workflow.workflow INFO executing verb chunk
20:17:42,327 datashaper.workflow.workflow INFO executing verb select
20:17:42,329 datashaper.workflow.workflow INFO executing verb unroll
20:17:42,332 datashaper.workflow.workflow INFO executing verb rename
20:17:42,334 datashaper.workflow.workflow INFO executing verb genid
20:17:42,336 datashaper.workflow.workflow INFO executing verb unzip
20:17:42,339 datashaper.workflow.workflow INFO executing verb copy
20:17:42,341 datashaper.workflow.workflow INFO executing verb filter
20:17:42,349 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:17:42,469 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
20:17:42,469 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:17:42,479 datashaper.workflow.workflow INFO executing verb entity_extract
20:17:42,482 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:17:42,519 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
20:17:42,519 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:17:44,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:17:44,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8602856850002354. input_tokens=1844, output_tokens=259
20:17:45,868 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:17:45,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4887829730000703. input_tokens=34, output_tokens=327
20:17:45,892 datashaper.workflow.workflow INFO executing verb snapshot
20:17:45,898 datashaper.workflow.workflow INFO executing verb merge_graphs
20:17:45,902 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:17:45,906 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
20:17:46,25 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
20:17:46,25 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
20:17:46,35 datashaper.workflow.workflow INFO executing verb summarize_descriptions
20:17:46,42 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:17:46,46 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
20:17:46,162 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
20:17:46,166 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
20:17:46,176 datashaper.workflow.workflow INFO executing verb cluster_graph
20:17:46,186 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:17:46,192 datashaper.workflow.workflow INFO executing verb embed_graph
20:17:46,195 root INFO Starting preprocessing of transition probabilities on graph with 4 nodes and 3 edges
20:17:46,196 root INFO Starting at time 1729102666.1960127
20:17:46,196 root INFO Beginning preprocessing of transition probabilities for 4 vertices
20:17:46,196 root INFO Completed 1 / 4 vertices
20:17:46,196 root INFO Completed 2 / 4 vertices
20:17:46,196 root INFO Completed 3 / 4 vertices
20:17:46,197 root INFO Completed 4 / 4 vertices
20:17:46,198 root INFO Completed preprocessing of transition probabilities for vertices
20:17:46,198 root INFO Beginning preprocessing of transition probabilities for 3 edges
20:17:46,199 root INFO Completed 1 / 3 edges
20:17:46,200 root INFO Completed 2 / 3 edges
20:17:46,201 root INFO Completed 3 / 3 edges
20:17:46,202 root INFO Completed preprocessing of transition probabilities for edges
20:17:46,203 root INFO Simulating walks on graph at time 1729102666.2030125
20:17:46,204 root INFO Walk iteration: 1/10
20:17:46,204 root INFO Walk iteration: 2/10
20:17:46,205 root INFO Walk iteration: 3/10
20:17:46,206 root INFO Walk iteration: 4/10
20:17:46,206 root INFO Walk iteration: 5/10
20:17:46,207 root INFO Walk iteration: 6/10
20:17:46,208 root INFO Walk iteration: 7/10
20:17:46,209 root INFO Walk iteration: 8/10
20:17:46,209 root INFO Walk iteration: 9/10
20:17:46,210 root INFO Walk iteration: 10/10
20:17:46,211 root INFO Learning embeddings at time 1729102666.2113938
20:17:46,212 gensim.models.word2vec INFO collecting all words and their counts
20:17:46,212 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:17:46,212 gensim.models.word2vec INFO collected 4 word types from a corpus of 640 raw words and 40 sentences
20:17:46,212 gensim.models.word2vec INFO Creating a fresh vocabulary
20:17:46,213 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 4 unique words (100.00% of original 4, drops 0)', 'datetime': '2024-10-16T20:17:46.213773', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:17:46,214 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 640 word corpus (100.00% of original 640, drops 0)', 'datetime': '2024-10-16T20:17:46.214555', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:17:46,214 gensim.models.word2vec INFO deleting the raw counts dictionary of 4 items
20:17:46,215 gensim.models.word2vec INFO sample=0.001 downsamples 4 most-common words
20:17:46,215 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 39.76724592377582 word corpus (6.2%% of prior 640)', 'datetime': '2024-10-16T20:17:46.215476', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:17:46,215 gensim.models.word2vec INFO estimated required memory for 4 words and 1536 dimensions: 51152 bytes
20:17:46,216 gensim.models.word2vec INFO resetting layer weights
20:17:46,216 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-16T20:17:46.216544', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:17:46,217 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-16T20:17:46.217022', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:17:46,222 gensim.models.word2vec INFO EPOCH 0: training on 640 raw words (46 effective words) took 0.0s, 41624 effective words/s
20:17:46,225 gensim.models.word2vec INFO EPOCH 1: training on 640 raw words (43 effective words) took 0.0s, 49624 effective words/s
20:17:46,228 gensim.models.word2vec INFO EPOCH 2: training on 640 raw words (45 effective words) took 0.0s, 60866 effective words/s
20:17:46,228 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1920 raw words (134 effective words) took 0.0s, 12539 effective words/s', 'datetime': '2024-10-16T20:17:46.228392', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:17:46,229 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-16T20:17:46.229237', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:17:46,229 root INFO Completed. Ending time is 1729102666.2299588 Elapsed time is -0.03394603729248047
20:17:46,242 datashaper.workflow.workflow INFO executing verb snapshot_rows
20:17:46,249 datashaper.workflow.workflow INFO executing verb select
20:17:46,252 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
20:17:46,373 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
20:17:46,373 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:17:46,386 datashaper.workflow.workflow INFO executing verb unpack_graph
20:17:46,391 datashaper.workflow.workflow INFO executing verb rename
20:17:46,395 datashaper.workflow.workflow INFO executing verb select
20:17:46,400 datashaper.workflow.workflow INFO executing verb dedupe
20:17:46,405 datashaper.workflow.workflow INFO executing verb rename
20:17:46,410 datashaper.workflow.workflow INFO executing verb filter
20:17:46,422 datashaper.workflow.workflow INFO executing verb text_split
20:17:46,427 datashaper.workflow.workflow INFO executing verb drop
20:17:46,433 datashaper.workflow.workflow INFO executing verb merge
20:17:46,439 datashaper.workflow.workflow INFO executing verb text_embed
20:17:46,440 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:17:46,478 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
20:17:46,478 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
20:17:46,478 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
20:17:47,808 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
20:17:47,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3519244540002546. input_tokens=103, output_tokens=0
20:17:47,842 datashaper.workflow.workflow INFO executing verb drop
20:17:47,847 datashaper.workflow.workflow INFO executing verb filter
20:17:47,858 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
20:17:47,986 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
20:17:47,987 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:17:48,2 datashaper.workflow.workflow INFO executing verb layout_graph
20:17:50,386 datashaper.workflow.workflow INFO executing verb unpack_graph
20:17:50,393 datashaper.workflow.workflow INFO executing verb unpack_graph
20:17:50,400 datashaper.workflow.workflow INFO executing verb drop
20:17:50,406 datashaper.workflow.workflow INFO executing verb filter
20:17:50,421 datashaper.workflow.workflow INFO executing verb select
20:17:50,428 datashaper.workflow.workflow INFO executing verb snapshot
20:17:50,437 datashaper.workflow.workflow INFO executing verb rename
20:17:50,444 datashaper.workflow.workflow INFO executing verb convert
20:17:50,459 datashaper.workflow.workflow INFO executing verb join
20:17:50,471 datashaper.workflow.workflow INFO executing verb rename
20:17:50,474 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
20:17:50,617 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
20:17:50,618 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:17:50,636 datashaper.workflow.workflow INFO executing verb create_final_communities
20:17:50,645 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
20:17:50,778 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
20:17:50,778 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:17:50,783 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:17:50,802 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
20:17:50,810 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
20:17:50,816 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
20:17:50,952 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
20:17:50,952 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:17:50,957 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
20:17:50,959 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:17:50,977 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
20:17:50,991 datashaper.workflow.workflow INFO executing verb select
20:17:50,993 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
20:17:51,131 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
20:17:51,131 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:17:51,136 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:17:51,155 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
20:17:51,164 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
20:17:51,173 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
20:17:51,183 datashaper.workflow.workflow INFO executing verb prepare_community_reports
20:17:51,183 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 5
20:17:51,201 datashaper.workflow.workflow INFO executing verb create_community_reports
20:17:55,121 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:17:55,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.915330016000098. input_tokens=2094, output_tokens=663
20:17:55,153 datashaper.workflow.workflow INFO executing verb window
20:17:55,156 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
20:17:55,301 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
20:17:55,301 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
20:17:55,324 datashaper.workflow.workflow INFO executing verb unroll
20:17:55,333 datashaper.workflow.workflow INFO executing verb select
20:17:55,343 datashaper.workflow.workflow INFO executing verb rename
20:17:55,352 datashaper.workflow.workflow INFO executing verb join
20:17:55,373 datashaper.workflow.workflow INFO executing verb aggregate_override
20:17:55,384 datashaper.workflow.workflow INFO executing verb join
20:17:55,396 datashaper.workflow.workflow INFO executing verb rename
20:17:55,405 datashaper.workflow.workflow INFO executing verb convert
20:17:55,409 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
20:17:55,549 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
20:17:55,549 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
20:17:55,574 datashaper.workflow.workflow INFO executing verb rename
20:17:55,577 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
20:17:55,616 graphrag.index.cli INFO All workflows completed successfully.
22:46:24,277 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:46:24,280 graphrag.index.cli INFO Starting pipeline run for: 20241016-224624, dryrun=False
22:46:24,280 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:46:24,282 graphrag.index.create_pipeline_config INFO skipping workflows 
22:46:24,282 graphrag.index.run.run INFO Running pipeline
22:46:24,282 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:46:24,283 graphrag.index.input.load_input INFO loading input from root_dir=input
22:46:24,283 graphrag.index.input.load_input INFO using file storage for input
22:46:24,284 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:46:24,285 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:46:24,286 graphrag.index.input.text INFO Found 1 files, loading 1
22:46:24,289 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:46:24,289 graphrag.index.run.run INFO Final # of rows loaded: 1
22:46:24,398 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:46:24,401 datashaper.workflow.workflow INFO executing verb orderby
22:46:24,407 datashaper.workflow.workflow INFO executing verb zip
22:46:24,409 datashaper.workflow.workflow INFO executing verb aggregate_override
22:46:24,416 datashaper.workflow.workflow INFO executing verb chunk
22:46:24,506 datashaper.workflow.workflow INFO executing verb select
22:46:24,511 datashaper.workflow.workflow INFO executing verb unroll
22:46:24,517 datashaper.workflow.workflow INFO executing verb rename
22:46:24,519 datashaper.workflow.workflow INFO executing verb genid
22:46:24,521 datashaper.workflow.workflow INFO executing verb unzip
22:46:24,524 datashaper.workflow.workflow INFO executing verb copy
22:46:24,526 datashaper.workflow.workflow INFO executing verb filter
22:46:24,535 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:46:24,655 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:46:24,656 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:46:24,665 datashaper.workflow.workflow INFO executing verb entity_extract
22:46:24,667 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:46:24,706 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:46:24,706 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:46:27,463 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:27,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.760681924999517. input_tokens=375, output_tokens=432
22:46:30,149 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:30,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.6839471219991538. input_tokens=34, output_tokens=661
22:46:30,173 datashaper.workflow.workflow INFO executing verb snapshot
22:46:30,181 datashaper.workflow.workflow INFO executing verb merge_graphs
22:46:30,190 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:46:30,195 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:46:30,317 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:46:30,317 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:46:30,327 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:46:30,333 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:46:30,338 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:46:30,452 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:46:30,452 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:46:30,463 datashaper.workflow.workflow INFO executing verb cluster_graph
22:46:30,463 graphrag.index.verbs.graph.clustering.cluster_graph WARNING Graph has no nodes
22:46:30,465 datashaper.workflow.workflow ERROR Error executing verb "cluster_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/clustering/cluster_graph.py", line 106, in cluster_graph
    output_df[[level_to, to]] = pd.DataFrame(
    ~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
22:46:30,468 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "cluster_graph" in create_base_entity_graph: Columns must be same length as key details=None
22:46:30,468 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/clustering/cluster_graph.py", line 106, in cluster_graph
    output_df[[level_to, to]] = pd.DataFrame(
    ~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
22:46:30,470 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:46:30,482 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:48:34,586 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:48:34,588 graphrag.index.cli INFO Starting pipeline run for: 20241016-224834, dryrun=False
22:48:34,588 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:48:34,590 graphrag.index.create_pipeline_config INFO skipping workflows 
22:48:34,590 graphrag.index.run.run INFO Running pipeline
22:48:34,591 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:48:34,592 graphrag.index.input.load_input INFO loading input from root_dir=input
22:48:34,592 graphrag.index.input.load_input INFO using file storage for input
22:48:34,594 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:48:34,595 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:48:34,595 graphrag.index.input.text INFO Found 1 files, loading 1
22:48:34,596 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:48:34,597 graphrag.index.run.run INFO Final # of rows loaded: 1
22:48:34,705 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:48:34,707 datashaper.workflow.workflow INFO executing verb orderby
22:48:34,709 datashaper.workflow.workflow INFO executing verb zip
22:48:34,711 datashaper.workflow.workflow INFO executing verb aggregate_override
22:48:34,714 datashaper.workflow.workflow INFO executing verb chunk
22:48:34,805 datashaper.workflow.workflow INFO executing verb select
22:48:34,807 datashaper.workflow.workflow INFO executing verb unroll
22:48:34,810 datashaper.workflow.workflow INFO executing verb rename
22:48:34,812 datashaper.workflow.workflow INFO executing verb genid
22:48:34,814 datashaper.workflow.workflow INFO executing verb unzip
22:48:34,817 datashaper.workflow.workflow INFO executing verb copy
22:48:34,819 datashaper.workflow.workflow INFO executing verb filter
22:48:34,827 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:48:34,947 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:48:34,948 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:48:34,957 datashaper.workflow.workflow INFO executing verb entity_extract
22:48:34,959 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:48:34,998 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:48:34,998 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:48:36,305 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:36,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.3107274959984352. input_tokens=1844, output_tokens=259
22:48:37,773 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:37,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4657622649992845. input_tokens=34, output_tokens=327
22:48:37,797 datashaper.workflow.workflow INFO executing verb snapshot
22:48:37,804 datashaper.workflow.workflow INFO executing verb merge_graphs
22:48:37,808 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:48:37,813 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:48:37,932 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:48:37,932 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:48:37,942 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:48:37,949 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:48:37,954 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:48:38,68 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:48:38,69 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:48:38,80 datashaper.workflow.workflow INFO executing verb cluster_graph
22:48:38,91 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:48:38,98 datashaper.workflow.workflow INFO executing verb embed_graph
22:48:38,102 root INFO Starting preprocessing of transition probabilities on graph with 4 nodes and 3 edges
22:48:38,102 root INFO Starting at time 1729111718.1024194
22:48:38,102 root INFO Beginning preprocessing of transition probabilities for 4 vertices
22:48:38,102 root INFO Completed 1 / 4 vertices
22:48:38,102 root INFO Completed 2 / 4 vertices
22:48:38,102 root INFO Completed 3 / 4 vertices
22:48:38,102 root INFO Completed 4 / 4 vertices
22:48:38,102 root INFO Completed preprocessing of transition probabilities for vertices
22:48:38,103 root INFO Beginning preprocessing of transition probabilities for 3 edges
22:48:38,103 root INFO Completed 1 / 3 edges
22:48:38,103 root INFO Completed 2 / 3 edges
22:48:38,103 root INFO Completed 3 / 3 edges
22:48:38,104 root INFO Completed preprocessing of transition probabilities for edges
22:48:38,104 root INFO Simulating walks on graph at time 1729111718.104537
22:48:38,105 root INFO Walk iteration: 1/10
22:48:38,106 root INFO Walk iteration: 2/10
22:48:38,106 root INFO Walk iteration: 3/10
22:48:38,107 root INFO Walk iteration: 4/10
22:48:38,108 root INFO Walk iteration: 5/10
22:48:38,108 root INFO Walk iteration: 6/10
22:48:38,109 root INFO Walk iteration: 7/10
22:48:38,110 root INFO Walk iteration: 8/10
22:48:38,110 root INFO Walk iteration: 9/10
22:48:38,111 root INFO Walk iteration: 10/10
22:48:38,112 root INFO Learning embeddings at time 1729111718.1128185
22:48:38,113 gensim.models.word2vec INFO collecting all words and their counts
22:48:38,113 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:48:38,114 gensim.models.word2vec INFO collected 4 word types from a corpus of 640 raw words and 40 sentences
22:48:38,114 gensim.models.word2vec INFO Creating a fresh vocabulary
22:48:38,114 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 4 unique words (100.00% of original 4, drops 0)', 'datetime': '2024-10-16T22:48:38.114688', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:38,115 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 640 word corpus (100.00% of original 640, drops 0)', 'datetime': '2024-10-16T22:48:38.115364', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:38,116 gensim.models.word2vec INFO deleting the raw counts dictionary of 4 items
22:48:38,116 gensim.models.word2vec INFO sample=0.001 downsamples 4 most-common words
22:48:38,116 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 39.76724592377582 word corpus (6.2%% of prior 640)', 'datetime': '2024-10-16T22:48:38.116898', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:38,117 gensim.models.word2vec INFO estimated required memory for 4 words and 1536 dimensions: 51152 bytes
22:48:38,118 gensim.models.word2vec INFO resetting layer weights
22:48:38,118 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-16T22:48:38.118708', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:48:38,119 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-16T22:48:38.119243', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:48:38,123 gensim.models.word2vec INFO EPOCH 0: training on 640 raw words (46 effective words) took 0.0s, 142878 effective words/s
22:48:38,125 gensim.models.word2vec INFO EPOCH 1: training on 640 raw words (43 effective words) took 0.0s, 63433 effective words/s
22:48:38,127 gensim.models.word2vec INFO EPOCH 2: training on 640 raw words (45 effective words) took 0.0s, 87397 effective words/s
22:48:38,127 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1920 raw words (134 effective words) took 0.0s, 18680 effective words/s', 'datetime': '2024-10-16T22:48:38.127478', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:48:38,128 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-16T22:48:38.128376', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:48:38,129 root INFO Completed. Ending time is 1729111718.129217 Elapsed time is -0.02679753303527832
22:48:38,138 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:48:38,145 datashaper.workflow.workflow INFO executing verb select
22:48:38,148 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:48:38,277 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:48:38,277 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:38,289 datashaper.workflow.workflow INFO executing verb unpack_graph
22:48:38,294 datashaper.workflow.workflow INFO executing verb rename
22:48:38,299 datashaper.workflow.workflow INFO executing verb select
22:48:38,304 datashaper.workflow.workflow INFO executing verb dedupe
22:48:38,309 datashaper.workflow.workflow INFO executing verb rename
22:48:38,314 datashaper.workflow.workflow INFO executing verb filter
22:48:38,325 datashaper.workflow.workflow INFO executing verb text_split
22:48:38,331 datashaper.workflow.workflow INFO executing verb drop
22:48:38,336 datashaper.workflow.workflow INFO executing verb merge
22:48:38,342 datashaper.workflow.workflow INFO executing verb text_embed
22:48:38,343 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:48:38,381 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:48:38,381 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:48:38,381 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:48:39,344 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:48:39,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9695660710003722. input_tokens=103, output_tokens=0
22:48:39,363 datashaper.workflow.workflow INFO executing verb drop
22:48:39,369 datashaper.workflow.workflow INFO executing verb filter
22:48:39,379 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:48:39,508 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:48:39,508 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:39,524 datashaper.workflow.workflow INFO executing verb layout_graph
22:48:41,903 datashaper.workflow.workflow INFO executing verb unpack_graph
22:48:41,910 datashaper.workflow.workflow INFO executing verb unpack_graph
22:48:41,916 datashaper.workflow.workflow INFO executing verb filter
22:48:41,931 datashaper.workflow.workflow INFO executing verb drop
22:48:41,937 datashaper.workflow.workflow INFO executing verb select
22:48:41,944 datashaper.workflow.workflow INFO executing verb snapshot
22:48:41,954 datashaper.workflow.workflow INFO executing verb rename
22:48:41,960 datashaper.workflow.workflow INFO executing verb convert
22:48:41,982 datashaper.workflow.workflow INFO executing verb join
22:48:41,994 datashaper.workflow.workflow INFO executing verb rename
22:48:41,997 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:48:42,143 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:48:42,143 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:42,161 datashaper.workflow.workflow INFO executing verb create_final_communities
22:48:42,174 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:48:42,308 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:48:42,308 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:48:42,313 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:42,330 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:48:42,339 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:48:42,344 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:48:42,480 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:48:42,480 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:48:42,485 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:48:42,487 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:48:42,505 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:48:42,520 datashaper.workflow.workflow INFO executing verb select
22:48:42,523 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:48:42,660 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:48:42,666 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:48:42,673 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:48:42,692 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:48:42,701 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:48:42,710 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:48:42,720 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:48:42,720 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 5
22:48:42,740 datashaper.workflow.workflow INFO executing verb create_community_reports
22:48:46,288 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:46,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5470644639990496. input_tokens=2093, output_tokens=606
22:48:46,320 datashaper.workflow.workflow INFO executing verb window
22:48:46,323 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:48:46,470 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:48:46,470 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:48:46,497 datashaper.workflow.workflow INFO executing verb unroll
22:48:46,507 datashaper.workflow.workflow INFO executing verb select
22:48:46,516 datashaper.workflow.workflow INFO executing verb rename
22:48:46,526 datashaper.workflow.workflow INFO executing verb join
22:48:46,538 datashaper.workflow.workflow INFO executing verb aggregate_override
22:48:46,548 datashaper.workflow.workflow INFO executing verb join
22:48:46,562 datashaper.workflow.workflow INFO executing verb rename
22:48:46,571 datashaper.workflow.workflow INFO executing verb convert
22:48:46,585 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:48:46,731 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:48:46,736 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:48:46,759 datashaper.workflow.workflow INFO executing verb rename
22:48:46,762 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:48:46,801 graphrag.index.cli INFO All workflows completed successfully.
22:49:51,305 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:49:51,306 graphrag.index.cli INFO Starting pipeline run for: 20241016-224951, dryrun=False
22:49:51,307 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph"
    ]
}
22:49:51,309 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph
22:49:51,309 graphrag.index.run.run INFO Running pipeline
22:49:51,309 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:49:51,310 graphrag.index.input.load_input INFO loading input from root_dir=input
22:49:51,310 graphrag.index.input.load_input INFO using file storage for input
22:49:51,311 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:49:51,311 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:49:51,312 graphrag.index.input.text INFO Found 1 files, loading 1
22:49:51,313 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:49:51,313 graphrag.index.run.run INFO Final # of rows loaded: 1
22:49:51,421 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:49:51,424 datashaper.workflow.workflow INFO executing verb orderby
22:49:51,426 datashaper.workflow.workflow INFO executing verb zip
22:49:51,427 datashaper.workflow.workflow INFO executing verb aggregate_override
22:49:51,430 datashaper.workflow.workflow INFO executing verb chunk
22:49:51,521 datashaper.workflow.workflow INFO executing verb select
22:49:51,523 datashaper.workflow.workflow INFO executing verb unroll
22:49:51,526 datashaper.workflow.workflow INFO executing verb rename
22:49:51,528 datashaper.workflow.workflow INFO executing verb genid
22:49:51,531 datashaper.workflow.workflow INFO executing verb unzip
22:49:51,533 datashaper.workflow.workflow INFO executing verb copy
22:49:51,535 datashaper.workflow.workflow INFO executing verb filter
22:49:51,543 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:49:51,663 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:49:51,663 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:49:51,673 datashaper.workflow.workflow INFO executing verb entity_extract
22:49:51,676 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:49:51,713 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:49:51,713 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:49:53,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:53,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8814375410001958. input_tokens=1844, output_tokens=260
22:49:55,187 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:55,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5929221120004513. input_tokens=34, output_tokens=380
22:49:55,210 datashaper.workflow.workflow INFO executing verb snapshot
22:49:55,217 datashaper.workflow.workflow INFO executing verb merge_graphs
22:49:55,221 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:49:55,226 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:49:55,341 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:49:55,344 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:49:55,353 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:49:55,360 datashaper.workflow.workflow INFO executing verb snapshot_rows
22:49:55,365 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:49:55,480 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:49:55,480 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:49:55,492 datashaper.workflow.workflow INFO executing verb cluster_graph
22:49:55,503 datashaper.workflow.workflow INFO executing verb select
22:49:55,505 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:49:55,619 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:49:55,620 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:49:55,631 datashaper.workflow.workflow INFO executing verb unpack_graph
22:49:55,635 datashaper.workflow.workflow INFO executing verb rename
22:49:55,640 datashaper.workflow.workflow INFO executing verb select
22:49:55,644 datashaper.workflow.workflow INFO executing verb dedupe
22:49:55,649 datashaper.workflow.workflow INFO executing verb rename
22:49:55,654 datashaper.workflow.workflow INFO executing verb filter
22:49:55,665 datashaper.workflow.workflow INFO executing verb text_split
22:49:55,670 datashaper.workflow.workflow INFO executing verb drop
22:49:55,675 datashaper.workflow.workflow INFO executing verb merge
22:49:55,681 datashaper.workflow.workflow INFO executing verb text_embed
22:49:55,682 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:49:55,719 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:49:55,719 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:49:55,719 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:49:56,698 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:49:56,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9881473079985881. input_tokens=100, output_tokens=0
22:49:56,718 datashaper.workflow.workflow INFO executing verb drop
22:49:56,724 datashaper.workflow.workflow INFO executing verb filter
22:49:56,733 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:49:56,861 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:49:56,861 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:49:56,876 datashaper.workflow.workflow INFO executing verb layout_graph
22:49:56,880 graphrag.index.verbs.graph.layout.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/layout/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
22:49:56,903 graphrag.index.reporting.file_workflow_callbacks INFO Error in Umap details=None
22:49:56,916 datashaper.workflow.workflow INFO executing verb unpack_graph
22:49:56,922 datashaper.workflow.workflow INFO executing verb unpack_graph
22:49:56,928 datashaper.workflow.workflow INFO executing verb filter
22:49:56,942 datashaper.workflow.workflow INFO executing verb drop
22:49:56,942 datashaper.workflow.workflow ERROR Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:49:56,966 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis" details=None
22:49:56,967 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:49:56,969 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:49:56,989 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:51:22,875 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:51:22,876 graphrag.index.cli INFO Starting pipeline run for: 20241016-225122, dryrun=False
22:51:22,877 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph"
    ]
}
22:51:22,879 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph
22:51:22,879 graphrag.index.run.run INFO Running pipeline
22:51:22,879 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:51:22,879 graphrag.index.input.load_input INFO loading input from root_dir=input
22:51:22,879 graphrag.index.input.load_input INFO using file storage for input
22:51:22,881 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:51:22,881 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:51:22,882 graphrag.index.input.text INFO Found 1 files, loading 1
22:51:22,883 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:51:22,883 graphrag.index.run.run INFO Final # of rows loaded: 1
22:51:22,989 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:51:22,992 datashaper.workflow.workflow INFO executing verb orderby
22:51:22,993 datashaper.workflow.workflow INFO executing verb zip
22:51:22,995 datashaper.workflow.workflow INFO executing verb aggregate_override
22:51:22,998 datashaper.workflow.workflow INFO executing verb chunk
22:51:23,87 datashaper.workflow.workflow INFO executing verb select
22:51:23,90 datashaper.workflow.workflow INFO executing verb unroll
22:51:23,92 datashaper.workflow.workflow INFO executing verb rename
22:51:23,94 datashaper.workflow.workflow INFO executing verb genid
22:51:23,97 datashaper.workflow.workflow INFO executing verb unzip
22:51:23,99 datashaper.workflow.workflow INFO executing verb copy
22:51:23,102 datashaper.workflow.workflow INFO executing verb filter
22:51:23,110 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:51:23,229 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:51:23,229 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:51:23,239 datashaper.workflow.workflow INFO executing verb entity_extract
22:51:23,241 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:51:23,280 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:51:23,280 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:51:25,107 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:25,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.830261631999747. input_tokens=1844, output_tokens=260
22:51:26,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:26,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5506082490010158. input_tokens=34, output_tokens=380
22:51:26,685 datashaper.workflow.workflow INFO executing verb snapshot
22:51:26,693 datashaper.workflow.workflow INFO executing verb merge_graphs
22:51:26,696 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:51:26,818 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:51:26,818 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:51:26,828 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:51:26,833 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:51:26,948 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:51:26,949 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:51:26,961 datashaper.workflow.workflow INFO executing verb cluster_graph
22:51:26,971 datashaper.workflow.workflow INFO executing verb select
22:51:26,974 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:51:27,89 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:51:27,89 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:51:27,100 datashaper.workflow.workflow INFO executing verb unpack_graph
22:51:27,105 datashaper.workflow.workflow INFO executing verb rename
22:51:27,108 datashaper.workflow.workflow INFO executing verb select
22:51:27,113 datashaper.workflow.workflow INFO executing verb dedupe
22:51:27,117 datashaper.workflow.workflow INFO executing verb rename
22:51:27,121 datashaper.workflow.workflow INFO executing verb filter
22:51:27,132 datashaper.workflow.workflow INFO executing verb text_split
22:51:27,137 datashaper.workflow.workflow INFO executing verb drop
22:51:27,142 datashaper.workflow.workflow INFO executing verb merge
22:51:27,148 datashaper.workflow.workflow INFO executing verb text_embed
22:51:27,149 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:51:27,187 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:51:27,187 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:51:27,187 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:51:28,174 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:28,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9962595530014369. input_tokens=100, output_tokens=0
22:51:28,194 datashaper.workflow.workflow INFO executing verb drop
22:51:28,199 datashaper.workflow.workflow INFO executing verb filter
22:51:28,209 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:51:28,334 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:51:28,334 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:51:28,349 datashaper.workflow.workflow INFO executing verb layout_graph
22:51:28,352 graphrag.index.verbs.graph.layout.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/layout/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
22:51:28,354 graphrag.index.reporting.file_workflow_callbacks INFO Error in Umap details=None
22:51:28,361 datashaper.workflow.workflow INFO executing verb unpack_graph
22:51:28,368 datashaper.workflow.workflow INFO executing verb unpack_graph
22:51:28,374 datashaper.workflow.workflow INFO executing verb drop
22:51:28,374 datashaper.workflow.workflow ERROR Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:51:28,378 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis" details=None
22:51:28,378 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:51:28,380 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:51:28,397 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:51:57,222 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:51:57,224 graphrag.index.cli INFO Starting pipeline run for: 20241016-225157, dryrun=False
22:51:57,224 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph",
        "create_final_nodes"
    ]
}
22:51:57,226 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph,create_final_nodes
22:51:57,226 graphrag.index.run.run INFO Running pipeline
22:51:57,226 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:51:57,227 graphrag.index.input.load_input INFO loading input from root_dir=input
22:51:57,227 graphrag.index.input.load_input INFO using file storage for input
22:51:57,228 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:51:57,228 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:51:57,230 graphrag.index.input.text INFO Found 1 files, loading 1
22:51:57,231 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_communities', 'create_final_nodes', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:51:57,231 graphrag.index.run.run INFO Final # of rows loaded: 1
22:51:57,339 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:51:57,342 datashaper.workflow.workflow INFO executing verb orderby
22:51:57,344 datashaper.workflow.workflow INFO executing verb zip
22:51:57,345 datashaper.workflow.workflow INFO executing verb aggregate_override
22:51:57,349 datashaper.workflow.workflow INFO executing verb chunk
22:51:57,439 datashaper.workflow.workflow INFO executing verb select
22:51:57,441 datashaper.workflow.workflow INFO executing verb unroll
22:51:57,444 datashaper.workflow.workflow INFO executing verb rename
22:51:57,446 datashaper.workflow.workflow INFO executing verb genid
22:51:57,448 datashaper.workflow.workflow INFO executing verb unzip
22:51:57,451 datashaper.workflow.workflow INFO executing verb copy
22:51:57,453 datashaper.workflow.workflow INFO executing verb filter
22:51:57,461 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:51:57,579 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:51:57,580 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:51:57,590 datashaper.workflow.workflow INFO executing verb entity_extract
22:51:57,592 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:51:57,630 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:51:57,630 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:51:59,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:59,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.12043480399916. input_tokens=1844, output_tokens=260
22:52:01,382 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:01,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6314532669985056. input_tokens=34, output_tokens=380
22:52:01,393 datashaper.workflow.workflow INFO executing verb snapshot
22:52:01,400 datashaper.workflow.workflow INFO executing verb merge_graphs
22:52:01,404 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:52:01,522 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:52:01,522 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:52:01,532 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:52:01,538 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:52:01,651 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:52:01,652 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:52:01,662 datashaper.workflow.workflow INFO executing verb cluster_graph
22:52:01,672 datashaper.workflow.workflow INFO executing verb select
22:52:01,674 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:52:01,791 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:52:01,791 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:01,802 datashaper.workflow.workflow INFO executing verb unpack_graph
22:52:01,807 datashaper.workflow.workflow INFO executing verb rename
22:52:01,811 datashaper.workflow.workflow INFO executing verb select
22:52:01,815 datashaper.workflow.workflow INFO executing verb dedupe
22:52:01,820 datashaper.workflow.workflow INFO executing verb rename
22:52:01,824 datashaper.workflow.workflow INFO executing verb filter
22:52:01,835 datashaper.workflow.workflow INFO executing verb text_split
22:52:01,840 datashaper.workflow.workflow INFO executing verb drop
22:52:01,845 datashaper.workflow.workflow INFO executing verb merge
22:52:01,850 datashaper.workflow.workflow INFO executing verb text_embed
22:52:01,851 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:52:01,888 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:52:01,888 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:52:01,888 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:52:02,865 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:02,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9940315869989718. input_tokens=100, output_tokens=0
22:52:02,893 datashaper.workflow.workflow INFO executing verb drop
22:52:02,898 datashaper.workflow.workflow INFO executing verb filter
22:52:02,908 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:52:03,34 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:52:03,34 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:03,49 datashaper.workflow.workflow INFO executing verb create_final_communities
22:52:03,58 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:52:03,178 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:52:03,179 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:03,194 datashaper.workflow.workflow INFO executing verb layout_graph
22:52:03,195 graphrag.index.verbs.graph.layout.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/layout/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
22:52:03,198 graphrag.index.reporting.file_workflow_callbacks INFO Error in Umap details=None
22:52:03,205 datashaper.workflow.workflow INFO executing verb unpack_graph
22:52:03,211 datashaper.workflow.workflow INFO executing verb unpack_graph
22:52:03,218 datashaper.workflow.workflow INFO executing verb drop
22:52:03,218 datashaper.workflow.workflow ERROR Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:52:03,222 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis" details=None
22:52:03,222 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:52:03,224 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:52:03,243 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:53:53,377 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:53:53,378 graphrag.index.cli INFO Starting pipeline run for: 20241016-225353, dryrun=False
22:53:53,379 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph",
        "create_final_nodes"
    ]
}
22:53:53,381 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph,create_final_nodes
22:53:53,381 graphrag.index.run.run INFO Running pipeline
22:53:53,381 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:53:53,382 graphrag.index.input.load_input INFO loading input from root_dir=input
22:53:53,382 graphrag.index.input.load_input INFO using file storage for input
22:53:53,383 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:53:53,383 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:53:53,385 graphrag.index.input.text INFO Found 1 files, loading 1
22:53:53,385 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_communities', 'create_final_nodes', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:53:53,386 graphrag.index.run.run INFO Final # of rows loaded: 1
22:53:53,493 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:53:53,496 datashaper.workflow.workflow INFO executing verb orderby
22:53:53,498 datashaper.workflow.workflow INFO executing verb zip
22:53:53,499 datashaper.workflow.workflow INFO executing verb aggregate_override
22:53:53,502 datashaper.workflow.workflow INFO executing verb chunk
22:53:53,593 datashaper.workflow.workflow INFO executing verb select
22:53:53,595 datashaper.workflow.workflow INFO executing verb unroll
22:53:53,598 datashaper.workflow.workflow INFO executing verb rename
22:53:53,600 datashaper.workflow.workflow INFO executing verb genid
22:53:53,603 datashaper.workflow.workflow INFO executing verb unzip
22:53:53,605 datashaper.workflow.workflow INFO executing verb copy
22:53:53,607 datashaper.workflow.workflow INFO executing verb filter
22:53:53,615 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:53:53,733 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:53:53,733 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:53:53,743 datashaper.workflow.workflow INFO executing verb entity_extract
22:53:53,746 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:53:53,784 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:53:53,784 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:53:55,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:53:55,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.027302567001243. input_tokens=1844, output_tokens=259
22:53:57,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:53:57,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3859716040005878. input_tokens=34, output_tokens=327
22:53:57,217 datashaper.workflow.workflow INFO executing verb snapshot
22:53:57,223 datashaper.workflow.workflow INFO executing verb merge_graphs
22:53:57,226 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:53:57,342 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:53:57,342 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:53:57,352 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:53:57,359 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:53:57,480 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:53:57,480 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:53:57,490 datashaper.workflow.workflow INFO executing verb cluster_graph
22:53:57,500 datashaper.workflow.workflow INFO executing verb select
22:53:57,502 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:53:57,618 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:53:57,619 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:53:57,629 datashaper.workflow.workflow INFO executing verb unpack_graph
22:53:57,634 datashaper.workflow.workflow INFO executing verb rename
22:53:57,638 datashaper.workflow.workflow INFO executing verb select
22:53:57,642 datashaper.workflow.workflow INFO executing verb dedupe
22:53:57,647 datashaper.workflow.workflow INFO executing verb rename
22:53:57,651 datashaper.workflow.workflow INFO executing verb filter
22:53:57,661 datashaper.workflow.workflow INFO executing verb text_split
22:53:57,666 datashaper.workflow.workflow INFO executing verb drop
22:53:57,671 datashaper.workflow.workflow INFO executing verb merge
22:53:57,676 datashaper.workflow.workflow INFO executing verb text_embed
22:53:57,677 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:53:57,714 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:53:57,714 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:53:57,714 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:53:58,668 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:58,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9619126470006449. input_tokens=103, output_tokens=0
22:53:58,687 datashaper.workflow.workflow INFO executing verb drop
22:53:58,692 datashaper.workflow.workflow INFO executing verb filter
22:53:58,701 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:53:58,827 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:53:58,827 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:53:58,841 datashaper.workflow.workflow INFO executing verb create_final_communities
22:53:58,850 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:53:58,969 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:53:58,969 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:53:58,984 datashaper.workflow.workflow INFO executing verb layout_graph
22:53:58,988 graphrag.index.verbs.graph.layout.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/layout/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
22:53:58,990 graphrag.index.reporting.file_workflow_callbacks INFO Error in Umap details=None
22:53:58,998 datashaper.workflow.workflow INFO executing verb unpack_graph
22:53:59,4 datashaper.workflow.workflow INFO executing verb unpack_graph
22:53:59,10 datashaper.workflow.workflow INFO executing verb drop
22:53:59,11 datashaper.workflow.workflow ERROR Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:53:59,15 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis" details=None
22:53:59,15 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:53:59,17 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:53:59,35 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:54:20,272 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:54:20,273 graphrag.index.cli INFO Starting pipeline run for: 20241016-225420, dryrun=False
22:54:20,273 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
22:54:20,276 graphrag.index.create_pipeline_config INFO skipping workflows 
22:54:20,276 graphrag.index.run.run INFO Running pipeline
22:54:20,276 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:54:20,277 graphrag.index.input.load_input INFO loading input from root_dir=input
22:54:20,277 graphrag.index.input.load_input INFO using file storage for input
22:54:20,278 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:54:20,278 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:54:20,279 graphrag.index.input.text INFO Found 1 files, loading 1
22:54:20,280 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:54:20,280 graphrag.index.run.run INFO Final # of rows loaded: 1
22:54:20,388 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:54:20,391 datashaper.workflow.workflow INFO executing verb orderby
22:54:20,392 datashaper.workflow.workflow INFO executing verb zip
22:54:20,394 datashaper.workflow.workflow INFO executing verb aggregate_override
22:54:20,397 datashaper.workflow.workflow INFO executing verb chunk
22:54:20,487 datashaper.workflow.workflow INFO executing verb select
22:54:20,489 datashaper.workflow.workflow INFO executing verb unroll
22:54:20,492 datashaper.workflow.workflow INFO executing verb rename
22:54:20,494 datashaper.workflow.workflow INFO executing verb genid
22:54:20,496 datashaper.workflow.workflow INFO executing verb unzip
22:54:20,499 datashaper.workflow.workflow INFO executing verb copy
22:54:20,501 datashaper.workflow.workflow INFO executing verb filter
22:54:20,509 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:54:20,628 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:54:20,628 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:54:20,638 datashaper.workflow.workflow INFO executing verb entity_extract
22:54:20,640 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:54:20,678 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:54:20,678 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:54:22,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:54:22,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8757141910009523. input_tokens=1844, output_tokens=259
22:54:24,0 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:54:24,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4471232329997292. input_tokens=34, output_tokens=327
22:54:24,23 datashaper.workflow.workflow INFO executing verb snapshot
22:54:24,32 datashaper.workflow.workflow INFO executing verb merge_graphs
22:54:24,38 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:54:24,159 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:54:24,159 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:54:24,169 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:54:24,175 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:54:24,288 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:54:24,289 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:54:24,301 datashaper.workflow.workflow INFO executing verb cluster_graph
22:54:24,310 datashaper.workflow.workflow INFO executing verb select
22:54:24,313 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:54:24,428 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:54:24,428 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:54:24,439 datashaper.workflow.workflow INFO executing verb unpack_graph
22:54:24,444 datashaper.workflow.workflow INFO executing verb rename
22:54:24,447 datashaper.workflow.workflow INFO executing verb select
22:54:24,452 datashaper.workflow.workflow INFO executing verb dedupe
22:54:24,456 datashaper.workflow.workflow INFO executing verb rename
22:54:24,460 datashaper.workflow.workflow INFO executing verb filter
22:54:24,471 datashaper.workflow.workflow INFO executing verb text_split
22:54:24,475 datashaper.workflow.workflow INFO executing verb drop
22:54:24,480 datashaper.workflow.workflow INFO executing verb merge
22:54:24,485 datashaper.workflow.workflow INFO executing verb text_embed
22:54:24,486 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:54:24,523 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:54:24,523 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:54:24,523 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:54:24,964 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:24,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.45158978600011324. input_tokens=103, output_tokens=0
22:54:24,985 datashaper.workflow.workflow INFO executing verb drop
22:54:24,990 datashaper.workflow.workflow INFO executing verb filter
22:54:25,0 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:54:25,125 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:54:25,125 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:54:25,140 datashaper.workflow.workflow INFO executing verb layout_graph
22:54:25,162 datashaper.workflow.workflow INFO executing verb unpack_graph
22:54:25,168 datashaper.workflow.workflow INFO executing verb unpack_graph
22:54:25,174 datashaper.workflow.workflow INFO executing verb filter
22:54:25,187 datashaper.workflow.workflow INFO executing verb drop
22:54:25,193 datashaper.workflow.workflow INFO executing verb select
22:54:25,199 datashaper.workflow.workflow INFO executing verb rename
22:54:25,205 datashaper.workflow.workflow INFO executing verb convert
22:54:25,224 datashaper.workflow.workflow INFO executing verb join
22:54:25,233 datashaper.workflow.workflow INFO executing verb rename
22:54:25,237 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:54:25,368 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:54:25,370 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:54:25,385 datashaper.workflow.workflow INFO executing verb create_final_communities
22:54:25,394 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:54:25,516 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:54:25,516 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:54:25,522 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:54:25,538 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
22:54:25,546 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
22:54:25,551 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:54:25,677 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
22:54:25,678 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:54:25,682 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:54:25,684 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:54:25,700 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
22:54:25,713 datashaper.workflow.workflow INFO executing verb select
22:54:25,716 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:54:25,841 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:54:25,841 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:54:25,846 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:54:25,863 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
22:54:25,872 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
22:54:25,880 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
22:54:25,889 datashaper.workflow.workflow INFO executing verb prepare_community_reports
22:54:25,890 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 5
22:54:25,907 datashaper.workflow.workflow INFO executing verb create_community_reports
22:54:28,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:54:28,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.886080017000495. input_tokens=2094, output_tokens=571
22:54:28,818 datashaper.workflow.workflow INFO executing verb window
22:54:28,822 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:54:28,955 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
22:54:28,955 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:54:28,976 datashaper.workflow.workflow INFO executing verb unroll
22:54:28,985 datashaper.workflow.workflow INFO executing verb select
22:54:28,994 datashaper.workflow.workflow INFO executing verb rename
22:54:29,3 datashaper.workflow.workflow INFO executing verb join
22:54:29,14 datashaper.workflow.workflow INFO executing verb aggregate_override
22:54:29,23 datashaper.workflow.workflow INFO executing verb join
22:54:29,34 datashaper.workflow.workflow INFO executing verb rename
22:54:29,43 datashaper.workflow.workflow INFO executing verb convert
22:54:29,57 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
22:54:29,186 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
22:54:29,186 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
22:54:29,210 datashaper.workflow.workflow INFO executing verb rename
22:54:29,213 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:54:29,251 graphrag.index.cli INFO All workflows completed successfully.
22:55:31,808 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:55:31,810 graphrag.index.cli INFO Starting pipeline run for: 20241016-225531, dryrun=False
22:55:31,810 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph",
        "create_final_nodes"
    ]
}
22:55:31,812 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph,create_final_nodes
22:55:31,812 graphrag.index.run.run INFO Running pipeline
22:55:31,812 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:55:31,812 graphrag.index.input.load_input INFO loading input from root_dir=input
22:55:31,812 graphrag.index.input.load_input INFO using file storage for input
22:55:31,814 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:55:31,815 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:55:31,816 graphrag.index.input.text INFO Found 1 files, loading 1
22:55:31,817 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_communities', 'create_final_nodes', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:55:31,817 graphrag.index.run.run INFO Final # of rows loaded: 1
22:55:31,923 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:55:31,926 datashaper.workflow.workflow INFO executing verb orderby
22:55:31,927 datashaper.workflow.workflow INFO executing verb zip
22:55:31,929 datashaper.workflow.workflow INFO executing verb aggregate_override
22:55:31,932 datashaper.workflow.workflow INFO executing verb chunk
22:55:32,23 datashaper.workflow.workflow INFO executing verb select
22:55:32,25 datashaper.workflow.workflow INFO executing verb unroll
22:55:32,28 datashaper.workflow.workflow INFO executing verb rename
22:55:32,30 datashaper.workflow.workflow INFO executing verb genid
22:55:32,32 datashaper.workflow.workflow INFO executing verb unzip
22:55:32,35 datashaper.workflow.workflow INFO executing verb copy
22:55:32,37 datashaper.workflow.workflow INFO executing verb filter
22:55:32,46 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:55:32,166 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:55:32,166 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:55:32,176 datashaper.workflow.workflow INFO executing verb entity_extract
22:55:32,179 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:55:32,217 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:55:32,217 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:55:34,324 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:34,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1095035890011786. input_tokens=1844, output_tokens=259
22:55:35,755 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:35,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4287697540003137. input_tokens=34, output_tokens=327
22:55:35,780 datashaper.workflow.workflow INFO executing verb snapshot
22:55:35,787 datashaper.workflow.workflow INFO executing verb merge_graphs
22:55:35,790 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:55:35,909 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:55:35,910 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:55:35,919 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:55:35,925 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:55:36,40 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:55:36,40 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:55:36,50 datashaper.workflow.workflow INFO executing verb cluster_graph
22:55:36,59 datashaper.workflow.workflow INFO executing verb select
22:55:36,61 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:55:36,177 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:55:36,177 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:55:36,187 datashaper.workflow.workflow INFO executing verb unpack_graph
22:55:36,192 datashaper.workflow.workflow INFO executing verb rename
22:55:36,196 datashaper.workflow.workflow INFO executing verb select
22:55:36,200 datashaper.workflow.workflow INFO executing verb dedupe
22:55:36,204 datashaper.workflow.workflow INFO executing verb rename
22:55:36,209 datashaper.workflow.workflow INFO executing verb filter
22:55:36,219 datashaper.workflow.workflow INFO executing verb text_split
22:55:36,224 datashaper.workflow.workflow INFO executing verb drop
22:55:36,229 datashaper.workflow.workflow INFO executing verb merge
22:55:36,234 datashaper.workflow.workflow INFO executing verb text_embed
22:55:36,235 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:55:36,272 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:55:36,272 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:55:36,273 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:55:37,209 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:37,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9463354029994662. input_tokens=103, output_tokens=0
22:55:37,230 datashaper.workflow.workflow INFO executing verb drop
22:55:37,235 datashaper.workflow.workflow INFO executing verb filter
22:55:37,245 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:55:37,371 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:55:37,371 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:55:37,385 datashaper.workflow.workflow INFO executing verb create_final_communities
22:55:37,394 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:55:37,514 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:55:37,514 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:55:37,528 datashaper.workflow.workflow INFO executing verb layout_graph
22:55:37,531 graphrag.index.verbs.graph.layout.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/layout/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
22:55:37,533 graphrag.index.reporting.file_workflow_callbacks INFO Error in Umap details=None
22:55:37,541 datashaper.workflow.workflow INFO executing verb unpack_graph
22:55:37,547 datashaper.workflow.workflow INFO executing verb unpack_graph
22:55:37,553 datashaper.workflow.workflow INFO executing verb filter
22:55:37,566 datashaper.workflow.workflow INFO executing verb drop
22:55:37,566 datashaper.workflow.workflow ERROR Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:55:37,570 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "drop" in create_final_nodes: "['x', 'y'] not found in axis" details=None
22:55:37,571 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/engine/verbs/drop.py", line 20, in drop
    output = output.drop(columns=columns)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
22:55:37,572 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:55:37,591 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
22:56:43,93 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:56:43,97 graphrag.index.cli INFO Starting pipeline run for: 20241016-225643, dryrun=False
22:56:43,98 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph",
        "create_final_nodes"
    ]
}
22:56:43,101 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph,create_final_nodes
22:56:43,101 graphrag.index.run.run INFO Running pipeline
22:56:43,101 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:56:43,103 graphrag.index.input.load_input INFO loading input from root_dir=input
22:56:43,103 graphrag.index.input.load_input INFO using file storage for input
22:56:43,105 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
22:56:43,107 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
22:56:43,109 graphrag.index.input.text INFO Found 1 files, loading 1
22:56:43,110 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_communities', 'create_final_nodes', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
22:56:43,110 graphrag.index.run.run INFO Final # of rows loaded: 1
22:56:43,220 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:56:43,222 datashaper.workflow.workflow INFO executing verb orderby
22:56:43,224 datashaper.workflow.workflow INFO executing verb zip
22:56:43,226 datashaper.workflow.workflow INFO executing verb aggregate_override
22:56:43,229 datashaper.workflow.workflow INFO executing verb chunk
22:56:43,319 datashaper.workflow.workflow INFO executing verb select
22:56:43,321 datashaper.workflow.workflow INFO executing verb unroll
22:56:43,324 datashaper.workflow.workflow INFO executing verb rename
22:56:43,326 datashaper.workflow.workflow INFO executing verb genid
22:56:43,328 datashaper.workflow.workflow INFO executing verb unzip
22:56:43,331 datashaper.workflow.workflow INFO executing verb copy
22:56:43,333 datashaper.workflow.workflow INFO executing verb filter
22:56:43,341 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:56:43,460 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
22:56:43,460 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:56:43,470 datashaper.workflow.workflow INFO executing verb entity_extract
22:56:43,473 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:56:43,511 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
22:56:43,511 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:56:45,873 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:45,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3657331769991288. input_tokens=375, output_tokens=432
22:56:48,585 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:48,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.710261870000977. input_tokens=34, output_tokens=660
22:56:48,608 datashaper.workflow.workflow INFO executing verb snapshot
22:56:48,614 datashaper.workflow.workflow INFO executing verb merge_graphs
22:56:48,617 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
22:56:48,732 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
22:56:48,732 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
22:56:48,742 datashaper.workflow.workflow INFO executing verb summarize_descriptions
22:56:48,748 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
22:56:48,861 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
22:56:48,861 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
22:56:48,871 datashaper.workflow.workflow INFO executing verb cluster_graph
22:56:48,871 graphrag.index.verbs.graph.clustering.cluster_graph WARNING Graph has no nodes
22:56:48,873 datashaper.workflow.workflow ERROR Error executing verb "cluster_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/clustering/cluster_graph.py", line 106, in cluster_graph
    output_df[[level_to, to]] = pd.DataFrame(
    ~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
22:56:48,876 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "cluster_graph" in create_base_entity_graph: Columns must be same length as key details=None
22:56:48,877 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 227, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 91, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 410, in _execute_verb
    result = node.verb.func(**verb_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/verbs/graph/clustering/cluster_graph.py", line 106, in cluster_graph
    output_df[[level_to, to]] = pd.DataFrame(
    ~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
22:56:48,878 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
22:56:48,888 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
11:22:40,828 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
11:22:40,830 graphrag.index.cli INFO Starting pipeline run for: 20241017-112240, dryrun=False
11:22:40,830 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_entity_graph",
        "create_final_nodes"
    ]
}
11:22:40,834 graphrag.index.create_pipeline_config INFO skipping workflows create_base_entity_graph,create_final_nodes
11:22:40,834 graphrag.index.run.run INFO Running pipeline
11:22:40,834 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
11:22:40,835 graphrag.index.input.load_input INFO loading input from root_dir=input
11:22:40,835 graphrag.index.input.load_input INFO using file storage for input
11:22:40,837 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
11:22:40,837 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
11:22:40,839 graphrag.index.input.text INFO Found 1 files, loading 1
11:22:40,843 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_communities', 'create_final_nodes', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
11:22:40,843 graphrag.index.run.run INFO Final # of rows loaded: 1
11:22:40,953 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
11:22:40,956 datashaper.workflow.workflow INFO executing verb create_base_text_units
11:22:42,111 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
11:22:42,282 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
11:22:42,283 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
11:22:42,319 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
11:22:42,321 root ERROR parallel transformation error
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 39, in execute
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 128, in run_strategy
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/strategies/graph_intelligence.py", line 37, in run_graph_intelligence
    llm = load_llm("entity_extraction", llm_type, callbacks, cache, llm_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/llm/load_llm.py", line 61, in load_llm
    raise ValueError(msg)
ValueError: Unknown LLM type None
11:22:42,334 graphrag.callbacks.file_workflow_callbacks INFO parallel transformation error details=None
11:22:42,334 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete!
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
11:22:42,340 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete! details=None
11:22:42,341 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
11:22:42,344 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
11:22:42,355 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
11:23:11,473 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
11:23:11,474 graphrag.index.cli INFO Starting pipeline run for: 20241017-112311, dryrun=False
11:23:11,474 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
11:23:11,476 graphrag.index.create_pipeline_config INFO skipping workflows 
11:23:11,476 graphrag.index.run.run INFO Running pipeline
11:23:11,477 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
11:23:11,479 graphrag.index.input.load_input INFO loading input from root_dir=input
11:23:11,479 graphrag.index.input.load_input INFO using file storage for input
11:23:11,480 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
11:23:11,481 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
11:23:11,482 graphrag.index.input.text INFO Found 1 files, loading 1
11:23:11,482 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
11:23:11,483 graphrag.index.run.run INFO Final # of rows loaded: 1
11:23:11,592 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
11:23:11,594 datashaper.workflow.workflow INFO executing verb create_base_text_units
11:23:11,985 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
11:23:12,110 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
11:23:12,110 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
11:23:12,118 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
11:23:12,119 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
11:23:12,158 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
11:23:12,158 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
11:23:14,93 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
11:23:14,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.938473387999693. input_tokens=1844, output_tokens=256
11:23:16,273 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
11:23:16,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.177519602999382. input_tokens=34, output_tokens=382
11:23:16,868 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
11:23:16,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5773403359999065. input_tokens=156, output_tokens=35
11:23:16,888 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
11:23:17,10 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
11:23:17,10 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
11:23:17,17 datashaper.workflow.workflow INFO executing verb create_final_entities
11:23:17,22 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
11:23:17,59 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
11:23:17,59 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
11:23:17,59 graphrag.index.operations.embed_text.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
11:23:18,290 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
11:23:18,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2407778599954327. input_tokens=106, output_tokens=0
11:23:18,307 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
11:23:18,444 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
11:23:18,444 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
11:23:18,451 datashaper.workflow.workflow INFO executing verb create_final_nodes
11:23:18,462 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
11:23:18,588 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
11:23:18,588 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
11:23:18,596 datashaper.workflow.workflow INFO executing verb create_final_communities
11:23:18,611 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
11:23:18,731 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
11:23:18,731 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
11:23:18,736 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
11:23:18,743 datashaper.workflow.workflow INFO executing verb create_final_relationships
11:23:18,749 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
11:23:18,873 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
11:23:18,873 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
11:23:18,878 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
11:23:18,881 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
11:23:18,888 datashaper.workflow.workflow INFO executing verb create_final_text_units
11:23:18,899 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
11:23:19,22 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
11:23:19,23 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
11:23:19,28 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
11:23:19,36 datashaper.workflow.workflow INFO executing verb create_final_community_reports
11:23:19,39 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 5
11:23:21,538 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
11:23:21,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.478813147994515. input_tokens=2139, output_tokens=414
11:23:21,549 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
11:23:21,682 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
11:23:21,685 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
11:23:21,694 datashaper.workflow.workflow INFO executing verb create_final_documents
11:23:21,699 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
11:23:21,715 graphrag.index.cli INFO All workflows completed successfully.
12:45:46,662 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
12:45:46,663 graphrag.index.cli INFO Starting pipeline run for: 20241017-124546, dryrun=False
12:45:46,663 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_text_units"
    ]
}
12:45:46,665 graphrag.index.create_pipeline_config INFO skipping workflows create_base_text_units
12:45:46,665 graphrag.index.run.run INFO Running pipeline
12:45:46,665 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
12:45:46,666 graphrag.index.input.load_input INFO loading input from root_dir=input
12:45:46,666 graphrag.index.input.load_input INFO using file storage for input
12:45:46,667 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
12:45:46,668 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
12:45:46,669 graphrag.index.input.text INFO Found 1 files, loading 1
12:45:46,669 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
12:45:46,670 graphrag.index.run.run INFO Final # of rows loaded: 1
12:45:46,778 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
12:45:46,780 datashaper.workflow.workflow INFO executing verb create_base_text_units
12:45:47,199 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
12:45:47,325 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
12:45:47,325 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
12:45:47,333 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
12:45:47,334 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
12:45:47,372 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
12:45:47,372 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
12:45:49,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
12:45:49,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1078588250020402. input_tokens=1844, output_tokens=256
12:45:50,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
12:45:50,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4769757209942327. input_tokens=34, output_tokens=324
12:45:51,504 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
12:45:51,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5393275239985087. input_tokens=156, output_tokens=35
12:45:51,524 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
12:45:51,645 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
12:45:51,646 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
12:45:51,653 datashaper.workflow.workflow INFO executing verb create_final_entities
12:45:51,656 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
12:45:51,693 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
12:45:51,693 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
12:45:51,693 graphrag.index.operations.embed_text.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
12:45:52,659 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
12:45:52,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9757783500026562. input_tokens=103, output_tokens=0
12:45:52,676 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
12:45:52,807 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
12:45:52,807 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
12:45:52,815 datashaper.workflow.workflow INFO executing verb create_final_nodes
12:45:52,823 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
12:45:52,944 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
12:45:52,944 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
12:45:52,953 datashaper.workflow.workflow INFO executing verb create_final_communities
12:45:52,963 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
12:45:53,82 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
12:45:53,82 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
12:45:53,87 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
12:45:53,94 datashaper.workflow.workflow INFO executing verb create_final_relationships
12:45:53,99 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
12:45:53,222 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
12:45:53,222 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
12:45:53,226 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
12:45:53,228 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
12:45:53,236 datashaper.workflow.workflow INFO executing verb create_final_text_units
12:45:53,245 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
12:45:53,366 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
12:45:53,367 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
12:45:53,371 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
12:45:53,379 datashaper.workflow.workflow INFO executing verb create_final_community_reports
12:45:53,382 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 5
12:45:56,105 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
12:45:56,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.713553501998831. input_tokens=2102, output_tokens=511
12:45:56,117 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
12:45:56,248 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
12:45:56,249 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
12:45:56,259 datashaper.workflow.workflow INFO executing verb create_final_documents
12:45:56,266 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
12:45:56,283 graphrag.index.cli INFO All workflows completed successfully.
12:46:36,94 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
12:46:36,95 graphrag.index.cli INFO Starting pipeline run for: 20241017-124636, dryrun=False
12:46:36,95 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_text_units",
        "create_base_entity_graph"
    ]
}
12:46:36,97 graphrag.index.create_pipeline_config INFO skipping workflows create_base_text_units,create_base_entity_graph
12:46:36,97 graphrag.index.run.run INFO Running pipeline
12:46:36,98 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
12:46:36,99 graphrag.index.input.load_input INFO loading input from root_dir=input
12:46:36,99 graphrag.index.input.load_input INFO using file storage for input
12:46:36,100 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
12:46:36,101 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
12:46:36,102 graphrag.index.input.text INFO Found 1 files, loading 1
12:46:36,102 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
12:46:36,102 graphrag.index.run.run INFO Final # of rows loaded: 1
12:46:36,211 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
12:46:36,214 datashaper.workflow.workflow INFO executing verb create_base_text_units
12:46:36,622 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
12:46:36,744 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
12:46:36,745 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
12:46:36,752 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
12:46:36,753 root ERROR parallel transformation error
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 39, in execute
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 128, in run_strategy
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/strategies/graph_intelligence.py", line 37, in run_graph_intelligence
    llm = load_llm("entity_extraction", llm_type, callbacks, cache, llm_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/llm/load_llm.py", line 61, in load_llm
    raise ValueError(msg)
ValueError: Unknown LLM type None
12:46:36,755 graphrag.callbacks.file_workflow_callbacks INFO parallel transformation error details=None
12:46:36,755 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete!
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
12:46:36,757 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete! details=None
12:46:36,757 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
12:46:36,758 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
12:46:36,763 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
12:47:08,192 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
12:47:08,193 graphrag.index.cli INFO Starting pipeline run for: 20241017-124708, dryrun=False
12:47:08,193 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        "create_base_text_units",
        "create_base_entity_graph"
    ]
}
12:47:08,196 graphrag.index.create_pipeline_config INFO skipping workflows create_base_text_units,create_base_entity_graph
12:47:08,196 graphrag.index.run.run INFO Running pipeline
12:47:08,197 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
12:47:08,198 graphrag.index.input.load_input INFO loading input from root_dir=input
12:47:08,198 graphrag.index.input.load_input INFO using file storage for input
12:47:08,200 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
12:47:08,201 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
12:47:08,203 graphrag.index.input.text INFO Found 1 files, loading 1
12:47:08,204 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
12:47:08,205 graphrag.index.run.run INFO Final # of rows loaded: 1
12:47:08,317 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
12:47:08,320 datashaper.workflow.workflow INFO executing verb create_base_text_units
12:47:08,622 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
12:47:08,745 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
12:47:08,745 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
12:47:08,752 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
12:47:08,753 root ERROR parallel transformation error
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 39, in execute
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 128, in run_strategy
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/strategies/graph_intelligence.py", line 37, in run_graph_intelligence
    llm = load_llm("entity_extraction", llm_type, callbacks, cache, llm_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/llm/load_llm.py", line 61, in load_llm
    raise ValueError(msg)
ValueError: Unknown LLM type None
12:47:08,755 graphrag.callbacks.file_workflow_callbacks INFO parallel transformation error details=None
12:47:08,756 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete!
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
12:47:08,757 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete! details=None
12:47:08,758 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 29, in derive_from_rows
    return await derive_from_rows_asyncio(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio.py", line 41, in derive_from_rows_asyncio
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
12:47:08,759 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
12:47:08,764 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
18:04:22,369 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:04:22,371 graphrag.index.cli INFO Starting pipeline run for: 20241017-180422, dryrun=False
18:04:22,371 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:04:22,374 graphrag.index.create_pipeline_config INFO skipping workflows 
18:04:22,374 graphrag.index.run.run INFO Running pipeline
18:04:22,375 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:04:22,376 graphrag.index.input.load_input INFO loading input from root_dir=input
18:04:22,376 graphrag.index.input.load_input INFO using file storage for input
18:04:22,379 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:04:22,380 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:04:22,382 graphrag.index.input.text INFO Found 1 files, loading 1
18:04:22,384 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:04:22,384 graphrag.index.run.run INFO Final # of rows loaded: 1
18:04:22,503 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:04:22,506 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:04:22,933 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:04:23,57 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:04:23,57 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:04:23,65 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:04:23,69 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:04:23,107 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:04:23,107 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:04:25,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:04:25,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9213007140060654. input_tokens=1844, output_tokens=259
18:04:26,540 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:04:26,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5121802590001607. input_tokens=34, output_tokens=327
18:04:27,117 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:04:27,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5687277880060719. input_tokens=159, output_tokens=37
18:04:27,141 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:04:27,264 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:04:27,264 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:04:27,271 datashaper.workflow.workflow INFO executing verb create_final_entities
18:04:27,275 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:04:27,312 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:04:27,312 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:04:27,312 graphrag.index.operations.embed_text.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:04:28,295 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:04:28,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9923916739935521. input_tokens=103, output_tokens=0
18:04:28,311 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:04:28,440 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:04:28,442 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:04:28,448 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:04:28,456 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:04:28,577 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:04:28,578 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:04:28,586 datashaper.workflow.workflow INFO executing verb create_final_communities
18:04:28,597 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:04:28,717 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:04:28,717 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:04:28,722 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:04:28,730 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:04:28,735 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:04:28,867 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
18:04:28,867 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:04:28,872 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:04:28,874 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:04:28,881 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:04:28,891 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:04:29,13 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:04:29,14 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:04:29,17 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:04:29,25 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:04:29,28 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 5
18:04:31,459 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:04:31,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4215573649998987. input_tokens=2104, output_tokens=443
18:04:31,476 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:04:31,621 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:04:31,622 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:04:31,632 datashaper.workflow.workflow INFO executing verb create_final_documents
18:04:31,639 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:04:31,656 graphrag.index.cli INFO All workflows completed successfully.
18:23:27,615 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:23:27,617 graphrag.index.cli INFO Starting pipeline run for: 20241017-182327, dryrun=False
18:23:27,617 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "Things"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:23:27,619 graphrag.index.create_pipeline_config INFO skipping workflows 
18:23:27,619 graphrag.index.run.run INFO Running pipeline
18:23:27,620 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:23:27,621 graphrag.index.input.load_input INFO loading input from root_dir=input
18:23:27,621 graphrag.index.input.load_input INFO using file storage for input
18:23:27,623 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:23:27,624 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:23:27,625 graphrag.index.input.text INFO Found 1 files, loading 1
18:23:27,625 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:23:27,625 graphrag.index.run.run INFO Final # of rows loaded: 1
18:23:27,735 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:23:27,738 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:23:28,187 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:23:28,311 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:23:28,311 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:23:28,319 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:23:28,321 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:23:28,359 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:23:28,359 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:23:32,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:23:32,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.286854737001704. input_tokens=841, output_tokens=924
18:23:34,53 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:23:34,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4064208919880912. input_tokens=34, output_tokens=260
18:23:34,64 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
18:23:34,68 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
18:23:34,84 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
18:23:34,84 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
18:23:34,88 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
18:23:34,97 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
18:23:54,520 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:23:54,521 graphrag.index.cli INFO Starting pipeline run for: 20241017-182354, dryrun=False
18:23:54,522 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:23:54,524 graphrag.index.create_pipeline_config INFO skipping workflows 
18:23:54,524 graphrag.index.run.run INFO Running pipeline
18:23:54,524 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:23:54,524 graphrag.index.input.load_input INFO loading input from root_dir=input
18:23:54,524 graphrag.index.input.load_input INFO using file storage for input
18:23:54,525 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:23:54,526 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:23:54,527 graphrag.index.input.text INFO Found 1 files, loading 1
18:23:54,527 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:23:54,528 graphrag.index.run.run INFO Final # of rows loaded: 1
18:23:54,637 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:23:54,640 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:23:54,925 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:23:55,49 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:23:55,50 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:23:55,57 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:23:55,58 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:23:55,96 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:23:55,96 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:23:57,3 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:23:57,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.910170417992049. input_tokens=843, output_tokens=464
18:23:58,635 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:23:58,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6292598629952408. input_tokens=34, output_tokens=336
18:23:58,647 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
18:23:58,651 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
18:23:58,655 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
18:23:58,655 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
18:23:58,658 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
18:23:58,664 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
18:26:54,7 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:26:54,9 graphrag.index.cli INFO Starting pipeline run for: 20241017-182653, dryrun=False
18:26:54,9 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:26:54,11 graphrag.index.create_pipeline_config INFO skipping workflows 
18:26:54,11 graphrag.index.run.run INFO Running pipeline
18:26:54,11 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:26:54,12 graphrag.index.input.load_input INFO loading input from root_dir=input
18:26:54,13 graphrag.index.input.load_input INFO using file storage for input
18:26:54,14 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:26:54,15 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:26:54,16 graphrag.index.input.text INFO Found 1 files, loading 1
18:26:54,16 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:26:54,16 graphrag.index.run.run INFO Final # of rows loaded: 1
18:26:54,125 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:26:54,128 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:26:54,523 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:26:54,647 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:26:54,648 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:26:54,655 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:26:54,657 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:26:54,695 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:26:54,695 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:26:56,924 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:26:56,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2329495229932945. input_tokens=2228, output_tokens=385
18:26:58,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:26:58,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7525988929992309. input_tokens=34, output_tokens=378
18:26:58,703 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:26:58,824 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:26:58,824 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:26:58,831 datashaper.workflow.workflow INFO executing verb create_final_entities
18:26:58,834 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:26:58,871 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:26:58,872 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:26:58,872 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:26:59,950 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:26:59,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0929341180017218. input_tokens=143, output_tokens=0
18:26:59,971 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:27:00,100 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:27:00,102 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:00,109 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:27:00,116 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:27:00,241 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:27:00,241 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:00,249 datashaper.workflow.workflow INFO executing verb create_final_communities
18:27:00,260 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:27:00,380 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:27:00,381 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:00,385 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:27:00,393 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:27:00,398 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:27:00,521 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
18:27:00,522 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:27:00,527 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:27:00,529 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:27:00,537 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:27:00,546 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:27:00,673 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:27:00,673 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:27:00,677 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:27:00,684 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:27:00,687 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
18:27:02,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:02,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.124452011004905. input_tokens=2022, output_tokens=293
18:27:05,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:05,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6274834469950292. input_tokens=2142, output_tokens=469
18:27:08,18 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:08,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5482280689902836. input_tokens=2194, output_tokens=450
18:27:08,29 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:27:08,174 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:27:08,177 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:27:08,186 datashaper.workflow.workflow INFO executing verb create_final_documents
18:27:08,193 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:27:08,210 graphrag.index.cli INFO All workflows completed successfully.
18:27:26,175 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:27:26,176 graphrag.index.cli INFO Starting pipeline run for: 20241017-182726, dryrun=False
18:27:26,176 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:27:26,179 graphrag.index.create_pipeline_config INFO skipping workflows 
18:27:26,179 graphrag.index.run.run INFO Running pipeline
18:27:26,179 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:27:26,179 graphrag.index.input.load_input INFO loading input from root_dir=input
18:27:26,179 graphrag.index.input.load_input INFO using file storage for input
18:27:26,180 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:27:26,181 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:27:26,182 graphrag.index.input.text INFO Found 1 files, loading 1
18:27:26,182 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:27:26,182 graphrag.index.run.run INFO Final # of rows loaded: 1
18:27:26,291 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:27:26,294 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:27:26,577 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:27:26,700 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:27:26,700 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:27:26,708 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:27:26,709 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:27:26,747 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:27:26,747 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:27:28,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:28,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1521737610019045. input_tokens=447, output_tokens=299
18:27:30,960 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:30,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0620100920059485. input_tokens=34, output_tokens=479
18:27:31,535 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:31,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5636433809995651. input_tokens=167, output_tokens=39
18:27:32,59 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:32,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5073555809940444. input_tokens=152, output_tokens=14
18:27:32,81 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:27:32,200 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:27:32,201 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:32,208 datashaper.workflow.workflow INFO executing verb create_final_entities
18:27:32,211 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:27:32,248 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:27:32,248 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:27:32,248 graphrag.index.operations.embed_text.strategies.openai INFO embedding 5 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:27:32,717 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:27:32,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.482058027992025. input_tokens=140, output_tokens=0
18:27:32,739 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:27:32,866 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:27:32,867 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:32,874 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:27:32,881 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:27:33,8 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:27:33,8 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:33,16 datashaper.workflow.workflow INFO executing verb create_final_communities
18:27:33,27 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:27:33,149 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:27:33,149 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:27:33,161 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:27:33,169 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:27:33,174 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:27:33,301 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
18:27:33,304 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:27:33,307 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:27:33,309 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:27:33,316 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:27:33,326 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:27:33,451 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:27:33,451 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:27:33,455 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:27:33,463 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:27:33,467 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 5
18:27:35,901 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:35,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.423947668998153. input_tokens=2136, output_tokens=424
18:27:37,940 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:27:43,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.032702518001315. input_tokens=2099, output_tokens=350
18:27:43,961 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:27:44,94 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:27:44,95 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:27:44,105 datashaper.workflow.workflow INFO executing verb create_final_documents
18:27:44,111 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:27:44,128 graphrag.index.cli INFO All workflows completed successfully.
18:28:23,284 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:28:23,285 graphrag.index.cli INFO Starting pipeline run for: 20241017-182823, dryrun=False
18:28:23,286 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:28:23,288 graphrag.index.create_pipeline_config INFO skipping workflows 
18:28:23,288 graphrag.index.run.run INFO Running pipeline
18:28:23,288 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:28:23,290 graphrag.index.input.load_input INFO loading input from root_dir=input
18:28:23,290 graphrag.index.input.load_input INFO using file storage for input
18:28:23,291 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:28:23,292 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:28:23,293 graphrag.index.input.text INFO Found 1 files, loading 1
18:28:23,294 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:28:23,294 graphrag.index.run.run INFO Final # of rows loaded: 1
18:28:23,403 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:28:23,406 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:28:23,807 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:28:23,933 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:28:23,933 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:28:23,941 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:28:23,942 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:28:23,981 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:28:23,981 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:28:25,815 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:28:25,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8384307939995779. input_tokens=1829, output_tokens=194
18:28:28,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:28:28,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0476437640027143. input_tokens=34, output_tokens=731
18:28:28,896 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:28:29,16 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:28:29,17 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:28:29,24 datashaper.workflow.workflow INFO executing verb create_final_entities
18:28:29,28 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:28:29,64 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:28:29,64 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:28:29,65 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:28:30,198 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:28:30,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1475435720058158. input_tokens=177, output_tokens=0
18:28:30,219 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:28:30,352 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:28:30,352 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:28:30,359 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:28:30,366 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:28:30,489 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:28:30,490 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:28:30,498 datashaper.workflow.workflow INFO executing verb create_final_communities
18:28:30,509 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:28:30,631 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:28:30,632 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:28:30,635 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:28:30,643 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:28:30,649 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:28:30,774 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:28:30,774 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:28:30,779 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:28:30,781 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:28:30,788 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:28:30,798 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:28:30,927 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:28:30,928 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:28:30,933 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:28:30,945 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:28:30,948 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 9
18:28:33,970 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:28:33,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.010031758007244. input_tokens=2271, output_tokens=570
18:28:36,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:28:36,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0947032569965813. input_tokens=2111, output_tokens=375
18:28:36,83 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:28:36,225 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:28:36,226 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:28:36,236 datashaper.workflow.workflow INFO executing verb create_final_documents
18:28:36,242 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:28:36,259 graphrag.index.cli INFO All workflows completed successfully.
18:31:19,298 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:31:19,299 graphrag.index.cli INFO Starting pipeline run for: 20241017-183119, dryrun=False
18:31:19,299 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:31:19,309 graphrag.index.create_pipeline_config INFO skipping workflows 
18:31:19,309 graphrag.index.run.run INFO Running pipeline
18:31:19,310 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:31:19,312 graphrag.index.input.load_input INFO loading input from root_dir=input
18:31:19,312 graphrag.index.input.load_input INFO using file storage for input
18:31:19,314 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:31:19,315 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:31:19,316 graphrag.index.input.text INFO Found 1 files, loading 1
18:31:19,317 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:31:19,317 graphrag.index.run.run INFO Final # of rows loaded: 1
18:31:19,426 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:31:19,429 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:31:19,798 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:31:19,921 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:31:19,921 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:31:19,928 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:31:19,929 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:31:19,967 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:31:19,967 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:31:21,53 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:31:21,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0900775129994145. input_tokens=1802, output_tokens=194
18:31:23,130 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:31:23,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.0736092349980026. input_tokens=34, output_tokens=471
18:31:23,159 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:31:23,291 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:31:23,292 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:31:23,299 datashaper.workflow.workflow INFO executing verb create_final_entities
18:31:23,302 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:31:23,340 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:31:23,340 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:31:23,342 graphrag.index.operations.embed_text.strategies.openai INFO embedding 7 inputs via 7 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:31:24,388 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:31:24,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0677563039935194. input_tokens=116, output_tokens=0
18:31:24,416 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:31:24,543 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:31:24,544 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:31:24,550 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:31:24,557 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:31:24,680 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:31:24,680 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:31:24,688 datashaper.workflow.workflow INFO executing verb create_final_communities
18:31:24,699 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:31:24,820 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:31:24,821 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:31:24,826 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:31:24,832 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:31:24,838 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:31:24,963 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
18:31:24,963 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:31:24,968 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:31:24,970 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:31:24,977 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:31:24,987 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:31:25,110 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:31:25,110 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:31:25,115 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:31:25,123 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:31:25,126 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 7
18:31:28,9 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:31:28,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8720907289971365. input_tokens=2150, output_tokens=411
18:31:30,266 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:31:30,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2482230870082276. input_tokens=2042, output_tokens=370
18:31:30,279 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:31:30,431 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:31:30,431 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:31:30,442 datashaper.workflow.workflow INFO executing verb create_final_documents
18:31:30,448 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:31:30,477 graphrag.index.cli INFO All workflows completed successfully.
18:32:27,276 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:32:27,277 graphrag.index.cli INFO Starting pipeline run for: 20241017-183227, dryrun=False
18:32:27,278 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:32:27,280 graphrag.index.create_pipeline_config INFO skipping workflows 
18:32:27,280 graphrag.index.run.run INFO Running pipeline
18:32:27,281 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:32:27,283 graphrag.index.input.load_input INFO loading input from root_dir=input
18:32:27,283 graphrag.index.input.load_input INFO using file storage for input
18:32:27,283 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:32:27,284 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:32:27,285 graphrag.index.input.text INFO Found 1 files, loading 1
18:32:27,286 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:32:27,286 graphrag.index.run.run INFO Final # of rows loaded: 1
18:32:27,398 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:32:27,400 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:32:27,800 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:32:27,925 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:32:27,925 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:32:27,933 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:32:27,934 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:32:27,972 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:32:27,972 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:32:29,490 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:32:29,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5260610319965053. input_tokens=1816, output_tokens=181
18:32:30,221 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:32:30,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.7231033729913179. input_tokens=34, output_tokens=72
18:32:30,244 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:32:30,367 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:32:30,369 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:32:30,375 datashaper.workflow.workflow INFO executing verb create_final_entities
18:32:30,378 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:32:30,416 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:32:30,416 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:32:30,416 graphrag.index.operations.embed_text.strategies.openai INFO embedding 4 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:32:30,816 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:32:30,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4074162430042634. input_tokens=61, output_tokens=0
18:32:30,831 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:32:30,958 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:32:30,958 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:32:30,966 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:32:30,976 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:32:31,98 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:32:31,99 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:32:31,107 datashaper.workflow.workflow INFO executing verb create_final_communities
18:32:31,117 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:32:31,237 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:32:31,237 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:32:31,241 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:32:31,249 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:32:31,254 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:32:31,377 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:32:31,380 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:32:31,383 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:32:31,385 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:32:31,392 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:32:31,401 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:32:31,528 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:32:31,528 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:32:31,532 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:32:31,540 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:32:31,543 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 4
18:32:33,464 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:32:33,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.91064680198906. input_tokens=2013, output_tokens=297
18:32:35,668 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:32:35,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.188945762012736. input_tokens=2014, output_tokens=396
18:32:35,679 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:32:35,817 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:32:35,817 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:32:35,828 datashaper.workflow.workflow INFO executing verb create_final_documents
18:32:35,834 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:32:35,851 graphrag.index.cli INFO All workflows completed successfully.
18:34:12,498 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:34:12,499 graphrag.index.cli INFO Starting pipeline run for: 20241017-183412, dryrun=False
18:34:12,499 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:34:12,502 graphrag.index.create_pipeline_config INFO skipping workflows 
18:34:12,502 graphrag.index.run.run INFO Running pipeline
18:34:12,502 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:34:12,503 graphrag.index.input.load_input INFO loading input from root_dir=input
18:34:12,503 graphrag.index.input.load_input INFO using file storage for input
18:34:12,505 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:34:12,506 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:34:12,507 graphrag.index.input.text INFO Found 1 files, loading 1
18:34:12,507 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:34:12,507 graphrag.index.run.run INFO Final # of rows loaded: 1
18:34:12,617 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:34:12,620 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:34:13,44 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:34:13,169 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:34:13,170 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:34:13,178 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:34:13,179 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:34:13,217 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:34:13,217 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:34:14,629 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:34:14,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4194228280102834. input_tokens=1804, output_tokens=194
18:34:15,789 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:34:15,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1537926029996015. input_tokens=34, output_tokens=256
18:34:15,812 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:34:15,933 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:34:15,934 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:34:15,940 datashaper.workflow.workflow INFO executing verb create_final_entities
18:34:15,944 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:34:15,980 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:34:15,980 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:34:15,981 graphrag.index.operations.embed_text.strategies.openai INFO embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:34:16,949 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:34:16,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9774056629976258. input_tokens=78, output_tokens=0
18:34:16,965 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:34:17,93 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:34:17,94 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:34:17,101 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:34:17,109 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:34:17,234 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:34:17,234 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:34:17,242 datashaper.workflow.workflow INFO executing verb create_final_communities
18:34:17,253 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:34:17,375 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:34:17,375 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:34:17,380 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:34:17,387 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:34:17,392 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:34:17,519 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
18:34:17,520 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:34:17,524 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:34:17,526 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:34:17,532 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:34:17,542 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:34:17,665 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:34:17,666 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:34:17,670 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:34:17,678 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:34:17,681 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 4
18:34:19,350 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:34:19,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6584382959990762. input_tokens=2022, output_tokens=289
18:34:21,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:34:21,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1199223949952284. input_tokens=2014, output_tokens=365
18:34:21,487 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:34:21,621 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:34:21,621 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:34:21,632 datashaper.workflow.workflow INFO executing verb create_final_documents
18:34:21,638 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:34:21,655 graphrag.index.cli INFO All workflows completed successfully.
18:34:58,478 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:34:58,479 graphrag.index.cli INFO Starting pipeline run for: 20241017-183458, dryrun=False
18:34:58,480 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:34:58,482 graphrag.index.create_pipeline_config INFO skipping workflows 
18:34:58,482 graphrag.index.run.run INFO Running pipeline
18:34:58,482 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:34:58,483 graphrag.index.input.load_input INFO loading input from root_dir=input
18:34:58,483 graphrag.index.input.load_input INFO using file storage for input
18:34:58,484 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:34:58,485 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:34:58,486 graphrag.index.input.text INFO Found 1 files, loading 1
18:34:58,486 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:34:58,486 graphrag.index.run.run INFO Final # of rows loaded: 1
18:34:58,595 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:34:58,598 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:34:58,892 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:34:59,16 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:34:59,16 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:34:59,24 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:34:59,25 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:34:59,64 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:34:59,64 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:35:00,720 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:35:00,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6607621580042178. input_tokens=1803, output_tokens=194
18:35:01,898 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:35:01,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1739910120086279. input_tokens=34, output_tokens=256
18:35:01,922 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:35:02,45 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:35:02,45 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:35:02,52 datashaper.workflow.workflow INFO executing verb create_final_entities
18:35:02,55 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:35:02,93 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:35:02,93 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:35:02,93 graphrag.index.operations.embed_text.strategies.openai INFO embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:35:02,631 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:35:02,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.546325536997756. input_tokens=78, output_tokens=0
18:35:02,646 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:35:02,775 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:35:02,775 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:35:02,783 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:35:02,790 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:35:02,912 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:35:02,912 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:35:02,921 datashaper.workflow.workflow INFO executing verb create_final_communities
18:35:02,931 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:35:03,52 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:35:03,52 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:35:03,56 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:35:03,63 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:35:03,68 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:35:03,190 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
18:35:03,190 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:35:03,195 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:35:03,198 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:35:03,205 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:35:03,213 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:35:03,336 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:35:03,337 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:35:03,341 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:35:03,348 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:35:03,351 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 4
18:35:05,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:35:05,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.711092326004291. input_tokens=2022, output_tokens=289
18:35:06,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:35:06,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.861733946003369. input_tokens=2014, output_tokens=335
18:35:06,964 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:35:07,97 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:35:07,97 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:35:07,108 datashaper.workflow.workflow INFO executing verb create_final_documents
18:35:07,114 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:35:07,132 graphrag.index.cli INFO All workflows completed successfully.
18:37:21,520 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:37:21,521 graphrag.index.cli INFO Starting pipeline run for: 20241017-183721, dryrun=False
18:37:21,521 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:37:21,523 graphrag.index.create_pipeline_config INFO skipping workflows 
18:37:21,524 graphrag.index.run.run INFO Running pipeline
18:37:21,524 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:37:21,529 graphrag.index.input.load_input INFO loading input from root_dir=input
18:37:21,529 graphrag.index.input.load_input INFO using file storage for input
18:37:21,531 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:37:21,531 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:37:21,533 graphrag.index.input.text INFO Found 1 files, loading 1
18:37:21,544 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:37:21,544 graphrag.index.run.run INFO Final # of rows loaded: 1
18:37:21,655 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:37:21,657 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:37:22,86 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:37:22,210 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:37:22,210 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:37:22,221 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:37:22,222 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:37:22,260 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:37:22,260 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:37:23,393 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:37:23,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.1360121620091377. input_tokens=1803, output_tokens=194
18:37:25,243 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:37:25,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.847092171999975. input_tokens=26, output_tokens=350
18:37:25,268 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:37:25,389 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:37:25,390 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:37:25,397 datashaper.workflow.workflow INFO executing verb create_final_entities
18:37:25,401 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:37:25,438 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:37:25,438 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:37:25,438 graphrag.index.operations.embed_text.strategies.openai INFO embedding 7 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:37:25,951 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:37:25,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5216639709979063. input_tokens=136, output_tokens=0
18:37:25,967 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:37:26,96 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:37:26,96 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:37:26,104 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:37:26,112 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:37:26,235 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:37:26,236 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:37:26,244 datashaper.workflow.workflow INFO executing verb create_final_communities
18:37:26,255 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:37:26,377 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:37:26,377 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:37:26,382 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:37:26,389 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:37:26,394 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:37:26,520 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
18:37:26,520 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:37:26,525 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:37:26,528 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:37:26,535 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:37:26,549 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:37:26,677 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:37:26,677 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:37:26,681 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:37:26,689 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:37:26,692 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 7
18:37:29,425 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:37:29,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.721033385998453. input_tokens=2141, output_tokens=483
18:37:32,694 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:37:32,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2534931740083266. input_tokens=2126, output_tokens=581
18:37:32,715 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:37:32,844 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:37:32,844 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:37:32,855 datashaper.workflow.workflow INFO executing verb create_final_documents
18:37:32,861 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:37:32,879 graphrag.index.cli INFO All workflows completed successfully.
18:38:36,300 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:38:36,301 graphrag.index.cli INFO Starting pipeline run for: 20241017-183836, dryrun=False
18:38:36,302 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:38:36,304 graphrag.index.create_pipeline_config INFO skipping workflows 
18:38:36,304 graphrag.index.run.run INFO Running pipeline
18:38:36,304 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:38:36,305 graphrag.index.input.load_input INFO loading input from root_dir=input
18:38:36,305 graphrag.index.input.load_input INFO using file storage for input
18:38:36,306 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:38:36,307 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:38:36,308 graphrag.index.input.text INFO Found 1 files, loading 1
18:38:36,309 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:38:36,309 graphrag.index.run.run INFO Final # of rows loaded: 1
18:38:36,417 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:38:36,419 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:38:36,818 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:38:36,941 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:38:36,942 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:38:36,949 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:38:36,950 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:38:36,989 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:38:36,989 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:38:38,635 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:38:38,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6548761229933007. input_tokens=1803, output_tokens=194
18:38:42,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:38:42,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.8129720830038423. input_tokens=26, output_tokens=676
18:38:42,482 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:38:42,605 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:38:42,605 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:38:42,611 datashaper.workflow.workflow INFO executing verb create_final_entities
18:38:42,615 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:38:42,652 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:38:42,652 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:38:42,653 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 10 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:38:43,225 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:38:43,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.593769881001208. input_tokens=578, output_tokens=0
18:38:43,253 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:38:43,382 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:38:43,384 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:38:43,391 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:38:43,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:38:43,531 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:38:43,531 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:38:43,539 datashaper.workflow.workflow INFO executing verb create_final_communities
18:38:43,550 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:38:43,671 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:38:43,672 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:38:43,677 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:38:43,691 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:38:43,697 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:38:43,819 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:38:43,819 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:38:43,824 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:38:43,826 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:38:43,842 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:38:43,852 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:38:43,973 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:38:43,973 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:38:43,978 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:38:43,986 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:38:43,988 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
18:38:46,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:38:46,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1769534979976015. input_tokens=2035, output_tokens=425
18:38:46,187 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:38:46,317 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:38:46,317 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:38:46,328 datashaper.workflow.workflow INFO executing verb create_final_documents
18:38:46,334 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:38:46,353 graphrag.index.cli INFO All workflows completed successfully.
18:41:10,896 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:41:10,897 graphrag.index.cli INFO Starting pipeline run for: 20241017-184110, dryrun=False
18:41:10,898 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:41:10,899 graphrag.index.create_pipeline_config INFO skipping workflows 
18:41:10,900 graphrag.index.run.run INFO Running pipeline
18:41:10,900 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:41:10,902 graphrag.index.input.load_input INFO loading input from root_dir=input
18:41:10,902 graphrag.index.input.load_input INFO using file storage for input
18:41:10,903 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:41:10,904 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:41:10,905 graphrag.index.input.text INFO Found 1 files, loading 1
18:41:10,906 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:41:10,906 graphrag.index.run.run INFO Final # of rows loaded: 1
18:41:11,16 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:41:11,18 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:41:11,442 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:41:11,568 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:41:11,569 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:41:11,576 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:41:11,577 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:41:11,614 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:41:11,615 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:41:13,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:41:13,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6645978450105758. input_tokens=1803, output_tokens=194
18:41:14,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:41:14,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3800614880019566. input_tokens=23, output_tokens=242
18:41:14,671 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:41:14,805 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:41:14,805 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:41:14,812 datashaper.workflow.workflow INFO executing verb create_final_entities
18:41:14,815 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:41:14,853 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:41:14,853 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:41:14,853 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:41:15,790 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:41:15,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.944124137997278. input_tokens=95, output_tokens=0
18:41:15,804 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:41:15,940 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:41:15,940 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:41:15,948 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:41:15,957 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:41:16,91 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:41:16,91 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:41:16,99 datashaper.workflow.workflow INFO executing verb create_final_communities
18:41:16,110 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:41:16,236 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:41:16,237 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:41:16,241 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:41:16,248 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:41:16,253 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:41:16,387 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
18:41:16,387 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:41:16,391 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:41:16,393 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:41:16,400 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:41:16,409 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:41:16,536 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:41:16,536 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:41:16,540 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:41:16,548 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:41:16,551 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
18:41:20,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:41:20,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.639948637006455. input_tokens=2182, output_tokens=674
18:41:20,224 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:41:20,356 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:41:20,356 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:41:20,367 datashaper.workflow.workflow INFO executing verb create_final_documents
18:41:20,373 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:41:20,391 graphrag.index.cli INFO All workflows completed successfully.
18:42:32,968 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:42:32,970 graphrag.index.cli INFO Starting pipeline run for: 20241017-184232, dryrun=False
18:42:32,970 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:42:32,973 graphrag.index.create_pipeline_config INFO skipping workflows 
18:42:32,973 graphrag.index.run.run INFO Running pipeline
18:42:32,973 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:42:32,975 graphrag.index.input.load_input INFO loading input from root_dir=input
18:42:32,975 graphrag.index.input.load_input INFO using file storage for input
18:42:32,978 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:42:32,979 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:42:32,981 graphrag.index.input.text INFO Found 1 files, loading 1
18:42:32,983 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:42:32,983 graphrag.index.run.run INFO Final # of rows loaded: 1
18:42:33,96 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:42:33,99 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:42:33,553 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:42:33,678 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:42:33,679 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:42:33,686 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:42:33,687 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:42:33,732 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:42:33,732 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:42:35,331 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:42:35,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.603042700997321. input_tokens=1803, output_tokens=194
18:42:36,685 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:42:36,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.351437265999266. input_tokens=23, output_tokens=242
18:42:36,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:42:36,829 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:42:36,829 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:42:36,836 datashaper.workflow.workflow INFO executing verb create_final_entities
18:42:36,840 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:42:36,877 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:42:36,877 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:42:36,877 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:42:37,320 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:42:37,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.45261789100186434. input_tokens=95, output_tokens=0
18:42:37,337 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:42:37,463 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:42:37,463 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:42:37,471 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:42:37,479 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:42:37,602 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:42:37,603 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:42:37,611 datashaper.workflow.workflow INFO executing verb create_final_communities
18:42:37,621 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:42:37,763 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:42:37,763 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:42:37,766 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:42:37,773 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:42:37,779 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:42:37,903 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:42:37,904 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:42:37,908 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:42:37,910 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:42:37,917 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:42:37,927 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:42:38,53 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:42:38,53 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:42:38,57 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:42:38,65 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:42:38,67 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
18:42:42,250 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:42:42,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.174254631987424. input_tokens=2182, output_tokens=767
18:42:42,270 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:42:42,402 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:42:42,403 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:42:42,413 datashaper.workflow.workflow INFO executing verb create_final_documents
18:42:42,419 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:42:42,436 graphrag.index.cli INFO All workflows completed successfully.
18:43:12,530 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:43:12,531 graphrag.index.cli INFO Starting pipeline run for: 20241017-184312, dryrun=False
18:43:12,532 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:43:12,533 graphrag.index.create_pipeline_config INFO skipping workflows 
18:43:12,533 graphrag.index.run.run INFO Running pipeline
18:43:12,534 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:43:12,535 graphrag.index.input.load_input INFO loading input from root_dir=input
18:43:12,535 graphrag.index.input.load_input INFO using file storage for input
18:43:12,537 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:43:12,538 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:43:12,539 graphrag.index.input.text INFO Found 1 files, loading 1
18:43:12,539 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:43:12,539 graphrag.index.run.run INFO Final # of rows loaded: 1
18:43:12,650 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:43:12,652 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:43:12,941 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:43:13,66 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:43:13,67 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:43:13,74 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:43:13,75 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:43:13,113 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:43:13,113 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:43:14,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:14,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.591949490000843. input_tokens=1803, output_tokens=194
18:43:16,121 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:16,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4168677459965693. input_tokens=24, output_tokens=242
18:43:16,141 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:43:16,263 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:43:16,263 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:16,271 datashaper.workflow.workflow INFO executing verb create_final_entities
18:43:16,274 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:43:16,310 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:43:16,311 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:43:16,311 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 5 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:43:16,904 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:43:16,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6020179420011118. input_tokens=95, output_tokens=0
18:43:16,921 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:43:17,48 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:43:17,49 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:17,57 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:43:17,65 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:43:17,187 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:43:17,188 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:17,196 datashaper.workflow.workflow INFO executing verb create_final_communities
18:43:17,207 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:43:17,328 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:43:17,328 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:43:17,334 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:17,340 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:43:17,346 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:43:17,471 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
18:43:17,471 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:43:17,475 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:43:17,478 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:43:17,485 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:43:17,494 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:43:17,623 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:43:17,623 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:43:17,627 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:43:17,634 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:43:17,637 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
18:43:21,414 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:21,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7686481950076995. input_tokens=2182, output_tokens=611
18:43:21,437 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:43:21,569 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:43:21,570 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:43:21,580 datashaper.workflow.workflow INFO executing verb create_final_documents
18:43:21,586 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:43:21,603 graphrag.index.cli INFO All workflows completed successfully.
18:43:47,978 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:43:47,979 graphrag.index.cli INFO Starting pipeline run for: 20241017-184347, dryrun=False
18:43:47,979 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:43:47,981 graphrag.index.create_pipeline_config INFO skipping workflows 
18:43:47,981 graphrag.index.run.run INFO Running pipeline
18:43:47,982 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:43:47,984 graphrag.index.input.load_input INFO loading input from root_dir=input
18:43:47,984 graphrag.index.input.load_input INFO using file storage for input
18:43:47,985 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:43:47,985 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:43:47,986 graphrag.index.input.text INFO Found 1 files, loading 1
18:43:47,993 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:43:47,993 graphrag.index.run.run INFO Final # of rows loaded: 1
18:43:48,103 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:43:48,106 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:43:48,500 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:43:48,630 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:43:48,630 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:43:48,638 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:43:48,640 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:43:48,678 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:43:48,678 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:43:49,903 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:49,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.2333885050029494. input_tokens=1803, output_tokens=194
18:43:51,857 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:51,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9452935709996382. input_tokens=28, output_tokens=401
18:43:51,878 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:43:52,3 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:43:52,3 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:52,10 datashaper.workflow.workflow INFO executing verb create_final_entities
18:43:52,14 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:43:52,50 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:43:52,50 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:43:52,51 graphrag.index.operations.embed_text.strategies.openai INFO embedding 8 inputs via 7 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:43:52,550 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:43:52,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5122016789973713. input_tokens=136, output_tokens=0
18:43:52,570 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:43:52,695 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:43:52,696 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:52,703 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:43:52,712 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:43:52,837 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:43:52,838 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:52,846 datashaper.workflow.workflow INFO executing verb create_final_communities
18:43:52,859 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:43:52,980 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:43:52,980 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:43:52,985 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:43:52,993 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:43:52,999 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:43:53,126 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
18:43:53,126 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:43:53,132 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:43:53,134 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:43:53,141 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:43:53,150 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:43:53,272 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:43:53,273 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:43:53,277 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:43:53,285 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:43:53,288 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 8
18:43:55,913 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:55,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6132975140062626. input_tokens=2144, output_tokens=435
18:43:58,984 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:43:58,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0673987560003297. input_tokens=2118, output_tokens=607
18:43:59,11 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:43:59,146 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:43:59,150 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:43:59,159 datashaper.workflow.workflow INFO executing verb create_final_documents
18:43:59,166 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:43:59,185 graphrag.index.cli INFO All workflows completed successfully.
18:44:37,659 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:44:37,660 graphrag.index.cli INFO Starting pipeline run for: 20241017-184437, dryrun=False
18:44:37,660 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:44:37,663 graphrag.index.create_pipeline_config INFO skipping workflows 
18:44:37,663 graphrag.index.run.run INFO Running pipeline
18:44:37,663 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:44:37,663 graphrag.index.input.load_input INFO loading input from root_dir=input
18:44:37,664 graphrag.index.input.load_input INFO using file storage for input
18:44:37,664 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:44:37,665 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:44:37,666 graphrag.index.input.text INFO Found 1 files, loading 1
18:44:37,667 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:44:37,667 graphrag.index.run.run INFO Final # of rows loaded: 1
18:44:37,776 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:44:37,779 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:44:38,121 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:44:38,243 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:44:38,244 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:44:38,251 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:44:38,252 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:44:38,290 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:44:38,290 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:44:39,899 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:44:39,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6123275639984058. input_tokens=1803, output_tokens=194
18:44:41,630 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:44:41,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7277238039969234. input_tokens=28, output_tokens=326
18:44:41,651 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:44:41,783 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:44:41,784 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:44:41,791 datashaper.workflow.workflow INFO executing verb create_final_entities
18:44:41,794 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:44:41,832 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:44:41,832 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:44:41,832 graphrag.index.operations.embed_text.strategies.openai INFO embedding 7 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:44:42,835 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:44:42,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0217151429969817. input_tokens=120, output_tokens=0
18:44:42,860 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:44:42,996 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:44:42,996 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:44:43,11 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:44:43,20 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:44:43,147 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:44:43,147 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:44:43,155 datashaper.workflow.workflow INFO executing verb create_final_communities
18:44:43,165 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:44:43,287 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:44:43,288 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:44:43,295 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:44:43,302 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:44:43,308 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:44:43,431 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
18:44:43,431 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:44:43,436 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:44:43,438 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:44:43,450 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:44:43,459 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:44:43,602 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:44:43,602 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:44:43,606 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:44:43,614 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:44:43,617 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 7
18:44:46,248 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:44:46,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6183206730056554. input_tokens=2131, output_tokens=484
18:44:49,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:44:49,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.205334143000073. input_tokens=2094, output_tokens=487
18:44:49,473 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:44:49,604 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:44:49,605 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:44:49,615 datashaper.workflow.workflow INFO executing verb create_final_documents
18:44:49,622 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:44:49,638 graphrag.index.cli INFO All workflows completed successfully.
18:47:26,966 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:47:26,967 graphrag.index.cli INFO Starting pipeline run for: 20241017-184726, dryrun=False
18:47:26,967 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:47:26,969 graphrag.index.create_pipeline_config INFO skipping workflows 
18:47:26,969 graphrag.index.run.run INFO Running pipeline
18:47:26,969 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:47:26,970 graphrag.index.input.load_input INFO loading input from root_dir=input
18:47:26,970 graphrag.index.input.load_input INFO using file storage for input
18:47:26,972 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:47:26,973 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:47:26,974 graphrag.index.input.text INFO Found 1 files, loading 1
18:47:26,975 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:47:26,975 graphrag.index.run.run INFO Final # of rows loaded: 1
18:47:27,84 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:47:27,86 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:47:27,427 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:47:27,550 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:47:27,550 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:47:27,558 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:47:27,559 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:47:27,597 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:47:27,597 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:47:29,164 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:47:29,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.57588822300022. input_tokens=1794, output_tokens=187
18:47:30,704 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:47:30,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.531294430998969. input_tokens=28, output_tokens=232
18:47:30,727 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:47:30,844 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:47:30,845 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:30,852 datashaper.workflow.workflow INFO executing verb create_final_entities
18:47:30,856 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:47:30,893 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:47:30,893 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:47:30,893 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:47:31,438 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:47:31,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5602391529973829. input_tokens=170, output_tokens=0
18:47:31,460 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:47:31,588 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:47:31,588 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:31,596 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:47:31,604 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:47:31,727 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:47:31,727 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:31,735 datashaper.workflow.workflow INFO executing verb create_final_communities
18:47:31,744 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:47:31,864 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
18:47:31,865 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:47:31,870 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:31,876 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:47:31,882 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:47:32,8 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
18:47:32,11 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:47:32,14 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:47:32,17 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:47:32,23 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:47:32,32 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:47:32,154 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:47:32,154 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:47:32,159 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:47:32,166 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:47:32,169 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 9
18:47:35,52 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:47:35,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.874428954004543. input_tokens=2035, output_tokens=438
18:47:35,67 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:47:35,203 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:47:35,203 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:47:35,213 datashaper.workflow.workflow INFO executing verb create_final_documents
18:47:35,220 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:47:35,237 graphrag.index.cli INFO All workflows completed successfully.
18:47:52,572 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:47:52,574 graphrag.index.cli INFO Starting pipeline run for: 20241017-184752, dryrun=False
18:47:52,574 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:47:52,576 graphrag.index.create_pipeline_config INFO skipping workflows 
18:47:52,576 graphrag.index.run.run INFO Running pipeline
18:47:52,576 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:47:52,576 graphrag.index.input.load_input INFO loading input from root_dir=input
18:47:52,577 graphrag.index.input.load_input INFO using file storage for input
18:47:52,577 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:47:52,578 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:47:52,579 graphrag.index.input.text INFO Found 1 files, loading 1
18:47:52,580 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:47:52,580 graphrag.index.run.run INFO Final # of rows loaded: 1
18:47:52,690 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:47:52,692 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:47:52,987 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:47:53,109 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:47:53,109 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:47:53,117 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:47:53,118 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:47:53,156 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:47:53,156 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:47:54,747 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:47:54,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5937055300018983. input_tokens=1794, output_tokens=187
18:47:56,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:47:56,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6657226079987595. input_tokens=28, output_tokens=336
18:47:56,435 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:47:56,562 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:47:56,562 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:56,570 datashaper.workflow.workflow INFO executing verb create_final_entities
18:47:56,573 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:47:56,611 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:47:56,611 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:47:56,611 graphrag.index.operations.embed_text.strategies.openai INFO embedding 7 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:47:57,145 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:47:57,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5456467590120155. input_tokens=104, output_tokens=0
18:47:57,163 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:47:57,289 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:47:57,289 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:57,297 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:47:57,304 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:47:57,425 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:47:57,426 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:57,434 datashaper.workflow.workflow INFO executing verb create_final_communities
18:47:57,444 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:47:57,564 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:47:57,565 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:47:57,569 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:47:57,576 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:47:57,582 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:47:57,708 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
18:47:57,708 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:47:57,724 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:47:57,726 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:47:57,735 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:47:57,744 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:47:57,865 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:47:57,865 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:47:57,879 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:47:57,887 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:47:57,890 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 7
18:48:00,580 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:48:00,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.675929051009007. input_tokens=2200, output_tokens=528
18:48:02,809 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:48:02,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2207281309965765. input_tokens=2006, output_tokens=396
18:48:02,819 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:48:02,953 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:48:02,954 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:48:02,965 datashaper.workflow.workflow INFO executing verb create_final_documents
18:48:02,971 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:48:02,987 graphrag.index.cli INFO All workflows completed successfully.
18:48:51,699 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:48:51,701 graphrag.index.cli INFO Starting pipeline run for: 20241017-184851, dryrun=False
18:48:51,701 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:48:51,703 graphrag.index.create_pipeline_config INFO skipping workflows 
18:48:51,703 graphrag.index.run.run INFO Running pipeline
18:48:51,703 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:48:51,705 graphrag.index.input.load_input INFO loading input from root_dir=input
18:48:51,705 graphrag.index.input.load_input INFO using file storage for input
18:48:51,706 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:48:51,707 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:48:51,709 graphrag.index.input.text INFO Found 1 files, loading 1
18:48:51,710 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:48:51,711 graphrag.index.run.run INFO Final # of rows loaded: 1
18:48:51,821 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:48:51,823 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:48:52,299 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:48:52,423 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:48:52,423 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:48:52,431 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:48:52,432 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:48:52,470 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:48:52,470 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:48:54,422 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:48:54,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.959777504001977. input_tokens=1800, output_tokens=199
18:48:56,434 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:48:56,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.004146535997279. input_tokens=28, output_tokens=416
18:48:56,461 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:48:56,587 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:48:56,588 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:48:56,595 datashaper.workflow.workflow INFO executing verb create_final_entities
18:48:56,598 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:48:56,635 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:48:56,635 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:48:56,636 graphrag.index.operations.embed_text.strategies.openai INFO embedding 8 inputs via 7 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:48:57,770 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:48:57,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1593454679969. input_tokens=152, output_tokens=0
18:48:57,802 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:48:57,929 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:48:57,930 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:48:57,938 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:48:57,947 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:48:58,70 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:48:58,70 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:48:58,79 datashaper.workflow.workflow INFO executing verb create_final_communities
18:48:58,90 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:48:58,212 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:48:58,212 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:48:58,216 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:48:58,224 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:48:58,230 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:48:58,354 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
18:48:58,354 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:48:58,359 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:48:58,361 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:48:58,368 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:48:58,377 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:48:58,500 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:48:58,500 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:48:58,505 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:48:58,513 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:48:58,516 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 8
18:49:01,898 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:01,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3718141650024336. input_tokens=2177, output_tokens=642
18:49:04,240 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:04,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.333235369005706. input_tokens=2080, output_tokens=384
18:49:04,250 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:49:04,399 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:49:04,399 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:49:04,410 datashaper.workflow.workflow INFO executing verb create_final_documents
18:49:04,417 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:49:04,434 graphrag.index.cli INFO All workflows completed successfully.
18:49:46,974 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:49:46,975 graphrag.index.cli INFO Starting pipeline run for: 20241017-184946, dryrun=False
18:49:46,976 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:49:46,978 graphrag.index.create_pipeline_config INFO skipping workflows 
18:49:46,978 graphrag.index.run.run INFO Running pipeline
18:49:46,978 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:49:46,978 graphrag.index.input.load_input INFO loading input from root_dir=input
18:49:46,978 graphrag.index.input.load_input INFO using file storage for input
18:49:46,980 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:49:46,980 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:49:46,981 graphrag.index.input.text INFO Found 1 files, loading 1
18:49:46,982 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:49:46,982 graphrag.index.run.run INFO Final # of rows loaded: 1
18:49:47,92 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:49:47,95 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:49:47,399 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:49:47,524 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:49:47,524 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:49:47,532 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:49:47,533 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:49:47,571 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:49:47,571 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:49:48,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:48,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0402059140033089. input_tokens=1802, output_tokens=178
18:49:50,437 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:50,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8266336639935616. input_tokens=28, output_tokens=388
18:49:50,459 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:49:50,581 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:49:50,581 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:49:50,589 datashaper.workflow.workflow INFO executing verb create_final_entities
18:49:50,592 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:49:50,628 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:49:50,628 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:49:50,629 graphrag.index.operations.embed_text.strategies.openai INFO embedding 8 inputs via 7 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:49:51,242 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:49:51,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.629895566002233. input_tokens=129, output_tokens=0
18:49:51,268 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:49:51,396 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:49:51,396 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:49:51,404 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:49:51,411 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:49:51,536 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:49:51,537 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:49:51,545 datashaper.workflow.workflow INFO executing verb create_final_communities
18:49:51,556 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:49:51,689 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:49:51,689 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:49:51,693 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:49:51,701 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:49:51,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:49:51,833 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
18:49:51,834 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:49:51,839 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:49:51,842 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:49:51,849 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:49:51,859 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:49:51,981 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
18:49:51,981 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:49:51,986 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:49:51,994 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:49:51,997 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 8
18:49:54,586 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:54,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.572689430002356. input_tokens=2074, output_tokens=471
18:49:57,420 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:49:57,423 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.82737959100632. input_tokens=2100, output_tokens=533
18:50:00,242 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:50:00,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.815325985007803. input_tokens=2075, output_tokens=549
18:50:00,252 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:50:00,383 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:50:00,384 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:50:00,394 datashaper.workflow.workflow INFO executing verb create_final_documents
18:50:00,400 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:50:00,419 graphrag.index.cli INFO All workflows completed successfully.
18:53:42,90 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
18:53:42,93 graphrag.index.cli INFO Starting pipeline run for: 20241017-185342, dryrun=False
18:53:42,94 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
18:53:42,97 graphrag.index.create_pipeline_config INFO skipping workflows 
18:53:42,97 graphrag.index.run.run INFO Running pipeline
18:53:42,98 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
18:53:42,101 graphrag.index.input.load_input INFO loading input from root_dir=input
18:53:42,101 graphrag.index.input.load_input INFO using file storage for input
18:53:42,104 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
18:53:42,105 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
18:53:42,107 graphrag.index.input.text INFO Found 1 files, loading 1
18:53:42,109 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
18:53:42,109 graphrag.index.run.run INFO Final # of rows loaded: 1
18:53:42,220 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
18:53:42,223 datashaper.workflow.workflow INFO executing verb create_base_text_units
18:53:42,687 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:53:42,811 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
18:53:42,812 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:53:42,819 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
18:53:42,820 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:53:42,858 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
18:53:42,858 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
18:53:43,931 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:53:43,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0768234629940707. input_tokens=1802, output_tokens=181
18:53:46,675 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:53:46,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.741676614008611. input_tokens=28, output_tokens=475
18:53:46,697 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
18:53:46,697 root INFO Starting at time 1729184026.6973403
18:53:46,697 root INFO Beginning preprocessing of transition probabilities for 10 vertices
18:53:46,697 root INFO Completed 1 / 10 vertices
18:53:46,697 root INFO Completed 2 / 10 vertices
18:53:46,698 root INFO Completed 3 / 10 vertices
18:53:46,699 root INFO Completed 4 / 10 vertices
18:53:46,700 root INFO Completed 5 / 10 vertices
18:53:46,700 root INFO Completed 6 / 10 vertices
18:53:46,701 root INFO Completed 7 / 10 vertices
18:53:46,702 root INFO Completed 8 / 10 vertices
18:53:46,703 root INFO Completed 9 / 10 vertices
18:53:46,704 root INFO Completed 10 / 10 vertices
18:53:46,705 root INFO Completed preprocessing of transition probabilities for vertices
18:53:46,705 root INFO Beginning preprocessing of transition probabilities for 11 edges
18:53:46,706 root INFO Completed 1 / 11 edges
18:53:46,707 root INFO Completed 2 / 11 edges
18:53:46,708 root INFO Completed 3 / 11 edges
18:53:46,708 root INFO Completed 4 / 11 edges
18:53:46,709 root INFO Completed 5 / 11 edges
18:53:46,710 root INFO Completed 6 / 11 edges
18:53:46,710 root INFO Completed 7 / 11 edges
18:53:46,710 root INFO Completed 8 / 11 edges
18:53:46,711 root INFO Completed 9 / 11 edges
18:53:46,712 root INFO Completed 10 / 11 edges
18:53:46,712 root INFO Completed 11 / 11 edges
18:53:46,713 root INFO Completed preprocessing of transition probabilities for edges
18:53:46,714 root INFO Simulating walks on graph at time 1729184026.71435
18:53:46,714 root INFO Walk iteration: 1/10
18:53:46,716 root INFO Walk iteration: 2/10
18:53:46,716 root INFO Walk iteration: 3/10
18:53:46,717 root INFO Walk iteration: 4/10
18:53:46,718 root INFO Walk iteration: 5/10
18:53:46,719 root INFO Walk iteration: 6/10
18:53:46,719 root INFO Walk iteration: 7/10
18:53:46,720 root INFO Walk iteration: 8/10
18:53:46,721 root INFO Walk iteration: 9/10
18:53:46,721 root INFO Walk iteration: 10/10
18:53:46,722 root INFO Learning embeddings at time 1729184026.7222972
18:53:46,722 gensim.models.word2vec INFO collecting all words and their counts
18:53:46,723 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
18:53:46,723 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
18:53:46,723 gensim.models.word2vec INFO Creating a fresh vocabulary
18:53:46,724 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T18:53:46.724029', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
18:53:46,724 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T18:53:46.724677', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
18:53:46,725 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
18:53:46,726 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
18:53:46,726 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T18:53:46.726933', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
18:53:46,727 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
18:53:46,728 gensim.models.word2vec INFO resetting layer weights
18:53:46,728 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T18:53:46.728670', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
18:53:46,729 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T18:53:46.729046', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
18:53:46,734 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 273334 effective words/s
18:53:46,738 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 100194 effective words/s
18:53:46,743 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 74380 effective words/s
18:53:46,744 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 44803 effective words/s', 'datetime': '2024-10-17T18:53:46.743954', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
18:53:46,744 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T18:53:46.744734', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
18:53:46,744 root INFO Completed. Ending time is 1729184026.7448478 Elapsed time is -0.047507524490356445
18:53:46,770 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:53:46,902 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:53:46,903 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:53:46,911 datashaper.workflow.workflow INFO executing verb create_final_entities
18:53:46,914 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
18:53:46,951 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
18:53:46,951 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
18:53:46,952 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
18:53:47,996 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
18:53:48,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0600658659968758. input_tokens=158, output_tokens=0
18:53:48,19 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
18:53:48,159 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
18:53:48,159 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:53:48,167 datashaper.workflow.workflow INFO executing verb create_final_nodes
18:53:50,616 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
18:53:50,768 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
18:53:50,769 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:53:50,778 datashaper.workflow.workflow INFO executing verb create_final_communities
18:53:50,788 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
18:53:50,918 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
18:53:50,918 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
18:53:50,935 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:53:50,944 datashaper.workflow.workflow INFO executing verb create_final_relationships
18:53:50,949 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
18:53:51,87 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
18:53:51,87 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
18:53:51,99 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:53:51,101 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
18:53:51,109 datashaper.workflow.workflow INFO executing verb create_final_text_units
18:53:51,126 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
18:53:51,259 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
18:53:51,260 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
18:53:51,264 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
18:53:51,271 datashaper.workflow.workflow INFO executing verb create_final_community_reports
18:53:51,274 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
18:53:54,340 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:53:54,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.05489010400197. input_tokens=2071, output_tokens=540
18:53:57,969 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
18:53:57,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.610876103004557. input_tokens=2240, output_tokens=740
18:53:57,980 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
18:53:58,124 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
18:53:58,124 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
18:53:58,134 datashaper.workflow.workflow INFO executing verb create_final_documents
18:53:58,139 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
18:53:58,155 graphrag.index.cli INFO All workflows completed successfully.
19:21:54,249 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:21:54,250 graphrag.index.cli INFO Starting pipeline run for: 20241017-192154, dryrun=False
19:21:54,261 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:21:54,263 graphrag.index.create_pipeline_config INFO skipping workflows 
19:21:54,263 graphrag.index.run.run INFO Running pipeline
19:21:54,263 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:21:54,265 graphrag.index.input.load_input INFO loading input from root_dir=input
19:21:54,265 graphrag.index.input.load_input INFO using file storage for input
19:21:54,267 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:21:54,267 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:21:54,269 graphrag.index.input.text INFO Found 1 files, loading 1
19:21:54,270 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:21:54,270 graphrag.index.run.run INFO Final # of rows loaded: 1
19:21:54,379 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:21:54,382 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:21:54,863 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:21:54,987 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:21:54,988 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:21:54,996 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:21:54,997 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:21:55,35 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
19:21:55,35 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
19:21:56,888 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:21:56,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8580309170065448. input_tokens=1802, output_tokens=181
19:21:59,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:21:59,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.1565024640003685. input_tokens=28, output_tokens=475
19:21:59,72 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
19:21:59,72 root INFO Starting at time 1729185719.0722427
19:21:59,72 root INFO Beginning preprocessing of transition probabilities for 10 vertices
19:21:59,72 root INFO Completed 1 / 10 vertices
19:21:59,72 root INFO Completed 2 / 10 vertices
19:21:59,72 root INFO Completed 3 / 10 vertices
19:21:59,72 root INFO Completed 4 / 10 vertices
19:21:59,73 root INFO Completed 5 / 10 vertices
19:21:59,73 root INFO Completed 6 / 10 vertices
19:21:59,73 root INFO Completed 7 / 10 vertices
19:21:59,73 root INFO Completed 8 / 10 vertices
19:21:59,73 root INFO Completed 9 / 10 vertices
19:21:59,73 root INFO Completed 10 / 10 vertices
19:21:59,74 root INFO Completed preprocessing of transition probabilities for vertices
19:21:59,74 root INFO Beginning preprocessing of transition probabilities for 11 edges
19:21:59,74 root INFO Completed 1 / 11 edges
19:21:59,74 root INFO Completed 2 / 11 edges
19:21:59,74 root INFO Completed 3 / 11 edges
19:21:59,74 root INFO Completed 4 / 11 edges
19:21:59,75 root INFO Completed 5 / 11 edges
19:21:59,75 root INFO Completed 6 / 11 edges
19:21:59,75 root INFO Completed 7 / 11 edges
19:21:59,75 root INFO Completed 8 / 11 edges
19:21:59,75 root INFO Completed 9 / 11 edges
19:21:59,76 root INFO Completed 10 / 11 edges
19:21:59,77 root INFO Completed 11 / 11 edges
19:21:59,78 root INFO Completed preprocessing of transition probabilities for edges
19:21:59,79 root INFO Simulating walks on graph at time 1729185719.0793905
19:21:59,80 root INFO Walk iteration: 1/10
19:21:59,81 root INFO Walk iteration: 2/10
19:21:59,82 root INFO Walk iteration: 3/10
19:21:59,83 root INFO Walk iteration: 4/10
19:21:59,83 root INFO Walk iteration: 5/10
19:21:59,84 root INFO Walk iteration: 6/10
19:21:59,85 root INFO Walk iteration: 7/10
19:21:59,85 root INFO Walk iteration: 8/10
19:21:59,86 root INFO Walk iteration: 9/10
19:21:59,87 root INFO Walk iteration: 10/10
19:21:59,88 root INFO Learning embeddings at time 1729185719.0880363
19:21:59,88 gensim.models.word2vec INFO collecting all words and their counts
19:21:59,89 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:21:59,89 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
19:21:59,89 gensim.models.word2vec INFO Creating a fresh vocabulary
19:21:59,89 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T19:21:59.089840', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:21:59,90 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T19:21:59.090469', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:21:59,90 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
19:21:59,91 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
19:21:59,92 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T19:21:59.092348', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:21:59,93 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
19:21:59,94 gensim.models.word2vec INFO resetting layer weights
19:21:59,95 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T19:21:59.095488', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:21:59,95 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T19:21:59.095986', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:21:59,101 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 119207 effective words/s
19:21:59,105 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 103754 effective words/s
19:21:59,109 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 82277 effective words/s
19:21:59,109 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 48307 effective words/s', 'datetime': '2024-10-17T19:21:59.109760', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:21:59,110 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T19:21:59.110509', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:21:59,111 root INFO Completed. Ending time is 1729185719.1113641 Elapsed time is -0.039121389389038086
19:21:59,139 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:21:59,270 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:21:59,270 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:21:59,278 datashaper.workflow.workflow INFO executing verb create_final_entities
19:21:59,281 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:21:59,318 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
19:21:59,318 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
19:21:59,319 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:22:00,373 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:22:00,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0685589269996854. input_tokens=158, output_tokens=0
19:22:00,395 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:22:00,527 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:22:00,528 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:22:00,536 datashaper.workflow.workflow INFO executing verb create_final_nodes
19:22:02,894 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:22:03,37 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:22:03,38 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:22:03,47 datashaper.workflow.workflow INFO executing verb create_final_communities
19:22:03,57 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:22:03,189 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
19:22:03,190 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:22:03,195 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:22:03,202 datashaper.workflow.workflow INFO executing verb create_final_relationships
19:22:03,208 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:22:03,341 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
19:22:03,344 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:22:03,348 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
19:22:03,353 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:22:03,359 datashaper.workflow.workflow INFO executing verb create_final_text_units
19:22:03,375 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:22:03,506 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
19:22:03,507 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:22:03,511 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:22:03,519 datashaper.workflow.workflow INFO executing verb create_final_community_reports
19:22:03,522 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
19:22:05,194 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:22:05,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6602460620051716. input_tokens=2071, output_tokens=293
19:22:08,919 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:22:08,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7197449569939636. input_tokens=2240, output_tokens=740
19:22:08,943 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:22:09,87 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
19:22:09,87 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
19:22:09,98 datashaper.workflow.workflow INFO executing verb create_final_documents
19:22:09,104 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:22:09,121 graphrag.index.cli INFO All workflows completed successfully.
19:23:10,713 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:23:10,715 graphrag.index.cli INFO Starting pipeline run for: 20241017-192310, dryrun=False
19:23:10,715 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:23:10,717 graphrag.index.create_pipeline_config INFO skipping workflows 
19:23:10,717 graphrag.index.run.run INFO Running pipeline
19:23:10,718 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:23:10,720 graphrag.index.input.load_input INFO loading input from root_dir=input
19:23:10,720 graphrag.index.input.load_input INFO using file storage for input
19:23:10,722 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:23:10,722 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:23:10,724 graphrag.index.input.text INFO Found 1 files, loading 1
19:23:10,724 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:23:10,724 graphrag.index.run.run INFO Final # of rows loaded: 1
19:23:10,831 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:23:10,834 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:23:11,312 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:23:11,434 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:23:11,434 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:23:11,443 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:23:11,444 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:23:11,483 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
19:23:11,483 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
19:23:13,119 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:23:13,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6401011119887698. input_tokens=1802, output_tokens=181
19:23:15,781 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:23:15,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.6594860699988203. input_tokens=28, output_tokens=475
19:23:15,802 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
19:23:15,802 root INFO Starting at time 1729185795.802586
19:23:15,802 root INFO Beginning preprocessing of transition probabilities for 10 vertices
19:23:15,802 root INFO Completed 1 / 10 vertices
19:23:15,802 root INFO Completed 2 / 10 vertices
19:23:15,802 root INFO Completed 3 / 10 vertices
19:23:15,802 root INFO Completed 4 / 10 vertices
19:23:15,803 root INFO Completed 5 / 10 vertices
19:23:15,803 root INFO Completed 6 / 10 vertices
19:23:15,803 root INFO Completed 7 / 10 vertices
19:23:15,803 root INFO Completed 8 / 10 vertices
19:23:15,803 root INFO Completed 9 / 10 vertices
19:23:15,803 root INFO Completed 10 / 10 vertices
19:23:15,804 root INFO Completed preprocessing of transition probabilities for vertices
19:23:15,804 root INFO Beginning preprocessing of transition probabilities for 11 edges
19:23:15,804 root INFO Completed 1 / 11 edges
19:23:15,804 root INFO Completed 2 / 11 edges
19:23:15,804 root INFO Completed 3 / 11 edges
19:23:15,805 root INFO Completed 4 / 11 edges
19:23:15,805 root INFO Completed 5 / 11 edges
19:23:15,805 root INFO Completed 6 / 11 edges
19:23:15,805 root INFO Completed 7 / 11 edges
19:23:15,806 root INFO Completed 8 / 11 edges
19:23:15,807 root INFO Completed 9 / 11 edges
19:23:15,808 root INFO Completed 10 / 11 edges
19:23:15,809 root INFO Completed 11 / 11 edges
19:23:15,810 root INFO Completed preprocessing of transition probabilities for edges
19:23:15,811 root INFO Simulating walks on graph at time 1729185795.8114476
19:23:15,812 root INFO Walk iteration: 1/10
19:23:15,814 root INFO Walk iteration: 2/10
19:23:15,814 root INFO Walk iteration: 3/10
19:23:15,815 root INFO Walk iteration: 4/10
19:23:15,816 root INFO Walk iteration: 5/10
19:23:15,818 root INFO Walk iteration: 6/10
19:23:15,818 root INFO Walk iteration: 7/10
19:23:15,819 root INFO Walk iteration: 8/10
19:23:15,820 root INFO Walk iteration: 9/10
19:23:15,821 root INFO Walk iteration: 10/10
19:23:15,822 root INFO Learning embeddings at time 1729185795.8225753
19:23:15,823 gensim.models.word2vec INFO collecting all words and their counts
19:23:15,823 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:23:15,824 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
19:23:15,824 gensim.models.word2vec INFO Creating a fresh vocabulary
19:23:15,824 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T19:23:15.824902', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:23:15,825 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T19:23:15.825567', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:23:15,826 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
19:23:15,827 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
19:23:15,828 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T19:23:15.828172', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:23:15,829 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
19:23:15,829 gensim.models.word2vec INFO resetting layer weights
19:23:15,830 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T19:23:15.830114', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:23:15,830 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T19:23:15.830653', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:23:15,837 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 226488 effective words/s
19:23:15,841 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 90257 effective words/s
19:23:15,847 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 116430 effective words/s
19:23:15,847 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 40181 effective words/s', 'datetime': '2024-10-17T19:23:15.847240', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:23:15,848 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T19:23:15.848175', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:23:15,849 root INFO Completed. Ending time is 1729185795.8490756 Elapsed time is -0.04648947715759277
19:23:15,877 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:23:16,12 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:23:16,13 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:23:16,20 datashaper.workflow.workflow INFO executing verb create_final_entities
19:23:16,24 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:23:16,61 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
19:23:16,61 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
19:23:16,61 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:23:17,114 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:23:17,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0681672259961488. input_tokens=158, output_tokens=0
19:23:17,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:23:17,267 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:23:17,267 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:23:17,275 datashaper.workflow.workflow INFO executing verb create_final_nodes
19:23:19,643 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:23:19,786 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:23:19,789 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:23:19,796 datashaper.workflow.workflow INFO executing verb create_final_communities
19:23:19,807 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:23:19,936 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
19:23:19,936 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:23:19,941 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:23:19,948 datashaper.workflow.workflow INFO executing verb create_final_relationships
19:23:19,954 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:23:20,85 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
19:23:20,86 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:23:20,91 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:23:20,93 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
19:23:20,100 datashaper.workflow.workflow INFO executing verb create_final_text_units
19:23:20,110 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:23:20,241 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
19:23:20,242 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:23:20,246 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:23:20,254 datashaper.workflow.workflow INFO executing verb create_final_community_reports
19:23:20,257 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
19:23:22,483 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:23:22,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2141561699972954. input_tokens=2071, output_tokens=392
19:23:26,86 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:23:26,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5977296339988243. input_tokens=2240, output_tokens=748
19:23:26,106 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:23:26,244 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
19:23:26,244 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
19:23:26,255 datashaper.workflow.workflow INFO executing verb create_final_documents
19:23:26,261 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:23:26,287 graphrag.index.cli INFO All workflows completed successfully.
19:24:41,359 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:24:41,364 graphrag.index.cli INFO Starting pipeline run for: 20241017-192441, dryrun=False
19:24:41,365 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "nltk"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:24:41,367 graphrag.index.create_pipeline_config INFO skipping workflows 
19:24:41,367 graphrag.index.run.run INFO Running pipeline
19:24:41,368 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:24:41,368 graphrag.index.input.load_input INFO loading input from root_dir=input
19:24:41,368 graphrag.index.input.load_input INFO using file storage for input
19:24:41,371 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:24:41,373 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:24:41,376 graphrag.index.input.text INFO Found 1 files, loading 1
19:24:41,378 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:24:41,379 graphrag.index.run.run INFO Final # of rows loaded: 1
19:24:41,496 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:24:41,499 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:24:41,996 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:24:42,121 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:24:42,121 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:24:42,129 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:24:43,797 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
19:24:43,798 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:24:43,801 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
19:24:43,802 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:24:43,803 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:24:43,815 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:24:59,626 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:24:59,627 graphrag.index.cli INFO Starting pipeline run for: 20241017-192459, dryrun=False
19:24:59,627 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "nltk"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:24:59,629 graphrag.index.create_pipeline_config INFO skipping workflows 
19:24:59,629 graphrag.index.run.run INFO Running pipeline
19:24:59,629 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:24:59,630 graphrag.index.input.load_input INFO loading input from root_dir=input
19:24:59,630 graphrag.index.input.load_input INFO using file storage for input
19:24:59,631 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:24:59,632 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:24:59,633 graphrag.index.input.text INFO Found 1 files, loading 1
19:24:59,633 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:24:59,633 graphrag.index.run.run INFO Final # of rows loaded: 1
19:24:59,742 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:24:59,744 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:25:00,26 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:25:00,150 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:25:00,150 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:25:00,158 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:25:01,721 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
19:25:01,723 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:25:01,725 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
19:25:01,725 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:25:01,727 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:25:01,738 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:25:35,752 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:25:35,753 graphrag.index.cli INFO Starting pipeline run for: 20241017-192535, dryrun=False
19:25:35,753 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "nltk"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:25:35,755 graphrag.index.create_pipeline_config INFO skipping workflows 
19:25:35,755 graphrag.index.run.run INFO Running pipeline
19:25:35,755 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:25:35,756 graphrag.index.input.load_input INFO loading input from root_dir=input
19:25:35,756 graphrag.index.input.load_input INFO using file storage for input
19:25:35,757 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:25:35,757 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:25:35,758 graphrag.index.input.text INFO Found 1 files, loading 1
19:25:35,759 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:25:35,759 graphrag.index.run.run INFO Final # of rows loaded: 1
19:25:35,867 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:25:35,870 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:25:36,112 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:25:36,236 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:25:36,236 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:25:36,244 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:25:37,785 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
19:25:37,786 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:25:37,793 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
19:25:37,793 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:25:37,795 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:25:37,806 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:28:43,780 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:28:43,781 graphrag.index.cli INFO Starting pipeline run for: 20241017-192843, dryrun=False
19:28:43,782 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "nltk"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:28:43,784 graphrag.index.create_pipeline_config INFO skipping workflows 
19:28:43,784 graphrag.index.run.run INFO Running pipeline
19:28:43,785 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:28:43,786 graphrag.index.input.load_input INFO loading input from root_dir=input
19:28:43,786 graphrag.index.input.load_input INFO using file storage for input
19:28:43,788 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:28:43,789 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:28:43,791 graphrag.index.input.text INFO Found 1 files, loading 1
19:28:43,791 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:28:43,791 graphrag.index.run.run INFO Final # of rows loaded: 1
19:28:43,903 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:28:43,905 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:28:44,271 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:28:44,396 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:28:44,396 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:28:44,404 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:28:45,974 graphrag.index.operations.cluster_graph WARNING Graph has no nodes
19:28:45,976 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:28:45,979 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Columns must be same length as key details=None
19:28:45,980 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 78, in create_base_entity_graph
    clustered = cluster_graph(
                ^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/cluster_graph.py", line 90, in cluster_graph
    output[[level_to, to]] = pd.DataFrame(output[to].tolist(), index=output.index)
    ~~~~~~^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4299, in __setitem__
    self._setitem_array(key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 4341, in _setitem_array
    check_key_length(self.columns, key, value)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexers/utils.py", line 390, in check_key_length
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
19:28:45,981 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:28:45,993 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:28:58,238 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:28:58,240 graphrag.index.cli INFO Starting pipeline run for: 20241017-192858, dryrun=False
19:28:58,242 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "sss"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:28:58,244 graphrag.index.create_pipeline_config INFO skipping workflows 
19:28:58,244 graphrag.index.run.run INFO Running pipeline
19:28:58,244 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:28:58,246 graphrag.index.input.load_input INFO loading input from root_dir=input
19:28:58,246 graphrag.index.input.load_input INFO using file storage for input
19:28:58,246 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:28:58,247 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:28:58,248 graphrag.index.input.text INFO Found 1 files, loading 1
19:28:58,249 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:28:58,249 graphrag.index.run.run INFO Final # of rows loaded: 1
19:28:58,359 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:28:58,362 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:28:58,594 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:28:58,718 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:28:58,718 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:28:58,726 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:28:58,726 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: sss
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: sss
19:28:58,727 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: sss details=None
19:28:58,728 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: sss
19:28:58,729 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:28:58,734 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:29:12,996 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:29:12,997 graphrag.index.cli INFO Starting pipeline run for: 20241017-192912, dryrun=False
19:29:12,998 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "graph_intelligence_json"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:29:13,0 graphrag.index.create_pipeline_config INFO skipping workflows 
19:29:13,0 graphrag.index.run.run INFO Running pipeline
19:29:13,0 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:29:13,1 graphrag.index.input.load_input INFO loading input from root_dir=input
19:29:13,1 graphrag.index.input.load_input INFO using file storage for input
19:29:13,6 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:29:13,7 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:29:13,8 graphrag.index.input.text INFO Found 1 files, loading 1
19:29:13,8 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:29:13,9 graphrag.index.run.run INFO Final # of rows loaded: 1
19:29:13,118 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:29:13,120 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:29:13,382 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:29:13,504 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:29:13,505 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:29:13,512 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:29:13,512 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: graph_intelligence_json
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: graph_intelligence_json
19:29:13,513 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: graph_intelligence_json details=None
19:29:13,514 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: graph_intelligence_json
19:29:13,515 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:29:13,520 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:29:40,349 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:29:40,351 graphrag.index.cli INFO Starting pipeline run for: 20241017-192940, dryrun=False
19:29:40,351 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "graph_intelligence_json"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:29:40,353 graphrag.index.create_pipeline_config INFO skipping workflows 
19:29:40,353 graphrag.index.run.run INFO Running pipeline
19:29:40,353 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:29:40,354 graphrag.index.input.load_input INFO loading input from root_dir=input
19:29:40,354 graphrag.index.input.load_input INFO using file storage for input
19:29:40,355 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:29:40,355 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:29:40,357 graphrag.index.input.text INFO Found 1 files, loading 1
19:29:40,357 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:29:40,357 graphrag.index.run.run INFO Final # of rows loaded: 1
19:29:40,468 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:29:40,471 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:29:40,711 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:29:40,843 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:29:40,843 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:29:40,851 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:29:40,851 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: graph_intelligence_json
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: graph_intelligence_json
19:29:40,852 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Unknown strategy: graph_intelligence_json details=None
19:29:40,853 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 117, in extract_entities
    strategy_exec = _load_strategy(
                    ^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 177, in _load_strategy
    raise ValueError(msg)
ValueError: Unknown strategy: graph_intelligence_json
19:29:40,854 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:29:40,859 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:29:54,106 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:29:54,107 graphrag.index.cli INFO Starting pipeline run for: 20241017-192954, dryrun=False
19:29:54,107 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": {
            "type": "graph_intelligence"
        },
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:29:54,109 graphrag.index.create_pipeline_config INFO skipping workflows 
19:29:54,109 graphrag.index.run.run INFO Running pipeline
19:29:54,110 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:29:54,112 graphrag.index.input.load_input INFO loading input from root_dir=input
19:29:54,112 graphrag.index.input.load_input INFO using file storage for input
19:29:54,113 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:29:54,113 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:29:54,115 graphrag.index.input.text INFO Found 1 files, loading 1
19:29:54,115 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:29:54,115 graphrag.index.run.run INFO Final # of rows loaded: 1
19:29:54,225 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:29:54,227 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:29:54,591 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:29:54,714 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:29:54,714 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:29:54,721 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:29:54,722 root ERROR parallel transformation error
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 39, in execute
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 128, in run_strategy
    result = await strategy_exec(
             ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/strategies/graph_intelligence.py", line 37, in run_graph_intelligence
    llm = load_llm("entity_extraction", llm_type, callbacks, cache, llm_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/llm/load_llm.py", line 61, in load_llm
    raise ValueError(msg)
ValueError: Unknown LLM type None
19:29:54,724 graphrag.callbacks.file_workflow_callbacks INFO parallel transformation error details=None
19:29:54,725 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete!
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 33, in derive_from_rows
    return await derive_from_rows_asyncio_threads(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio_threads.py", line 40, in derive_from_rows_asyncio_threads
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
19:29:54,726 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: 1 Errors occurred while running parallel transformation, could not complete! details=None
19:29:54,726 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 50, in create_base_entity_graph
    entities, entity_graphs = await extract_entities(
                              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/extract_entities/extract_entities.py", line 138, in extract_entities
    results = await derive_from_rows(
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows.py", line 33, in derive_from_rows
    return await derive_from_rows_asyncio_threads(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_asyncio_threads.py", line 40, in derive_from_rows_asyncio_threads
    return await derive_from_rows_base(input, transform, callbacks, gather)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/execution/derive_from_rows_base.py", line 57, in derive_from_rows_base
    raise VerbParallelizationError(len(errors))
datashaper.errors.VerbParallelizationError: 1 Errors occurred while running parallel transformation, could not complete!
19:29:54,727 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
19:29:54,732 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
19:30:13,30 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:30:13,32 graphrag.index.cli INFO Starting pipeline run for: 20241017-193013, dryrun=False
19:30:13,32 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:30:13,34 graphrag.index.create_pipeline_config INFO skipping workflows 
19:30:13,34 graphrag.index.run.run INFO Running pipeline
19:30:13,34 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:30:13,35 graphrag.index.input.load_input INFO loading input from root_dir=input
19:30:13,35 graphrag.index.input.load_input INFO using file storage for input
19:30:13,35 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:30:13,36 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:30:13,37 graphrag.index.input.text INFO Found 1 files, loading 1
19:30:13,37 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:30:13,38 graphrag.index.run.run INFO Final # of rows loaded: 1
19:30:13,151 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:30:13,154 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:30:13,441 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:30:13,566 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:30:13,566 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:30:13,574 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:30:13,575 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:30:13,613 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
19:30:13,613 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
19:30:14,782 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:14,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.172683088996564. input_tokens=1802, output_tokens=181
19:30:17,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:17,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2340460139967036. input_tokens=28, output_tokens=475
19:30:17,47 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
19:30:17,47 root INFO Starting at time 1729186217.0472403
19:30:17,47 root INFO Beginning preprocessing of transition probabilities for 10 vertices
19:30:17,47 root INFO Completed 1 / 10 vertices
19:30:17,47 root INFO Completed 2 / 10 vertices
19:30:17,50 root INFO Completed 3 / 10 vertices
19:30:17,50 root INFO Completed 4 / 10 vertices
19:30:17,50 root INFO Completed 5 / 10 vertices
19:30:17,50 root INFO Completed 6 / 10 vertices
19:30:17,50 root INFO Completed 7 / 10 vertices
19:30:17,51 root INFO Completed 8 / 10 vertices
19:30:17,52 root INFO Completed 9 / 10 vertices
19:30:17,53 root INFO Completed 10 / 10 vertices
19:30:17,54 root INFO Completed preprocessing of transition probabilities for vertices
19:30:17,55 root INFO Beginning preprocessing of transition probabilities for 11 edges
19:30:17,56 root INFO Completed 1 / 11 edges
19:30:17,56 root INFO Completed 2 / 11 edges
19:30:17,63 root INFO Completed 3 / 11 edges
19:30:17,63 root INFO Completed 4 / 11 edges
19:30:17,64 root INFO Completed 5 / 11 edges
19:30:17,65 root INFO Completed 6 / 11 edges
19:30:17,66 root INFO Completed 7 / 11 edges
19:30:17,66 root INFO Completed 8 / 11 edges
19:30:17,66 root INFO Completed 9 / 11 edges
19:30:17,67 root INFO Completed 10 / 11 edges
19:30:17,67 root INFO Completed 11 / 11 edges
19:30:17,68 root INFO Completed preprocessing of transition probabilities for edges
19:30:17,75 root INFO Simulating walks on graph at time 1729186217.075787
19:30:17,77 root INFO Walk iteration: 1/10
19:30:17,79 root INFO Walk iteration: 2/10
19:30:17,80 root INFO Walk iteration: 3/10
19:30:17,81 root INFO Walk iteration: 4/10
19:30:17,83 root INFO Walk iteration: 5/10
19:30:17,84 root INFO Walk iteration: 6/10
19:30:17,85 root INFO Walk iteration: 7/10
19:30:17,86 root INFO Walk iteration: 8/10
19:30:17,87 root INFO Walk iteration: 9/10
19:30:17,88 root INFO Walk iteration: 10/10
19:30:17,89 root INFO Learning embeddings at time 1729186217.0894423
19:30:17,89 gensim.models.word2vec INFO collecting all words and their counts
19:30:17,90 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:30:17,91 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
19:30:17,91 gensim.models.word2vec INFO Creating a fresh vocabulary
19:30:17,92 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T19:30:17.092934', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:17,93 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T19:30:17.093598', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:17,94 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
19:30:17,95 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
19:30:17,95 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T19:30:17.095859', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:17,96 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
19:30:17,97 gensim.models.word2vec INFO resetting layer weights
19:30:17,97 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T19:30:17.097840', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:30:17,98 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T19:30:17.098191', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:30:17,105 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 77978 effective words/s
19:30:17,111 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 75037 effective words/s
19:30:17,116 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 73244 effective words/s
19:30:17,117 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 34898 effective words/s', 'datetime': '2024-10-17T19:30:17.117029', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:30:17,117 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T19:30:17.117818', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:30:17,118 root INFO Completed. Ending time is 1729186217.1186066 Elapsed time is -0.0713663101196289
19:30:17,149 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:30:17,296 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:30:17,297 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:17,305 datashaper.workflow.workflow INFO executing verb create_final_entities
19:30:17,308 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:30:17,345 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
19:30:17,345 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
19:30:17,345 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:30:18,517 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:30:18,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1853262420045212. input_tokens=158, output_tokens=0
19:30:18,544 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:30:18,685 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:30:18,686 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:18,694 datashaper.workflow.workflow INFO executing verb create_final_nodes
19:30:21,51 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:30:21,198 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:30:21,198 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:21,207 datashaper.workflow.workflow INFO executing verb create_final_communities
19:30:21,218 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:30:21,345 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
19:30:21,345 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:30:21,351 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:21,358 datashaper.workflow.workflow INFO executing verb create_final_relationships
19:30:21,363 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:30:21,501 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
19:30:21,501 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:30:21,506 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:30:21,507 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
19:30:21,515 datashaper.workflow.workflow INFO executing verb create_final_text_units
19:30:21,524 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:30:21,663 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
19:30:21,664 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:30:21,669 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:30:21,677 datashaper.workflow.workflow INFO executing verb create_final_community_reports
19:30:21,680 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
19:30:24,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:24,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5097190070082434. input_tokens=2071, output_tokens=386
19:30:28,433 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:28,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.225745124000241. input_tokens=2240, output_tokens=740
19:30:28,454 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:30:28,594 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
19:30:28,594 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
19:30:28,604 datashaper.workflow.workflow INFO executing verb create_final_documents
19:30:28,611 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:30:28,628 graphrag.index.cli INFO All workflows completed successfully.
19:30:41,889 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
19:30:41,891 graphrag.index.cli INFO Starting pipeline run for: 20241017-193041, dryrun=False
19:30:41,891 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
19:30:41,893 graphrag.index.create_pipeline_config INFO skipping workflows 
19:30:41,893 graphrag.index.run.run INFO Running pipeline
19:30:41,893 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
19:30:41,893 graphrag.index.input.load_input INFO loading input from root_dir=input
19:30:41,894 graphrag.index.input.load_input INFO using file storage for input
19:30:41,895 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.txt$
19:30:41,895 graphrag.index.input.text INFO found text files from input, found [('doc.txt', {})]
19:30:41,897 graphrag.index.input.text INFO Found 1 files, loading 1
19:30:41,897 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
19:30:41,897 graphrag.index.run.run INFO Final # of rows loaded: 1
19:30:42,5 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:30:42,8 datashaper.workflow.workflow INFO executing verb create_base_text_units
19:30:42,272 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:30:42,394 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
19:30:42,395 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:30:42,402 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
19:30:42,403 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:30:42,441 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
19:30:42,441 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
19:30:44,23 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:44,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5856540910026524. input_tokens=1802, output_tokens=181
19:30:46,197 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:46,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.1692091849981807. input_tokens=28, output_tokens=475
19:30:46,207 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
19:30:46,207 root INFO Starting at time 1729186246.2074902
19:30:46,207 root INFO Beginning preprocessing of transition probabilities for 10 vertices
19:30:46,207 root INFO Completed 1 / 10 vertices
19:30:46,207 root INFO Completed 2 / 10 vertices
19:30:46,207 root INFO Completed 3 / 10 vertices
19:30:46,213 root INFO Completed 4 / 10 vertices
19:30:46,213 root INFO Completed 5 / 10 vertices
19:30:46,214 root INFO Completed 6 / 10 vertices
19:30:46,214 root INFO Completed 7 / 10 vertices
19:30:46,214 root INFO Completed 8 / 10 vertices
19:30:46,214 root INFO Completed 9 / 10 vertices
19:30:46,215 root INFO Completed 10 / 10 vertices
19:30:46,215 root INFO Completed preprocessing of transition probabilities for vertices
19:30:46,215 root INFO Beginning preprocessing of transition probabilities for 11 edges
19:30:46,216 root INFO Completed 1 / 11 edges
19:30:46,216 root INFO Completed 2 / 11 edges
19:30:46,217 root INFO Completed 3 / 11 edges
19:30:46,217 root INFO Completed 4 / 11 edges
19:30:46,224 root INFO Completed 5 / 11 edges
19:30:46,224 root INFO Completed 6 / 11 edges
19:30:46,225 root INFO Completed 7 / 11 edges
19:30:46,225 root INFO Completed 8 / 11 edges
19:30:46,225 root INFO Completed 9 / 11 edges
19:30:46,225 root INFO Completed 10 / 11 edges
19:30:46,226 root INFO Completed 11 / 11 edges
19:30:46,226 root INFO Completed preprocessing of transition probabilities for edges
19:30:46,227 root INFO Simulating walks on graph at time 1729186246.2273765
19:30:46,227 root INFO Walk iteration: 1/10
19:30:46,228 root INFO Walk iteration: 2/10
19:30:46,229 root INFO Walk iteration: 3/10
19:30:46,230 root INFO Walk iteration: 4/10
19:30:46,230 root INFO Walk iteration: 5/10
19:30:46,231 root INFO Walk iteration: 6/10
19:30:46,232 root INFO Walk iteration: 7/10
19:30:46,233 root INFO Walk iteration: 8/10
19:30:46,233 root INFO Walk iteration: 9/10
19:30:46,234 root INFO Walk iteration: 10/10
19:30:46,235 root INFO Learning embeddings at time 1729186246.2350228
19:30:46,235 gensim.models.word2vec INFO collecting all words and their counts
19:30:46,236 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:30:46,236 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
19:30:46,237 gensim.models.word2vec INFO Creating a fresh vocabulary
19:30:46,237 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T19:30:46.237848', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:46,238 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T19:30:46.238517', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:46,239 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
19:30:46,239 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
19:30:46,240 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T19:30:46.240007', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:30:46,240 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
19:30:46,241 gensim.models.word2vec INFO resetting layer weights
19:30:46,241 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T19:30:46.241744', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:30:46,242 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T19:30:46.242306', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:30:46,244 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 310123 effective words/s
19:30:46,246 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 330125 effective words/s
19:30:46,254 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 329618 effective words/s
19:30:46,254 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 53745 effective words/s', 'datetime': '2024-10-17T19:30:46.254773', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:30:46,255 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T19:30:46.255599', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:30:46,256 root INFO Completed. Ending time is 1729186246.25645 Elapsed time is -0.04895973205566406
19:30:46,278 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:30:46,413 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:30:46,413 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:46,421 datashaper.workflow.workflow INFO executing verb create_final_entities
19:30:46,424 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:30:46,462 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
19:30:46,462 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
19:30:46,462 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
19:30:47,512 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:30:47,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.063088555005379. input_tokens=158, output_tokens=0
19:30:47,533 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:30:47,675 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:30:47,675 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:47,683 datashaper.workflow.workflow INFO executing verb create_final_nodes
19:30:50,132 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:30:50,278 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:30:50,278 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:50,290 datashaper.workflow.workflow INFO executing verb create_final_communities
19:30:50,301 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:30:50,429 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
19:30:50,429 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:30:50,435 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:30:50,442 datashaper.workflow.workflow INFO executing verb create_final_relationships
19:30:50,447 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:30:50,584 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
19:30:50,584 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:30:50,589 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:30:50,591 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
19:30:50,598 datashaper.workflow.workflow INFO executing verb create_final_text_units
19:30:50,607 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:30:50,738 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
19:30:50,739 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:30:50,743 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:30:50,754 datashaper.workflow.workflow INFO executing verb create_final_community_reports
19:30:50,757 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
19:30:52,953 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:52,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.185734863000107. input_tokens=2071, output_tokens=382
19:30:56,569 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:30:56,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.610381154008792. input_tokens=2240, output_tokens=748
19:30:56,592 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:30:56,730 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
19:30:56,730 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
19:30:56,741 datashaper.workflow.workflow INFO executing verb create_final_documents
19:30:56,747 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:30:56,764 graphrag.index.cli INFO All workflows completed successfully.
20:26:58,383 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:26:58,385 graphrag.index.cli INFO Starting pipeline run for: 20241017-202658, dryrun=False
20:26:58,385 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
20:26:58,387 graphrag.index.create_pipeline_config INFO skipping workflows 
20:26:58,387 graphrag.index.run.run INFO Running pipeline
20:26:58,387 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:26:58,389 graphrag.index.input.load_input INFO loading input from root_dir=input
20:26:58,389 graphrag.index.input.load_input INFO using file storage for input
20:26:58,391 graphrag.index.input.csv INFO Loading csv files from input
20:26:58,391 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
20:35:37,48 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:35:37,50 graphrag.index.cli INFO Starting pipeline run for: 20241017-203537, dryrun=False
20:35:37,50 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
20:35:37,52 graphrag.index.create_pipeline_config INFO skipping workflows 
20:35:37,52 graphrag.index.run.run INFO Running pipeline
20:35:37,53 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:35:37,55 graphrag.index.input.load_input INFO loading input from root_dir=input
20:35:37,55 graphrag.index.input.load_input INFO using file storage for input
20:35:37,56 graphrag.index.input.csv INFO Loading csv files from input
20:35:37,56 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
20:35:37,75 graphrag.index.input.csv INFO Found 1 csv files, loading 1
20:35:37,75 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
20:35:37,77 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
20:35:37,77 graphrag.index.run.run INFO Final # of rows loaded: 1
20:35:37,189 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:35:37,192 datashaper.workflow.workflow INFO executing verb create_base_text_units
20:35:37,573 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:35:37,696 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
20:35:37,696 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:35:37,704 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
20:35:37,705 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:35:37,743 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
20:35:37,743 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:35:39,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:35:39,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6513671430002432. input_tokens=1802, output_tokens=178
20:35:41,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:35:41,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7960298489924753. input_tokens=28, output_tokens=388
20:35:41,211 root INFO Starting preprocessing of transition probabilities on graph with 8 nodes and 10 edges
20:35:41,211 root INFO Starting at time 1729190141.2110684
20:35:41,211 root INFO Beginning preprocessing of transition probabilities for 8 vertices
20:35:41,211 root INFO Completed 1 / 8 vertices
20:35:41,211 root INFO Completed 2 / 8 vertices
20:35:41,211 root INFO Completed 3 / 8 vertices
20:35:41,211 root INFO Completed 4 / 8 vertices
20:35:41,212 root INFO Completed 5 / 8 vertices
20:35:41,212 root INFO Completed 6 / 8 vertices
20:35:41,213 root INFO Completed 7 / 8 vertices
20:35:41,214 root INFO Completed 8 / 8 vertices
20:35:41,215 root INFO Completed preprocessing of transition probabilities for vertices
20:35:41,216 root INFO Beginning preprocessing of transition probabilities for 10 edges
20:35:41,216 root INFO Completed 1 / 10 edges
20:35:41,217 root INFO Completed 2 / 10 edges
20:35:41,218 root INFO Completed 3 / 10 edges
20:35:41,218 root INFO Completed 4 / 10 edges
20:35:41,219 root INFO Completed 5 / 10 edges
20:35:41,219 root INFO Completed 6 / 10 edges
20:35:41,219 root INFO Completed 7 / 10 edges
20:35:41,219 root INFO Completed 8 / 10 edges
20:35:41,220 root INFO Completed 9 / 10 edges
20:35:41,220 root INFO Completed 10 / 10 edges
20:35:41,220 root INFO Completed preprocessing of transition probabilities for edges
20:35:41,220 root INFO Simulating walks on graph at time 1729190141.2208004
20:35:41,221 root INFO Walk iteration: 1/10
20:35:41,223 root INFO Walk iteration: 2/10
20:35:41,223 root INFO Walk iteration: 3/10
20:35:41,224 root INFO Walk iteration: 4/10
20:35:41,225 root INFO Walk iteration: 5/10
20:35:41,225 root INFO Walk iteration: 6/10
20:35:41,226 root INFO Walk iteration: 7/10
20:35:41,226 root INFO Walk iteration: 8/10
20:35:41,227 root INFO Walk iteration: 9/10
20:35:41,228 root INFO Walk iteration: 10/10
20:35:41,229 root INFO Learning embeddings at time 1729190141.2290814
20:35:41,229 gensim.models.word2vec INFO collecting all words and their counts
20:35:41,230 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:35:41,230 gensim.models.word2vec INFO collected 8 word types from a corpus of 1500 raw words and 80 sentences
20:35:41,231 gensim.models.word2vec INFO Creating a fresh vocabulary
20:35:41,232 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 8 unique words (100.00% of original 8, drops 0)', 'datetime': '2024-10-17T20:35:41.232362', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:35:41,232 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 1500 word corpus (100.00% of original 1500, drops 0)', 'datetime': '2024-10-17T20:35:41.232963', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:35:41,233 gensim.models.word2vec INFO deleting the raw counts dictionary of 8 items
20:35:41,234 gensim.models.word2vec INFO sample=0.001 downsamples 8 most-common words
20:35:41,234 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 136.66562766264752 word corpus (9.1%% of prior 1500)', 'datetime': '2024-10-17T20:35:41.234494', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:35:41,235 gensim.models.word2vec INFO estimated required memory for 8 words and 1536 dimensions: 102304 bytes
20:35:41,235 gensim.models.word2vec INFO resetting layer weights
20:35:41,236 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T20:35:41.236083', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:35:41,236 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 8 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T20:35:41.236439', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:35:41,241 gensim.models.word2vec INFO EPOCH 0: training on 1500 raw words (142 effective words) took 0.0s, 413854 effective words/s
20:35:41,245 gensim.models.word2vec INFO EPOCH 1: training on 1500 raw words (140 effective words) took 0.0s, 79946 effective words/s
20:35:41,249 gensim.models.word2vec INFO EPOCH 2: training on 1500 raw words (129 effective words) took 0.0s, 77736 effective words/s
20:35:41,249 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 4500 raw words (411 effective words) took 0.0s, 32464 effective words/s', 'datetime': '2024-10-17T20:35:41.249729', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:35:41,250 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=8, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T20:35:41.250469', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:35:41,251 root INFO Completed. Ending time is 1729190141.2510705 Elapsed time is -0.04000210762023926
20:35:41,273 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
20:35:41,412 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
20:35:41,413 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:35:41,420 datashaper.workflow.workflow INFO executing verb create_final_entities
20:35:41,424 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:35:41,461 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
20:35:41,461 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
20:35:41,461 graphrag.index.operations.embed_text.strategies.openai INFO embedding 8 inputs via 7 snippets using 1 batches. max_batch_size=16, max_tokens=8191
20:35:42,494 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
20:35:42,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.051785824005492. input_tokens=129, output_tokens=0
20:35:42,520 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
20:35:42,653 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
20:35:42,654 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:35:42,661 datashaper.workflow.workflow INFO executing verb create_final_nodes
20:35:45,27 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
20:35:45,175 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
20:35:45,177 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:35:45,184 datashaper.workflow.workflow INFO executing verb create_final_communities
20:35:45,194 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
20:35:45,322 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
20:35:45,322 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:35:45,327 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:35:45,334 datashaper.workflow.workflow INFO executing verb create_final_relationships
20:35:45,340 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
20:35:45,472 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
20:35:45,472 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:35:45,477 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:35:45,479 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
20:35:45,486 datashaper.workflow.workflow INFO executing verb create_final_text_units
20:35:45,496 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
20:35:45,624 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
20:35:45,625 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:35:45,642 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:35:45,650 datashaper.workflow.workflow INFO executing verb create_final_community_reports
20:35:45,653 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 8
20:35:48,172 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:35:48,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.50685416199849. input_tokens=2074, output_tokens=471
20:35:51,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:35:51,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5533001970034093. input_tokens=2100, output_tokens=570
20:35:54,233 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:35:54,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4937131789920386. input_tokens=2075, output_tokens=444
20:35:54,250 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
20:35:54,393 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
20:35:54,393 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
20:35:54,403 datashaper.workflow.workflow INFO executing verb create_final_documents
20:35:54,410 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
20:35:54,429 graphrag.index.cli INFO All workflows completed successfully.
20:36:05,854 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:36:05,856 graphrag.index.cli INFO Starting pipeline run for: 20241017-203605, dryrun=False
20:36:05,856 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
20:36:05,857 graphrag.index.create_pipeline_config INFO skipping workflows 
20:36:05,857 graphrag.index.run.run INFO Running pipeline
20:36:05,858 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:36:05,860 graphrag.index.input.load_input INFO loading input from root_dir=input
20:36:05,860 graphrag.index.input.load_input INFO using file storage for input
20:36:05,861 graphrag.index.input.csv INFO Loading csv files from input
20:36:05,861 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
20:36:05,864 graphrag.index.input.csv INFO Found 1 csv files, loading 1
20:36:05,864 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
20:36:05,865 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
20:36:05,866 graphrag.index.run.run INFO Final # of rows loaded: 1
20:36:05,974 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:36:05,976 datashaper.workflow.workflow INFO executing verb create_base_text_units
20:36:06,211 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:36:06,340 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
20:36:06,340 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:36:06,347 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
20:36:06,348 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:36:06,387 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
20:36:06,387 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:36:08,21 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:36:08,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6404613360064104. input_tokens=1802, output_tokens=181
20:36:10,186 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:36:10,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.15958620600577. input_tokens=28, output_tokens=475
20:36:10,206 root INFO Starting preprocessing of transition probabilities on graph with 10 nodes and 11 edges
20:36:10,206 root INFO Starting at time 1729190170.206662
20:36:10,206 root INFO Beginning preprocessing of transition probabilities for 10 vertices
20:36:10,206 root INFO Completed 1 / 10 vertices
20:36:10,206 root INFO Completed 2 / 10 vertices
20:36:10,206 root INFO Completed 3 / 10 vertices
20:36:10,206 root INFO Completed 4 / 10 vertices
20:36:10,207 root INFO Completed 5 / 10 vertices
20:36:10,207 root INFO Completed 6 / 10 vertices
20:36:10,207 root INFO Completed 7 / 10 vertices
20:36:10,207 root INFO Completed 8 / 10 vertices
20:36:10,208 root INFO Completed 9 / 10 vertices
20:36:10,208 root INFO Completed 10 / 10 vertices
20:36:10,208 root INFO Completed preprocessing of transition probabilities for vertices
20:36:10,208 root INFO Beginning preprocessing of transition probabilities for 11 edges
20:36:10,208 root INFO Completed 1 / 11 edges
20:36:10,208 root INFO Completed 2 / 11 edges
20:36:10,209 root INFO Completed 3 / 11 edges
20:36:10,209 root INFO Completed 4 / 11 edges
20:36:10,209 root INFO Completed 5 / 11 edges
20:36:10,209 root INFO Completed 6 / 11 edges
20:36:10,210 root INFO Completed 7 / 11 edges
20:36:10,210 root INFO Completed 8 / 11 edges
20:36:10,210 root INFO Completed 9 / 11 edges
20:36:10,210 root INFO Completed 10 / 11 edges
20:36:10,211 root INFO Completed 11 / 11 edges
20:36:10,211 root INFO Completed preprocessing of transition probabilities for edges
20:36:10,211 root INFO Simulating walks on graph at time 1729190170.2115746
20:36:10,212 root INFO Walk iteration: 1/10
20:36:10,213 root INFO Walk iteration: 2/10
20:36:10,214 root INFO Walk iteration: 3/10
20:36:10,214 root INFO Walk iteration: 4/10
20:36:10,215 root INFO Walk iteration: 5/10
20:36:10,216 root INFO Walk iteration: 6/10
20:36:10,217 root INFO Walk iteration: 7/10
20:36:10,217 root INFO Walk iteration: 8/10
20:36:10,218 root INFO Walk iteration: 9/10
20:36:10,219 root INFO Walk iteration: 10/10
20:36:10,220 root INFO Learning embeddings at time 1729190170.2200534
20:36:10,220 gensim.models.word2vec INFO collecting all words and their counts
20:36:10,220 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:36:10,221 gensim.models.word2vec INFO collected 10 word types from a corpus of 2000 raw words and 100 sentences
20:36:10,222 gensim.models.word2vec INFO Creating a fresh vocabulary
20:36:10,222 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 10 unique words (100.00% of original 10, drops 0)', 'datetime': '2024-10-17T20:36:10.222387', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:36:10,223 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2000 word corpus (100.00% of original 2000, drops 0)', 'datetime': '2024-10-17T20:36:10.223112', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:36:10,223 gensim.models.word2vec INFO deleting the raw counts dictionary of 10 items
20:36:10,224 gensim.models.word2vec INFO sample=0.001 downsamples 10 most-common words
20:36:10,224 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 206.25916641138846 word corpus (10.3%% of prior 2000)', 'datetime': '2024-10-17T20:36:10.224452', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:36:10,225 gensim.models.word2vec INFO estimated required memory for 10 words and 1536 dimensions: 127880 bytes
20:36:10,225 gensim.models.word2vec INFO resetting layer weights
20:36:10,226 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-17T20:36:10.226618', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:36:10,227 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 10 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-17T20:36:10.227074', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:36:10,231 gensim.models.word2vec INFO EPOCH 0: training on 2000 raw words (214 effective words) took 0.0s, 270233 effective words/s
20:36:10,234 gensim.models.word2vec INFO EPOCH 1: training on 2000 raw words (208 effective words) took 0.0s, 140816 effective words/s
20:36:10,237 gensim.models.word2vec INFO EPOCH 2: training on 2000 raw words (211 effective words) took 0.0s, 127666 effective words/s
20:36:10,237 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 6000 raw words (633 effective words) took 0.0s, 62138 effective words/s', 'datetime': '2024-10-17T20:36:10.237871', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:36:10,238 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=10, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-17T20:36:10.238623', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:36:10,239 root INFO Completed. Ending time is 1729190170.239351 Elapsed time is -0.03268909454345703
20:36:10,275 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
20:36:10,407 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
20:36:10,407 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:36:10,416 datashaper.workflow.workflow INFO executing verb create_final_entities
20:36:10,419 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:36:10,455 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
20:36:10,455 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
20:36:10,456 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
20:36:11,537 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
20:36:11,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0853320629976224. input_tokens=158, output_tokens=0
20:36:11,558 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
20:36:11,701 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
20:36:11,701 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:36:11,709 datashaper.workflow.workflow INFO executing verb create_final_nodes
20:36:14,111 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
20:36:14,254 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
20:36:14,254 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:36:14,263 datashaper.workflow.workflow INFO executing verb create_final_communities
20:36:14,274 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
20:36:14,401 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
20:36:14,401 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:36:14,407 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:36:14,414 datashaper.workflow.workflow INFO executing verb create_final_relationships
20:36:14,419 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
20:36:14,552 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
20:36:14,552 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:36:14,556 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
20:36:14,559 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:36:14,569 datashaper.workflow.workflow INFO executing verb create_final_text_units
20:36:14,583 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
20:36:14,713 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
20:36:14,714 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:36:14,718 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:36:14,726 datashaper.workflow.workflow INFO executing verb create_final_community_reports
20:36:14,729 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 10
20:36:16,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:36:16,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7576073969976278. input_tokens=2071, output_tokens=308
20:36:20,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:36:20,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6861696889973246. input_tokens=2240, output_tokens=748
20:36:20,199 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
20:36:20,339 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
20:36:20,339 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
20:36:20,350 datashaper.workflow.workflow INFO executing verb create_final_documents
20:36:20,356 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
20:36:20,372 graphrag.index.cli INFO All workflows completed successfully.
20:38:08,285 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
20:38:08,287 graphrag.index.cli INFO Starting pipeline run for: 20241017-203808, dryrun=False
20:38:08,287 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
20:38:08,289 graphrag.index.create_pipeline_config INFO skipping workflows 
20:38:08,289 graphrag.index.run.run INFO Running pipeline
20:38:08,289 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
20:38:08,290 graphrag.index.input.load_input INFO loading input from root_dir=input
20:38:08,290 graphrag.index.input.load_input INFO using file storage for input
20:38:08,291 graphrag.index.input.csv INFO Loading csv files from input
20:38:08,291 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
20:38:08,343 graphrag.index.input.csv INFO Found 1 csv files, loading 1
20:38:08,343 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 3231
20:38:08,344 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
20:38:08,344 graphrag.index.run.run INFO Final # of rows loaded: 3231
20:38:08,453 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:38:08,456 datashaper.workflow.workflow INFO executing verb create_base_text_units
20:38:09,178 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:38:09,340 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
20:38:09,340 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:38:09,353 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
20:38:09,403 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:38:09,439 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
20:38:09,439 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:38:13,948 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:13,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.513246240006993. input_tokens=1797, output_tokens=962
20:38:17,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:17,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.522479288993054. input_tokens=28, output_tokens=892
20:38:19,175 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:19,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6943080950004514. input_tokens=1803, output_tokens=370
20:38:20,606 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:20,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4292656250036089. input_tokens=28, output_tokens=299
20:38:21,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:21,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.1754359109909274. input_tokens=1791, output_tokens=189
20:38:23,237 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:23,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4408407820010325. input_tokens=28, output_tokens=271
20:38:41,710 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:41,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.463493353003287. input_tokens=1895, output_tokens=4112
20:38:54,197 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:38:54,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.48246519100212. input_tokens=28, output_tokens=4003
20:38:56,89 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:39:02,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.876468507995014. input_tokens=1814, output_tokens=376
20:39:10,408 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:39:10,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.3045952360116644. input_tokens=28, output_tokens=349
20:39:21,236 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:39:27,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.816404723009327. input_tokens=1876, output_tokens=1194
20:39:35,304 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:39:35,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.051714291999815. input_tokens=28, output_tokens=491
20:39:42,384 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
