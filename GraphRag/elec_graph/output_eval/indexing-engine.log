17:27:32,431 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
17:27:32,432 graphrag.index.cli INFO Starting pipeline run for: 20241101-172732, dryrun=False
17:27:32,433 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
17:27:32,434 graphrag.index.create_pipeline_config INFO skipping workflows 
17:27:32,434 graphrag.index.run.run INFO Running pipeline
17:27:32,435 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
17:27:32,436 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
17:27:32,436 graphrag.index.input.load_input INFO using file storage for input
17:27:32,437 graphrag.index.input.csv INFO Loading csv files from input_eval
17:27:32,437 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
17:27:32,440 graphrag.index.input.csv INFO Found 1 csv files, loading 1
17:27:32,440 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
17:27:32,441 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
17:27:32,442 graphrag.index.run.run INFO Final # of rows loaded: 1
17:27:32,551 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:27:32,553 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:27:34,833 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
17:27:34,958 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:27:34,958 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:27:34,966 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:27:34,983 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:27:35,21 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
17:27:35,21 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
17:27:35,46 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and identify all entities from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Which class the entity belogns to? The class has to be an specific term\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n######################\n-Real Data-\n######################\nText:\nPrinted circuit board\nElectronics assemblies are based on use of a printed circuit board of one\nform or another to hold components. Construction of these printed circuit\nboards is critical to soldering processes in that different printed circuit\nboard types have different thermal characteristics which can greatly affect\nhow they must be soldered.\nIn principle a printed circuit board (PCB) sometimes called a printed\nwiring board (PWB) or simply printed board comprises: a base which is\na thin board of insulating material supporting all the components which\nmake up a circuit; conducting tracks usually copper on one or bOth sides\nof the base making up the interconnections between components. Component\nconnecting leads are electrically connected in some form of permanent\nor semi-permanent way usually by soldering to lands sometimes called\npads ~ the areas of track specially designated for component connection\npurposes. If lands have holes drilled or punched through the board to\nfacilitate component mounting the board is a through-hole printed circuit\nboard. If lands have no holes the board is a surface mounted printed circuit\nboard.\nTo clarify the term printed is somewhat misleading as tracks are not\nprinted directly onto the board. It refers instead to just one stage within the\nwhole printed circuit board manufacturing process where the conducting\ntrack layout sometimes called pattern or image may be produced using\nsome form of printing technique.\nPrinted circuit boards can be made in one of two main ways. First in an\nadditive process the conductive track may be added to the surface of the\nbase material. There\'s a number of ways in which this can be done. Second\nin a subtractive process where base material is supplied with its whole\nsurface covered with a conductive layer track pattern is defined and excess\nconductive material is removed leaving the required track. Sometimes\nboth processes may be combined to produce printed circuit boards with\nmore than one layer of conductive track.\n######################\nOutput:'}
17:27:36,855 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and identify all entities from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Which class the entity belogns to? The class has to be an specific term\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n######################\n-Real Data-\n######################\nText:\nPrinted circuit board\nElectronics assemblies are based on use of a printed circuit board of one\nform or another to hold components. Construction of these printed circuit\nboards is critical to soldering processes in that different printed circuit\nboard types have different thermal characteristics which can greatly affect\nhow they must be soldered.\nIn principle a printed circuit board (PCB) sometimes called a printed\nwiring board (PWB) or simply printed board comprises: a base which is\na thin board of insulating material supporting all the components which\nmake up a circuit; conducting tracks usually copper on one or bOth sides\nof the base making up the interconnections between components. Component\nconnecting leads are electrically connected in some form of permanent\nor semi-permanent way usually by soldering to lands sometimes called\npads ~ the areas of track specially designated for component connection\npurposes. If lands have holes drilled or punched through the board to\nfacilitate component mounting the board is a through-hole printed circuit\nboard. If lands have no holes the board is a surface mounted printed circuit\nboard.\nTo clarify the term printed is somewhat misleading as tracks are not\nprinted directly onto the board. It refers instead to just one stage within the\nwhole printed circuit board manufacturing process where the conducting\ntrack layout sometimes called pattern or image may be produced using\nsome form of printing technique.\nPrinted circuit boards can be made in one of two main ways. First in an\nadditive process the conductive track may be added to the surface of the\nbase material. There\'s a number of ways in which this can be done. Second\nin a subtractive process where base material is supplied with its whole\nsurface covered with a conductive layer track pattern is defined and excess\nconductive material is removed leaving the required track. Sometimes\nboth processes may be combined to produce printed circuit boards with\nmore than one layer of conductive track.\n######################\nOutput:'}
17:27:39,200 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and identify all entities from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: Which class the entity belogns to? The class has to be an specific term\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n######################\n-Real Data-\n######################\nText:\nPrinted circuit board\nElectronics assemblies are based on use of a printed circuit board of one\nform or another to hold components. Construction of these printed circuit\nboards is critical to soldering processes in that different printed circuit\nboard types have different thermal characteristics which can greatly affect\nhow they must be soldered.\nIn principle a printed circuit board (PCB) sometimes called a printed\nwiring board (PWB) or simply printed board comprises: a base which is\na thin board of insulating material supporting all the components which\nmake up a circuit; conducting tracks usually copper on one or bOth sides\nof the base making up the interconnections between components. Component\nconnecting leads are electrically connected in some form of permanent\nor semi-permanent way usually by soldering to lands sometimes called\npads ~ the areas of track specially designated for component connection\npurposes. If lands have holes drilled or punched through the board to\nfacilitate component mounting the board is a through-hole printed circuit\nboard. If lands have no holes the board is a surface mounted printed circuit\nboard.\nTo clarify the term printed is somewhat misleading as tracks are not\nprinted directly onto the board. It refers instead to just one stage within the\nwhole printed circuit board manufacturing process where the conducting\ntrack layout sometimes called pattern or image may be produced using\nsome form of printing technique.\nPrinted circuit boards can be made in one of two main ways. First in an\nadditive process the conductive track may be added to the surface of the\nbase material. There\'s a number of ways in which this can be done. Second\nin a subtractive process where base material is supplied with its whole\nsurface covered with a conductive layer track pattern is defined and excess\nconductive material is removed leaving the required track. Sometimes\nboth processes may be combined to produce printed circuit boards with\nmore than one layer of conductive track.\n######################\nOutput:'}
17:27:46,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:27:46,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 2.43818480707705. input_tokens=2089, output_tokens=537
17:27:50,197 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:27:50,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.8983835568651557. input_tokens=28, output_tokens=921
17:27:50,763 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:27:50,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5632481111679226. input_tokens=30, output_tokens=1
17:27:53,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:27:53,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.8733228009659797. input_tokens=28, output_tokens=615
17:27:53,699 root INFO Starting preprocessing of transition probabilities on graph with 23 nodes and 22 edges
17:27:53,699 root INFO Starting at time 1730478473.6998258
17:27:53,699 root INFO Beginning preprocessing of transition probabilities for 23 vertices
17:27:53,699 root INFO Completed 1 / 23 vertices
17:27:53,700 root INFO Completed 3 / 23 vertices
17:27:53,700 root INFO Completed 5 / 23 vertices
17:27:53,700 root INFO Completed 7 / 23 vertices
17:27:53,701 root INFO Completed 9 / 23 vertices
17:27:53,702 root INFO Completed 11 / 23 vertices
17:27:53,703 root INFO Completed 13 / 23 vertices
17:27:53,703 root INFO Completed 15 / 23 vertices
17:27:53,704 root INFO Completed 17 / 23 vertices
17:27:53,704 root INFO Completed 19 / 23 vertices
17:27:53,705 root INFO Completed 21 / 23 vertices
17:27:53,706 root INFO Completed 23 / 23 vertices
17:27:53,707 root INFO Completed preprocessing of transition probabilities for vertices
17:27:53,707 root INFO Beginning preprocessing of transition probabilities for 22 edges
17:27:53,707 root INFO Completed 1 / 22 edges
17:27:53,708 root INFO Completed 3 / 22 edges
17:27:53,709 root INFO Completed 5 / 22 edges
17:27:53,709 root INFO Completed 7 / 22 edges
17:27:53,710 root INFO Completed 9 / 22 edges
17:27:53,711 root INFO Completed 11 / 22 edges
17:27:53,712 root INFO Completed 13 / 22 edges
17:27:53,712 root INFO Completed 15 / 22 edges
17:27:53,713 root INFO Completed 17 / 22 edges
17:27:53,714 root INFO Completed 19 / 22 edges
17:27:53,715 root INFO Completed 21 / 22 edges
17:27:53,715 root INFO Completed preprocessing of transition probabilities for edges
17:27:53,715 root INFO Simulating walks on graph at time 1730478473.7152255
17:27:53,716 root INFO Walk iteration: 1/10
17:27:53,719 root INFO Walk iteration: 2/10
17:27:53,720 root INFO Walk iteration: 3/10
17:27:53,722 root INFO Walk iteration: 4/10
17:27:53,723 root INFO Walk iteration: 5/10
17:27:53,724 root INFO Walk iteration: 6/10
17:27:53,725 root INFO Walk iteration: 7/10
17:27:53,726 root INFO Walk iteration: 8/10
17:27:53,727 root INFO Walk iteration: 9/10
17:27:53,727 root INFO Walk iteration: 10/10
17:27:53,728 root INFO Learning embeddings at time 1730478473.7283804
17:27:53,728 gensim.models.word2vec INFO collecting all words and their counts
17:27:53,729 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
17:27:53,730 gensim.models.word2vec INFO collected 23 word types from a corpus of 3440 raw words and 230 sentences
17:27:53,730 gensim.models.word2vec INFO Creating a fresh vocabulary
17:27:53,730 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 23 unique words (100.00% of original 23, drops 0)', 'datetime': '2024-11-01T17:27:53.730624', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,731 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3440 word corpus (100.00% of original 3440, drops 0)', 'datetime': '2024-11-01T17:27:53.731194', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,731 gensim.models.word2vec INFO deleting the raw counts dictionary of 23 items
17:27:53,732 gensim.models.word2vec INFO sample=0.001 downsamples 23 most-common words
17:27:53,732 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 535.3260677822248 word corpus (15.6%% of prior 3440)', 'datetime': '2024-11-01T17:27:53.732811', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,733 gensim.models.word2vec INFO estimated required memory for 23 words and 1536 dimensions: 294124 bytes
17:27:53,733 gensim.models.word2vec INFO resetting layer weights
17:27:53,734 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T17:27:53.734219', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
17:27:53,734 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 23 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T17:27:53.734422', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:27:53,740 gensim.models.word2vec INFO EPOCH 0: training on 3440 raw words (540 effective words) took 0.0s, 139607 effective words/s
17:27:53,748 gensim.models.word2vec INFO EPOCH 1: training on 3440 raw words (507 effective words) took 0.0s, 122971 effective words/s
17:27:53,757 gensim.models.word2vec INFO EPOCH 2: training on 3440 raw words (530 effective words) took 0.0s, 76222 effective words/s
17:27:53,757 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10320 raw words (1577 effective words) took 0.0s, 68645 effective words/s', 'datetime': '2024-11-01T17:27:53.757458', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:27:53,758 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T17:27:53.758299', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
17:27:53,758 root INFO Completed. Ending time is 1730478473.7584157 Elapsed time is -0.058589935302734375
17:27:53,769 root INFO Starting preprocessing of transition probabilities on graph with 23 nodes and 22 edges
17:27:53,769 root INFO Starting at time 1730478473.7697577
17:27:53,769 root INFO Beginning preprocessing of transition probabilities for 23 vertices
17:27:53,770 root INFO Completed 1 / 23 vertices
17:27:53,771 root INFO Completed 3 / 23 vertices
17:27:53,772 root INFO Completed 5 / 23 vertices
17:27:53,772 root INFO Completed 7 / 23 vertices
17:27:53,773 root INFO Completed 9 / 23 vertices
17:27:53,774 root INFO Completed 11 / 23 vertices
17:27:53,775 root INFO Completed 13 / 23 vertices
17:27:53,775 root INFO Completed 15 / 23 vertices
17:27:53,776 root INFO Completed 17 / 23 vertices
17:27:53,776 root INFO Completed 19 / 23 vertices
17:27:53,776 root INFO Completed 21 / 23 vertices
17:27:53,776 root INFO Completed 23 / 23 vertices
17:27:53,777 root INFO Completed preprocessing of transition probabilities for vertices
17:27:53,777 root INFO Beginning preprocessing of transition probabilities for 22 edges
17:27:53,778 root INFO Completed 1 / 22 edges
17:27:53,778 root INFO Completed 3 / 22 edges
17:27:53,779 root INFO Completed 5 / 22 edges
17:27:53,779 root INFO Completed 7 / 22 edges
17:27:53,780 root INFO Completed 9 / 22 edges
17:27:53,781 root INFO Completed 11 / 22 edges
17:27:53,781 root INFO Completed 13 / 22 edges
17:27:53,782 root INFO Completed 15 / 22 edges
17:27:53,783 root INFO Completed 17 / 22 edges
17:27:53,784 root INFO Completed 19 / 22 edges
17:27:53,784 root INFO Completed 21 / 22 edges
17:27:53,784 root INFO Completed preprocessing of transition probabilities for edges
17:27:53,785 root INFO Simulating walks on graph at time 1730478473.7854004
17:27:53,785 root INFO Walk iteration: 1/10
17:27:53,788 root INFO Walk iteration: 2/10
17:27:53,789 root INFO Walk iteration: 3/10
17:27:53,791 root INFO Walk iteration: 4/10
17:27:53,792 root INFO Walk iteration: 5/10
17:27:53,793 root INFO Walk iteration: 6/10
17:27:53,794 root INFO Walk iteration: 7/10
17:27:53,795 root INFO Walk iteration: 8/10
17:27:53,796 root INFO Walk iteration: 9/10
17:27:53,796 root INFO Walk iteration: 10/10
17:27:53,797 root INFO Learning embeddings at time 1730478473.7975402
17:27:53,797 gensim.models.word2vec INFO collecting all words and their counts
17:27:53,798 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
17:27:53,799 gensim.models.word2vec INFO collected 23 word types from a corpus of 3440 raw words and 230 sentences
17:27:53,799 gensim.models.word2vec INFO Creating a fresh vocabulary
17:27:53,799 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 23 unique words (100.00% of original 23, drops 0)', 'datetime': '2024-11-01T17:27:53.799920', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,800 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3440 word corpus (100.00% of original 3440, drops 0)', 'datetime': '2024-11-01T17:27:53.800482', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,800 gensim.models.word2vec INFO deleting the raw counts dictionary of 23 items
17:27:53,801 gensim.models.word2vec INFO sample=0.001 downsamples 23 most-common words
17:27:53,801 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 535.3260677822248 word corpus (15.6%% of prior 3440)', 'datetime': '2024-11-01T17:27:53.801191', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
17:27:53,801 gensim.models.word2vec INFO estimated required memory for 23 words and 1536 dimensions: 294124 bytes
17:27:53,802 gensim.models.word2vec INFO resetting layer weights
17:27:53,802 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T17:27:53.802871', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
17:27:53,803 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 23 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T17:27:53.803321', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:27:53,808 gensim.models.word2vec INFO EPOCH 0: training on 3440 raw words (540 effective words) took 0.0s, 177504 effective words/s
17:27:53,815 gensim.models.word2vec INFO EPOCH 1: training on 3440 raw words (507 effective words) took 0.0s, 111207 effective words/s
17:27:53,825 gensim.models.word2vec INFO EPOCH 2: training on 3440 raw words (530 effective words) took 0.0s, 66978 effective words/s
17:27:53,825 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10320 raw words (1577 effective words) took 0.0s, 72470 effective words/s', 'datetime': '2024-11-01T17:27:53.825763', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
17:27:53,826 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T17:27:53.826580', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
17:27:53,826 root INFO Completed. Ending time is 1730478473.8266962 Elapsed time is -0.05693840980529785
17:27:53,872 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
17:27:54,19 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:27:54,19 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:27:54,28 datashaper.workflow.workflow INFO executing verb create_final_entities
17:27:54,32 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
17:27:54,69 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
17:27:54,69 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
17:27:54,70 graphrag.index.operations.embed_text.strategies.openai INFO embedding 23 inputs via 21 snippets using 2 batches. max_batch_size=16, max_tokens=8191
17:27:55,282 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
17:27:55,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2241781789343804. input_tokens=143, output_tokens=0
17:27:55,302 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
17:27:55,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2541242649313062. input_tokens=534, output_tokens=0
17:27:55,329 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:27:55,486 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:27:55,486 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:27:55,494 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:27:58,399 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:27:58,551 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:27:58,552 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:27:58,561 datashaper.workflow.workflow INFO executing verb create_final_communities
17:27:58,573 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:27:58,705 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
17:27:58,708 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:27:58,713 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
17:27:58,722 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:27:58,728 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:27:58,863 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
17:27:58,864 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
17:27:58,868 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:27:58,870 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:27:58,882 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:27:58,914 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:27:59,62 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
17:27:59,63 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:27:59,67 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:27:59,76 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:27:59,80 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 15
17:27:59,91 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 47
17:28:01,889 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:01,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7790869548916817. input_tokens=2090, output_tokens=357
17:28:03,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:03,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9074111408554018. input_tokens=2825, output_tokens=629
17:28:05,239 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:05,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.212538796942681. input_tokens=2067, output_tokens=333
17:28:05,284 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:05,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2552718641236424. input_tokens=2068, output_tokens=275
17:28:05,363 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:05,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3386263898573816. input_tokens=2166, output_tokens=424
17:28:06,290 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
17:28:06,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2671288170386106. input_tokens=3036, output_tokens=609
17:28:06,299 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:28:06,447 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
17:28:06,448 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:28:06,458 datashaper.workflow.workflow INFO executing verb create_final_documents
17:28:06,464 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:28:06,482 graphrag.index.cli INFO All workflows completed successfully.
20:57:18,503 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
20:57:18,505 graphrag.index.cli INFO Starting pipeline run for: 20241101-205718, dryrun=False
20:57:18,505 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 3,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
20:57:18,507 graphrag.index.create_pipeline_config INFO skipping workflows 
20:57:18,507 graphrag.index.run.run INFO Running pipeline
20:57:18,507 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
20:57:18,509 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
20:57:18,509 graphrag.index.input.load_input INFO using file storage for input
20:57:18,510 graphrag.index.input.csv INFO Loading csv files from input_eval
20:57:18,510 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
20:57:18,513 graphrag.index.input.csv INFO Found 1 csv files, loading 1
20:57:18,513 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
20:57:18,514 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
20:57:18,514 graphrag.index.run.run INFO Final # of rows loaded: 1
20:57:18,628 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
20:57:18,631 datashaper.workflow.workflow INFO executing verb create_base_text_units
20:57:19,39 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
20:57:19,168 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
20:57:19,168 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:57:19,175 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
20:57:19,176 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:57:19,214 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
20:57:19,214 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
20:57:21,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:21,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.350236813072115. input_tokens=2087, output_tokens=527
20:57:25,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:25,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.611787650967017. input_tokens=28, output_tokens=923
20:57:25,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:25,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5630977239925414. input_tokens=30, output_tokens=1
20:57:29,385 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:29,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.641359989065677. input_tokens=28, output_tokens=942
20:57:30,61 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:30,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6727615781128407. input_tokens=30, output_tokens=1
20:57:31,696 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:31,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 1.6338680339977145. input_tokens=28, output_tokens=276
20:57:32,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:32,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5801330918911844. input_tokens=153, output_tokens=27
20:57:32,328 root INFO Starting preprocessing of transition probabilities on graph with 28 nodes and 38 edges
20:57:32,328 root INFO Starting at time 1730491052.3284042
20:57:32,328 root INFO Beginning preprocessing of transition probabilities for 28 vertices
20:57:32,328 root INFO Completed 1 / 28 vertices
20:57:32,328 root INFO Completed 3 / 28 vertices
20:57:32,329 root INFO Completed 5 / 28 vertices
20:57:32,329 root INFO Completed 7 / 28 vertices
20:57:32,330 root INFO Completed 9 / 28 vertices
20:57:32,331 root INFO Completed 11 / 28 vertices
20:57:32,332 root INFO Completed 13 / 28 vertices
20:57:32,332 root INFO Completed 15 / 28 vertices
20:57:32,333 root INFO Completed 17 / 28 vertices
20:57:32,334 root INFO Completed 19 / 28 vertices
20:57:32,334 root INFO Completed 21 / 28 vertices
20:57:32,335 root INFO Completed 23 / 28 vertices
20:57:32,335 root INFO Completed 25 / 28 vertices
20:57:32,335 root INFO Completed 27 / 28 vertices
20:57:32,335 root INFO Completed preprocessing of transition probabilities for vertices
20:57:32,336 root INFO Beginning preprocessing of transition probabilities for 38 edges
20:57:32,336 root INFO Completed 1 / 38 edges
20:57:32,336 root INFO Completed 4 / 38 edges
20:57:32,337 root INFO Completed 7 / 38 edges
20:57:32,337 root INFO Completed 10 / 38 edges
20:57:32,338 root INFO Completed 13 / 38 edges
20:57:32,339 root INFO Completed 16 / 38 edges
20:57:32,339 root INFO Completed 19 / 38 edges
20:57:32,340 root INFO Completed 22 / 38 edges
20:57:32,341 root INFO Completed 25 / 38 edges
20:57:32,341 root INFO Completed 28 / 38 edges
20:57:32,342 root INFO Completed 31 / 38 edges
20:57:32,343 root INFO Completed 34 / 38 edges
20:57:32,343 root INFO Completed 37 / 38 edges
20:57:32,343 root INFO Completed preprocessing of transition probabilities for edges
20:57:32,343 root INFO Simulating walks on graph at time 1730491052.3439767
20:57:32,344 root INFO Walk iteration: 1/10
20:57:32,346 root INFO Walk iteration: 2/10
20:57:32,347 root INFO Walk iteration: 3/10
20:57:32,349 root INFO Walk iteration: 4/10
20:57:32,350 root INFO Walk iteration: 5/10
20:57:32,351 root INFO Walk iteration: 6/10
20:57:32,352 root INFO Walk iteration: 7/10
20:57:32,353 root INFO Walk iteration: 8/10
20:57:32,353 root INFO Walk iteration: 9/10
20:57:32,354 root INFO Walk iteration: 10/10
20:57:32,355 root INFO Learning embeddings at time 1730491052.35519
20:57:32,355 gensim.models.word2vec INFO collecting all words and their counts
20:57:32,355 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:57:32,355 gensim.models.word2vec INFO collected 28 word types from a corpus of 5480 raw words and 280 sentences
20:57:32,356 gensim.models.word2vec INFO Creating a fresh vocabulary
20:57:32,356 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 28 unique words (100.00% of original 28, drops 0)', 'datetime': '2024-11-01T20:57:32.356490', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,356 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5480 word corpus (100.00% of original 5480, drops 0)', 'datetime': '2024-11-01T20:57:32.356969', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,357 gensim.models.word2vec INFO deleting the raw counts dictionary of 28 items
20:57:32,357 gensim.models.word2vec INFO sample=0.001 downsamples 28 most-common words
20:57:32,357 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 982.1999370949211 word corpus (17.9%% of prior 5480)', 'datetime': '2024-11-01T20:57:32.357612', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,358 gensim.models.word2vec INFO estimated required memory for 28 words and 1536 dimensions: 358064 bytes
20:57:32,358 gensim.models.word2vec INFO resetting layer weights
20:57:32,358 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T20:57:32.358949', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:57:32,359 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 28 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T20:57:32.359422', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:57:32,363 gensim.models.word2vec INFO EPOCH 0: training on 5480 raw words (1022 effective words) took 0.0s, 327324 effective words/s
20:57:32,368 gensim.models.word2vec INFO EPOCH 1: training on 5480 raw words (1003 effective words) took 0.0s, 303147 effective words/s
20:57:32,372 gensim.models.word2vec INFO EPOCH 2: training on 5480 raw words (969 effective words) took 0.0s, 258351 effective words/s
20:57:32,372 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16440 raw words (2994 effective words) took 0.0s, 224179 effective words/s', 'datetime': '2024-11-01T20:57:32.372801', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:57:32,372 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=28, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T20:57:32.372870', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:57:32,373 root INFO Completed. Ending time is 1730491052.373666 Elapsed time is -0.04526185989379883
20:57:32,376 root INFO Starting preprocessing of transition probabilities on graph with 28 nodes and 38 edges
20:57:32,377 root INFO Starting at time 1730491052.3769896
20:57:32,377 root INFO Beginning preprocessing of transition probabilities for 28 vertices
20:57:32,377 root INFO Completed 1 / 28 vertices
20:57:32,377 root INFO Completed 3 / 28 vertices
20:57:32,377 root INFO Completed 5 / 28 vertices
20:57:32,377 root INFO Completed 7 / 28 vertices
20:57:32,378 root INFO Completed 9 / 28 vertices
20:57:32,378 root INFO Completed 11 / 28 vertices
20:57:32,378 root INFO Completed 13 / 28 vertices
20:57:32,378 root INFO Completed 15 / 28 vertices
20:57:32,378 root INFO Completed 17 / 28 vertices
20:57:32,379 root INFO Completed 19 / 28 vertices
20:57:32,379 root INFO Completed 21 / 28 vertices
20:57:32,379 root INFO Completed 23 / 28 vertices
20:57:32,379 root INFO Completed 25 / 28 vertices
20:57:32,379 root INFO Completed 27 / 28 vertices
20:57:32,380 root INFO Completed preprocessing of transition probabilities for vertices
20:57:32,380 root INFO Beginning preprocessing of transition probabilities for 38 edges
20:57:32,380 root INFO Completed 1 / 38 edges
20:57:32,380 root INFO Completed 4 / 38 edges
20:57:32,380 root INFO Completed 7 / 38 edges
20:57:32,381 root INFO Completed 10 / 38 edges
20:57:32,390 root INFO Completed 13 / 38 edges
20:57:32,391 root INFO Completed 16 / 38 edges
20:57:32,392 root INFO Completed 19 / 38 edges
20:57:32,393 root INFO Completed 22 / 38 edges
20:57:32,394 root INFO Completed 25 / 38 edges
20:57:32,395 root INFO Completed 28 / 38 edges
20:57:32,396 root INFO Completed 31 / 38 edges
20:57:32,397 root INFO Completed 34 / 38 edges
20:57:32,405 root INFO Completed 37 / 38 edges
20:57:32,405 root INFO Completed preprocessing of transition probabilities for edges
20:57:32,412 root INFO Simulating walks on graph at time 1730491052.4122458
20:57:32,412 root INFO Walk iteration: 1/10
20:57:32,416 root INFO Walk iteration: 2/10
20:57:32,420 root INFO Walk iteration: 3/10
20:57:32,423 root INFO Walk iteration: 4/10
20:57:32,427 root INFO Walk iteration: 5/10
20:57:32,429 root INFO Walk iteration: 6/10
20:57:32,431 root INFO Walk iteration: 7/10
20:57:32,433 root INFO Walk iteration: 8/10
20:57:32,445 root INFO Walk iteration: 9/10
20:57:32,452 root INFO Walk iteration: 10/10
20:57:32,454 root INFO Learning embeddings at time 1730491052.4548733
20:57:32,461 gensim.models.word2vec INFO collecting all words and their counts
20:57:32,461 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
20:57:32,462 gensim.models.word2vec INFO collected 28 word types from a corpus of 5480 raw words and 280 sentences
20:57:32,463 gensim.models.word2vec INFO Creating a fresh vocabulary
20:57:32,463 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 28 unique words (100.00% of original 28, drops 0)', 'datetime': '2024-11-01T20:57:32.463265', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,464 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5480 word corpus (100.00% of original 5480, drops 0)', 'datetime': '2024-11-01T20:57:32.464048', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,464 gensim.models.word2vec INFO deleting the raw counts dictionary of 28 items
20:57:32,464 gensim.models.word2vec INFO sample=0.001 downsamples 28 most-common words
20:57:32,464 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 982.1999370949211 word corpus (17.9%% of prior 5480)', 'datetime': '2024-11-01T20:57:32.464833', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
20:57:32,465 gensim.models.word2vec INFO estimated required memory for 28 words and 1536 dimensions: 358064 bytes
20:57:32,465 gensim.models.word2vec INFO resetting layer weights
20:57:32,466 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T20:57:32.466302', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
20:57:32,466 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 28 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T20:57:32.466602', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:57:32,482 gensim.models.word2vec INFO EPOCH 0: training on 5480 raw words (1022 effective words) took 0.0s, 77758 effective words/s
20:57:32,498 gensim.models.word2vec INFO EPOCH 1: training on 5480 raw words (1003 effective words) took 0.0s, 75954 effective words/s
20:57:32,511 gensim.models.word2vec INFO EPOCH 2: training on 5480 raw words (969 effective words) took 0.0s, 80344 effective words/s
20:57:32,512 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16440 raw words (2994 effective words) took 0.0s, 65735 effective words/s', 'datetime': '2024-11-01T20:57:32.512228', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
20:57:32,513 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=28, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T20:57:32.513001', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
20:57:32,513 root INFO Completed. Ending time is 1730491052.5136766 Elapsed time is -0.1366870403289795
20:57:32,555 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
20:57:32,710 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
20:57:32,710 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:57:32,718 datashaper.workflow.workflow INFO executing verb create_final_entities
20:57:32,722 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
20:57:32,760 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
20:57:32,760 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
20:57:32,761 graphrag.index.operations.embed_text.strategies.openai INFO embedding 28 inputs via 25 snippets using 2 batches. max_batch_size=16, max_tokens=8191
20:57:33,577 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
20:57:33,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8302580330055207. input_tokens=227, output_tokens=0
20:57:33,594 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
20:57:33,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8577762099448591. input_tokens=469, output_tokens=0
20:57:33,622 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
20:57:33,776 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
20:57:33,776 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:57:33,785 datashaper.workflow.workflow INFO executing verb create_final_nodes
20:57:36,630 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
20:57:36,790 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
20:57:36,792 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:57:36,800 datashaper.workflow.workflow INFO executing verb create_final_communities
20:57:36,813 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
20:57:36,949 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
20:57:36,949 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:57:36,959 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
20:57:36,967 datashaper.workflow.workflow INFO executing verb create_final_relationships
20:57:36,975 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
20:57:37,113 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
20:57:37,113 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
20:57:37,117 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:57:37,119 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
20:57:37,127 datashaper.workflow.workflow INFO executing verb create_final_text_units
20:57:37,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
20:57:37,272 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
20:57:37,272 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
20:57:37,276 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
20:57:37,285 datashaper.workflow.workflow INFO executing verb create_final_community_reports
20:57:37,290 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 14
20:57:37,302 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 70
20:57:39,671 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:39,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.344927490921691. input_tokens=2216, output_tokens=401
20:57:40,134 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:40,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8134521888568997. input_tokens=2512, output_tokens=488
20:57:40,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:40,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2095002790447325. input_tokens=2169, output_tokens=412
20:57:42,555 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:42,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.985279300948605. input_tokens=2113, output_tokens=315
20:57:43,221 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:43,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6662989801261574. input_tokens=2359, output_tokens=487
20:57:44,247 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:44,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6840461189858615. input_tokens=3057, output_tokens=698
20:57:44,291 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
20:57:44,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7234915629960597. input_tokens=2286, output_tokens=628
20:57:44,299 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
20:57:44,464 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
20:57:44,464 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
20:57:44,474 datashaper.workflow.workflow INFO executing verb create_final_documents
20:57:44,480 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
20:57:44,497 graphrag.index.cli INFO All workflows completed successfully.
21:06:02,893 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:06:02,895 graphrag.index.cli INFO Starting pipeline run for: 20241101-210602, dryrun=False
21:06:02,895 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 3,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:06:02,897 graphrag.index.create_pipeline_config INFO skipping workflows 
21:06:02,898 graphrag.index.run.run INFO Running pipeline
21:06:02,898 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:06:02,900 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:06:02,900 graphrag.index.input.load_input INFO using file storage for input
21:06:02,902 graphrag.index.input.csv INFO Loading csv files from input_eval
21:06:02,902 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:06:02,908 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:06:02,908 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:06:02,909 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:06:02,909 graphrag.index.run.run INFO Final # of rows loaded: 1
21:06:03,36 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:06:03,38 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:06:03,416 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:06:03,549 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:06:03,550 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:06:03,556 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:06:03,557 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:06:03,596 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:06:03,596 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:06:06,345 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:06,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7533521768637. input_tokens=2087, output_tokens=527
21:06:09,403 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:09,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.054359990172088. input_tokens=34, output_tokens=854
21:06:09,973 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:09,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5672052630688995. input_tokens=30, output_tokens=1
21:06:10,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:10,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.920863897074014. input_tokens=34, output_tokens=126
21:06:11,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:11,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5154037300962955. input_tokens=30, output_tokens=1
21:06:12,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:12,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.8974647531285882. input_tokens=34, output_tokens=138
21:06:12,336 root INFO Starting preprocessing of transition probabilities on graph with 18 nodes and 19 edges
21:06:12,336 root INFO Starting at time 1730491572.3367343
21:06:12,336 root INFO Beginning preprocessing of transition probabilities for 18 vertices
21:06:12,336 root INFO Completed 1 / 18 vertices
21:06:12,337 root INFO Completed 2 / 18 vertices
21:06:12,338 root INFO Completed 3 / 18 vertices
21:06:12,338 root INFO Completed 4 / 18 vertices
21:06:12,339 root INFO Completed 5 / 18 vertices
21:06:12,339 root INFO Completed 6 / 18 vertices
21:06:12,339 root INFO Completed 7 / 18 vertices
21:06:12,339 root INFO Completed 8 / 18 vertices
21:06:12,339 root INFO Completed 9 / 18 vertices
21:06:12,339 root INFO Completed 10 / 18 vertices
21:06:12,339 root INFO Completed 11 / 18 vertices
21:06:12,340 root INFO Completed 12 / 18 vertices
21:06:12,340 root INFO Completed 13 / 18 vertices
21:06:12,340 root INFO Completed 14 / 18 vertices
21:06:12,340 root INFO Completed 15 / 18 vertices
21:06:12,340 root INFO Completed 16 / 18 vertices
21:06:12,340 root INFO Completed 17 / 18 vertices
21:06:12,340 root INFO Completed 18 / 18 vertices
21:06:12,340 root INFO Completed preprocessing of transition probabilities for vertices
21:06:12,341 root INFO Beginning preprocessing of transition probabilities for 19 edges
21:06:12,341 root INFO Completed 1 / 19 edges
21:06:12,341 root INFO Completed 2 / 19 edges
21:06:12,341 root INFO Completed 3 / 19 edges
21:06:12,342 root INFO Completed 4 / 19 edges
21:06:12,343 root INFO Completed 5 / 19 edges
21:06:12,343 root INFO Completed 6 / 19 edges
21:06:12,343 root INFO Completed 7 / 19 edges
21:06:12,344 root INFO Completed 8 / 19 edges
21:06:12,344 root INFO Completed 9 / 19 edges
21:06:12,344 root INFO Completed 10 / 19 edges
21:06:12,344 root INFO Completed 11 / 19 edges
21:06:12,344 root INFO Completed 12 / 19 edges
21:06:12,345 root INFO Completed 13 / 19 edges
21:06:12,345 root INFO Completed 14 / 19 edges
21:06:12,345 root INFO Completed 15 / 19 edges
21:06:12,345 root INFO Completed 16 / 19 edges
21:06:12,345 root INFO Completed 17 / 19 edges
21:06:12,346 root INFO Completed 18 / 19 edges
21:06:12,346 root INFO Completed 19 / 19 edges
21:06:12,346 root INFO Completed preprocessing of transition probabilities for edges
21:06:12,346 root INFO Simulating walks on graph at time 1730491572.3462937
21:06:12,346 root INFO Walk iteration: 1/10
21:06:12,347 root INFO Walk iteration: 2/10
21:06:12,348 root INFO Walk iteration: 3/10
21:06:12,348 root INFO Walk iteration: 4/10
21:06:12,349 root INFO Walk iteration: 5/10
21:06:12,349 root INFO Walk iteration: 6/10
21:06:12,350 root INFO Walk iteration: 7/10
21:06:12,351 root INFO Walk iteration: 8/10
21:06:12,351 root INFO Walk iteration: 9/10
21:06:12,352 root INFO Walk iteration: 10/10
21:06:12,352 root INFO Learning embeddings at time 1730491572.3529572
21:06:12,353 gensim.models.word2vec INFO collecting all words and their counts
21:06:12,353 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:06:12,353 gensim.models.word2vec INFO collected 18 word types from a corpus of 2880 raw words and 180 sentences
21:06:12,354 gensim.models.word2vec INFO Creating a fresh vocabulary
21:06:12,354 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 18 unique words (100.00% of original 18, drops 0)', 'datetime': '2024-11-01T21:06:12.354488', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:06:12,354 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2880 word corpus (100.00% of original 2880, drops 0)', 'datetime': '2024-11-01T21:06:12.354957', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:06:12,355 gensim.models.word2vec INFO deleting the raw counts dictionary of 18 items
21:06:12,355 gensim.models.word2vec INFO sample=0.001 downsamples 18 most-common words
21:06:12,355 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 392.7283413350299 word corpus (13.6%% of prior 2880)', 'datetime': '2024-11-01T21:06:12.355497', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:06:12,356 gensim.models.word2vec INFO estimated required memory for 18 words and 1536 dimensions: 230184 bytes
21:06:12,356 gensim.models.word2vec INFO resetting layer weights
21:06:12,356 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:06:12.356849', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:06:12,357 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 18 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:06:12.357213', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:06:12,359 gensim.models.word2vec INFO EPOCH 0: training on 2880 raw words (409 effective words) took 0.0s, 404694 effective words/s
21:06:12,361 gensim.models.word2vec INFO EPOCH 1: training on 2880 raw words (360 effective words) took 0.0s, 377134 effective words/s
21:06:12,364 gensim.models.word2vec INFO EPOCH 2: training on 2880 raw words (397 effective words) took 0.0s, 261424 effective words/s
21:06:12,364 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 8640 raw words (1166 effective words) took 0.0s, 169729 effective words/s', 'datetime': '2024-11-01T21:06:12.364107', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:06:12,364 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=18, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:06:12.364743', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:06:12,364 root INFO Completed. Ending time is 1730491572.3647778 Elapsed time is -0.028043508529663086
21:06:12,390 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:06:12,531 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:06:12,531 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:06:12,539 datashaper.workflow.workflow INFO executing verb create_final_entities
21:06:12,542 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:06:12,581 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:06:12,581 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:06:12,581 graphrag.index.operations.embed_text.strategies.openai INFO embedding 18 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
21:06:13,418 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:06:13,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.859306053025648. input_tokens=430, output_tokens=0
21:06:13,447 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:06:13,588 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:06:13,589 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:06:13,596 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:06:15,987 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:06:16,152 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:06:16,153 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:06:16,160 datashaper.workflow.workflow INFO executing verb create_final_communities
21:06:16,171 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:06:16,305 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:06:16,306 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:06:16,312 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:06:16,320 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:06:16,325 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:06:16,464 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
21:06:16,464 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:06:16,468 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:06:16,471 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:06:16,478 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:06:16,487 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:06:16,623 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:06:16,624 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:06:16,627 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:06:16,635 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:06:16,638 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 18
21:06:19,46 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:19,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.388048392953351. input_tokens=2034, output_tokens=302
21:06:19,187 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:19,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.532034564996138. input_tokens=2101, output_tokens=317
21:06:19,303 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:19,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6462970899883658. input_tokens=2082, output_tokens=337
21:06:19,938 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:06:19,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2860916680656374. input_tokens=2795, output_tokens=607
21:06:19,951 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:06:20,107 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:06:20,107 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:06:20,116 datashaper.workflow.workflow INFO executing verb create_final_documents
21:06:20,123 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:06:20,140 graphrag.index.cli INFO All workflows completed successfully.
21:08:02,214 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:08:02,215 graphrag.index.cli INFO Starting pipeline run for: 20241101-210802, dryrun=False
21:08:02,216 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 4,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:08:02,218 graphrag.index.create_pipeline_config INFO skipping workflows 
21:08:02,218 graphrag.index.run.run INFO Running pipeline
21:08:02,218 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:08:02,219 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:08:02,219 graphrag.index.input.load_input INFO using file storage for input
21:08:02,221 graphrag.index.input.csv INFO Loading csv files from input_eval
21:08:02,221 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:08:02,224 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:08:02,224 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:08:02,225 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:08:02,225 graphrag.index.run.run INFO Final # of rows loaded: 1
21:08:02,337 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:08:02,339 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:08:02,820 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:08:02,948 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:08:02,948 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:08:02,955 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:08:02,956 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:08:02,995 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:08:02,995 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:08:05,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:05,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7670257398858666. input_tokens=2087, output_tokens=527
21:08:08,749 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:08,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.9857661130372435. input_tokens=34, output_tokens=854
21:08:09,239 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:09,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4908088441006839. input_tokens=30, output_tokens=1
21:08:10,162 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:10,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.9226394249126315. input_tokens=34, output_tokens=126
21:08:10,688 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:10,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5238189471419901. input_tokens=30, output_tokens=1
21:08:11,563 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:11,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.8735332060605288. input_tokens=34, output_tokens=138
21:08:12,47 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:12,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.4830685539636761. input_tokens=30, output_tokens=1
21:08:12,851 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:12,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.8032352509908378. input_tokens=34, output_tokens=87
21:08:12,881 root INFO Starting preprocessing of transition probabilities on graph with 19 nodes and 20 edges
21:08:12,881 root INFO Starting at time 1730491692.881443
21:08:12,881 root INFO Beginning preprocessing of transition probabilities for 19 vertices
21:08:12,881 root INFO Completed 1 / 19 vertices
21:08:12,881 root INFO Completed 2 / 19 vertices
21:08:12,881 root INFO Completed 3 / 19 vertices
21:08:12,881 root INFO Completed 4 / 19 vertices
21:08:12,882 root INFO Completed 5 / 19 vertices
21:08:12,882 root INFO Completed 6 / 19 vertices
21:08:12,882 root INFO Completed 7 / 19 vertices
21:08:12,883 root INFO Completed 8 / 19 vertices
21:08:12,884 root INFO Completed 9 / 19 vertices
21:08:12,885 root INFO Completed 10 / 19 vertices
21:08:12,885 root INFO Completed 11 / 19 vertices
21:08:12,885 root INFO Completed 12 / 19 vertices
21:08:12,885 root INFO Completed 13 / 19 vertices
21:08:12,885 root INFO Completed 14 / 19 vertices
21:08:12,886 root INFO Completed 15 / 19 vertices
21:08:12,890 root INFO Completed 16 / 19 vertices
21:08:12,890 root INFO Completed 17 / 19 vertices
21:08:12,890 root INFO Completed 18 / 19 vertices
21:08:12,891 root INFO Completed 19 / 19 vertices
21:08:12,892 root INFO Completed preprocessing of transition probabilities for vertices
21:08:12,893 root INFO Beginning preprocessing of transition probabilities for 20 edges
21:08:12,893 root INFO Completed 1 / 20 edges
21:08:12,894 root INFO Completed 3 / 20 edges
21:08:12,895 root INFO Completed 5 / 20 edges
21:08:12,896 root INFO Completed 7 / 20 edges
21:08:12,897 root INFO Completed 9 / 20 edges
21:08:12,898 root INFO Completed 11 / 20 edges
21:08:12,898 root INFO Completed 13 / 20 edges
21:08:12,899 root INFO Completed 15 / 20 edges
21:08:12,899 root INFO Completed 17 / 20 edges
21:08:12,899 root INFO Completed 19 / 20 edges
21:08:12,900 root INFO Completed preprocessing of transition probabilities for edges
21:08:12,901 root INFO Simulating walks on graph at time 1730491692.901027
21:08:12,901 root INFO Walk iteration: 1/10
21:08:12,903 root INFO Walk iteration: 2/10
21:08:12,904 root INFO Walk iteration: 3/10
21:08:12,905 root INFO Walk iteration: 4/10
21:08:12,906 root INFO Walk iteration: 5/10
21:08:12,907 root INFO Walk iteration: 6/10
21:08:12,908 root INFO Walk iteration: 7/10
21:08:12,909 root INFO Walk iteration: 8/10
21:08:12,909 root INFO Walk iteration: 9/10
21:08:12,910 root INFO Walk iteration: 10/10
21:08:12,910 root INFO Learning embeddings at time 1730491692.9109654
21:08:12,911 gensim.models.word2vec INFO collecting all words and their counts
21:08:12,911 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:08:12,911 gensim.models.word2vec INFO collected 19 word types from a corpus of 3360 raw words and 190 sentences
21:08:12,912 gensim.models.word2vec INFO Creating a fresh vocabulary
21:08:12,912 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 19 unique words (100.00% of original 19, drops 0)', 'datetime': '2024-11-01T21:08:12.912659', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:12,913 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3360 word corpus (100.00% of original 3360, drops 0)', 'datetime': '2024-11-01T21:08:12.913292', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:12,914 gensim.models.word2vec INFO deleting the raw counts dictionary of 19 items
21:08:12,914 gensim.models.word2vec INFO sample=0.001 downsamples 19 most-common words
21:08:12,914 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 472.26268115200037 word corpus (14.1%% of prior 3360)', 'datetime': '2024-11-01T21:08:12.914845', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:12,915 gensim.models.word2vec INFO estimated required memory for 19 words and 1536 dimensions: 242972 bytes
21:08:12,916 gensim.models.word2vec INFO resetting layer weights
21:08:12,916 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:08:12.916399', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:08:12,916 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 19 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:08:12.916903', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:08:12,922 gensim.models.word2vec INFO EPOCH 0: training on 3360 raw words (487 effective words) took 0.0s, 172767 effective words/s
21:08:12,927 gensim.models.word2vec INFO EPOCH 1: training on 3360 raw words (457 effective words) took 0.0s, 127080 effective words/s
21:08:12,934 gensim.models.word2vec INFO EPOCH 2: training on 3360 raw words (492 effective words) took 0.0s, 109034 effective words/s
21:08:12,934 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10080 raw words (1436 effective words) took 0.0s, 86204 effective words/s', 'datetime': '2024-11-01T21:08:12.934272', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:08:12,935 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=19, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:08:12.934982', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:08:12,935 root INFO Completed. Ending time is 1730491692.9350994 Elapsed time is -0.05365633964538574
21:08:12,941 root INFO Starting preprocessing of transition probabilities on graph with 19 nodes and 20 edges
21:08:12,942 root INFO Starting at time 1730491692.9420817
21:08:12,942 root INFO Beginning preprocessing of transition probabilities for 19 vertices
21:08:12,942 root INFO Completed 1 / 19 vertices
21:08:12,943 root INFO Completed 2 / 19 vertices
21:08:12,943 root INFO Completed 3 / 19 vertices
21:08:12,944 root INFO Completed 4 / 19 vertices
21:08:12,944 root INFO Completed 5 / 19 vertices
21:08:12,945 root INFO Completed 6 / 19 vertices
21:08:12,945 root INFO Completed 7 / 19 vertices
21:08:12,945 root INFO Completed 8 / 19 vertices
21:08:12,946 root INFO Completed 9 / 19 vertices
21:08:12,946 root INFO Completed 10 / 19 vertices
21:08:12,950 root INFO Completed 11 / 19 vertices
21:08:12,950 root INFO Completed 12 / 19 vertices
21:08:12,950 root INFO Completed 13 / 19 vertices
21:08:12,950 root INFO Completed 14 / 19 vertices
21:08:12,951 root INFO Completed 15 / 19 vertices
21:08:12,951 root INFO Completed 16 / 19 vertices
21:08:12,951 root INFO Completed 17 / 19 vertices
21:08:12,952 root INFO Completed 18 / 19 vertices
21:08:12,952 root INFO Completed 19 / 19 vertices
21:08:12,958 root INFO Completed preprocessing of transition probabilities for vertices
21:08:12,958 root INFO Beginning preprocessing of transition probabilities for 20 edges
21:08:12,959 root INFO Completed 1 / 20 edges
21:08:12,959 root INFO Completed 3 / 20 edges
21:08:12,960 root INFO Completed 5 / 20 edges
21:08:12,960 root INFO Completed 7 / 20 edges
21:08:12,961 root INFO Completed 9 / 20 edges
21:08:12,962 root INFO Completed 11 / 20 edges
21:08:12,963 root INFO Completed 13 / 20 edges
21:08:12,963 root INFO Completed 15 / 20 edges
21:08:12,964 root INFO Completed 17 / 20 edges
21:08:12,965 root INFO Completed 19 / 20 edges
21:08:12,965 root INFO Completed preprocessing of transition probabilities for edges
21:08:12,965 root INFO Simulating walks on graph at time 1730491692.965845
21:08:12,966 root INFO Walk iteration: 1/10
21:08:12,974 root INFO Walk iteration: 2/10
21:08:12,976 root INFO Walk iteration: 3/10
21:08:12,978 root INFO Walk iteration: 4/10
21:08:12,980 root INFO Walk iteration: 5/10
21:08:12,982 root INFO Walk iteration: 6/10
21:08:12,984 root INFO Walk iteration: 7/10
21:08:12,989 root INFO Walk iteration: 8/10
21:08:12,991 root INFO Walk iteration: 9/10
21:08:12,992 root INFO Walk iteration: 10/10
21:08:12,994 root INFO Learning embeddings at time 1730491692.9940355
21:08:12,994 gensim.models.word2vec INFO collecting all words and their counts
21:08:12,994 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:08:12,995 gensim.models.word2vec INFO collected 19 word types from a corpus of 3360 raw words and 190 sentences
21:08:12,996 gensim.models.word2vec INFO Creating a fresh vocabulary
21:08:13,2 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 19 unique words (100.00% of original 19, drops 0)', 'datetime': '2024-11-01T21:08:13.002477', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:13,3 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3360 word corpus (100.00% of original 3360, drops 0)', 'datetime': '2024-11-01T21:08:13.003225', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:13,4 gensim.models.word2vec INFO deleting the raw counts dictionary of 19 items
21:08:13,4 gensim.models.word2vec INFO sample=0.001 downsamples 19 most-common words
21:08:13,11 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 472.26268115200037 word corpus (14.1%% of prior 3360)', 'datetime': '2024-11-01T21:08:13.011089', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:08:13,17 gensim.models.word2vec INFO estimated required memory for 19 words and 1536 dimensions: 242972 bytes
21:08:13,18 gensim.models.word2vec INFO resetting layer weights
21:08:13,18 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:08:13.018773', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:08:13,24 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 19 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:08:13.024250', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:08:13,35 gensim.models.word2vec INFO EPOCH 0: training on 3360 raw words (487 effective words) took 0.0s, 75204 effective words/s
21:08:13,41 gensim.models.word2vec INFO EPOCH 1: training on 3360 raw words (457 effective words) took 0.0s, 107460 effective words/s
21:08:13,50 gensim.models.word2vec INFO EPOCH 2: training on 3360 raw words (492 effective words) took 0.0s, 77397 effective words/s
21:08:13,50 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10080 raw words (1436 effective words) took 0.0s, 56707 effective words/s', 'datetime': '2024-11-01T21:08:13.050302', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:08:13,51 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=19, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:08:13.051108', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:08:13,51 root INFO Completed. Ending time is 1730491693.051132 Elapsed time is -0.10905027389526367
21:08:13,95 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:08:13,239 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:08:13,240 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:08:13,248 datashaper.workflow.workflow INFO executing verb create_final_entities
21:08:13,252 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:08:13,289 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:08:13,289 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:08:13,290 graphrag.index.operations.embed_text.strategies.openai INFO embedding 19 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
21:08:13,903 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:08:13,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6348197339102626. input_tokens=430, output_tokens=0
21:08:13,931 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:08:14,69 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:08:14,69 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:08:14,78 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:08:16,756 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:08:16,921 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:08:16,921 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:08:16,930 datashaper.workflow.workflow INFO executing verb create_final_communities
21:08:16,943 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:08:17,74 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:08:17,74 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:08:17,79 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:08:17,87 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:08:17,95 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:08:17,228 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
21:08:17,228 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:08:17,232 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:08:17,234 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:08:17,242 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:08:17,251 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:08:17,383 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:08:17,384 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:08:17,389 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:08:17,400 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:08:17,404 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 12
21:08:17,415 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 40
21:08:20,30 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:20,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5979238259606063. input_tokens=2062, output_tokens=380
21:08:20,802 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:20,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3724953630007803. input_tokens=2579, output_tokens=626
21:08:22,773 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:22,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9589681969955564. input_tokens=2129, output_tokens=332
21:08:23,9 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:23,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.19308151002042. input_tokens=2034, output_tokens=248
21:08:23,774 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:23,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9626041299197823. input_tokens=2101, output_tokens=437
21:08:24,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:08:24,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.285936882952228. input_tokens=2795, output_tokens=629
21:08:24,117 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:08:24,260 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:08:24,261 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:08:24,271 datashaper.workflow.workflow INFO executing verb create_final_documents
21:08:24,278 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:08:24,294 graphrag.index.cli INFO All workflows completed successfully.
21:11:35,743 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:11:35,744 graphrag.index.cli INFO Starting pipeline run for: 20241101-211135, dryrun=False
21:11:35,745 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 4,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:11:35,746 graphrag.index.create_pipeline_config INFO skipping workflows 
21:11:35,747 graphrag.index.run.run INFO Running pipeline
21:11:35,747 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:11:35,747 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:11:35,747 graphrag.index.input.load_input INFO using file storage for input
21:11:35,749 graphrag.index.input.csv INFO Loading csv files from input_eval
21:11:35,749 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:11:35,752 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:11:35,752 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:11:35,752 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:11:35,753 graphrag.index.run.run INFO Final # of rows loaded: 1
21:11:35,872 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:11:35,874 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:11:36,208 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:11:36,346 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:11:36,347 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:11:36,354 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:11:36,355 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:11:36,393 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:11:36,393 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:11:38,773 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:38,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3842197789344937. input_tokens=2087, output_tokens=527
21:11:42,584 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:42,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.8074387540109456. input_tokens=18, output_tokens=1131
21:11:43,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:43,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.507532199146226. input_tokens=30, output_tokens=1
21:11:50,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:50,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.103242228040472. input_tokens=18, output_tokens=2276
21:11:50,890 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:50,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6857682319823653. input_tokens=30, output_tokens=1
21:11:57,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:57,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.969211365096271. input_tokens=18, output_tokens=2514
21:11:58,659 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:11:58,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7950351180043072. input_tokens=30, output_tokens=1
21:12:08,801 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:08,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 10.141206285916269. input_tokens=18, output_tokens=3348
21:12:09,335 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:09,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.524565105792135. input_tokens=145, output_tokens=17
21:12:09,867 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:09,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5243915538303554. input_tokens=148, output_tokens=26
21:12:09,943 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:09,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1236359260510653. input_tokens=149, output_tokens=24
21:12:09,986 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:09,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1718053580261767. input_tokens=151, output_tokens=35
21:12:10,31 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2147062791045755. input_tokens=143, output_tokens=18
21:12:10,74 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2560137750115246. input_tokens=143, output_tokens=29
21:12:10,416 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5414054919965565. input_tokens=147, output_tokens=21
21:12:10,433 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4896077199373394. input_tokens=150, output_tokens=27
21:12:10,516 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5277879419736564. input_tokens=145, output_tokens=23
21:12:10,583 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5507540521211922. input_tokens=144, output_tokens=22
21:12:10,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5332621887791902. input_tokens=157, output_tokens=32
21:12:10,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.517401441000402. input_tokens=141, output_tokens=18
21:12:10,973 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5403924928978086. input_tokens=147, output_tokens=23
21:12:10,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:10,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4625239430461079. input_tokens=165, output_tokens=21
21:12:11,0 root INFO Starting preprocessing of transition probabilities on graph with 29 nodes and 43 edges
21:12:11,1 root INFO Starting at time 1730491931.0010056
21:12:11,1 root INFO Beginning preprocessing of transition probabilities for 29 vertices
21:12:11,1 root INFO Completed 1 / 29 vertices
21:12:11,1 root INFO Completed 3 / 29 vertices
21:12:11,1 root INFO Completed 5 / 29 vertices
21:12:11,1 root INFO Completed 7 / 29 vertices
21:12:11,2 root INFO Completed 9 / 29 vertices
21:12:11,2 root INFO Completed 11 / 29 vertices
21:12:11,3 root INFO Completed 13 / 29 vertices
21:12:11,3 root INFO Completed 15 / 29 vertices
21:12:11,3 root INFO Completed 17 / 29 vertices
21:12:11,4 root INFO Completed 19 / 29 vertices
21:12:11,4 root INFO Completed 21 / 29 vertices
21:12:11,4 root INFO Completed 23 / 29 vertices
21:12:11,5 root INFO Completed 25 / 29 vertices
21:12:11,5 root INFO Completed 27 / 29 vertices
21:12:11,6 root INFO Completed 29 / 29 vertices
21:12:11,6 root INFO Completed preprocessing of transition probabilities for vertices
21:12:11,7 root INFO Beginning preprocessing of transition probabilities for 43 edges
21:12:11,8 root INFO Completed 1 / 43 edges
21:12:11,8 root INFO Completed 5 / 43 edges
21:12:11,9 root INFO Completed 9 / 43 edges
21:12:11,10 root INFO Completed 13 / 43 edges
21:12:11,10 root INFO Completed 17 / 43 edges
21:12:11,11 root INFO Completed 21 / 43 edges
21:12:11,11 root INFO Completed 25 / 43 edges
21:12:11,12 root INFO Completed 29 / 43 edges
21:12:11,12 root INFO Completed 33 / 43 edges
21:12:11,12 root INFO Completed 37 / 43 edges
21:12:11,13 root INFO Completed 41 / 43 edges
21:12:11,14 root INFO Completed preprocessing of transition probabilities for edges
21:12:11,14 root INFO Simulating walks on graph at time 1730491931.0144198
21:12:11,14 root INFO Walk iteration: 1/10
21:12:11,16 root INFO Walk iteration: 2/10
21:12:11,29 root INFO Walk iteration: 3/10
21:12:11,31 root INFO Walk iteration: 4/10
21:12:11,33 root INFO Walk iteration: 5/10
21:12:11,34 root INFO Walk iteration: 6/10
21:12:11,35 root INFO Walk iteration: 7/10
21:12:11,36 root INFO Walk iteration: 8/10
21:12:11,38 root INFO Walk iteration: 9/10
21:12:11,46 root INFO Walk iteration: 10/10
21:12:11,48 root INFO Learning embeddings at time 1730491931.0480814
21:12:11,48 gensim.models.word2vec INFO collecting all words and their counts
21:12:11,49 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:12:11,49 gensim.models.word2vec INFO collected 29 word types from a corpus of 4860 raw words and 290 sentences
21:12:11,56 gensim.models.word2vec INFO Creating a fresh vocabulary
21:12:11,57 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 29 unique words (100.00% of original 29, drops 0)', 'datetime': '2024-11-01T21:12:11.056986', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:12:11,59 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4860 word corpus (100.00% of original 4860, drops 0)', 'datetime': '2024-11-01T21:12:11.058973', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:12:11,60 gensim.models.word2vec INFO deleting the raw counts dictionary of 29 items
21:12:11,60 gensim.models.word2vec INFO sample=0.001 downsamples 29 most-common words
21:12:11,61 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 908.6495420424176 word corpus (18.7%% of prior 4860)', 'datetime': '2024-11-01T21:12:11.061710', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:12:11,62 gensim.models.word2vec INFO estimated required memory for 29 words and 1536 dimensions: 370852 bytes
21:12:11,65 gensim.models.word2vec INFO resetting layer weights
21:12:11,66 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:12:11.066702', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:12:11,66 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 29 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:12:11.066873', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:12:11,85 gensim.models.word2vec INFO EPOCH 0: training on 4860 raw words (927 effective words) took 0.0s, 72630 effective words/s
21:12:11,97 gensim.models.word2vec INFO EPOCH 1: training on 4860 raw words (936 effective words) took 0.0s, 139732 effective words/s
21:12:11,104 gensim.models.word2vec INFO EPOCH 2: training on 4860 raw words (890 effective words) took 0.0s, 155906 effective words/s
21:12:11,104 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 14580 raw words (2753 effective words) took 0.0s, 74149 effective words/s', 'datetime': '2024-11-01T21:12:11.104078', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:12:11,104 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=29, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:12:11.104139', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:12:11,104 root INFO Completed. Ending time is 1730491931.104165 Elapsed time is -0.10315942764282227
21:12:11,159 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:12:11,302 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:12:11,303 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:12:11,311 datashaper.workflow.workflow INFO executing verb create_final_entities
21:12:11,315 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:12:11,352 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:12:11,352 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:12:11,353 graphrag.index.operations.embed_text.strategies.openai INFO embedding 29 inputs via 24 snippets using 2 batches. max_batch_size=16, max_tokens=8191
21:12:12,57 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:12:12,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7165082551073283. input_tokens=190, output_tokens=0
21:12:12,166 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:12:12,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8364593170117587. input_tokens=502, output_tokens=0
21:12:12,196 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:12:12,345 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:12:12,345 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:12:12,355 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:12:14,779 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:12:14,937 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:12:14,937 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:12:14,947 datashaper.workflow.workflow INFO executing verb create_final_communities
21:12:14,959 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:12:15,95 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:12:15,95 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:12:15,101 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:12:15,108 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:12:15,115 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:12:15,260 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
21:12:15,261 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:12:15,266 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:12:15,269 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:12:15,276 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:12:15,286 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:12:15,420 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:12:15,420 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:12:15,425 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:12:15,434 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:12:15,437 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 29
21:12:18,60 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:18,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6039593450259417. input_tokens=2376, output_tokens=478
21:12:18,572 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:18,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.113839096855372. input_tokens=2470, output_tokens=535
21:12:18,849 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:18,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.395396832143888. input_tokens=2250, output_tokens=605
21:12:18,896 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:18,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4344177020248026. input_tokens=2790, output_tokens=557
21:12:20,51 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:12:20,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.59137905202806. input_tokens=2363, output_tokens=727
21:12:20,69 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:12:20,219 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:12:20,219 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:12:20,229 datashaper.workflow.workflow INFO executing verb create_final_documents
21:12:20,236 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:12:20,253 graphrag.index.cli INFO All workflows completed successfully.
21:18:25,521 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:18:25,523 graphrag.index.cli INFO Starting pipeline run for: 20241101-211825, dryrun=False
21:18:25,523 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 4,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:18:25,525 graphrag.index.create_pipeline_config INFO skipping workflows 
21:18:25,525 graphrag.index.run.run INFO Running pipeline
21:18:25,525 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:18:25,525 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:18:25,525 graphrag.index.input.load_input INFO using file storage for input
21:18:25,527 graphrag.index.input.csv INFO Loading csv files from input_eval
21:18:25,527 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:18:25,530 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:18:25,530 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:18:25,530 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:18:25,531 graphrag.index.run.run INFO Final # of rows loaded: 1
21:18:25,644 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:18:25,647 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:18:26,58 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:18:26,187 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:18:26,188 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:18:26,195 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:18:26,196 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:18:26,235 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:18:26,235 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:18:29,73 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:29,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8462946228682995. input_tokens=2087, output_tokens=527
21:18:36,953 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:36,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.871684513986111. input_tokens=38, output_tokens=2563
21:18:37,627 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:37,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6698461000341922. input_tokens=30, output_tokens=1
21:18:45,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:45,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.421460867160931. input_tokens=38, output_tokens=2426
21:18:45,873 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:45,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.8184579331427813. input_tokens=30, output_tokens=1
21:18:54,221 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:54,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 8.34691006410867. input_tokens=38, output_tokens=2854
21:18:55,91 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:18:55,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8648867700248957. input_tokens=30, output_tokens=1
21:19:03,289 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:03,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 8.197283310815692. input_tokens=38, output_tokens=2863
21:19:03,332 root INFO Starting preprocessing of transition probabilities on graph with 25 nodes and 38 edges
21:19:03,332 root INFO Starting at time 1730492343.3327708
21:19:03,332 root INFO Beginning preprocessing of transition probabilities for 25 vertices
21:19:03,332 root INFO Completed 1 / 25 vertices
21:19:03,332 root INFO Completed 3 / 25 vertices
21:19:03,336 root INFO Completed 5 / 25 vertices
21:19:03,337 root INFO Completed 7 / 25 vertices
21:19:03,337 root INFO Completed 9 / 25 vertices
21:19:03,338 root INFO Completed 11 / 25 vertices
21:19:03,338 root INFO Completed 13 / 25 vertices
21:19:03,338 root INFO Completed 15 / 25 vertices
21:19:03,338 root INFO Completed 17 / 25 vertices
21:19:03,345 root INFO Completed 19 / 25 vertices
21:19:03,346 root INFO Completed 21 / 25 vertices
21:19:03,353 root INFO Completed 23 / 25 vertices
21:19:03,353 root INFO Completed 25 / 25 vertices
21:19:03,354 root INFO Completed preprocessing of transition probabilities for vertices
21:19:03,354 root INFO Beginning preprocessing of transition probabilities for 38 edges
21:19:03,355 root INFO Completed 1 / 38 edges
21:19:03,356 root INFO Completed 4 / 38 edges
21:19:03,356 root INFO Completed 7 / 38 edges
21:19:03,357 root INFO Completed 10 / 38 edges
21:19:03,358 root INFO Completed 13 / 38 edges
21:19:03,362 root INFO Completed 16 / 38 edges
21:19:03,362 root INFO Completed 19 / 38 edges
21:19:03,363 root INFO Completed 22 / 38 edges
21:19:03,371 root INFO Completed 25 / 38 edges
21:19:03,371 root INFO Completed 28 / 38 edges
21:19:03,372 root INFO Completed 31 / 38 edges
21:19:03,373 root INFO Completed 34 / 38 edges
21:19:03,373 root INFO Completed 37 / 38 edges
21:19:03,374 root INFO Completed preprocessing of transition probabilities for edges
21:19:03,374 root INFO Simulating walks on graph at time 1730492343.3748152
21:19:03,376 root INFO Walk iteration: 1/10
21:19:03,379 root INFO Walk iteration: 2/10
21:19:03,381 root INFO Walk iteration: 3/10
21:19:03,383 root INFO Walk iteration: 4/10
21:19:03,385 root INFO Walk iteration: 5/10
21:19:03,386 root INFO Walk iteration: 6/10
21:19:03,387 root INFO Walk iteration: 7/10
21:19:03,388 root INFO Walk iteration: 8/10
21:19:03,389 root INFO Walk iteration: 9/10
21:19:03,390 root INFO Walk iteration: 10/10
21:19:03,391 root INFO Learning embeddings at time 1730492343.3910718
21:19:03,391 gensim.models.word2vec INFO collecting all words and their counts
21:19:03,391 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:19:03,392 gensim.models.word2vec INFO collected 25 word types from a corpus of 4680 raw words and 250 sentences
21:19:03,400 gensim.models.word2vec INFO Creating a fresh vocabulary
21:19:03,407 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 25 unique words (100.00% of original 25, drops 0)', 'datetime': '2024-11-01T21:19:03.407879', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,408 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4680 word corpus (100.00% of original 4680, drops 0)', 'datetime': '2024-11-01T21:19:03.408345', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,408 gensim.models.word2vec INFO deleting the raw counts dictionary of 25 items
21:19:03,409 gensim.models.word2vec INFO sample=0.001 downsamples 25 most-common words
21:19:03,409 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 789.4076468615714 word corpus (16.9%% of prior 4680)', 'datetime': '2024-11-01T21:19:03.409087', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,410 gensim.models.word2vec INFO estimated required memory for 25 words and 1536 dimensions: 319700 bytes
21:19:03,410 gensim.models.word2vec INFO resetting layer weights
21:19:03,411 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:19:03.410987', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:19:03,411 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 25 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:19:03.411292', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:19:03,426 gensim.models.word2vec INFO EPOCH 0: training on 4680 raw words (811 effective words) took 0.0s, 70789 effective words/s
21:19:03,438 gensim.models.word2vec INFO EPOCH 1: training on 4680 raw words (772 effective words) took 0.0s, 82560 effective words/s
21:19:03,448 gensim.models.word2vec INFO EPOCH 2: training on 4680 raw words (764 effective words) took 0.0s, 98495 effective words/s
21:19:03,448 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 14040 raw words (2347 effective words) took 0.0s, 64076 effective words/s', 'datetime': '2024-11-01T21:19:03.448451', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:19:03,449 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=25, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:19:03.449124', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:19:03,449 root INFO Completed. Ending time is 1730492343.449256 Elapsed time is -0.1164851188659668
21:19:03,470 root INFO Starting preprocessing of transition probabilities on graph with 25 nodes and 38 edges
21:19:03,470 root INFO Starting at time 1730492343.4701533
21:19:03,470 root INFO Beginning preprocessing of transition probabilities for 25 vertices
21:19:03,471 root INFO Completed 1 / 25 vertices
21:19:03,471 root INFO Completed 3 / 25 vertices
21:19:03,471 root INFO Completed 5 / 25 vertices
21:19:03,472 root INFO Completed 7 / 25 vertices
21:19:03,472 root INFO Completed 9 / 25 vertices
21:19:03,472 root INFO Completed 11 / 25 vertices
21:19:03,480 root INFO Completed 13 / 25 vertices
21:19:03,480 root INFO Completed 15 / 25 vertices
21:19:03,480 root INFO Completed 17 / 25 vertices
21:19:03,480 root INFO Completed 19 / 25 vertices
21:19:03,481 root INFO Completed 21 / 25 vertices
21:19:03,481 root INFO Completed 23 / 25 vertices
21:19:03,482 root INFO Completed 25 / 25 vertices
21:19:03,482 root INFO Completed preprocessing of transition probabilities for vertices
21:19:03,482 root INFO Beginning preprocessing of transition probabilities for 38 edges
21:19:03,482 root INFO Completed 1 / 38 edges
21:19:03,489 root INFO Completed 4 / 38 edges
21:19:03,489 root INFO Completed 7 / 38 edges
21:19:03,490 root INFO Completed 10 / 38 edges
21:19:03,491 root INFO Completed 13 / 38 edges
21:19:03,491 root INFO Completed 16 / 38 edges
21:19:03,492 root INFO Completed 19 / 38 edges
21:19:03,493 root INFO Completed 22 / 38 edges
21:19:03,493 root INFO Completed 25 / 38 edges
21:19:03,494 root INFO Completed 28 / 38 edges
21:19:03,494 root INFO Completed 31 / 38 edges
21:19:03,495 root INFO Completed 34 / 38 edges
21:19:03,496 root INFO Completed 37 / 38 edges
21:19:03,496 root INFO Completed preprocessing of transition probabilities for edges
21:19:03,497 root INFO Simulating walks on graph at time 1730492343.4973674
21:19:03,497 root INFO Walk iteration: 1/10
21:19:03,501 root INFO Walk iteration: 2/10
21:19:03,504 root INFO Walk iteration: 3/10
21:19:03,510 root INFO Walk iteration: 4/10
21:19:03,512 root INFO Walk iteration: 5/10
21:19:03,516 root INFO Walk iteration: 6/10
21:19:03,518 root INFO Walk iteration: 7/10
21:19:03,525 root INFO Walk iteration: 8/10
21:19:03,528 root INFO Walk iteration: 9/10
21:19:03,531 root INFO Walk iteration: 10/10
21:19:03,534 root INFO Learning embeddings at time 1730492343.5341213
21:19:03,535 gensim.models.word2vec INFO collecting all words and their counts
21:19:03,535 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:19:03,536 gensim.models.word2vec INFO collected 25 word types from a corpus of 4680 raw words and 250 sentences
21:19:03,536 gensim.models.word2vec INFO Creating a fresh vocabulary
21:19:03,537 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 25 unique words (100.00% of original 25, drops 0)', 'datetime': '2024-11-01T21:19:03.537112', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,537 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4680 word corpus (100.00% of original 4680, drops 0)', 'datetime': '2024-11-01T21:19:03.537574', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,538 gensim.models.word2vec INFO deleting the raw counts dictionary of 25 items
21:19:03,538 gensim.models.word2vec INFO sample=0.001 downsamples 25 most-common words
21:19:03,538 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 789.4076468615714 word corpus (16.9%% of prior 4680)', 'datetime': '2024-11-01T21:19:03.538912', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:19:03,539 gensim.models.word2vec INFO estimated required memory for 25 words and 1536 dimensions: 319700 bytes
21:19:03,539 gensim.models.word2vec INFO resetting layer weights
21:19:03,540 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:19:03.540112', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:19:03,540 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 25 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:19:03.540332', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:19:03,550 gensim.models.word2vec INFO EPOCH 0: training on 4680 raw words (811 effective words) took 0.0s, 124277 effective words/s
21:19:03,554 gensim.models.word2vec INFO EPOCH 1: training on 4680 raw words (772 effective words) took 0.0s, 265432 effective words/s
21:19:03,560 gensim.models.word2vec INFO EPOCH 2: training on 4680 raw words (764 effective words) took 0.0s, 174517 effective words/s
21:19:03,560 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 14040 raw words (2347 effective words) took 0.0s, 121745 effective words/s', 'datetime': '2024-11-01T21:19:03.560219', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:19:03,560 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=25, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:19:03.560725', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:19:03,560 root INFO Completed. Ending time is 1730492343.5607557 Elapsed time is -0.09060239791870117
21:19:03,587 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:19:03,733 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:19:03,734 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:19:03,741 datashaper.workflow.workflow INFO executing verb create_final_entities
21:19:03,745 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:19:03,783 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:19:03,783 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:19:03,784 graphrag.index.operations.embed_text.strategies.openai INFO embedding 25 inputs via 25 snippets using 2 batches. max_batch_size=16, max_tokens=8191
21:19:04,592 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:19:04,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8350407178513706. input_tokens=475, output_tokens=0
21:19:05,19 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:19:05,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2546892559621483. input_tokens=224, output_tokens=0
21:19:05,45 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:19:05,208 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:19:05,208 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:19:05,216 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:19:08,43 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:19:08,209 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:19:08,209 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:19:08,218 datashaper.workflow.workflow INFO executing verb create_final_communities
21:19:08,231 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:19:08,369 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:19:08,369 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:19:08,374 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:19:08,383 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:19:08,390 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:19:08,531 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
21:19:08,531 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:19:08,534 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:19:08,539 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:19:08,546 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:19:08,556 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:19:08,702 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:19:08,702 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:19:08,706 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:19:08,714 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:19:08,720 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 10
21:19:08,730 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 70
21:19:10,976 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:10,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2241547119338065. input_tokens=2055, output_tokens=251
21:19:12,971 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:12,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.2211172638926655. input_tokens=2409, output_tokens=709
21:19:16,272 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:16,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2900837981142104. input_tokens=2827, output_tokens=635
21:19:16,724 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:16,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7401459829416126. input_tokens=2569, output_tokens=585
21:19:17,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:19:17,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.411618301877752. input_tokens=2597, output_tokens=898
21:19:17,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:19:17,551 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:19:17,551 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:19:17,560 datashaper.workflow.workflow INFO executing verb create_final_documents
21:19:17,567 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:19:17,584 graphrag.index.cli INFO All workflows completed successfully.
21:27:35,245 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:27:35,246 graphrag.index.cli INFO Starting pipeline run for: 20241101-212735, dryrun=False
21:27:35,247 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:27:35,249 graphrag.index.create_pipeline_config INFO skipping workflows 
21:27:35,249 graphrag.index.run.run INFO Running pipeline
21:27:35,249 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:27:35,250 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:27:35,250 graphrag.index.input.load_input INFO using file storage for input
21:27:35,250 graphrag.index.input.csv INFO Loading csv files from input_eval
21:27:35,251 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:27:35,253 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:27:35,253 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:27:35,254 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:27:35,255 graphrag.index.run.run INFO Final # of rows loaded: 1
21:27:35,365 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:27:35,368 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:27:35,795 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:27:35,924 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:27:35,924 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:27:35,932 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:27:35,933 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:27:35,972 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:27:35,972 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:27:38,744 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:27:38,755 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.780426725046709. input_tokens=2087, output_tokens=527
21:27:46,376 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:27:46,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.622642578091472. input_tokens=38, output_tokens=2383
21:27:47,191 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:27:47,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8098029689863324. input_tokens=30, output_tokens=1
21:27:54,44 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:27:54,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.852728472091258. input_tokens=38, output_tokens=2163
21:27:54,68 root INFO Starting preprocessing of transition probabilities on graph with 17 nodes and 22 edges
21:27:54,68 root INFO Starting at time 1730492874.068509
21:27:54,68 root INFO Beginning preprocessing of transition probabilities for 17 vertices
21:27:54,68 root INFO Completed 1 / 17 vertices
21:27:54,68 root INFO Completed 2 / 17 vertices
21:27:54,68 root INFO Completed 3 / 17 vertices
21:27:54,69 root INFO Completed 4 / 17 vertices
21:27:54,70 root INFO Completed 5 / 17 vertices
21:27:54,70 root INFO Completed 6 / 17 vertices
21:27:54,70 root INFO Completed 7 / 17 vertices
21:27:54,70 root INFO Completed 8 / 17 vertices
21:27:54,70 root INFO Completed 9 / 17 vertices
21:27:54,70 root INFO Completed 10 / 17 vertices
21:27:54,70 root INFO Completed 11 / 17 vertices
21:27:54,70 root INFO Completed 12 / 17 vertices
21:27:54,70 root INFO Completed 13 / 17 vertices
21:27:54,70 root INFO Completed 14 / 17 vertices
21:27:54,70 root INFO Completed 15 / 17 vertices
21:27:54,70 root INFO Completed 16 / 17 vertices
21:27:54,70 root INFO Completed 17 / 17 vertices
21:27:54,70 root INFO Completed preprocessing of transition probabilities for vertices
21:27:54,70 root INFO Beginning preprocessing of transition probabilities for 22 edges
21:27:54,70 root INFO Completed 1 / 22 edges
21:27:54,71 root INFO Completed 3 / 22 edges
21:27:54,72 root INFO Completed 5 / 22 edges
21:27:54,73 root INFO Completed 7 / 22 edges
21:27:54,73 root INFO Completed 9 / 22 edges
21:27:54,74 root INFO Completed 11 / 22 edges
21:27:54,75 root INFO Completed 13 / 22 edges
21:27:54,76 root INFO Completed 15 / 22 edges
21:27:54,76 root INFO Completed 17 / 22 edges
21:27:54,77 root INFO Completed 19 / 22 edges
21:27:54,78 root INFO Completed 21 / 22 edges
21:27:54,79 root INFO Completed preprocessing of transition probabilities for edges
21:27:54,79 root INFO Simulating walks on graph at time 1730492874.0799086
21:27:54,80 root INFO Walk iteration: 1/10
21:27:54,81 root INFO Walk iteration: 2/10
21:27:54,83 root INFO Walk iteration: 3/10
21:27:54,84 root INFO Walk iteration: 4/10
21:27:54,84 root INFO Walk iteration: 5/10
21:27:54,85 root INFO Walk iteration: 6/10
21:27:54,86 root INFO Walk iteration: 7/10
21:27:54,87 root INFO Walk iteration: 8/10
21:27:54,87 root INFO Walk iteration: 9/10
21:27:54,88 root INFO Walk iteration: 10/10
21:27:54,89 root INFO Learning embeddings at time 1730492874.0893831
21:27:54,89 gensim.models.word2vec INFO collecting all words and their counts
21:27:54,90 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:27:54,91 gensim.models.word2vec INFO collected 17 word types from a corpus of 3280 raw words and 170 sentences
21:27:54,91 gensim.models.word2vec INFO Creating a fresh vocabulary
21:27:54,91 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 17 unique words (100.00% of original 17, drops 0)', 'datetime': '2024-11-01T21:27:54.091864', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:27:54,92 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3280 word corpus (100.00% of original 3280, drops 0)', 'datetime': '2024-11-01T21:27:54.092510', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:27:54,93 gensim.models.word2vec INFO deleting the raw counts dictionary of 17 items
21:27:54,93 gensim.models.word2vec INFO sample=0.001 downsamples 17 most-common words
21:27:54,94 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 444.05783035515105 word corpus (13.5%% of prior 3280)', 'datetime': '2024-11-01T21:27:54.094741', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:27:54,95 gensim.models.word2vec INFO estimated required memory for 17 words and 1536 dimensions: 217396 bytes
21:27:54,96 gensim.models.word2vec INFO resetting layer weights
21:27:54,96 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:27:54.096541', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:27:54,97 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 17 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:27:54.097007', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:27:54,102 gensim.models.word2vec INFO EPOCH 0: training on 3280 raw words (461 effective words) took 0.0s, 158054 effective words/s
21:27:54,109 gensim.models.word2vec INFO EPOCH 1: training on 3280 raw words (480 effective words) took 0.0s, 106815 effective words/s
21:27:54,116 gensim.models.word2vec INFO EPOCH 2: training on 3280 raw words (417 effective words) took 0.0s, 74677 effective words/s
21:27:54,117 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 9840 raw words (1358 effective words) took 0.0s, 70952 effective words/s', 'datetime': '2024-11-01T21:27:54.117094', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:27:54,117 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=17, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:27:54.117926', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:27:54,118 root INFO Completed. Ending time is 1730492874.1186683 Elapsed time is -0.05015921592712402
21:27:54,151 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:27:54,289 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:27:54,290 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:27:54,298 datashaper.workflow.workflow INFO executing verb create_final_entities
21:27:54,301 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:27:54,338 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:27:54,338 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:27:54,339 graphrag.index.operations.embed_text.strategies.openai INFO embedding 17 inputs via 17 snippets using 2 batches. max_batch_size=16, max_tokens=8191
21:27:54,842 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:27:54,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5073255270253867. input_tokens=40, output_tokens=0
21:27:55,227 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:27:55,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9101216308772564. input_tokens=587, output_tokens=0
21:27:55,256 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:27:55,400 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:27:55,400 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:27:55,408 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:27:57,810 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:27:57,960 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:27:57,960 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:27:57,970 datashaper.workflow.workflow INFO executing verb create_final_communities
21:27:57,982 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:27:58,117 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:27:58,117 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:27:58,123 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:27:58,131 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:27:58,137 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:27:58,275 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
21:27:58,276 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:27:58,281 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:27:58,283 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:27:58,291 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:27:58,300 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:27:58,438 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:27:58,439 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:27:58,443 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:27:58,451 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:27:58,454 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 17
21:28:00,932 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:28:00,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4633273009676486. input_tokens=2291, output_tokens=420
21:28:01,3 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:28:01,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.52762188995257. input_tokens=2121, output_tokens=417
21:28:01,467 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:28:01,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.996427508071065. input_tokens=2325, output_tokens=522
21:28:02,357 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:28:02,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.884104226017371. input_tokens=2657, output_tokens=648
21:28:02,374 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:28:02,530 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:28:02,536 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:28:02,545 datashaper.workflow.workflow INFO executing verb create_final_documents
21:28:02,552 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:28:02,570 graphrag.index.cli INFO All workflows completed successfully.
21:29:02,447 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:29:02,448 graphrag.index.cli INFO Starting pipeline run for: 20241101-212902, dryrun=False
21:29:02,448 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:29:02,451 graphrag.index.create_pipeline_config INFO skipping workflows 
21:29:02,451 graphrag.index.run.run INFO Running pipeline
21:29:02,451 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:29:02,453 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:29:02,453 graphrag.index.input.load_input INFO using file storage for input
21:29:02,453 graphrag.index.input.csv INFO Loading csv files from input_eval
21:29:02,454 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:29:02,457 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:29:02,457 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:29:02,458 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:29:02,459 graphrag.index.run.run INFO Final # of rows loaded: 1
21:29:02,575 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:29:02,577 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:29:03,48 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:29:03,177 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:29:03,177 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:29:03,184 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:29:03,185 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:29:03,226 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:29:03,226 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:29:05,961 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:05,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7397334100678563. input_tokens=2087, output_tokens=527
21:29:13,37 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:13,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.071970816934481. input_tokens=38, output_tokens=2282
21:29:13,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:13,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6584608680568635. input_tokens=30, output_tokens=1
21:29:20,152 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:20,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.4515236569568515. input_tokens=38, output_tokens=2118
21:29:20,912 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:20,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.7554754200391471. input_tokens=30, output_tokens=1
21:29:28,835 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:28,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 7.921772253001109. input_tokens=38, output_tokens=2583
21:29:29,643 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:29,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8051082869060338. input_tokens=30, output_tokens=1
21:29:38,433 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:38,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 8.788576985010877. input_tokens=38, output_tokens=3041
21:29:39,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:39,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8345981629099697. input_tokens=30, output_tokens=1
21:29:49,156 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:49,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 9.883129680063576. input_tokens=38, output_tokens=3346
21:29:50,91 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:29:50,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9315969189628959. input_tokens=30, output_tokens=1
21:30:01,945 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:01,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 11.852331890026107. input_tokens=38, output_tokens=3926
21:30:03,66 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:03,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.113973799161613. input_tokens=30, output_tokens=1
21:30:16,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:16,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 13.211187402019277. input_tokens=38, output_tokens=4613
21:30:16,310 root INFO Starting preprocessing of transition probabilities on graph with 31 nodes and 53 edges
21:30:16,310 root INFO Starting at time 1730493016.3107507
21:30:16,310 root INFO Beginning preprocessing of transition probabilities for 31 vertices
21:30:16,310 root INFO Completed 1 / 31 vertices
21:30:16,310 root INFO Completed 4 / 31 vertices
21:30:16,310 root INFO Completed 7 / 31 vertices
21:30:16,311 root INFO Completed 10 / 31 vertices
21:30:16,311 root INFO Completed 13 / 31 vertices
21:30:16,311 root INFO Completed 16 / 31 vertices
21:30:16,311 root INFO Completed 19 / 31 vertices
21:30:16,312 root INFO Completed 22 / 31 vertices
21:30:16,312 root INFO Completed 25 / 31 vertices
21:30:16,312 root INFO Completed 28 / 31 vertices
21:30:16,312 root INFO Completed 31 / 31 vertices
21:30:16,312 root INFO Completed preprocessing of transition probabilities for vertices
21:30:16,312 root INFO Beginning preprocessing of transition probabilities for 53 edges
21:30:16,312 root INFO Completed 1 / 53 edges
21:30:16,312 root INFO Completed 6 / 53 edges
21:30:16,313 root INFO Completed 11 / 53 edges
21:30:16,313 root INFO Completed 16 / 53 edges
21:30:16,313 root INFO Completed 21 / 53 edges
21:30:16,314 root INFO Completed 26 / 53 edges
21:30:16,314 root INFO Completed 31 / 53 edges
21:30:16,315 root INFO Completed 36 / 53 edges
21:30:16,315 root INFO Completed 41 / 53 edges
21:30:16,315 root INFO Completed 46 / 53 edges
21:30:16,315 root INFO Completed 51 / 53 edges
21:30:16,316 root INFO Completed preprocessing of transition probabilities for edges
21:30:16,317 root INFO Simulating walks on graph at time 1730493016.317158
21:30:16,317 root INFO Walk iteration: 1/10
21:30:16,318 root INFO Walk iteration: 2/10
21:30:16,319 root INFO Walk iteration: 3/10
21:30:16,320 root INFO Walk iteration: 4/10
21:30:16,320 root INFO Walk iteration: 5/10
21:30:16,321 root INFO Walk iteration: 6/10
21:30:16,322 root INFO Walk iteration: 7/10
21:30:16,322 root INFO Walk iteration: 8/10
21:30:16,323 root INFO Walk iteration: 9/10
21:30:16,324 root INFO Walk iteration: 10/10
21:30:16,325 root INFO Learning embeddings at time 1730493016.3252091
21:30:16,325 gensim.models.word2vec INFO collecting all words and their counts
21:30:16,325 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:30:16,325 gensim.models.word2vec INFO collected 31 word types from a corpus of 5960 raw words and 310 sentences
21:30:16,326 gensim.models.word2vec INFO Creating a fresh vocabulary
21:30:16,326 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 31 unique words (100.00% of original 31, drops 0)', 'datetime': '2024-11-01T21:30:16.326477', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,327 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5960 word corpus (100.00% of original 5960, drops 0)', 'datetime': '2024-11-01T21:30:16.327027', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,327 gensim.models.word2vec INFO deleting the raw counts dictionary of 31 items
21:30:16,327 gensim.models.word2vec INFO sample=0.001 downsamples 31 most-common words
21:30:16,327 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1124.45948316988 word corpus (18.9%% of prior 5960)', 'datetime': '2024-11-01T21:30:16.327627', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,328 gensim.models.word2vec INFO estimated required memory for 31 words and 1536 dimensions: 396428 bytes
21:30:16,328 gensim.models.word2vec INFO resetting layer weights
21:30:16,328 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:30:16.328901', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:30:16,329 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 31 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:30:16.329326', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:30:16,334 gensim.models.word2vec INFO EPOCH 0: training on 5960 raw words (1158 effective words) took 0.0s, 374475 effective words/s
21:30:16,340 gensim.models.word2vec INFO EPOCH 1: training on 5960 raw words (1116 effective words) took 0.0s, 241973 effective words/s
21:30:16,347 gensim.models.word2vec INFO EPOCH 2: training on 5960 raw words (1161 effective words) took 0.0s, 181087 effective words/s
21:30:16,347 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 17880 raw words (3435 effective words) took 0.0s, 193817 effective words/s', 'datetime': '2024-11-01T21:30:16.347503', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:30:16,347 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=31, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:30:16.347538', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:30:16,348 root INFO Completed. Ending time is 1730493016.3483 Elapsed time is -0.03754925727844238
21:30:16,356 root INFO Starting preprocessing of transition probabilities on graph with 31 nodes and 53 edges
21:30:16,356 root INFO Starting at time 1730493016.3560793
21:30:16,356 root INFO Beginning preprocessing of transition probabilities for 31 vertices
21:30:16,356 root INFO Completed 1 / 31 vertices
21:30:16,356 root INFO Completed 4 / 31 vertices
21:30:16,357 root INFO Completed 7 / 31 vertices
21:30:16,357 root INFO Completed 10 / 31 vertices
21:30:16,358 root INFO Completed 13 / 31 vertices
21:30:16,358 root INFO Completed 16 / 31 vertices
21:30:16,359 root INFO Completed 19 / 31 vertices
21:30:16,360 root INFO Completed 22 / 31 vertices
21:30:16,360 root INFO Completed 25 / 31 vertices
21:30:16,360 root INFO Completed 28 / 31 vertices
21:30:16,360 root INFO Completed 31 / 31 vertices
21:30:16,361 root INFO Completed preprocessing of transition probabilities for vertices
21:30:16,361 root INFO Beginning preprocessing of transition probabilities for 53 edges
21:30:16,362 root INFO Completed 1 / 53 edges
21:30:16,362 root INFO Completed 6 / 53 edges
21:30:16,363 root INFO Completed 11 / 53 edges
21:30:16,363 root INFO Completed 16 / 53 edges
21:30:16,364 root INFO Completed 21 / 53 edges
21:30:16,364 root INFO Completed 26 / 53 edges
21:30:16,365 root INFO Completed 31 / 53 edges
21:30:16,365 root INFO Completed 36 / 53 edges
21:30:16,366 root INFO Completed 41 / 53 edges
21:30:16,366 root INFO Completed 46 / 53 edges
21:30:16,367 root INFO Completed 51 / 53 edges
21:30:16,368 root INFO Completed preprocessing of transition probabilities for edges
21:30:16,368 root INFO Simulating walks on graph at time 1730493016.3685007
21:30:16,368 root INFO Walk iteration: 1/10
21:30:16,371 root INFO Walk iteration: 2/10
21:30:16,373 root INFO Walk iteration: 3/10
21:30:16,375 root INFO Walk iteration: 4/10
21:30:16,376 root INFO Walk iteration: 5/10
21:30:16,377 root INFO Walk iteration: 6/10
21:30:16,378 root INFO Walk iteration: 7/10
21:30:16,379 root INFO Walk iteration: 8/10
21:30:16,380 root INFO Walk iteration: 9/10
21:30:16,381 root INFO Walk iteration: 10/10
21:30:16,382 root INFO Learning embeddings at time 1730493016.3820121
21:30:16,382 gensim.models.word2vec INFO collecting all words and their counts
21:30:16,382 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:30:16,382 gensim.models.word2vec INFO collected 31 word types from a corpus of 5960 raw words and 310 sentences
21:30:16,383 gensim.models.word2vec INFO Creating a fresh vocabulary
21:30:16,383 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 31 unique words (100.00% of original 31, drops 0)', 'datetime': '2024-11-01T21:30:16.383359', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,383 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5960 word corpus (100.00% of original 5960, drops 0)', 'datetime': '2024-11-01T21:30:16.383868', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,384 gensim.models.word2vec INFO deleting the raw counts dictionary of 31 items
21:30:16,385 gensim.models.word2vec INFO sample=0.001 downsamples 31 most-common words
21:30:16,385 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1124.45948316988 word corpus (18.9%% of prior 5960)', 'datetime': '2024-11-01T21:30:16.385251', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:30:16,385 gensim.models.word2vec INFO estimated required memory for 31 words and 1536 dimensions: 396428 bytes
21:30:16,386 gensim.models.word2vec INFO resetting layer weights
21:30:16,386 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:30:16.386619', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:30:16,386 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 31 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:30:16.386944', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:30:16,393 gensim.models.word2vec INFO EPOCH 0: training on 5960 raw words (1158 effective words) took 0.0s, 269859 effective words/s
21:30:16,402 gensim.models.word2vec INFO EPOCH 1: training on 5960 raw words (1116 effective words) took 0.0s, 146399 effective words/s
21:30:16,418 gensim.models.word2vec INFO EPOCH 2: training on 5960 raw words (1161 effective words) took 0.0s, 85033 effective words/s
21:30:16,418 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 17880 raw words (3435 effective words) took 0.0s, 111191 effective words/s', 'datetime': '2024-11-01T21:30:16.418419', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:30:16,419 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=31, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:30:16.419175', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:30:16,419 root INFO Completed. Ending time is 1730493016.4197948 Elapsed time is -0.06371545791625977
21:30:16,473 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:30:16,637 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:30:16,637 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:30:16,646 datashaper.workflow.workflow INFO executing verb create_final_entities
21:30:16,652 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:30:16,689 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:30:16,689 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:30:16,691 graphrag.index.operations.embed_text.strategies.openai INFO embedding 38 inputs via 38 snippets using 3 batches. max_batch_size=16, max_tokens=8191
21:30:17,571 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:30:17,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9049803719390184. input_tokens=474, output_tokens=0
21:30:17,598 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:30:17,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9292850489728153. input_tokens=478, output_tokens=0
21:30:17,883 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:30:17,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2019797759130597. input_tokens=239, output_tokens=0
21:30:17,900 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:30:18,69 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:30:18,70 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:30:18,79 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:30:20,788 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:30:20,953 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:30:20,954 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:30:20,965 datashaper.workflow.workflow INFO executing verb create_final_communities
21:30:20,979 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:30:21,116 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:30:21,117 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:30:21,125 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:30:21,138 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:30:21,148 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:30:21,293 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
21:30:21,294 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:30:21,297 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:30:21,301 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:30:21,308 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:30:21,318 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:30:21,452 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:30:21,452 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:30:21,459 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:30:21,468 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:30:21,475 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 17
21:30:21,488 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 101
21:30:23,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:23,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.912813439965248. input_tokens=2055, output_tokens=282
21:30:23,715 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:23,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.194671052042395. input_tokens=2079, output_tokens=340
21:30:23,829 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:23,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.314176922896877. input_tokens=2070, output_tokens=363
21:30:24,686 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:24,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1689622302073985. input_tokens=2271, output_tokens=478
21:30:27,662 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:27,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9523412380367517. input_tokens=2830, output_tokens=552
21:30:27,680 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:27,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.967665940988809. input_tokens=2485, output_tokens=538
21:30:27,699 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:27,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9916367779951543. input_tokens=2883, output_tokens=520
21:30:27,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:30:27,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2008697220589966. input_tokens=2407, output_tokens=608
21:30:27,923 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:30:28,69 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:30:28,72 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:30:28,81 datashaper.workflow.workflow INFO executing verb create_final_documents
21:30:28,87 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:30:28,105 graphrag.index.cli INFO All workflows completed successfully.
21:34:15,621 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:34:15,622 graphrag.index.cli INFO Starting pipeline run for: 20241101-213415, dryrun=False
21:34:15,623 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:34:15,625 graphrag.index.create_pipeline_config INFO skipping workflows 
21:34:15,625 graphrag.index.run.run INFO Running pipeline
21:34:15,625 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:34:15,626 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:34:15,626 graphrag.index.input.load_input INFO using file storage for input
21:34:15,626 graphrag.index.input.csv INFO Loading csv files from input_eval
21:34:15,626 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:34:15,630 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:34:15,630 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:34:15,630 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:34:15,631 graphrag.index.run.run INFO Final # of rows loaded: 1
21:34:15,757 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:34:15,760 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:34:16,75 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:34:16,215 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:34:16,215 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:34:16,221 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:34:16,222 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:34:16,262 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:34:16,262 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:34:18,555 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:18,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.296473175054416. input_tokens=2087, output_tokens=523
21:34:21,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:21,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.9852589769288898. input_tokens=34, output_tokens=794
21:34:22,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:22,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4693449689075351. input_tokens=30, output_tokens=1
21:34:23,453 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:23,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.4370519490912557. input_tokens=34, output_tokens=258
21:34:23,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:23,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5214865789748728. input_tokens=30, output_tokens=1
21:34:25,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:25,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 1.1959722449537367. input_tokens=34, output_tokens=241
21:34:25,697 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:25,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.5218319618143141. input_tokens=30, output_tokens=1
21:34:26,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:26,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.8629539120011032. input_tokens=34, output_tokens=75
21:34:27,110 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:27,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.5464152172207832. input_tokens=30, output_tokens=1
21:34:28,40 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:28,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.9297431439626962. input_tokens=34, output_tokens=125
21:34:28,576 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:28,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5366926579736173. input_tokens=30, output_tokens=1
21:34:29,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:29,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.7999105348717421. input_tokens=34, output_tokens=79
21:34:29,973 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:29,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.5941290110349655. input_tokens=30, output_tokens=1
21:34:30,791 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:30,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.8165072090923786. input_tokens=34, output_tokens=87
21:34:30,824 root INFO Starting preprocessing of transition probabilities on graph with 25 nodes and 27 edges
21:34:30,824 root INFO Starting at time 1730493270.8240654
21:34:30,824 root INFO Beginning preprocessing of transition probabilities for 25 vertices
21:34:30,824 root INFO Completed 1 / 25 vertices
21:34:30,824 root INFO Completed 3 / 25 vertices
21:34:30,825 root INFO Completed 5 / 25 vertices
21:34:30,825 root INFO Completed 7 / 25 vertices
21:34:30,825 root INFO Completed 9 / 25 vertices
21:34:30,826 root INFO Completed 11 / 25 vertices
21:34:30,835 root INFO Completed 13 / 25 vertices
21:34:30,836 root INFO Completed 15 / 25 vertices
21:34:30,837 root INFO Completed 17 / 25 vertices
21:34:30,837 root INFO Completed 19 / 25 vertices
21:34:30,838 root INFO Completed 21 / 25 vertices
21:34:30,839 root INFO Completed 23 / 25 vertices
21:34:30,840 root INFO Completed 25 / 25 vertices
21:34:30,841 root INFO Completed preprocessing of transition probabilities for vertices
21:34:30,842 root INFO Beginning preprocessing of transition probabilities for 27 edges
21:34:30,842 root INFO Completed 1 / 27 edges
21:34:30,844 root INFO Completed 3 / 27 edges
21:34:30,844 root INFO Completed 5 / 27 edges
21:34:30,845 root INFO Completed 7 / 27 edges
21:34:30,846 root INFO Completed 9 / 27 edges
21:34:30,847 root INFO Completed 11 / 27 edges
21:34:30,848 root INFO Completed 13 / 27 edges
21:34:30,849 root INFO Completed 15 / 27 edges
21:34:30,850 root INFO Completed 17 / 27 edges
21:34:30,850 root INFO Completed 19 / 27 edges
21:34:30,850 root INFO Completed 21 / 27 edges
21:34:30,851 root INFO Completed 23 / 27 edges
21:34:30,852 root INFO Completed 25 / 27 edges
21:34:30,852 root INFO Completed 27 / 27 edges
21:34:30,853 root INFO Completed preprocessing of transition probabilities for edges
21:34:30,853 root INFO Simulating walks on graph at time 1730493270.853496
21:34:30,854 root INFO Walk iteration: 1/10
21:34:30,858 root INFO Walk iteration: 2/10
21:34:30,861 root INFO Walk iteration: 3/10
21:34:30,863 root INFO Walk iteration: 4/10
21:34:30,864 root INFO Walk iteration: 5/10
21:34:30,866 root INFO Walk iteration: 6/10
21:34:30,867 root INFO Walk iteration: 7/10
21:34:30,868 root INFO Walk iteration: 8/10
21:34:30,868 root INFO Walk iteration: 9/10
21:34:30,869 root INFO Walk iteration: 10/10
21:34:30,870 root INFO Learning embeddings at time 1730493270.8704302
21:34:30,870 gensim.models.word2vec INFO collecting all words and their counts
21:34:30,871 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:34:30,871 gensim.models.word2vec INFO collected 25 word types from a corpus of 4560 raw words and 250 sentences
21:34:30,872 gensim.models.word2vec INFO Creating a fresh vocabulary
21:34:30,873 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 25 unique words (100.00% of original 25, drops 0)', 'datetime': '2024-11-01T21:34:30.873040', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:34:30,873 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4560 word corpus (100.00% of original 4560, drops 0)', 'datetime': '2024-11-01T21:34:30.873860', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:34:30,874 gensim.models.word2vec INFO deleting the raw counts dictionary of 25 items
21:34:30,875 gensim.models.word2vec INFO sample=0.001 downsamples 25 most-common words
21:34:30,876 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 758.5718442943083 word corpus (16.6%% of prior 4560)', 'datetime': '2024-11-01T21:34:30.876295', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:34:30,877 gensim.models.word2vec INFO estimated required memory for 25 words and 1536 dimensions: 319700 bytes
21:34:30,878 gensim.models.word2vec INFO resetting layer weights
21:34:30,879 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:34:30.879225', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:34:30,879 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 25 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:34:30.879669', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:34:30,887 gensim.models.word2vec INFO EPOCH 0: training on 4560 raw words (764 effective words) took 0.0s, 161244 effective words/s
21:34:30,900 gensim.models.word2vec INFO EPOCH 1: training on 4560 raw words (772 effective words) took 0.0s, 73991 effective words/s
21:34:30,913 gensim.models.word2vec INFO EPOCH 2: training on 4560 raw words (734 effective words) took 0.0s, 73666 effective words/s
21:34:30,913 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 13680 raw words (2270 effective words) took 0.0s, 66968 effective words/s', 'datetime': '2024-11-01T21:34:30.913621', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:34:30,914 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=25, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:34:30.914531', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:34:30,915 root INFO Completed. Ending time is 1730493270.9154792 Elapsed time is -0.09141373634338379
21:34:30,975 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:34:31,114 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:34:31,114 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:34:31,123 datashaper.workflow.workflow INFO executing verb create_final_entities
21:34:31,127 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:34:31,166 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:34:31,166 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:34:31,167 graphrag.index.operations.embed_text.strategies.openai INFO embedding 27 inputs via 19 snippets using 2 batches. max_batch_size=16, max_tokens=8191
21:34:32,156 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:34:32,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0029336588922888. input_tokens=66, output_tokens=0
21:34:32,461 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:34:32,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3166199079714715. input_tokens=453, output_tokens=0
21:34:32,490 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:34:32,636 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:34:32,637 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:34:32,646 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:34:34,923 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:34:35,92 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:34:35,92 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:34:35,103 datashaper.workflow.workflow INFO executing verb create_final_communities
21:34:35,115 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:34:35,248 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:34:35,248 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:34:35,255 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:34:35,263 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:34:35,270 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:34:35,409 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
21:34:35,409 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:34:35,414 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:34:35,416 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:34:35,427 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:34:35,436 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:34:35,567 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:34:35,568 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:34:35,573 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:34:35,581 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:34:35,584 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 27
21:34:37,625 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:37,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0197619879618287. input_tokens=2023, output_tokens=304
21:34:37,693 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:37,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0800882130861282. input_tokens=2067, output_tokens=237
21:34:37,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:37,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1302331008482724. input_tokens=2070, output_tokens=320
21:34:38,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:38,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.470071626128629. input_tokens=2057, output_tokens=393
21:34:39,356 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:39,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6109510590322316. input_tokens=2047, output_tokens=274
21:34:39,774 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:39,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.133553473977372. input_tokens=2118, output_tokens=405
21:34:39,790 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:39,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.095488494960591. input_tokens=2116, output_tokens=395
21:34:39,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:34:39,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.217844689032063. input_tokens=2628, output_tokens=781
21:34:39,833 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:34:39,991 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:34:39,991 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:34:40,1 datashaper.workflow.workflow INFO executing verb create_final_documents
21:34:40,7 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:34:40,23 graphrag.index.cli INFO All workflows completed successfully.
21:36:17,12 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:36:17,13 graphrag.index.cli INFO Starting pipeline run for: 20241101-213617, dryrun=False
21:36:17,13 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:36:17,15 graphrag.index.create_pipeline_config INFO skipping workflows 
21:36:17,15 graphrag.index.run.run INFO Running pipeline
21:36:17,15 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:36:17,17 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:36:17,17 graphrag.index.input.load_input INFO using file storage for input
21:36:17,18 graphrag.index.input.csv INFO Loading csv files from input_eval
21:36:17,18 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:36:17,22 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:36:17,22 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:36:17,23 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:36:17,23 graphrag.index.run.run INFO Final # of rows loaded: 1
21:36:17,137 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:36:17,137 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:36:43,187 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:36:43,188 graphrag.index.cli INFO Starting pipeline run for: 20241101-213643, dryrun=False
21:36:43,189 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:36:43,191 graphrag.index.create_pipeline_config INFO skipping workflows 
21:36:43,191 graphrag.index.run.run INFO Running pipeline
21:36:43,191 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:36:43,193 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:36:43,193 graphrag.index.input.load_input INFO using file storage for input
21:36:43,194 graphrag.index.input.csv INFO Loading csv files from input_eval
21:36:43,194 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:36:43,197 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:36:43,197 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:36:43,198 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:36:43,199 graphrag.index.run.run INFO Final # of rows loaded: 1
21:36:43,317 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:36:43,319 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:36:43,582 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:36:43,716 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:36:43,716 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:36:43,723 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:36:43,724 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:36:43,763 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:36:43,763 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:36:46,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:36:46,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.9323490720707923. input_tokens=2087, output_tokens=527
21:36:52,523 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:36:52,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.827135445084423. input_tokens=21, output_tokens=1702
21:36:53,98 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:36:53,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5718351409304887. input_tokens=30, output_tokens=1
21:37:00,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:00,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.951522154035047. input_tokens=21, output_tokens=2276
21:37:00,722 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:00,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6676031609531492. input_tokens=30, output_tokens=1
21:37:07,997 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:08,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 7.27331071998924. input_tokens=21, output_tokens=2507
21:37:08,803 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:08,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.802539526950568. input_tokens=30, output_tokens=1
21:37:17,93 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:17,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 8.288119663018733. input_tokens=21, output_tokens=2863
21:37:18,7 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:18,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.9106448509264737. input_tokens=30, output_tokens=1
21:37:29,443 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:29,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 11.434646171983331. input_tokens=21, output_tokens=3956
21:37:30,410 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:30,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9635884419549257. input_tokens=30, output_tokens=1
21:37:43,156 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:43,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.744801582070068. input_tokens=21, output_tokens=4413
21:37:44,259 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:44,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.0994885601103306. input_tokens=30, output_tokens=1
21:37:57,205 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:57,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.9434430799447. input_tokens=21, output_tokens=4541
21:37:57,778 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:37:57,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5565195251256227. input_tokens=150, output_tokens=36
21:37:57,817 root INFO Starting preprocessing of transition probabilities on graph with 46 nodes and 69 edges
21:37:57,817 root INFO Starting at time 1730493477.8172402
21:37:57,817 root INFO Beginning preprocessing of transition probabilities for 46 vertices
21:37:57,817 root INFO Completed 1 / 46 vertices
21:37:57,817 root INFO Completed 5 / 46 vertices
21:37:57,817 root INFO Completed 9 / 46 vertices
21:37:57,818 root INFO Completed 13 / 46 vertices
21:37:57,819 root INFO Completed 17 / 46 vertices
21:37:57,820 root INFO Completed 21 / 46 vertices
21:37:57,821 root INFO Completed 25 / 46 vertices
21:37:57,821 root INFO Completed 29 / 46 vertices
21:37:57,822 root INFO Completed 33 / 46 vertices
21:37:57,823 root INFO Completed 37 / 46 vertices
21:37:57,824 root INFO Completed 41 / 46 vertices
21:37:57,824 root INFO Completed 45 / 46 vertices
21:37:57,824 root INFO Completed preprocessing of transition probabilities for vertices
21:37:57,824 root INFO Beginning preprocessing of transition probabilities for 69 edges
21:37:57,825 root INFO Completed 1 / 69 edges
21:37:57,826 root INFO Completed 7 / 69 edges
21:37:57,827 root INFO Completed 13 / 69 edges
21:37:57,828 root INFO Completed 19 / 69 edges
21:37:57,829 root INFO Completed 25 / 69 edges
21:37:57,829 root INFO Completed 31 / 69 edges
21:37:57,830 root INFO Completed 37 / 69 edges
21:37:57,831 root INFO Completed 43 / 69 edges
21:37:57,831 root INFO Completed 49 / 69 edges
21:37:57,832 root INFO Completed 55 / 69 edges
21:37:57,833 root INFO Completed 61 / 69 edges
21:37:57,834 root INFO Completed 67 / 69 edges
21:37:57,834 root INFO Completed preprocessing of transition probabilities for edges
21:37:57,835 root INFO Simulating walks on graph at time 1730493477.8354495
21:37:57,835 root INFO Walk iteration: 1/10
21:37:57,838 root INFO Walk iteration: 2/10
21:37:57,840 root INFO Walk iteration: 3/10
21:37:57,842 root INFO Walk iteration: 4/10
21:37:57,843 root INFO Walk iteration: 5/10
21:37:57,844 root INFO Walk iteration: 6/10
21:37:57,845 root INFO Walk iteration: 7/10
21:37:57,847 root INFO Walk iteration: 8/10
21:37:57,848 root INFO Walk iteration: 9/10
21:37:57,849 root INFO Walk iteration: 10/10
21:37:57,850 root INFO Learning embeddings at time 1730493477.8503668
21:37:57,850 gensim.models.word2vec INFO collecting all words and their counts
21:37:57,851 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:37:57,851 gensim.models.word2vec INFO collected 46 word types from a corpus of 9120 raw words and 460 sentences
21:37:57,851 gensim.models.word2vec INFO Creating a fresh vocabulary
21:37:57,851 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 46 unique words (100.00% of original 46, drops 0)', 'datetime': '2024-11-01T21:37:57.851857', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,852 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 9120 word corpus (100.00% of original 9120, drops 0)', 'datetime': '2024-11-01T21:37:57.852523', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,853 gensim.models.word2vec INFO deleting the raw counts dictionary of 46 items
21:37:57,853 gensim.models.word2vec INFO sample=0.001 downsamples 44 most-common words
21:37:57,854 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2178.8003398215383 word corpus (23.9%% of prior 9120)', 'datetime': '2024-11-01T21:37:57.854703', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,855 gensim.models.word2vec INFO estimated required memory for 46 words and 1536 dimensions: 588248 bytes
21:37:57,856 gensim.models.word2vec INFO resetting layer weights
21:37:57,857 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:37:57.857227', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:37:57,857 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 46 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:37:57.857856', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:37:57,869 gensim.models.word2vec INFO EPOCH 0: training on 9120 raw words (2225 effective words) took 0.0s, 251836 effective words/s
21:37:57,881 gensim.models.word2vec INFO EPOCH 1: training on 9120 raw words (2165 effective words) took 0.0s, 198926 effective words/s
21:37:57,890 gensim.models.word2vec INFO EPOCH 2: training on 9120 raw words (2229 effective words) took 0.0s, 263432 effective words/s
21:37:57,890 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 27360 raw words (6619 effective words) took 0.0s, 208176 effective words/s', 'datetime': '2024-11-01T21:37:57.890508', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:37:57,890 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=46, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:37:57.890543', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:37:57,891 root INFO Completed. Ending time is 1730493477.891379 Elapsed time is -0.07413887977600098
21:37:57,899 root INFO Starting preprocessing of transition probabilities on graph with 46 nodes and 69 edges
21:37:57,899 root INFO Starting at time 1730493477.8999596
21:37:57,899 root INFO Beginning preprocessing of transition probabilities for 46 vertices
21:37:57,900 root INFO Completed 1 / 46 vertices
21:37:57,900 root INFO Completed 5 / 46 vertices
21:37:57,901 root INFO Completed 9 / 46 vertices
21:37:57,902 root INFO Completed 13 / 46 vertices
21:37:57,903 root INFO Completed 17 / 46 vertices
21:37:57,903 root INFO Completed 21 / 46 vertices
21:37:57,904 root INFO Completed 25 / 46 vertices
21:37:57,905 root INFO Completed 29 / 46 vertices
21:37:57,905 root INFO Completed 33 / 46 vertices
21:37:57,906 root INFO Completed 37 / 46 vertices
21:37:57,906 root INFO Completed 41 / 46 vertices
21:37:57,906 root INFO Completed 45 / 46 vertices
21:37:57,906 root INFO Completed preprocessing of transition probabilities for vertices
21:37:57,907 root INFO Beginning preprocessing of transition probabilities for 69 edges
21:37:57,907 root INFO Completed 1 / 69 edges
21:37:57,908 root INFO Completed 7 / 69 edges
21:37:57,909 root INFO Completed 13 / 69 edges
21:37:57,910 root INFO Completed 19 / 69 edges
21:37:57,911 root INFO Completed 25 / 69 edges
21:37:57,911 root INFO Completed 31 / 69 edges
21:37:57,912 root INFO Completed 37 / 69 edges
21:37:57,913 root INFO Completed 43 / 69 edges
21:37:57,914 root INFO Completed 49 / 69 edges
21:37:57,914 root INFO Completed 55 / 69 edges
21:37:57,915 root INFO Completed 61 / 69 edges
21:37:57,916 root INFO Completed 67 / 69 edges
21:37:57,917 root INFO Completed preprocessing of transition probabilities for edges
21:37:57,918 root INFO Simulating walks on graph at time 1730493477.9180028
21:37:57,918 root INFO Walk iteration: 1/10
21:37:57,923 root INFO Walk iteration: 2/10
21:37:57,926 root INFO Walk iteration: 3/10
21:37:57,928 root INFO Walk iteration: 4/10
21:37:57,929 root INFO Walk iteration: 5/10
21:37:57,931 root INFO Walk iteration: 6/10
21:37:57,932 root INFO Walk iteration: 7/10
21:37:57,933 root INFO Walk iteration: 8/10
21:37:57,934 root INFO Walk iteration: 9/10
21:37:57,935 root INFO Walk iteration: 10/10
21:37:57,936 root INFO Learning embeddings at time 1730493477.936627
21:37:57,936 gensim.models.word2vec INFO collecting all words and their counts
21:37:57,937 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:37:57,938 gensim.models.word2vec INFO collected 46 word types from a corpus of 9120 raw words and 460 sentences
21:37:57,939 gensim.models.word2vec INFO Creating a fresh vocabulary
21:37:57,940 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 46 unique words (100.00% of original 46, drops 0)', 'datetime': '2024-11-01T21:37:57.940323', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,940 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 9120 word corpus (100.00% of original 9120, drops 0)', 'datetime': '2024-11-01T21:37:57.940954', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,941 gensim.models.word2vec INFO deleting the raw counts dictionary of 46 items
21:37:57,942 gensim.models.word2vec INFO sample=0.001 downsamples 44 most-common words
21:37:57,943 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2178.8003398215383 word corpus (23.9%% of prior 9120)', 'datetime': '2024-11-01T21:37:57.943425', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:37:57,944 gensim.models.word2vec INFO estimated required memory for 46 words and 1536 dimensions: 588248 bytes
21:37:57,944 gensim.models.word2vec INFO resetting layer weights
21:37:57,945 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:37:57.945979', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:37:57,946 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 46 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:37:57.946455', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:37:57,965 gensim.models.word2vec INFO EPOCH 0: training on 9120 raw words (2225 effective words) took 0.0s, 131061 effective words/s
21:37:57,975 gensim.models.word2vec INFO EPOCH 1: training on 9120 raw words (2165 effective words) took 0.0s, 244172 effective words/s
21:37:57,984 gensim.models.word2vec INFO EPOCH 2: training on 9120 raw words (2229 effective words) took 0.0s, 288135 effective words/s
21:37:57,984 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 27360 raw words (6619 effective words) took 0.0s, 174895 effective words/s', 'datetime': '2024-11-01T21:37:57.984988', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:37:57,985 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=46, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:37:57.985881', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:37:57,985 root INFO Completed. Ending time is 1730493477.985911 Elapsed time is -0.08595132827758789
21:37:58,31 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:37:58,191 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:37:58,191 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:37:58,201 datashaper.workflow.workflow INFO executing verb create_final_entities
21:37:58,208 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:37:58,246 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:37:58,246 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:37:58,247 graphrag.index.operations.embed_text.strategies.openai INFO embedding 56 inputs via 55 snippets using 4 batches. max_batch_size=16, max_tokens=8191
21:37:59,97 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:37:59,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8715044530108571. input_tokens=360, output_tokens=0
21:37:59,518 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:37:59,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2804148618597537. input_tokens=141, output_tokens=0
21:37:59,631 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:37:59,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4071311699226499. input_tokens=421, output_tokens=0
21:37:59,726 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:37:59,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5023518330417573. input_tokens=339, output_tokens=0
21:37:59,755 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:37:59,927 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:37:59,928 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:37:59,938 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:38:02,662 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:38:02,826 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:38:02,827 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:38:02,838 datashaper.workflow.workflow INFO executing verb create_final_communities
21:38:02,854 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:38:02,987 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:38:02,987 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:38:02,995 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:38:03,4 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:38:03,14 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:38:03,148 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
21:38:03,149 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:38:03,154 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:38:03,158 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:38:03,164 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:38:03,174 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:38:03,308 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:38:03,308 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:38:03,316 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:38:03,325 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:38:03,330 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 36
21:38:03,347 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 116
21:38:06,164 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:06,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7940588230267167. input_tokens=2489, output_tokens=477
21:38:06,392 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:06,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0136227519251406. input_tokens=2290, output_tokens=520
21:38:06,400 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:06,401 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0260311898309737. input_tokens=2257, output_tokens=459
21:38:06,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:06,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.450800344115123. input_tokens=2237, output_tokens=607
21:38:06,870 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:06,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.493097851984203. input_tokens=2268, output_tokens=520
21:38:09,137 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:09,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9669647309929132. input_tokens=2511, output_tokens=581
21:38:11,36 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:11,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8861386079806834. input_tokens=2098, output_tokens=263
21:38:11,262 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:11,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.11553469998762. input_tokens=2045, output_tokens=351
21:38:12,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:12,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1708846539258957. input_tokens=2445, output_tokens=565
21:38:12,642 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:12,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.489766398910433. input_tokens=3419, output_tokens=656
21:38:12,759 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:12,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.610425506019965. input_tokens=2768, output_tokens=675
21:38:13,639 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:38:13,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.596766628790647. input_tokens=2478, output_tokens=511
21:38:13,652 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:38:13,815 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:38:13,815 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:38:13,826 datashaper.workflow.workflow INFO executing verb create_final_documents
21:38:13,833 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:38:13,851 graphrag.index.cli INFO All workflows completed successfully.
21:43:15,500 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:43:15,501 graphrag.index.cli INFO Starting pipeline run for: 20241101-214315, dryrun=False
21:43:15,501 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:43:15,503 graphrag.index.create_pipeline_config INFO skipping workflows 
21:43:15,503 graphrag.index.run.run INFO Running pipeline
21:43:15,503 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:43:15,504 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:43:15,504 graphrag.index.input.load_input INFO using file storage for input
21:43:15,505 graphrag.index.input.csv INFO Loading csv files from input_eval
21:43:15,505 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:43:15,508 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:43:15,508 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:43:15,509 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:43:15,509 graphrag.index.run.run INFO Final # of rows loaded: 1
21:43:15,619 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:43:15,622 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:43:15,990 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:43:16,116 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:43:16,116 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:43:16,124 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:43:16,125 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:43:16,163 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:43:16,163 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:43:18,898 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:18,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7435859700199217. input_tokens=2087, output_tokens=527
21:43:26,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:26,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.05842431099154. input_tokens=38, output_tokens=2561
21:43:27,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:27,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6225681889336556. input_tokens=30, output_tokens=1
21:43:35,776 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:35,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 8.18361053802073. input_tokens=38, output_tokens=2753
21:43:36,616 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:36,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.8341636089608073. input_tokens=30, output_tokens=1
21:43:48,90 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:48,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 11.47289807908237. input_tokens=38, output_tokens=3802
21:43:48,965 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:43:48,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8664363820571452. input_tokens=30, output_tokens=1
21:44:01,113 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:01,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 12.147651764098555. input_tokens=38, output_tokens=4099
21:44:02,180 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:02,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 1.0631375638768077. input_tokens=30, output_tokens=1
21:44:16,180 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:16,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 13.997786009917036. input_tokens=38, output_tokens=0
21:44:17,158 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:17,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9773889719508588. input_tokens=30, output_tokens=1
21:44:30,633 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:30,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 13.473458649823442. input_tokens=38, output_tokens=4529
21:44:31,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:31,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.1077780961059034. input_tokens=30, output_tokens=1
21:44:46,618 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:46,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 14.870442813029513. input_tokens=38, output_tokens=4487
21:44:47,214 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:47,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5847433970775455. input_tokens=167, output_tokens=34
21:44:47,802 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:47,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1700773402117193. input_tokens=195, output_tokens=53
21:44:47,828 root INFO Starting preprocessing of transition probabilities on graph with 33 nodes and 47 edges
21:44:47,828 root INFO Starting at time 1730493887.82809
21:44:47,828 root INFO Beginning preprocessing of transition probabilities for 33 vertices
21:44:47,828 root INFO Completed 1 / 33 vertices
21:44:47,828 root INFO Completed 4 / 33 vertices
21:44:47,829 root INFO Completed 7 / 33 vertices
21:44:47,830 root INFO Completed 10 / 33 vertices
21:44:47,830 root INFO Completed 13 / 33 vertices
21:44:47,831 root INFO Completed 16 / 33 vertices
21:44:47,832 root INFO Completed 19 / 33 vertices
21:44:47,833 root INFO Completed 22 / 33 vertices
21:44:47,834 root INFO Completed 25 / 33 vertices
21:44:47,835 root INFO Completed 28 / 33 vertices
21:44:47,836 root INFO Completed 31 / 33 vertices
21:44:47,836 root INFO Completed preprocessing of transition probabilities for vertices
21:44:47,837 root INFO Beginning preprocessing of transition probabilities for 47 edges
21:44:47,838 root INFO Completed 1 / 47 edges
21:44:47,839 root INFO Completed 5 / 47 edges
21:44:47,840 root INFO Completed 9 / 47 edges
21:44:47,840 root INFO Completed 13 / 47 edges
21:44:47,841 root INFO Completed 17 / 47 edges
21:44:47,842 root INFO Completed 21 / 47 edges
21:44:47,843 root INFO Completed 25 / 47 edges
21:44:47,843 root INFO Completed 29 / 47 edges
21:44:47,844 root INFO Completed 33 / 47 edges
21:44:47,845 root INFO Completed 37 / 47 edges
21:44:47,845 root INFO Completed 41 / 47 edges
21:44:47,846 root INFO Completed 45 / 47 edges
21:44:47,846 root INFO Completed preprocessing of transition probabilities for edges
21:44:47,847 root INFO Simulating walks on graph at time 1730493887.8475175
21:44:47,847 root INFO Walk iteration: 1/10
21:44:47,850 root INFO Walk iteration: 2/10
21:44:47,851 root INFO Walk iteration: 3/10
21:44:47,853 root INFO Walk iteration: 4/10
21:44:47,854 root INFO Walk iteration: 5/10
21:44:47,855 root INFO Walk iteration: 6/10
21:44:47,856 root INFO Walk iteration: 7/10
21:44:47,857 root INFO Walk iteration: 8/10
21:44:47,858 root INFO Walk iteration: 9/10
21:44:47,858 root INFO Walk iteration: 10/10
21:44:47,859 root INFO Learning embeddings at time 1730493887.8596723
21:44:47,859 gensim.models.word2vec INFO collecting all words and their counts
21:44:47,860 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:44:47,861 gensim.models.word2vec INFO collected 33 word types from a corpus of 6560 raw words and 330 sentences
21:44:47,861 gensim.models.word2vec INFO Creating a fresh vocabulary
21:44:47,862 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 33 unique words (100.00% of original 33, drops 0)', 'datetime': '2024-11-01T21:44:47.862309', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,862 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6560 word corpus (100.00% of original 6560, drops 0)', 'datetime': '2024-11-01T21:44:47.862934', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,863 gensim.models.word2vec INFO deleting the raw counts dictionary of 33 items
21:44:47,864 gensim.models.word2vec INFO sample=0.001 downsamples 33 most-common words
21:44:47,865 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1280.0814162306954 word corpus (19.5%% of prior 6560)', 'datetime': '2024-11-01T21:44:47.865220', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,866 gensim.models.word2vec INFO estimated required memory for 33 words and 1536 dimensions: 422004 bytes
21:44:47,866 gensim.models.word2vec INFO resetting layer weights
21:44:47,866 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:44:47.866816', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:44:47,867 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 33 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:44:47.867281', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:44:47,876 gensim.models.word2vec INFO EPOCH 0: training on 6560 raw words (1309 effective words) took 0.0s, 209382 effective words/s
21:44:47,886 gensim.models.word2vec INFO EPOCH 1: training on 6560 raw words (1325 effective words) took 0.0s, 142139 effective words/s
21:44:47,894 gensim.models.word2vec INFO EPOCH 2: training on 6560 raw words (1212 effective words) took 0.0s, 179366 effective words/s
21:44:47,894 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 19680 raw words (3846 effective words) took 0.0s, 144128 effective words/s', 'datetime': '2024-11-01T21:44:47.894597', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:44:47,894 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=33, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:44:47.894633', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:44:47,895 root INFO Completed. Ending time is 1730493887.8958113 Elapsed time is -0.06772136688232422
21:44:47,905 root INFO Starting preprocessing of transition probabilities on graph with 33 nodes and 47 edges
21:44:47,905 root INFO Starting at time 1730493887.9055824
21:44:47,905 root INFO Beginning preprocessing of transition probabilities for 33 vertices
21:44:47,906 root INFO Completed 1 / 33 vertices
21:44:47,906 root INFO Completed 4 / 33 vertices
21:44:47,907 root INFO Completed 7 / 33 vertices
21:44:47,907 root INFO Completed 10 / 33 vertices
21:44:47,908 root INFO Completed 13 / 33 vertices
21:44:47,909 root INFO Completed 16 / 33 vertices
21:44:47,909 root INFO Completed 19 / 33 vertices
21:44:47,910 root INFO Completed 22 / 33 vertices
21:44:47,910 root INFO Completed 25 / 33 vertices
21:44:47,910 root INFO Completed 28 / 33 vertices
21:44:47,910 root INFO Completed 31 / 33 vertices
21:44:47,911 root INFO Completed preprocessing of transition probabilities for vertices
21:44:47,911 root INFO Beginning preprocessing of transition probabilities for 47 edges
21:44:47,912 root INFO Completed 1 / 47 edges
21:44:47,912 root INFO Completed 5 / 47 edges
21:44:47,913 root INFO Completed 9 / 47 edges
21:44:47,913 root INFO Completed 13 / 47 edges
21:44:47,914 root INFO Completed 17 / 47 edges
21:44:47,915 root INFO Completed 21 / 47 edges
21:44:47,916 root INFO Completed 25 / 47 edges
21:44:47,916 root INFO Completed 29 / 47 edges
21:44:47,917 root INFO Completed 33 / 47 edges
21:44:47,918 root INFO Completed 37 / 47 edges
21:44:47,919 root INFO Completed 41 / 47 edges
21:44:47,919 root INFO Completed 45 / 47 edges
21:44:47,920 root INFO Completed preprocessing of transition probabilities for edges
21:44:47,921 root INFO Simulating walks on graph at time 1730493887.921272
21:44:47,921 root INFO Walk iteration: 1/10
21:44:47,926 root INFO Walk iteration: 2/10
21:44:47,928 root INFO Walk iteration: 3/10
21:44:47,930 root INFO Walk iteration: 4/10
21:44:47,931 root INFO Walk iteration: 5/10
21:44:47,933 root INFO Walk iteration: 6/10
21:44:47,934 root INFO Walk iteration: 7/10
21:44:47,934 root INFO Walk iteration: 8/10
21:44:47,935 root INFO Walk iteration: 9/10
21:44:47,936 root INFO Walk iteration: 10/10
21:44:47,937 root INFO Learning embeddings at time 1730493887.937491
21:44:47,937 gensim.models.word2vec INFO collecting all words and their counts
21:44:47,938 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:44:47,938 gensim.models.word2vec INFO collected 33 word types from a corpus of 6560 raw words and 330 sentences
21:44:47,939 gensim.models.word2vec INFO Creating a fresh vocabulary
21:44:47,939 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 33 unique words (100.00% of original 33, drops 0)', 'datetime': '2024-11-01T21:44:47.939907', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,940 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6560 word corpus (100.00% of original 6560, drops 0)', 'datetime': '2024-11-01T21:44:47.940613', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,941 gensim.models.word2vec INFO deleting the raw counts dictionary of 33 items
21:44:47,942 gensim.models.word2vec INFO sample=0.001 downsamples 33 most-common words
21:44:47,942 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1280.0814162306954 word corpus (19.5%% of prior 6560)', 'datetime': '2024-11-01T21:44:47.942673', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:44:47,943 gensim.models.word2vec INFO estimated required memory for 33 words and 1536 dimensions: 422004 bytes
21:44:47,944 gensim.models.word2vec INFO resetting layer weights
21:44:47,945 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:44:47.945248', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:44:47,945 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 33 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:44:47.945760', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:44:47,955 gensim.models.word2vec INFO EPOCH 0: training on 6560 raw words (1309 effective words) took 0.0s, 173137 effective words/s
21:44:47,973 gensim.models.word2vec INFO EPOCH 1: training on 6560 raw words (1325 effective words) took 0.0s, 84224 effective words/s
21:44:47,988 gensim.models.word2vec INFO EPOCH 2: training on 6560 raw words (1212 effective words) took 0.0s, 93536 effective words/s
21:44:47,988 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 19680 raw words (3846 effective words) took 0.0s, 90805 effective words/s', 'datetime': '2024-11-01T21:44:47.988742', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:44:47,989 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=33, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:44:47.989696', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:44:47,990 root INFO Completed. Ending time is 1730493887.9905577 Elapsed time is -0.0849752426147461
21:44:48,47 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:44:48,202 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:44:48,202 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:44:48,211 datashaper.workflow.workflow INFO executing verb create_final_entities
21:44:48,218 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:44:48,254 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:44:48,254 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:44:48,256 graphrag.index.operations.embed_text.strategies.openai INFO embedding 54 inputs via 54 snippets using 4 batches. max_batch_size=16, max_tokens=8191
21:44:49,13 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:44:49,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7678156110923737. input_tokens=167, output_tokens=0
21:44:49,194 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:44:49,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9691788039635867. input_tokens=474, output_tokens=0
21:44:49,741 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:44:49,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5071834190748632. input_tokens=454, output_tokens=0
21:44:49,767 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:44:49,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.533132869983092. input_tokens=456, output_tokens=0
21:44:49,796 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:44:49,963 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:44:49,964 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:44:49,973 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:44:52,697 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:44:52,864 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:44:52,864 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:44:52,875 datashaper.workflow.workflow INFO executing verb create_final_communities
21:44:52,891 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:44:53,26 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:44:53,26 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:44:53,33 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:44:53,42 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:44:53,50 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:44:53,188 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
21:44:53,189 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:44:53,192 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:44:53,203 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:44:53,210 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:44:53,220 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:44:53,361 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:44:53,361 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:44:53,366 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:44:53,375 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:44:53,381 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 31
21:44:53,393 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 123
21:44:55,343 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:55,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9214535329956561. input_tokens=2066, output_tokens=323
21:44:55,781 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:55,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3618899898137897. input_tokens=2173, output_tokens=316
21:44:56,544 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:56,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.125302132917568. input_tokens=2330, output_tokens=514
21:44:58,991 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:58,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.437269473914057. input_tokens=2060, output_tokens=422
21:44:59,517 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:59,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9682029779069126. input_tokens=2586, output_tokens=553
21:44:59,555 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:59,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0035537439398468. input_tokens=2631, output_tokens=581
21:44:59,697 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:44:59,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1398048708215356. input_tokens=2139, output_tokens=486
21:45:00,574 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:45:00,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.018890031147748. input_tokens=2239, output_tokens=622
21:45:02,160 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:45:02,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1630576299503446. input_tokens=2979, output_tokens=627
21:45:02,171 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:45:02,321 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:45:02,321 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:45:02,331 datashaper.workflow.workflow INFO executing verb create_final_documents
21:45:02,338 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:45:02,356 graphrag.index.cli INFO All workflows completed successfully.
21:48:23,276 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:48:23,277 graphrag.index.cli INFO Starting pipeline run for: 20241101-214823, dryrun=False
21:48:23,277 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:48:23,279 graphrag.index.create_pipeline_config INFO skipping workflows 
21:48:23,279 graphrag.index.run.run INFO Running pipeline
21:48:23,279 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:48:23,280 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:48:23,280 graphrag.index.input.load_input INFO using file storage for input
21:48:23,281 graphrag.index.input.csv INFO Loading csv files from input_eval
21:48:23,281 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:48:23,284 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:48:23,284 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:48:23,285 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:48:23,285 graphrag.index.run.run INFO Final # of rows loaded: 1
21:48:23,393 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:48:23,395 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:48:23,722 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:48:23,848 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:48:23,848 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:48:23,856 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:48:23,857 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:48:23,895 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:48:23,895 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:48:26,613 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:26,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.722022379981354. input_tokens=2087, output_tokens=527
21:48:29,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:29,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2957959950435907. input_tokens=27, output_tokens=821
21:48:30,418 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:30,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5036369981244206. input_tokens=30, output_tokens=1
21:48:33,282 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:33,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.854013190837577. input_tokens=27, output_tokens=691
21:48:33,860 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:33,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5751245021820068. input_tokens=30, output_tokens=1
21:48:36,863 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:36,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 3.0024441259447485. input_tokens=27, output_tokens=612
21:48:37,471 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:37,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6056962399743497. input_tokens=30, output_tokens=1
21:48:40,154 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:40,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 2.681630526203662. input_tokens=27, output_tokens=541
21:48:40,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:40,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6772990848403424. input_tokens=30, output_tokens=1
21:48:43,824 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:43,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 2.9889862169511616. input_tokens=27, output_tokens=596
21:48:44,451 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:44,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6236798749305308. input_tokens=30, output_tokens=1
21:48:47,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:47,281 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 2.826505196047947. input_tokens=27, output_tokens=543
21:48:47,961 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:47,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.6808743991423398. input_tokens=30, output_tokens=1
21:48:50,694 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:50,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 2.7321621829178184. input_tokens=27, output_tokens=516
21:48:51,246 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:51,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5387432950083166. input_tokens=169, output_tokens=33
21:48:51,277 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 31 edges
21:48:51,277 root INFO Starting at time 1730494131.277086
21:48:51,277 root INFO Beginning preprocessing of transition probabilities for 22 vertices
21:48:51,277 root INFO Completed 1 / 22 vertices
21:48:51,277 root INFO Completed 3 / 22 vertices
21:48:51,278 root INFO Completed 5 / 22 vertices
21:48:51,279 root INFO Completed 7 / 22 vertices
21:48:51,280 root INFO Completed 9 / 22 vertices
21:48:51,280 root INFO Completed 11 / 22 vertices
21:48:51,281 root INFO Completed 13 / 22 vertices
21:48:51,281 root INFO Completed 15 / 22 vertices
21:48:51,281 root INFO Completed 17 / 22 vertices
21:48:51,281 root INFO Completed 19 / 22 vertices
21:48:51,282 root INFO Completed 21 / 22 vertices
21:48:51,282 root INFO Completed preprocessing of transition probabilities for vertices
21:48:51,282 root INFO Beginning preprocessing of transition probabilities for 31 edges
21:48:51,283 root INFO Completed 1 / 31 edges
21:48:51,283 root INFO Completed 4 / 31 edges
21:48:51,284 root INFO Completed 7 / 31 edges
21:48:51,285 root INFO Completed 10 / 31 edges
21:48:51,285 root INFO Completed 13 / 31 edges
21:48:51,286 root INFO Completed 16 / 31 edges
21:48:51,287 root INFO Completed 19 / 31 edges
21:48:51,288 root INFO Completed 22 / 31 edges
21:48:51,288 root INFO Completed 25 / 31 edges
21:48:51,288 root INFO Completed 28 / 31 edges
21:48:51,289 root INFO Completed 31 / 31 edges
21:48:51,289 root INFO Completed preprocessing of transition probabilities for edges
21:48:51,290 root INFO Simulating walks on graph at time 1730494131.2902763
21:48:51,290 root INFO Walk iteration: 1/10
21:48:51,293 root INFO Walk iteration: 2/10
21:48:51,295 root INFO Walk iteration: 3/10
21:48:51,298 root INFO Walk iteration: 4/10
21:48:51,300 root INFO Walk iteration: 5/10
21:48:51,301 root INFO Walk iteration: 6/10
21:48:51,302 root INFO Walk iteration: 7/10
21:48:51,303 root INFO Walk iteration: 8/10
21:48:51,304 root INFO Walk iteration: 9/10
21:48:51,305 root INFO Walk iteration: 10/10
21:48:51,306 root INFO Learning embeddings at time 1730494131.306165
21:48:51,306 gensim.models.word2vec INFO collecting all words and their counts
21:48:51,306 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:48:51,306 gensim.models.word2vec INFO collected 22 word types from a corpus of 4520 raw words and 220 sentences
21:48:51,307 gensim.models.word2vec INFO Creating a fresh vocabulary
21:48:51,308 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T21:48:51.308178', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,308 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4520 word corpus (100.00% of original 4520, drops 0)', 'datetime': '2024-11-01T21:48:51.308857', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,309 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
21:48:51,310 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
21:48:51,310 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 696.329842087694 word corpus (15.4%% of prior 4520)', 'datetime': '2024-11-01T21:48:51.310164', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,311 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
21:48:51,311 gensim.models.word2vec INFO resetting layer weights
21:48:51,311 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:48:51.311916', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:48:51,312 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:48:51.312215', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:48:51,317 gensim.models.word2vec INFO EPOCH 0: training on 4520 raw words (712 effective words) took 0.0s, 203391 effective words/s
21:48:51,323 gensim.models.word2vec INFO EPOCH 1: training on 4520 raw words (683 effective words) took 0.0s, 135503 effective words/s
21:48:51,331 gensim.models.word2vec INFO EPOCH 2: training on 4520 raw words (735 effective words) took 0.0s, 132329 effective words/s
21:48:51,331 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 13560 raw words (2130 effective words) took 0.0s, 111620 effective words/s', 'datetime': '2024-11-01T21:48:51.331333', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:48:51,332 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:48:51.332096', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:48:51,332 root INFO Completed. Ending time is 1730494131.3327737 Elapsed time is -0.055687665939331055
21:48:51,337 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 31 edges
21:48:51,337 root INFO Starting at time 1730494131.337924
21:48:51,337 root INFO Beginning preprocessing of transition probabilities for 22 vertices
21:48:51,338 root INFO Completed 1 / 22 vertices
21:48:51,338 root INFO Completed 3 / 22 vertices
21:48:51,339 root INFO Completed 5 / 22 vertices
21:48:51,340 root INFO Completed 7 / 22 vertices
21:48:51,340 root INFO Completed 9 / 22 vertices
21:48:51,340 root INFO Completed 11 / 22 vertices
21:48:51,340 root INFO Completed 13 / 22 vertices
21:48:51,341 root INFO Completed 15 / 22 vertices
21:48:51,342 root INFO Completed 17 / 22 vertices
21:48:51,342 root INFO Completed 19 / 22 vertices
21:48:51,343 root INFO Completed 21 / 22 vertices
21:48:51,343 root INFO Completed preprocessing of transition probabilities for vertices
21:48:51,343 root INFO Beginning preprocessing of transition probabilities for 31 edges
21:48:51,344 root INFO Completed 1 / 31 edges
21:48:51,344 root INFO Completed 4 / 31 edges
21:48:51,345 root INFO Completed 7 / 31 edges
21:48:51,345 root INFO Completed 10 / 31 edges
21:48:51,346 root INFO Completed 13 / 31 edges
21:48:51,347 root INFO Completed 16 / 31 edges
21:48:51,348 root INFO Completed 19 / 31 edges
21:48:51,348 root INFO Completed 22 / 31 edges
21:48:51,349 root INFO Completed 25 / 31 edges
21:48:51,350 root INFO Completed 28 / 31 edges
21:48:51,350 root INFO Completed 31 / 31 edges
21:48:51,351 root INFO Completed preprocessing of transition probabilities for edges
21:48:51,351 root INFO Simulating walks on graph at time 1730494131.3515368
21:48:51,352 root INFO Walk iteration: 1/10
21:48:51,355 root INFO Walk iteration: 2/10
21:48:51,358 root INFO Walk iteration: 3/10
21:48:51,361 root INFO Walk iteration: 4/10
21:48:51,363 root INFO Walk iteration: 5/10
21:48:51,364 root INFO Walk iteration: 6/10
21:48:51,366 root INFO Walk iteration: 7/10
21:48:51,367 root INFO Walk iteration: 8/10
21:48:51,368 root INFO Walk iteration: 9/10
21:48:51,370 root INFO Walk iteration: 10/10
21:48:51,372 root INFO Learning embeddings at time 1730494131.3722675
21:48:51,372 gensim.models.word2vec INFO collecting all words and their counts
21:48:51,373 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:48:51,373 gensim.models.word2vec INFO collected 22 word types from a corpus of 4520 raw words and 220 sentences
21:48:51,373 gensim.models.word2vec INFO Creating a fresh vocabulary
21:48:51,373 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T21:48:51.373883', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,374 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4520 word corpus (100.00% of original 4520, drops 0)', 'datetime': '2024-11-01T21:48:51.374441', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,374 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
21:48:51,375 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
21:48:51,375 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 696.329842087694 word corpus (15.4%% of prior 4520)', 'datetime': '2024-11-01T21:48:51.375280', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:48:51,376 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
21:48:51,376 gensim.models.word2vec INFO resetting layer weights
21:48:51,377 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:48:51.377409', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:48:51,377 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:48:51.377544', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:48:51,391 gensim.models.word2vec INFO EPOCH 0: training on 4520 raw words (712 effective words) took 0.0s, 70975 effective words/s
21:48:51,403 gensim.models.word2vec INFO EPOCH 1: training on 4520 raw words (683 effective words) took 0.0s, 71896 effective words/s
21:48:51,416 gensim.models.word2vec INFO EPOCH 2: training on 4520 raw words (735 effective words) took 0.0s, 69814 effective words/s
21:48:51,416 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 13560 raw words (2130 effective words) took 0.0s, 55202 effective words/s', 'datetime': '2024-11-01T21:48:51.416681', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:48:51,417 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:48:51.417588', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:48:51,417 root INFO Completed. Ending time is 1730494131.417732 Elapsed time is -0.07980799674987793
21:48:51,473 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:48:51,613 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:48:51,613 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:48:51,622 datashaper.workflow.workflow INFO executing verb create_final_entities
21:48:51,627 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:48:51,664 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:48:51,664 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:48:51,665 graphrag.index.operations.embed_text.strategies.openai INFO embedding 36 inputs via 33 snippets using 3 batches. max_batch_size=16, max_tokens=8191
21:48:52,710 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:48:52,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0513921650126576. input_tokens=30, output_tokens=0
21:48:52,943 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:48:52,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3000243718270212. input_tokens=560, output_tokens=0
21:48:53,41 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:48:53,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4050748439040035. input_tokens=454, output_tokens=0
21:48:53,76 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:48:53,226 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:48:53,226 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:48:53,235 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:48:55,909 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:48:56,63 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:48:56,63 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:48:56,73 datashaper.workflow.workflow INFO executing verb create_final_communities
21:48:56,87 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:48:56,220 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:48:56,220 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:48:56,226 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:48:56,239 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:48:56,246 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:48:56,381 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
21:48:56,381 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:48:56,386 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:48:56,388 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:48:56,396 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:48:56,406 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:48:56,541 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
21:48:56,542 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:48:56,546 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:48:56,555 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:48:56,560 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 24
21:48:56,570 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 72
21:48:59,81 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:48:59,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4908780509140342. input_tokens=2071, output_tokens=310
21:49:00,433 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:00,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8444993181619793. input_tokens=2425, output_tokens=569
21:49:02,584 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:02,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1287525920197368. input_tokens=2140, output_tokens=241
21:49:03,153 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:03,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.703475959133357. input_tokens=2525, output_tokens=523
21:49:03,300 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:03,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8530699419789016. input_tokens=2330, output_tokens=511
21:49:04,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:04,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6473363330587745. input_tokens=2735, output_tokens=593
21:49:04,112 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:49:04,263 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:49:04,264 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:49:04,274 datashaper.workflow.workflow INFO executing verb create_final_documents
21:49:04,281 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:49:04,299 graphrag.index.cli INFO All workflows completed successfully.
21:49:42,501 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:49:42,503 graphrag.index.cli INFO Starting pipeline run for: 20241101-214942, dryrun=False
21:49:42,504 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:49:42,506 graphrag.index.create_pipeline_config INFO skipping workflows 
21:49:42,506 graphrag.index.run.run INFO Running pipeline
21:49:42,506 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:49:42,507 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:49:42,507 graphrag.index.input.load_input INFO using file storage for input
21:49:42,507 graphrag.index.input.csv INFO Loading csv files from input_eval
21:49:42,508 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:49:42,510 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:49:42,510 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:49:42,511 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:49:42,511 graphrag.index.run.run INFO Final # of rows loaded: 1
21:49:42,624 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:49:42,627 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:49:43,103 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:49:43,230 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:49:43,230 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:49:43,238 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:49:43,239 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:49:43,278 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:49:43,278 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:49:45,920 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:45,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6465781470760703. input_tokens=2087, output_tokens=523
21:49:48,820 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:48,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.894243769114837. input_tokens=27, output_tokens=668
21:49:49,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:49,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.47149768797680736. input_tokens=30, output_tokens=1
21:49:52,190 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:52,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.8956252008210868. input_tokens=27, output_tokens=618
21:49:52,774 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:52,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5813746131025255. input_tokens=30, output_tokens=1
21:49:55,144 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:55,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 2.369402016978711. input_tokens=27, output_tokens=484
21:49:55,796 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:55,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6502540020737797. input_tokens=30, output_tokens=1
21:49:59,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:59,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 3.2971503210719675. input_tokens=27, output_tokens=754
21:49:59,732 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:49:59,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6340697510167956. input_tokens=30, output_tokens=1
21:50:03,194 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:03,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 3.461903669172898. input_tokens=27, output_tokens=762
21:50:03,780 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:03,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5834077019244432. input_tokens=30, output_tokens=1
21:50:06,920 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:06,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 3.1379564499948174. input_tokens=27, output_tokens=688
21:50:07,608 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:07,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.6856438869144768. input_tokens=30, output_tokens=1
21:50:10,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:10,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 3.356989299878478. input_tokens=27, output_tokens=863
21:50:10,991 root INFO Starting preprocessing of transition probabilities on graph with 40 nodes and 46 edges
21:50:10,991 root INFO Starting at time 1730494210.9915302
21:50:10,991 root INFO Beginning preprocessing of transition probabilities for 40 vertices
21:50:10,991 root INFO Completed 1 / 40 vertices
21:50:10,991 root INFO Completed 5 / 40 vertices
21:50:10,991 root INFO Completed 9 / 40 vertices
21:50:10,991 root INFO Completed 13 / 40 vertices
21:50:10,992 root INFO Completed 17 / 40 vertices
21:50:10,992 root INFO Completed 21 / 40 vertices
21:50:10,993 root INFO Completed 25 / 40 vertices
21:50:10,993 root INFO Completed 29 / 40 vertices
21:50:10,993 root INFO Completed 33 / 40 vertices
21:50:10,993 root INFO Completed 37 / 40 vertices
21:50:10,993 root INFO Completed preprocessing of transition probabilities for vertices
21:50:10,994 root INFO Beginning preprocessing of transition probabilities for 46 edges
21:50:10,994 root INFO Completed 1 / 46 edges
21:50:10,995 root INFO Completed 5 / 46 edges
21:50:10,996 root INFO Completed 9 / 46 edges
21:50:10,997 root INFO Completed 13 / 46 edges
21:50:10,998 root INFO Completed 17 / 46 edges
21:50:10,999 root INFO Completed 21 / 46 edges
21:50:11,0 root INFO Completed 25 / 46 edges
21:50:11,0 root INFO Completed 29 / 46 edges
21:50:11,1 root INFO Completed 33 / 46 edges
21:50:11,2 root INFO Completed 37 / 46 edges
21:50:11,2 root INFO Completed 41 / 46 edges
21:50:11,3 root INFO Completed 45 / 46 edges
21:50:11,4 root INFO Completed preprocessing of transition probabilities for edges
21:50:11,4 root INFO Simulating walks on graph at time 1730494211.0041952
21:50:11,5 root INFO Walk iteration: 1/10
21:50:11,7 root INFO Walk iteration: 2/10
21:50:11,8 root INFO Walk iteration: 3/10
21:50:11,9 root INFO Walk iteration: 4/10
21:50:11,10 root INFO Walk iteration: 5/10
21:50:11,11 root INFO Walk iteration: 6/10
21:50:11,12 root INFO Walk iteration: 7/10
21:50:11,13 root INFO Walk iteration: 8/10
21:50:11,13 root INFO Walk iteration: 9/10
21:50:11,14 root INFO Walk iteration: 10/10
21:50:11,15 root INFO Learning embeddings at time 1730494211.0156326
21:50:11,15 gensim.models.word2vec INFO collecting all words and their counts
21:50:11,16 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:50:11,16 gensim.models.word2vec INFO collected 40 word types from a corpus of 7200 raw words and 400 sentences
21:50:11,17 gensim.models.word2vec INFO Creating a fresh vocabulary
21:50:11,17 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 40 unique words (100.00% of original 40, drops 0)', 'datetime': '2024-11-01T21:50:11.017381', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:50:11,18 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7200 word corpus (100.00% of original 7200, drops 0)', 'datetime': '2024-11-01T21:50:11.018216', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:50:11,18 gensim.models.word2vec INFO deleting the raw counts dictionary of 40 items
21:50:11,19 gensim.models.word2vec INFO sample=0.001 downsamples 40 most-common words
21:50:11,19 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1600.5077240600851 word corpus (22.2%% of prior 7200)', 'datetime': '2024-11-01T21:50:11.019302', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:50:11,20 gensim.models.word2vec INFO estimated required memory for 40 words and 1536 dimensions: 511520 bytes
21:50:11,20 gensim.models.word2vec INFO resetting layer weights
21:50:11,20 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:50:11.020748', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:50:11,20 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 40 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:50:11.020990', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:50:11,29 gensim.models.word2vec INFO EPOCH 0: training on 7200 raw words (1627 effective words) took 0.0s, 244295 effective words/s
21:50:11,45 gensim.models.word2vec INFO EPOCH 1: training on 7200 raw words (1574 effective words) took 0.0s, 110722 effective words/s
21:50:11,63 gensim.models.word2vec INFO EPOCH 2: training on 7200 raw words (1611 effective words) took 0.0s, 98113 effective words/s
21:50:11,63 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 21600 raw words (4812 effective words) took 0.0s, 112020 effective words/s', 'datetime': '2024-11-01T21:50:11.063968', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:50:11,64 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=40, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:50:11.064023', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:50:11,64 root INFO Completed. Ending time is 1730494211.0640676 Elapsed time is -0.07253742218017578
21:50:11,113 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:50:11,261 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:50:11,261 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:11,274 datashaper.workflow.workflow INFO executing verb create_final_entities
21:50:11,278 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:50:11,315 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:50:11,315 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:50:11,316 graphrag.index.operations.embed_text.strategies.openai INFO embedding 44 inputs via 40 snippets using 3 batches. max_batch_size=16, max_tokens=8191
21:50:12,506 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:50:12,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2029808510560542. input_tokens=243, output_tokens=0
21:50:12,609 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:50:12,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3166041190270334. input_tokens=358, output_tokens=0
21:50:12,722 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:50:12,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4312955541536212. input_tokens=452, output_tokens=0
21:50:12,753 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:50:12,916 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:50:12,917 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:12,926 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:50:15,369 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:50:15,535 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:50:15,535 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:15,544 datashaper.workflow.workflow INFO executing verb create_final_communities
21:50:15,556 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:50:15,691 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:50:15,694 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:50:15,698 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:50:15,707 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:50:15,713 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:50:15,863 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
21:50:15,863 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:50:15,869 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:50:15,871 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:50:15,878 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:50:15,887 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:50:16,21 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:50:16,22 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:50:16,29 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:50:16,37 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:50:16,40 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 44
21:50:18,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:18,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3910807699430734. input_tokens=2073, output_tokens=385
21:50:18,810 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:18,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7493096319958568. input_tokens=2080, output_tokens=353
21:50:18,897 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:18,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.830218569841236. input_tokens=2119, output_tokens=407
21:50:19,339 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:19,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2713720791507512. input_tokens=2276, output_tokens=498
21:50:19,694 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:19,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.63035721401684. input_tokens=2328, output_tokens=492
21:50:20,217 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:20,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.75025777798146. input_tokens=2041, output_tokens=300
21:50:21,623 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:21,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.267599147045985. input_tokens=2084, output_tokens=448
21:50:21,937 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:21,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1191856381483376. input_tokens=2288, output_tokens=599
21:50:22,583 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:50:22,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.684213026892394. input_tokens=2939, output_tokens=744
21:50:22,595 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:50:22,749 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:50:22,752 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:50:22,761 datashaper.workflow.workflow INFO executing verb create_final_documents
21:50:22,767 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:50:22,784 graphrag.index.cli INFO All workflows completed successfully.
21:53:51,678 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:53:51,679 graphrag.index.cli INFO Starting pipeline run for: 20241101-215351, dryrun=False
21:53:51,680 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:53:51,682 graphrag.index.create_pipeline_config INFO skipping workflows 
21:53:51,682 graphrag.index.run.run INFO Running pipeline
21:53:51,682 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:53:51,683 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:53:51,683 graphrag.index.input.load_input INFO using file storage for input
21:53:51,685 graphrag.index.input.csv INFO Loading csv files from input_eval
21:53:51,685 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:53:51,689 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:53:51,689 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:53:51,690 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:53:51,690 graphrag.index.run.run INFO Final # of rows loaded: 1
21:53:51,803 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:53:51,805 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:53:52,262 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:53:52,388 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:53:52,389 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:53:52,396 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:53:52,397 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:53:52,435 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:53:52,435 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:53:55,194 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:53:55,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.768090914003551. input_tokens=2087, output_tokens=527
21:54:00,187 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:00,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.983403833117336. input_tokens=34, output_tokens=1212
21:54:00,675 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:00,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.48520971299149096. input_tokens=30, output_tokens=1
21:54:02,663 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:02,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.9861425659619272. input_tokens=34, output_tokens=446
21:54:03,299 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:03,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.633754262002185. input_tokens=30, output_tokens=1
21:54:04,688 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:04,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 1.3872322798706591. input_tokens=34, output_tokens=221
21:54:05,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:05,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.5752194088418037. input_tokens=30, output_tokens=1
21:54:06,420 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:06,423 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 1.1553512259852141. input_tokens=34, output_tokens=145
21:54:06,988 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:06,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.566628233063966. input_tokens=30, output_tokens=1
21:54:08,98 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:08,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 1.1082925589289516. input_tokens=34, output_tokens=127
21:54:08,676 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:08,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.576856680912897. input_tokens=30, output_tokens=1
21:54:09,674 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:09,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.9965101259294897. input_tokens=34, output_tokens=124
21:54:10,272 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:10,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.597015816019848. input_tokens=30, output_tokens=1
21:54:11,234 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:11,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.9605786339379847. input_tokens=34, output_tokens=112
21:54:11,257 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 21 edges
21:54:11,257 root INFO Starting at time 1730494451.257528
21:54:11,257 root INFO Beginning preprocessing of transition probabilities for 22 vertices
21:54:11,257 root INFO Completed 1 / 22 vertices
21:54:11,257 root INFO Completed 3 / 22 vertices
21:54:11,258 root INFO Completed 5 / 22 vertices
21:54:11,259 root INFO Completed 7 / 22 vertices
21:54:11,259 root INFO Completed 9 / 22 vertices
21:54:11,260 root INFO Completed 11 / 22 vertices
21:54:11,260 root INFO Completed 13 / 22 vertices
21:54:11,261 root INFO Completed 15 / 22 vertices
21:54:11,262 root INFO Completed 17 / 22 vertices
21:54:11,262 root INFO Completed 19 / 22 vertices
21:54:11,263 root INFO Completed 21 / 22 vertices
21:54:11,264 root INFO Completed preprocessing of transition probabilities for vertices
21:54:11,264 root INFO Beginning preprocessing of transition probabilities for 21 edges
21:54:11,265 root INFO Completed 1 / 21 edges
21:54:11,266 root INFO Completed 3 / 21 edges
21:54:11,266 root INFO Completed 5 / 21 edges
21:54:11,267 root INFO Completed 7 / 21 edges
21:54:11,267 root INFO Completed 9 / 21 edges
21:54:11,268 root INFO Completed 11 / 21 edges
21:54:11,269 root INFO Completed 13 / 21 edges
21:54:11,269 root INFO Completed 15 / 21 edges
21:54:11,269 root INFO Completed 17 / 21 edges
21:54:11,270 root INFO Completed 19 / 21 edges
21:54:11,271 root INFO Completed 21 / 21 edges
21:54:11,271 root INFO Completed preprocessing of transition probabilities for edges
21:54:11,272 root INFO Simulating walks on graph at time 1730494451.2719903
21:54:11,272 root INFO Walk iteration: 1/10
21:54:11,273 root INFO Walk iteration: 2/10
21:54:11,274 root INFO Walk iteration: 3/10
21:54:11,275 root INFO Walk iteration: 4/10
21:54:11,276 root INFO Walk iteration: 5/10
21:54:11,277 root INFO Walk iteration: 6/10
21:54:11,277 root INFO Walk iteration: 7/10
21:54:11,278 root INFO Walk iteration: 8/10
21:54:11,279 root INFO Walk iteration: 9/10
21:54:11,279 root INFO Walk iteration: 10/10
21:54:11,280 root INFO Learning embeddings at time 1730494451.2803197
21:54:11,280 gensim.models.word2vec INFO collecting all words and their counts
21:54:11,281 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:54:11,282 gensim.models.word2vec INFO collected 22 word types from a corpus of 3520 raw words and 220 sentences
21:54:11,282 gensim.models.word2vec INFO Creating a fresh vocabulary
21:54:11,282 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T21:54:11.282634', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:54:11,283 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3520 word corpus (100.00% of original 3520, drops 0)', 'datetime': '2024-11-01T21:54:11.283123', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:54:11,283 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
21:54:11,284 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
21:54:11,284 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 537.5117390260028 word corpus (15.3%% of prior 3520)', 'datetime': '2024-11-01T21:54:11.284255', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:54:11,284 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
21:54:11,285 gensim.models.word2vec INFO resetting layer weights
21:54:11,285 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:54:11.285833', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:54:11,286 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:54:11.286239', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:54:11,291 gensim.models.word2vec INFO EPOCH 0: training on 3520 raw words (543 effective words) took 0.0s, 207343 effective words/s
21:54:11,294 gensim.models.word2vec INFO EPOCH 1: training on 3520 raw words (554 effective words) took 0.0s, 226404 effective words/s
21:54:11,298 gensim.models.word2vec INFO EPOCH 2: training on 3520 raw words (549 effective words) took 0.0s, 191465 effective words/s
21:54:11,298 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10560 raw words (1646 effective words) took 0.0s, 141038 effective words/s', 'datetime': '2024-11-01T21:54:11.298500', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:54:11,299 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:54:11.299230', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:54:11,299 root INFO Completed. Ending time is 1730494451.299879 Elapsed time is -0.04235100746154785
21:54:11,324 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:54:11,460 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:54:11,460 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:54:11,471 datashaper.workflow.workflow INFO executing verb create_final_entities
21:54:11,475 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:54:11,513 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:54:11,513 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:54:11,513 graphrag.index.operations.embed_text.strategies.openai INFO embedding 33 inputs via 28 snippets using 2 batches. max_batch_size=16, max_tokens=8191
21:54:12,688 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:54:12,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1972794998437166. input_tokens=253, output_tokens=0
21:54:12,968 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:54:12,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4802017039619386. input_tokens=450, output_tokens=0
21:54:13,0 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:54:13,145 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:54:13,145 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:54:13,153 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:54:15,444 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:54:15,614 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:54:15,615 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:54:15,624 datashaper.workflow.workflow INFO executing verb create_final_communities
21:54:15,635 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:54:15,767 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
21:54:15,768 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:54:15,776 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:54:15,784 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:54:15,789 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:54:15,926 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
21:54:15,929 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:54:15,934 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:54:15,936 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:54:15,942 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:54:15,952 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:54:16,84 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:54:16,84 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:54:16,90 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:54:16,98 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:54:16,101 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 33
21:54:18,644 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:18,647 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.529860433889553. input_tokens=2331, output_tokens=465
21:54:18,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:18,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5620835558511317. input_tokens=2163, output_tokens=322
21:54:20,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:54:20,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.125398122007027. input_tokens=2555, output_tokens=639
21:54:20,264 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:54:20,416 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:54:20,416 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:54:20,427 datashaper.workflow.workflow INFO executing verb create_final_documents
21:54:20,433 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:54:20,450 graphrag.index.cli INFO All workflows completed successfully.
21:57:52,93 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:57:52,94 graphrag.index.cli INFO Starting pipeline run for: 20241101-215752, dryrun=False
21:57:52,95 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:57:52,97 graphrag.index.create_pipeline_config INFO skipping workflows 
21:57:52,97 graphrag.index.run.run INFO Running pipeline
21:57:52,98 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:57:52,99 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:57:52,99 graphrag.index.input.load_input INFO using file storage for input
21:57:52,100 graphrag.index.input.csv INFO Loading csv files from input_eval
21:57:52,100 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:57:52,102 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:57:52,102 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:57:52,103 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:57:52,103 graphrag.index.run.run INFO Final # of rows loaded: 1
21:57:52,213 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:57:52,215 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:57:52,553 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:57:52,679 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:57:52,679 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:57:52,687 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:57:52,688 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:57:52,726 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:57:52,726 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:57:55,85 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:57:55,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.362803113879636. input_tokens=2087, output_tokens=523
21:58:00,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:00,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.928392211906612. input_tokens=34, output_tokens=1290
21:58:00,511 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:00,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4913355929311365. input_tokens=30, output_tokens=1
21:58:03,693 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:03,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.1812429749406874. input_tokens=34, output_tokens=767
21:58:04,304 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:04,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6095535669010133. input_tokens=30, output_tokens=1
21:58:07,531 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:07,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 3.224958614911884. input_tokens=34, output_tokens=789
21:58:08,766 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:08,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 1.233297083992511. input_tokens=30, output_tokens=1
21:58:11,866 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:11,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 3.0990383860189468. input_tokens=34, output_tokens=725
21:58:12,405 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:12,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.5365233039483428. input_tokens=30, output_tokens=1
21:58:15,471 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:15,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 3.0631323938723654. input_tokens=34, output_tokens=729
21:58:16,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:16,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.7294168469961733. input_tokens=30, output_tokens=1
21:58:18,892 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:18,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 2.691154669970274. input_tokens=34, output_tokens=604
21:58:20,440 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:20,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.54444334609434. input_tokens=30, output_tokens=1
21:58:22,881 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:22,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 2.440188125008717. input_tokens=34, output_tokens=527
21:58:22,891 root INFO Starting preprocessing of transition probabilities on graph with 6 nodes and 5 edges
21:58:22,892 root INFO Starting at time 1730494702.891996
21:58:22,892 root INFO Beginning preprocessing of transition probabilities for 6 vertices
21:58:22,892 root INFO Completed 1 / 6 vertices
21:58:22,892 root INFO Completed 2 / 6 vertices
21:58:22,892 root INFO Completed 3 / 6 vertices
21:58:22,892 root INFO Completed 4 / 6 vertices
21:58:22,892 root INFO Completed 5 / 6 vertices
21:58:22,892 root INFO Completed 6 / 6 vertices
21:58:22,892 root INFO Completed preprocessing of transition probabilities for vertices
21:58:22,892 root INFO Beginning preprocessing of transition probabilities for 5 edges
21:58:22,892 root INFO Completed 1 / 5 edges
21:58:22,892 root INFO Completed 2 / 5 edges
21:58:22,892 root INFO Completed 3 / 5 edges
21:58:22,893 root INFO Completed 4 / 5 edges
21:58:22,893 root INFO Completed 5 / 5 edges
21:58:22,893 root INFO Completed preprocessing of transition probabilities for edges
21:58:22,893 root INFO Simulating walks on graph at time 1730494702.8934584
21:58:22,893 root INFO Walk iteration: 1/10
21:58:22,894 root INFO Walk iteration: 2/10
21:58:22,894 root INFO Walk iteration: 3/10
21:58:22,895 root INFO Walk iteration: 4/10
21:58:22,895 root INFO Walk iteration: 5/10
21:58:22,896 root INFO Walk iteration: 6/10
21:58:22,896 root INFO Walk iteration: 7/10
21:58:22,897 root INFO Walk iteration: 8/10
21:58:22,898 root INFO Walk iteration: 9/10
21:58:22,898 root INFO Walk iteration: 10/10
21:58:22,899 root INFO Learning embeddings at time 1730494702.899598
21:58:22,900 gensim.models.word2vec INFO collecting all words and their counts
21:58:22,900 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
21:58:22,900 gensim.models.word2vec INFO collected 6 word types from a corpus of 800 raw words and 60 sentences
21:58:22,900 gensim.models.word2vec INFO Creating a fresh vocabulary
21:58:22,901 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 6 unique words (100.00% of original 6, drops 0)', 'datetime': '2024-11-01T21:58:22.901426', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:58:22,901 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 800 word corpus (100.00% of original 800, drops 0)', 'datetime': '2024-11-01T21:58:22.901929', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:58:22,901 gensim.models.word2vec INFO deleting the raw counts dictionary of 6 items
21:58:22,908 gensim.models.word2vec INFO sample=0.001 downsamples 6 most-common words
21:58:22,908 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 61.961470799410435 word corpus (7.7%% of prior 800)', 'datetime': '2024-11-01T21:58:22.908673', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
21:58:22,909 gensim.models.word2vec INFO estimated required memory for 6 words and 1536 dimensions: 76728 bytes
21:58:22,910 gensim.models.word2vec INFO resetting layer weights
21:58:22,910 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T21:58:22.910219', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
21:58:22,910 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 6 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T21:58:22.910723', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:58:22,912 gensim.models.word2vec INFO EPOCH 0: training on 800 raw words (77 effective words) took 0.0s, 215860 effective words/s
21:58:22,913 gensim.models.word2vec INFO EPOCH 1: training on 800 raw words (55 effective words) took 0.0s, 182113 effective words/s
21:58:22,914 gensim.models.word2vec INFO EPOCH 2: training on 800 raw words (72 effective words) took 0.0s, 1132697 effective words/s
21:58:22,914 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2400 raw words (204 effective words) took 0.0s, 52753 effective words/s', 'datetime': '2024-11-01T21:58:22.914611', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
21:58:22,915 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=6, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T21:58:22.915228', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
21:58:22,915 root INFO Completed. Ending time is 1730494702.9152522 Elapsed time is -0.023256301879882812
21:58:22,938 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
21:58:23,75 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
21:58:23,75 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:58:23,82 datashaper.workflow.workflow INFO executing verb create_final_entities
21:58:23,86 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:58:23,123 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
21:58:23,124 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
21:58:23,124 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
21:58:23,602 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
21:58:23,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.49076880095526576. input_tokens=214, output_tokens=0
21:58:23,621 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
21:58:23,753 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
21:58:23,753 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:58:23,760 datashaper.workflow.workflow INFO executing verb create_final_nodes
21:58:26,270 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
21:58:26,412 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
21:58:26,412 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:58:26,421 datashaper.workflow.workflow INFO executing verb create_final_communities
21:58:26,431 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
21:58:26,562 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
21:58:26,562 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
21:58:26,566 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:58:26,574 datashaper.workflow.workflow INFO executing verb create_final_relationships
21:58:26,580 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
21:58:26,712 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
21:58:26,713 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:58:26,717 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
21:58:26,721 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:58:26,728 datashaper.workflow.workflow INFO executing verb create_final_text_units
21:58:26,737 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
21:58:26,869 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
21:58:26,870 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
21:58:26,874 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
21:58:26,882 datashaper.workflow.workflow INFO executing verb create_final_community_reports
21:58:26,885 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
21:58:29,531 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:58:29,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.637052084086463. input_tokens=2277, output_tokens=476
21:58:29,541 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
21:58:29,683 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
21:58:29,683 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
21:58:29,694 datashaper.workflow.workflow INFO executing verb create_final_documents
21:58:29,700 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
21:58:29,718 graphrag.index.cli INFO All workflows completed successfully.
21:59:10,150 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
21:59:10,153 graphrag.index.cli INFO Starting pipeline run for: 20241101-215910, dryrun=False
21:59:10,154 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
21:59:10,157 graphrag.index.create_pipeline_config INFO skipping workflows 
21:59:10,157 graphrag.index.run.run INFO Running pipeline
21:59:10,158 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
21:59:10,159 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
21:59:10,159 graphrag.index.input.load_input INFO using file storage for input
21:59:10,161 graphrag.index.input.csv INFO Loading csv files from input_eval
21:59:10,161 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
21:59:10,165 graphrag.index.input.csv INFO Found 1 csv files, loading 1
21:59:10,165 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
21:59:10,166 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
21:59:10,166 graphrag.index.run.run INFO Final # of rows loaded: 1
21:59:10,291 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
21:59:10,294 datashaper.workflow.workflow INFO executing verb create_base_text_units
21:59:10,753 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
21:59:10,883 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
21:59:10,883 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
21:59:10,890 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
21:59:10,891 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
21:59:10,930 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
21:59:10,930 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
21:59:13,563 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:13,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.641534976894036. input_tokens=2087, output_tokens=527
21:59:20,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:20,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.453266878146678. input_tokens=33, output_tokens=2029
21:59:20,664 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:20,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6349929771386087. input_tokens=30, output_tokens=1
21:59:28,213 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:28,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.547059410950169. input_tokens=33, output_tokens=2496
21:59:29,502 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:29,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 1.2857316450681537. input_tokens=30, output_tokens=1
21:59:39,862 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:39,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 10.359798398101702. input_tokens=33, output_tokens=3373
21:59:40,618 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:40,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7483783341012895. input_tokens=30, output_tokens=1
21:59:51,625 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:51,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 11.006236148998141. input_tokens=33, output_tokens=3848
21:59:52,531 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
21:59:52,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.9033832331188023. input_tokens=30, output_tokens=1
22:00:04,601 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:04,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.068574097007513. input_tokens=33, output_tokens=4271
22:00:05,709 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:05,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.0996143650263548. input_tokens=30, output_tokens=1
22:00:18,822 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:18,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 13.111219383077696. input_tokens=33, output_tokens=4559
22:00:19,940 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:19,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.1149303319398314. input_tokens=30, output_tokens=1
22:00:32,829 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:32,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.886774186044931. input_tokens=33, output_tokens=4568
22:00:33,372 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:33,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5196487540379167. input_tokens=163, output_tokens=34
22:00:33,396 root INFO Starting preprocessing of transition probabilities on graph with 35 nodes and 64 edges
22:00:33,396 root INFO Starting at time 1730494833.3967817
22:00:33,396 root INFO Beginning preprocessing of transition probabilities for 35 vertices
22:00:33,396 root INFO Completed 1 / 35 vertices
22:00:33,396 root INFO Completed 4 / 35 vertices
22:00:33,397 root INFO Completed 7 / 35 vertices
22:00:33,397 root INFO Completed 10 / 35 vertices
22:00:33,398 root INFO Completed 13 / 35 vertices
22:00:33,399 root INFO Completed 16 / 35 vertices
22:00:33,400 root INFO Completed 19 / 35 vertices
22:00:33,401 root INFO Completed 22 / 35 vertices
22:00:33,401 root INFO Completed 25 / 35 vertices
22:00:33,402 root INFO Completed 28 / 35 vertices
22:00:33,403 root INFO Completed 31 / 35 vertices
22:00:33,403 root INFO Completed 34 / 35 vertices
22:00:33,404 root INFO Completed preprocessing of transition probabilities for vertices
22:00:33,405 root INFO Beginning preprocessing of transition probabilities for 64 edges
22:00:33,405 root INFO Completed 1 / 64 edges
22:00:33,406 root INFO Completed 7 / 64 edges
22:00:33,407 root INFO Completed 13 / 64 edges
22:00:33,407 root INFO Completed 19 / 64 edges
22:00:33,408 root INFO Completed 25 / 64 edges
22:00:33,408 root INFO Completed 31 / 64 edges
22:00:33,409 root INFO Completed 37 / 64 edges
22:00:33,409 root INFO Completed 43 / 64 edges
22:00:33,410 root INFO Completed 49 / 64 edges
22:00:33,410 root INFO Completed 55 / 64 edges
22:00:33,411 root INFO Completed 61 / 64 edges
22:00:33,412 root INFO Completed preprocessing of transition probabilities for edges
22:00:33,412 root INFO Simulating walks on graph at time 1730494833.4125721
22:00:33,412 root INFO Walk iteration: 1/10
22:00:33,414 root INFO Walk iteration: 2/10
22:00:33,416 root INFO Walk iteration: 3/10
22:00:33,417 root INFO Walk iteration: 4/10
22:00:33,418 root INFO Walk iteration: 5/10
22:00:33,419 root INFO Walk iteration: 6/10
22:00:33,420 root INFO Walk iteration: 7/10
22:00:33,421 root INFO Walk iteration: 8/10
22:00:33,422 root INFO Walk iteration: 9/10
22:00:33,422 root INFO Walk iteration: 10/10
22:00:33,423 root INFO Learning embeddings at time 1730494833.423713
22:00:33,424 gensim.models.word2vec INFO collecting all words and their counts
22:00:33,424 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:00:33,425 gensim.models.word2vec INFO collected 35 word types from a corpus of 7240 raw words and 350 sentences
22:00:33,425 gensim.models.word2vec INFO Creating a fresh vocabulary
22:00:33,426 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 35 unique words (100.00% of original 35, drops 0)', 'datetime': '2024-11-01T22:00:33.426603', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,427 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7240 word corpus (100.00% of original 7240, drops 0)', 'datetime': '2024-11-01T22:00:33.427203', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,427 gensim.models.word2vec INFO deleting the raw counts dictionary of 35 items
22:00:33,428 gensim.models.word2vec INFO sample=0.001 downsamples 35 most-common words
22:00:33,429 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1485.4809590381558 word corpus (20.5%% of prior 7240)', 'datetime': '2024-11-01T22:00:33.429138', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,429 gensim.models.word2vec INFO estimated required memory for 35 words and 1536 dimensions: 447580 bytes
22:00:33,430 gensim.models.word2vec INFO resetting layer weights
22:00:33,430 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:00:33.430343', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:00:33,430 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 35 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:00:33.430653', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:00:33,439 gensim.models.word2vec INFO EPOCH 0: training on 7240 raw words (1515 effective words) took 0.0s, 266093 effective words/s
22:00:33,448 gensim.models.word2vec INFO EPOCH 1: training on 7240 raw words (1505 effective words) took 0.0s, 189904 effective words/s
22:00:33,456 gensim.models.word2vec INFO EPOCH 2: training on 7240 raw words (1534 effective words) took 0.0s, 230732 effective words/s
22:00:33,456 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 21720 raw words (4554 effective words) took 0.0s, 181350 effective words/s', 'datetime': '2024-11-01T22:00:33.456371', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:00:33,456 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=35, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:00:33.456404', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:00:33,457 root INFO Completed. Ending time is 1730494833.4571323 Elapsed time is -0.060350656509399414
22:00:33,462 root INFO Starting preprocessing of transition probabilities on graph with 35 nodes and 64 edges
22:00:33,462 root INFO Starting at time 1730494833.462306
22:00:33,462 root INFO Beginning preprocessing of transition probabilities for 35 vertices
22:00:33,462 root INFO Completed 1 / 35 vertices
22:00:33,462 root INFO Completed 4 / 35 vertices
22:00:33,463 root INFO Completed 7 / 35 vertices
22:00:33,463 root INFO Completed 10 / 35 vertices
22:00:33,464 root INFO Completed 13 / 35 vertices
22:00:33,464 root INFO Completed 16 / 35 vertices
22:00:33,465 root INFO Completed 19 / 35 vertices
22:00:33,465 root INFO Completed 22 / 35 vertices
22:00:33,465 root INFO Completed 25 / 35 vertices
22:00:33,465 root INFO Completed 28 / 35 vertices
22:00:33,466 root INFO Completed 31 / 35 vertices
22:00:33,466 root INFO Completed 34 / 35 vertices
22:00:33,467 root INFO Completed preprocessing of transition probabilities for vertices
22:00:33,467 root INFO Beginning preprocessing of transition probabilities for 64 edges
22:00:33,467 root INFO Completed 1 / 64 edges
22:00:33,467 root INFO Completed 7 / 64 edges
22:00:33,468 root INFO Completed 13 / 64 edges
22:00:33,469 root INFO Completed 19 / 64 edges
22:00:33,469 root INFO Completed 25 / 64 edges
22:00:33,470 root INFO Completed 31 / 64 edges
22:00:33,471 root INFO Completed 37 / 64 edges
22:00:33,471 root INFO Completed 43 / 64 edges
22:00:33,472 root INFO Completed 49 / 64 edges
22:00:33,473 root INFO Completed 55 / 64 edges
22:00:33,474 root INFO Completed 61 / 64 edges
22:00:33,474 root INFO Completed preprocessing of transition probabilities for edges
22:00:33,475 root INFO Simulating walks on graph at time 1730494833.475343
22:00:33,475 root INFO Walk iteration: 1/10
22:00:33,478 root INFO Walk iteration: 2/10
22:00:33,480 root INFO Walk iteration: 3/10
22:00:33,481 root INFO Walk iteration: 4/10
22:00:33,482 root INFO Walk iteration: 5/10
22:00:33,484 root INFO Walk iteration: 6/10
22:00:33,485 root INFO Walk iteration: 7/10
22:00:33,486 root INFO Walk iteration: 8/10
22:00:33,486 root INFO Walk iteration: 9/10
22:00:33,487 root INFO Walk iteration: 10/10
22:00:33,488 root INFO Learning embeddings at time 1730494833.4887013
22:00:33,488 gensim.models.word2vec INFO collecting all words and their counts
22:00:33,489 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:00:33,489 gensim.models.word2vec INFO collected 35 word types from a corpus of 7240 raw words and 350 sentences
22:00:33,490 gensim.models.word2vec INFO Creating a fresh vocabulary
22:00:33,490 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 35 unique words (100.00% of original 35, drops 0)', 'datetime': '2024-11-01T22:00:33.490935', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,491 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7240 word corpus (100.00% of original 7240, drops 0)', 'datetime': '2024-11-01T22:00:33.491702', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,492 gensim.models.word2vec INFO deleting the raw counts dictionary of 35 items
22:00:33,493 gensim.models.word2vec INFO sample=0.001 downsamples 35 most-common words
22:00:33,493 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1485.4809590381558 word corpus (20.5%% of prior 7240)', 'datetime': '2024-11-01T22:00:33.493562', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:00:33,494 gensim.models.word2vec INFO estimated required memory for 35 words and 1536 dimensions: 447580 bytes
22:00:33,494 gensim.models.word2vec INFO resetting layer weights
22:00:33,495 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:00:33.495014', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:00:33,495 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 35 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:00:33.495533', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:00:33,503 gensim.models.word2vec INFO EPOCH 0: training on 7240 raw words (1515 effective words) took 0.0s, 244825 effective words/s
22:00:33,513 gensim.models.word2vec INFO EPOCH 1: training on 7240 raw words (1505 effective words) took 0.0s, 161856 effective words/s
22:00:33,522 gensim.models.word2vec INFO EPOCH 2: training on 7240 raw words (1534 effective words) took 0.0s, 193859 effective words/s
22:00:33,523 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 21720 raw words (4554 effective words) took 0.0s, 171124 effective words/s', 'datetime': '2024-11-01T22:00:33.522999', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:00:33,523 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=35, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:00:33.523664', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:00:33,523 root INFO Completed. Ending time is 1730494833.5237324 Elapsed time is -0.061426401138305664
22:00:33,576 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:00:33,732 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:00:33,734 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:33,742 datashaper.workflow.workflow INFO executing verb create_final_entities
22:00:33,748 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:00:33,785 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:00:33,785 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:00:33,786 graphrag.index.operations.embed_text.strategies.openai INFO embedding 36 inputs via 36 snippets using 3 batches. max_batch_size=16, max_tokens=8191
22:00:34,883 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:00:34,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1031793751753867. input_tokens=82, output_tokens=0
22:00:35,181 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:00:35,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4193163830786943. input_tokens=347, output_tokens=0
22:00:35,542 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:00:35,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7791536280419677. input_tokens=453, output_tokens=0
22:00:35,572 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:00:35,733 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:00:35,733 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:35,743 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:00:38,514 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:00:38,685 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:00:38,685 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:38,696 datashaper.workflow.workflow INFO executing verb create_final_communities
22:00:38,712 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:00:38,854 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:00:38,857 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:00:38,867 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:00:38,875 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:00:38,883 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:00:39,29 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:00:39,30 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:00:39,35 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:00:39,36 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:00:39,51 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:00:39,61 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:00:39,205 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:00:39,205 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:00:39,213 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:00:39,221 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:00:39,225 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 36
22:00:39,243 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 36
22:00:42,98 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:42,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.835210761986673. input_tokens=2551, output_tokens=398
22:00:42,568 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:42,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3009916450828314. input_tokens=2386, output_tokens=497
22:00:42,690 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:42,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4281464219093323. input_tokens=2422, output_tokens=507
22:00:42,728 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:42,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4688648879528046. input_tokens=2502, output_tokens=522
22:00:42,878 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:42,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6136767650023103. input_tokens=2493, output_tokens=578
22:00:44,589 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:44,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0044626789167523. input_tokens=2221, output_tokens=383
22:00:46,44 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:46,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.931381094036624. input_tokens=2609, output_tokens=788
22:00:49,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:49,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.69347549485974. input_tokens=3305, output_tokens=690
22:00:49,760 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:49,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6981319738551974. input_tokens=2816, output_tokens=720
22:00:50,20 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:00:50,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9617714651394635. input_tokens=2869, output_tokens=769
22:00:50,42 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:00:50,192 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:00:50,192 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:00:50,202 datashaper.workflow.workflow INFO executing verb create_final_documents
22:00:50,209 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:00:50,226 graphrag.index.cli INFO All workflows completed successfully.
22:15:31,268 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:15:31,270 graphrag.index.cli INFO Starting pipeline run for: 20241101-221531, dryrun=False
22:15:31,270 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:15:31,272 graphrag.index.create_pipeline_config INFO skipping workflows 
22:15:31,272 graphrag.index.run.run INFO Running pipeline
22:15:31,272 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:15:31,272 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:15:31,272 graphrag.index.input.load_input INFO using file storage for input
22:15:31,273 graphrag.index.input.csv INFO Loading csv files from input_eval
22:15:31,273 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:15:31,276 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:15:31,276 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:15:31,277 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:15:31,278 graphrag.index.run.run INFO Final # of rows loaded: 1
22:15:31,390 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:15:31,392 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:15:31,874 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:15:32,4 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:15:32,4 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:15:32,11 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:15:32,12 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:15:32,50 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:15:32,50 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:15:34,781 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:34,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7363949809223413. input_tokens=2087, output_tokens=527
22:15:40,682 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:40,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.895090167177841. input_tokens=34, output_tokens=1794
22:15:41,345 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:41,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6582573519553989. input_tokens=30, output_tokens=1
22:15:48,839 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:48,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.495072491001338. input_tokens=34, output_tokens=2559
22:15:49,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:49,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.7111354188527912. input_tokens=30, output_tokens=1
22:15:59,287 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:15:59,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 9.729969058884308. input_tokens=34, output_tokens=3382
22:16:00,118 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:00,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8250388239976019. input_tokens=30, output_tokens=1
22:16:10,555 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:10,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 10.435549083165824. input_tokens=34, output_tokens=3824
22:16:11,444 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:11,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8823037222027779. input_tokens=30, output_tokens=1
22:16:24,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:24,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.746471060905606. input_tokens=34, output_tokens=4353
22:16:25,296 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:25,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.0959030990488827. input_tokens=30, output_tokens=1
22:16:37,672 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:37,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.374436263926327. input_tokens=34, output_tokens=4353
22:16:38,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:38,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.1443392129149288. input_tokens=30, output_tokens=1
22:16:51,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:51,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.451795251108706. input_tokens=34, output_tokens=4353
22:16:51,761 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:51,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46486333990469575. input_tokens=157, output_tokens=18
22:16:52,341 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:16:52,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.044479412958026. input_tokens=178, output_tokens=18
22:16:52,374 root INFO Starting preprocessing of transition probabilities on graph with 32 nodes and 65 edges
22:16:52,374 root INFO Starting at time 1730495812.374393
22:16:52,374 root INFO Beginning preprocessing of transition probabilities for 32 vertices
22:16:52,374 root INFO Completed 1 / 32 vertices
22:16:52,375 root INFO Completed 4 / 32 vertices
22:16:52,375 root INFO Completed 7 / 32 vertices
22:16:52,376 root INFO Completed 10 / 32 vertices
22:16:52,377 root INFO Completed 13 / 32 vertices
22:16:52,377 root INFO Completed 16 / 32 vertices
22:16:52,377 root INFO Completed 19 / 32 vertices
22:16:52,378 root INFO Completed 22 / 32 vertices
22:16:52,378 root INFO Completed 25 / 32 vertices
22:16:52,378 root INFO Completed 28 / 32 vertices
22:16:52,378 root INFO Completed 31 / 32 vertices
22:16:52,379 root INFO Completed preprocessing of transition probabilities for vertices
22:16:52,379 root INFO Beginning preprocessing of transition probabilities for 65 edges
22:16:52,379 root INFO Completed 1 / 65 edges
22:16:52,380 root INFO Completed 7 / 65 edges
22:16:52,380 root INFO Completed 13 / 65 edges
22:16:52,381 root INFO Completed 19 / 65 edges
22:16:52,382 root INFO Completed 25 / 65 edges
22:16:52,382 root INFO Completed 31 / 65 edges
22:16:52,383 root INFO Completed 37 / 65 edges
22:16:52,383 root INFO Completed 43 / 65 edges
22:16:52,384 root INFO Completed 49 / 65 edges
22:16:52,384 root INFO Completed 55 / 65 edges
22:16:52,385 root INFO Completed 61 / 65 edges
22:16:52,385 root INFO Completed preprocessing of transition probabilities for edges
22:16:52,386 root INFO Simulating walks on graph at time 1730495812.3863144
22:16:52,387 root INFO Walk iteration: 1/10
22:16:52,390 root INFO Walk iteration: 2/10
22:16:52,392 root INFO Walk iteration: 3/10
22:16:52,394 root INFO Walk iteration: 4/10
22:16:52,397 root INFO Walk iteration: 5/10
22:16:52,399 root INFO Walk iteration: 6/10
22:16:52,400 root INFO Walk iteration: 7/10
22:16:52,402 root INFO Walk iteration: 8/10
22:16:52,403 root INFO Walk iteration: 9/10
22:16:52,404 root INFO Walk iteration: 10/10
22:16:52,405 root INFO Learning embeddings at time 1730495812.4054284
22:16:52,405 gensim.models.word2vec INFO collecting all words and their counts
22:16:52,406 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:16:52,406 gensim.models.word2vec INFO collected 32 word types from a corpus of 6220 raw words and 320 sentences
22:16:52,406 gensim.models.word2vec INFO Creating a fresh vocabulary
22:16:52,406 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 32 unique words (100.00% of original 32, drops 0)', 'datetime': '2024-11-01T22:16:52.406694', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,407 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6220 word corpus (100.00% of original 6220, drops 0)', 'datetime': '2024-11-01T22:16:52.407131', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,407 gensim.models.word2vec INFO deleting the raw counts dictionary of 32 items
22:16:52,407 gensim.models.word2vec INFO sample=0.001 downsamples 32 most-common words
22:16:52,407 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1227.3808819845633 word corpus (19.7%% of prior 6220)', 'datetime': '2024-11-01T22:16:52.407633', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,407 gensim.models.word2vec INFO estimated required memory for 32 words and 1536 dimensions: 409216 bytes
22:16:52,408 gensim.models.word2vec INFO resetting layer weights
22:16:52,408 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:16:52.408508', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:16:52,408 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 32 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:16:52.408774', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:16:52,417 gensim.models.word2vec INFO EPOCH 0: training on 6220 raw words (1252 effective words) took 0.0s, 193423 effective words/s
22:16:52,426 gensim.models.word2vec INFO EPOCH 1: training on 6220 raw words (1181 effective words) took 0.0s, 173015 effective words/s
22:16:52,433 gensim.models.word2vec INFO EPOCH 2: training on 6220 raw words (1214 effective words) took 0.0s, 184844 effective words/s
22:16:52,433 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 18660 raw words (3647 effective words) took 0.0s, 149549 effective words/s', 'datetime': '2024-11-01T22:16:52.433661', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:16:52,434 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=32, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:16:52.434379', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:16:52,434 root INFO Completed. Ending time is 1730495812.4344356 Elapsed time is -0.060042619705200195
22:16:52,440 root INFO Starting preprocessing of transition probabilities on graph with 32 nodes and 65 edges
22:16:52,440 root INFO Starting at time 1730495812.4408376
22:16:52,440 root INFO Beginning preprocessing of transition probabilities for 32 vertices
22:16:52,441 root INFO Completed 1 / 32 vertices
22:16:52,441 root INFO Completed 4 / 32 vertices
22:16:52,442 root INFO Completed 7 / 32 vertices
22:16:52,443 root INFO Completed 10 / 32 vertices
22:16:52,443 root INFO Completed 13 / 32 vertices
22:16:52,444 root INFO Completed 16 / 32 vertices
22:16:52,445 root INFO Completed 19 / 32 vertices
22:16:52,445 root INFO Completed 22 / 32 vertices
22:16:52,445 root INFO Completed 25 / 32 vertices
22:16:52,446 root INFO Completed 28 / 32 vertices
22:16:52,446 root INFO Completed 31 / 32 vertices
22:16:52,447 root INFO Completed preprocessing of transition probabilities for vertices
22:16:52,447 root INFO Beginning preprocessing of transition probabilities for 65 edges
22:16:52,447 root INFO Completed 1 / 65 edges
22:16:52,448 root INFO Completed 7 / 65 edges
22:16:52,448 root INFO Completed 13 / 65 edges
22:16:52,449 root INFO Completed 19 / 65 edges
22:16:52,450 root INFO Completed 25 / 65 edges
22:16:52,450 root INFO Completed 31 / 65 edges
22:16:52,451 root INFO Completed 37 / 65 edges
22:16:52,452 root INFO Completed 43 / 65 edges
22:16:52,453 root INFO Completed 49 / 65 edges
22:16:52,453 root INFO Completed 55 / 65 edges
22:16:52,454 root INFO Completed 61 / 65 edges
22:16:52,455 root INFO Completed preprocessing of transition probabilities for edges
22:16:52,455 root INFO Simulating walks on graph at time 1730495812.4557512
22:16:52,456 root INFO Walk iteration: 1/10
22:16:52,458 root INFO Walk iteration: 2/10
22:16:52,461 root INFO Walk iteration: 3/10
22:16:52,462 root INFO Walk iteration: 4/10
22:16:52,464 root INFO Walk iteration: 5/10
22:16:52,465 root INFO Walk iteration: 6/10
22:16:52,466 root INFO Walk iteration: 7/10
22:16:52,467 root INFO Walk iteration: 8/10
22:16:52,468 root INFO Walk iteration: 9/10
22:16:52,468 root INFO Walk iteration: 10/10
22:16:52,469 root INFO Learning embeddings at time 1730495812.4697492
22:16:52,470 gensim.models.word2vec INFO collecting all words and their counts
22:16:52,470 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:16:52,471 gensim.models.word2vec INFO collected 32 word types from a corpus of 6220 raw words and 320 sentences
22:16:52,471 gensim.models.word2vec INFO Creating a fresh vocabulary
22:16:52,471 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 32 unique words (100.00% of original 32, drops 0)', 'datetime': '2024-11-01T22:16:52.471826', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,472 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6220 word corpus (100.00% of original 6220, drops 0)', 'datetime': '2024-11-01T22:16:52.472505', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,473 gensim.models.word2vec INFO deleting the raw counts dictionary of 32 items
22:16:52,473 gensim.models.word2vec INFO sample=0.001 downsamples 32 most-common words
22:16:52,473 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1227.3808819845633 word corpus (19.7%% of prior 6220)', 'datetime': '2024-11-01T22:16:52.473654', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:16:52,474 gensim.models.word2vec INFO estimated required memory for 32 words and 1536 dimensions: 409216 bytes
22:16:52,474 gensim.models.word2vec INFO resetting layer weights
22:16:52,475 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:16:52.475949', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:16:52,476 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 32 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:16:52.476392', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:16:52,483 gensim.models.word2vec INFO EPOCH 0: training on 6220 raw words (1252 effective words) took 0.0s, 209479 effective words/s
22:16:52,494 gensim.models.word2vec INFO EPOCH 1: training on 6220 raw words (1181 effective words) took 0.0s, 126317 effective words/s
22:16:52,506 gensim.models.word2vec INFO EPOCH 2: training on 6220 raw words (1214 effective words) took 0.0s, 105173 effective words/s
22:16:52,506 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 18660 raw words (3647 effective words) took 0.0s, 122070 effective words/s', 'datetime': '2024-11-01T22:16:52.506825', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:16:52,507 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=32, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:16:52.507595', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:16:52,507 root INFO Completed. Ending time is 1730495812.5076423 Elapsed time is -0.06680464744567871
22:16:52,562 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:16:52,727 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:16:52,728 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:52,736 datashaper.workflow.workflow INFO executing verb create_final_entities
22:16:52,743 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:16:52,782 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:16:52,782 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:16:52,783 graphrag.index.operations.embed_text.strategies.openai INFO embedding 48 inputs via 48 snippets using 3 batches. max_batch_size=16, max_tokens=8191
22:16:54,85 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:16:54,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3257065929938108. input_tokens=339, output_tokens=0
22:16:54,113 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:16:54,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3526962550822645. input_tokens=406, output_tokens=0
22:16:54,276 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:16:54,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5228490501176566. input_tokens=452, output_tokens=0
22:16:54,312 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:16:54,487 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:16:54,487 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:54,496 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:16:57,383 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:16:57,551 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:16:57,551 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:57,560 datashaper.workflow.workflow INFO executing verb create_final_communities
22:16:57,576 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:16:57,711 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:16:57,711 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:16:57,717 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:16:57,726 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:16:57,734 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:16:57,872 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
22:16:57,872 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:16:57,877 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:16:57,880 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:16:57,887 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:16:57,896 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:16:58,36 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:16:58,36 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:16:58,39 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:16:58,49 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:16:58,56 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 27
22:16:58,70 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 111
22:17:00,484 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:00,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3783832699991763. input_tokens=2055, output_tokens=310
22:17:00,547 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:00,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4429615950211883. input_tokens=2223, output_tokens=428
22:17:01,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:01,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9485978048760444. input_tokens=2102, output_tokens=444
22:17:01,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:01,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2429757099598646. input_tokens=2312, output_tokens=507
22:17:04,106 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:04,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7467735570389777. input_tokens=2241, output_tokens=495
22:17:04,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:04,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.406163599807769. input_tokens=2638, output_tokens=598
22:17:04,843 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:04,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.486883766017854. input_tokens=2289, output_tokens=511
22:17:04,847 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:04,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4865154589060694. input_tokens=3198, output_tokens=682
22:17:05,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:17:05,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.338599788956344. input_tokens=2740, output_tokens=808
22:17:05,706 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:17:05,853 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:17:05,853 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:17:05,866 datashaper.workflow.workflow INFO executing verb create_final_documents
22:17:05,872 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:17:05,901 graphrag.index.cli INFO All workflows completed successfully.
22:20:30,937 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:20:30,939 graphrag.index.cli INFO Starting pipeline run for: 20241101-222030, dryrun=False
22:20:30,939 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:20:30,942 graphrag.index.create_pipeline_config INFO skipping workflows 
22:20:30,942 graphrag.index.run.run INFO Running pipeline
22:20:30,942 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:20:30,944 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:20:30,944 graphrag.index.input.load_input INFO using file storage for input
22:20:30,947 graphrag.index.input.csv INFO Loading csv files from input_eval
22:20:30,947 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:20:30,953 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:20:30,954 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:20:30,955 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:20:30,955 graphrag.index.run.run INFO Final # of rows loaded: 1
22:20:31,76 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:20:31,78 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:20:31,711 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:20:31,841 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:20:31,841 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:20:31,847 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:20:31,849 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:20:31,888 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:20:31,888 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:20:34,580 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:34,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7002224850002676. input_tokens=2087, output_tokens=527
22:20:40,878 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:40,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.289686913136393. input_tokens=32, output_tokens=2075
22:20:41,470 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:41,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5872133090160787. input_tokens=30, output_tokens=1
22:20:46,916 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:46,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.444010148057714. input_tokens=32, output_tokens=1766
22:20:47,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:47,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.8265071350615472. input_tokens=30, output_tokens=1
22:20:54,36 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:54,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.291859483113512. input_tokens=32, output_tokens=2112
22:20:54,759 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:20:54,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7180611100047827. input_tokens=30, output_tokens=1
22:21:01,448 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:01,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 6.6885336430277675. input_tokens=32, output_tokens=2238
22:21:02,251 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:02,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.7967754339333624. input_tokens=30, output_tokens=1
22:21:09,525 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:09,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 7.273551326012239. input_tokens=32, output_tokens=2429
22:21:10,295 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:10,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.7673444750253111. input_tokens=30, output_tokens=1
22:21:17,771 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:17,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 7.474772008135915. input_tokens=32, output_tokens=2565
22:21:18,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:18,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.9026838040445. input_tokens=30, output_tokens=1
22:21:26,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:26,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 7.832339043030515. input_tokens=32, output_tokens=2709
22:21:26,535 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 41 edges
22:21:26,535 root INFO Starting at time 1730496086.5353246
22:21:26,535 root INFO Beginning preprocessing of transition probabilities for 22 vertices
22:21:26,535 root INFO Completed 1 / 22 vertices
22:21:26,535 root INFO Completed 3 / 22 vertices
22:21:26,538 root INFO Completed 5 / 22 vertices
22:21:26,539 root INFO Completed 7 / 22 vertices
22:21:26,539 root INFO Completed 9 / 22 vertices
22:21:26,540 root INFO Completed 11 / 22 vertices
22:21:26,540 root INFO Completed 13 / 22 vertices
22:21:26,540 root INFO Completed 15 / 22 vertices
22:21:26,540 root INFO Completed 17 / 22 vertices
22:21:26,540 root INFO Completed 19 / 22 vertices
22:21:26,540 root INFO Completed 21 / 22 vertices
22:21:26,540 root INFO Completed preprocessing of transition probabilities for vertices
22:21:26,541 root INFO Beginning preprocessing of transition probabilities for 41 edges
22:21:26,541 root INFO Completed 1 / 41 edges
22:21:26,541 root INFO Completed 5 / 41 edges
22:21:26,542 root INFO Completed 9 / 41 edges
22:21:26,542 root INFO Completed 13 / 41 edges
22:21:26,543 root INFO Completed 17 / 41 edges
22:21:26,544 root INFO Completed 21 / 41 edges
22:21:26,544 root INFO Completed 25 / 41 edges
22:21:26,545 root INFO Completed 29 / 41 edges
22:21:26,546 root INFO Completed 33 / 41 edges
22:21:26,546 root INFO Completed 37 / 41 edges
22:21:26,547 root INFO Completed 41 / 41 edges
22:21:26,547 root INFO Completed preprocessing of transition probabilities for edges
22:21:26,548 root INFO Simulating walks on graph at time 1730496086.5485306
22:21:26,549 root INFO Walk iteration: 1/10
22:21:26,551 root INFO Walk iteration: 2/10
22:21:26,553 root INFO Walk iteration: 3/10
22:21:26,555 root INFO Walk iteration: 4/10
22:21:26,555 root INFO Walk iteration: 5/10
22:21:26,556 root INFO Walk iteration: 6/10
22:21:26,556 root INFO Walk iteration: 7/10
22:21:26,557 root INFO Walk iteration: 8/10
22:21:26,558 root INFO Walk iteration: 9/10
22:21:26,558 root INFO Walk iteration: 10/10
22:21:26,559 root INFO Learning embeddings at time 1730496086.559154
22:21:26,559 gensim.models.word2vec INFO collecting all words and their counts
22:21:26,559 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:21:26,559 gensim.models.word2vec INFO collected 22 word types from a corpus of 4160 raw words and 220 sentences
22:21:26,560 gensim.models.word2vec INFO Creating a fresh vocabulary
22:21:26,560 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T22:21:26.560490', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,561 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4160 word corpus (100.00% of original 4160, drops 0)', 'datetime': '2024-11-01T22:21:26.561204', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,561 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
22:21:26,561 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
22:21:26,562 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 657.49516805063 word corpus (15.8%% of prior 4160)', 'datetime': '2024-11-01T22:21:26.562631', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,562 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
22:21:26,563 gensim.models.word2vec INFO resetting layer weights
22:21:26,564 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:21:26.564081', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:21:26,564 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:21:26.564460', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:21:26,568 gensim.models.word2vec INFO EPOCH 0: training on 4160 raw words (668 effective words) took 0.0s, 281829 effective words/s
22:21:26,571 gensim.models.word2vec INFO EPOCH 1: training on 4160 raw words (641 effective words) took 0.0s, 303140 effective words/s
22:21:26,574 gensim.models.word2vec INFO EPOCH 2: training on 4160 raw words (626 effective words) took 0.0s, 278743 effective words/s
22:21:26,574 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 12480 raw words (1935 effective words) took 0.0s, 211248 effective words/s', 'datetime': '2024-11-01T22:21:26.574297', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:21:26,574 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:21:26.574338', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:21:26,574 root INFO Completed. Ending time is 1730496086.574365 Elapsed time is -0.039040327072143555
22:21:26,576 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 41 edges
22:21:26,576 root INFO Starting at time 1730496086.57691
22:21:26,576 root INFO Beginning preprocessing of transition probabilities for 22 vertices
22:21:26,577 root INFO Completed 1 / 22 vertices
22:21:26,577 root INFO Completed 3 / 22 vertices
22:21:26,578 root INFO Completed 5 / 22 vertices
22:21:26,579 root INFO Completed 7 / 22 vertices
22:21:26,579 root INFO Completed 9 / 22 vertices
22:21:26,579 root INFO Completed 11 / 22 vertices
22:21:26,580 root INFO Completed 13 / 22 vertices
22:21:26,580 root INFO Completed 15 / 22 vertices
22:21:26,580 root INFO Completed 17 / 22 vertices
22:21:26,580 root INFO Completed 19 / 22 vertices
22:21:26,581 root INFO Completed 21 / 22 vertices
22:21:26,581 root INFO Completed preprocessing of transition probabilities for vertices
22:21:26,582 root INFO Beginning preprocessing of transition probabilities for 41 edges
22:21:26,582 root INFO Completed 1 / 41 edges
22:21:26,583 root INFO Completed 5 / 41 edges
22:21:26,583 root INFO Completed 9 / 41 edges
22:21:26,584 root INFO Completed 13 / 41 edges
22:21:26,584 root INFO Completed 17 / 41 edges
22:21:26,585 root INFO Completed 21 / 41 edges
22:21:26,585 root INFO Completed 25 / 41 edges
22:21:26,586 root INFO Completed 29 / 41 edges
22:21:26,586 root INFO Completed 33 / 41 edges
22:21:26,587 root INFO Completed 37 / 41 edges
22:21:26,588 root INFO Completed 41 / 41 edges
22:21:26,588 root INFO Completed preprocessing of transition probabilities for edges
22:21:26,589 root INFO Simulating walks on graph at time 1730496086.589269
22:21:26,589 root INFO Walk iteration: 1/10
22:21:26,590 root INFO Walk iteration: 2/10
22:21:26,590 root INFO Walk iteration: 3/10
22:21:26,591 root INFO Walk iteration: 4/10
22:21:26,591 root INFO Walk iteration: 5/10
22:21:26,592 root INFO Walk iteration: 6/10
22:21:26,593 root INFO Walk iteration: 7/10
22:21:26,593 root INFO Walk iteration: 8/10
22:21:26,594 root INFO Walk iteration: 9/10
22:21:26,594 root INFO Walk iteration: 10/10
22:21:26,595 root INFO Learning embeddings at time 1730496086.5953572
22:21:26,595 gensim.models.word2vec INFO collecting all words and their counts
22:21:26,595 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:21:26,596 gensim.models.word2vec INFO collected 22 word types from a corpus of 4160 raw words and 220 sentences
22:21:26,596 gensim.models.word2vec INFO Creating a fresh vocabulary
22:21:26,596 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T22:21:26.596321', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,596 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4160 word corpus (100.00% of original 4160, drops 0)', 'datetime': '2024-11-01T22:21:26.596821', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,597 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
22:21:26,597 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
22:21:26,598 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 657.49516805063 word corpus (15.8%% of prior 4160)', 'datetime': '2024-11-01T22:21:26.598378', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:21:26,598 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
22:21:26,599 gensim.models.word2vec INFO resetting layer weights
22:21:26,599 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:21:26.599639', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:21:26,600 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:21:26.600054', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:21:26,603 gensim.models.word2vec INFO EPOCH 0: training on 4160 raw words (668 effective words) took 0.0s, 309176 effective words/s
22:21:26,606 gensim.models.word2vec INFO EPOCH 1: training on 4160 raw words (641 effective words) took 0.0s, 319193 effective words/s
22:21:26,608 gensim.models.word2vec INFO EPOCH 2: training on 4160 raw words (626 effective words) took 0.0s, 332281 effective words/s
22:21:26,608 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 12480 raw words (1935 effective words) took 0.0s, 232467 effective words/s', 'datetime': '2024-11-01T22:21:26.608870', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:21:26,609 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:21:26.609485', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:21:26,609 root INFO Completed. Ending time is 1730496086.60952 Elapsed time is -0.03260993957519531
22:21:26,635 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:21:26,783 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:21:26,784 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:26,791 datashaper.workflow.workflow INFO executing verb create_final_entities
22:21:26,796 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:21:26,833 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:21:26,833 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:21:26,834 graphrag.index.operations.embed_text.strategies.openai INFO embedding 22 inputs via 22 snippets using 2 batches. max_batch_size=16, max_tokens=8191
22:21:28,23 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:21:28,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1986796481069177. input_tokens=148, output_tokens=0
22:21:28,37 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:21:28,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2269170340150595. input_tokens=458, output_tokens=0
22:21:28,65 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:21:28,237 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:21:28,237 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:28,246 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:21:31,118 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:21:31,285 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:21:31,285 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:31,294 datashaper.workflow.workflow INFO executing verb create_final_communities
22:21:31,307 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:21:31,443 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:21:31,445 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:21:31,449 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:21:31,457 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:21:31,464 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:21:31,604 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
22:21:31,604 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:21:31,608 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:21:31,610 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:21:31,617 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:21:31,626 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:21:31,764 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:21:31,764 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:21:31,768 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:21:31,777 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:21:31,782 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 11
22:21:31,794 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 55
22:21:34,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:34,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8777550468221307. input_tokens=2217, output_tokens=445
22:21:35,137 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:35,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.320109887048602. input_tokens=2492, output_tokens=544
22:21:37,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:37,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.613655585097149. input_tokens=2625, output_tokens=506
22:21:38,542 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:38,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3821967581752688. input_tokens=3006, output_tokens=670
22:21:38,690 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:21:38,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5331262869294733. input_tokens=2486, output_tokens=607
22:21:38,698 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:21:38,862 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:21:38,862 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:21:38,872 datashaper.workflow.workflow INFO executing verb create_final_documents
22:21:38,879 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:21:38,899 graphrag.index.cli INFO All workflows completed successfully.
22:23:50,667 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:23:50,668 graphrag.index.cli INFO Starting pipeline run for: 20241101-222350, dryrun=False
22:23:50,669 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:23:50,670 graphrag.index.create_pipeline_config INFO skipping workflows 
22:23:50,670 graphrag.index.run.run INFO Running pipeline
22:23:50,671 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:23:50,672 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:23:50,672 graphrag.index.input.load_input INFO using file storage for input
22:23:50,674 graphrag.index.input.csv INFO Loading csv files from input_eval
22:23:50,674 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:23:50,677 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:23:50,677 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:23:50,678 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:23:50,678 graphrag.index.run.run INFO Final # of rows loaded: 1
22:23:50,792 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:23:50,794 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:23:51,278 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:23:51,407 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:23:51,408 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:23:51,414 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:23:51,415 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:23:51,454 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:23:51,454 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:23:53,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:53,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3039583871141076. input_tokens=2087, output_tokens=527
22:24:00,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:00,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.328846370102838. input_tokens=32, output_tokens=2075
22:24:00,742 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:00,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6499515541363508. input_tokens=34, output_tokens=1
22:24:00,765 root INFO Starting preprocessing of transition probabilities on graph with 14 nodes and 17 edges
22:24:00,765 root INFO Starting at time 1730496240.7654626
22:24:00,765 root INFO Beginning preprocessing of transition probabilities for 14 vertices
22:24:00,765 root INFO Completed 1 / 14 vertices
22:24:00,765 root INFO Completed 2 / 14 vertices
22:24:00,765 root INFO Completed 3 / 14 vertices
22:24:00,765 root INFO Completed 4 / 14 vertices
22:24:00,765 root INFO Completed 5 / 14 vertices
22:24:00,765 root INFO Completed 6 / 14 vertices
22:24:00,766 root INFO Completed 7 / 14 vertices
22:24:00,766 root INFO Completed 8 / 14 vertices
22:24:00,766 root INFO Completed 9 / 14 vertices
22:24:00,767 root INFO Completed 10 / 14 vertices
22:24:00,767 root INFO Completed 11 / 14 vertices
22:24:00,767 root INFO Completed 12 / 14 vertices
22:24:00,767 root INFO Completed 13 / 14 vertices
22:24:00,767 root INFO Completed 14 / 14 vertices
22:24:00,767 root INFO Completed preprocessing of transition probabilities for vertices
22:24:00,767 root INFO Beginning preprocessing of transition probabilities for 17 edges
22:24:00,768 root INFO Completed 1 / 17 edges
22:24:00,768 root INFO Completed 2 / 17 edges
22:24:00,769 root INFO Completed 3 / 17 edges
22:24:00,769 root INFO Completed 4 / 17 edges
22:24:00,769 root INFO Completed 5 / 17 edges
22:24:00,769 root INFO Completed 6 / 17 edges
22:24:00,770 root INFO Completed 7 / 17 edges
22:24:00,771 root INFO Completed 8 / 17 edges
22:24:00,771 root INFO Completed 9 / 17 edges
22:24:00,771 root INFO Completed 10 / 17 edges
22:24:00,772 root INFO Completed 11 / 17 edges
22:24:00,772 root INFO Completed 12 / 17 edges
22:24:00,773 root INFO Completed 13 / 17 edges
22:24:00,773 root INFO Completed 14 / 17 edges
22:24:00,773 root INFO Completed 15 / 17 edges
22:24:00,773 root INFO Completed 16 / 17 edges
22:24:00,774 root INFO Completed 17 / 17 edges
22:24:00,774 root INFO Completed preprocessing of transition probabilities for edges
22:24:00,774 root INFO Simulating walks on graph at time 1730496240.7749076
22:24:00,775 root INFO Walk iteration: 1/10
22:24:00,776 root INFO Walk iteration: 2/10
22:24:00,776 root INFO Walk iteration: 3/10
22:24:00,777 root INFO Walk iteration: 4/10
22:24:00,777 root INFO Walk iteration: 5/10
22:24:00,778 root INFO Walk iteration: 6/10
22:24:00,779 root INFO Walk iteration: 7/10
22:24:00,779 root INFO Walk iteration: 8/10
22:24:00,780 root INFO Walk iteration: 9/10
22:24:00,780 root INFO Walk iteration: 10/10
22:24:00,781 root INFO Learning embeddings at time 1730496240.7811978
22:24:00,781 gensim.models.word2vec INFO collecting all words and their counts
22:24:00,782 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:24:00,782 gensim.models.word2vec INFO collected 14 word types from a corpus of 2680 raw words and 140 sentences
22:24:00,782 gensim.models.word2vec INFO Creating a fresh vocabulary
22:24:00,782 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 14 unique words (100.00% of original 14, drops 0)', 'datetime': '2024-11-01T22:24:00.782703', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:00,783 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2680 word corpus (100.00% of original 2680, drops 0)', 'datetime': '2024-11-01T22:24:00.783171', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:00,783 gensim.models.word2vec INFO deleting the raw counts dictionary of 14 items
22:24:00,783 gensim.models.word2vec INFO sample=0.001 downsamples 14 most-common words
22:24:00,783 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 318.3233331547982 word corpus (11.9%% of prior 2680)', 'datetime': '2024-11-01T22:24:00.783766', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:00,783 gensim.models.word2vec INFO estimated required memory for 14 words and 1536 dimensions: 179032 bytes
22:24:00,784 gensim.models.word2vec INFO resetting layer weights
22:24:00,785 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:24:00.785092', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:24:00,785 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 14 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:24:00.785547', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:24:00,788 gensim.models.word2vec INFO EPOCH 0: training on 2680 raw words (313 effective words) took 0.0s, 271475 effective words/s
22:24:00,796 gensim.models.word2vec INFO EPOCH 1: training on 2680 raw words (314 effective words) took 0.0s, 166235 effective words/s
22:24:00,799 gensim.models.word2vec INFO EPOCH 2: training on 2680 raw words (336 effective words) took 0.0s, 132496 effective words/s
22:24:00,800 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 8040 raw words (963 effective words) took 0.0s, 66310 effective words/s', 'datetime': '2024-11-01T22:24:00.800106', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:24:00,800 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=14, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:24:00.800780', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:24:00,801 root INFO Completed. Ending time is 1730496240.8015862 Elapsed time is -0.03612351417541504
22:24:00,831 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:24:00,971 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:24:00,972 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:00,979 datashaper.workflow.workflow INFO executing verb create_final_entities
22:24:00,982 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:24:01,20 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:24:01,20 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:24:01,21 graphrag.index.operations.embed_text.strategies.openai INFO embedding 14 inputs via 14 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:24:02,326 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:24:02,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3345567528158426. input_tokens=405, output_tokens=0
22:24:02,361 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:24:02,503 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:24:02,503 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:02,511 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:24:05,50 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:24:05,200 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:24:05,201 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:05,208 datashaper.workflow.workflow INFO executing verb create_final_communities
22:24:05,219 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:24:05,354 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:24:05,354 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:24:05,360 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:05,367 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:24:05,373 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:24:05,512 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
22:24:05,512 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:24:05,515 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:24:05,518 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:24:05,525 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:24:05,534 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:24:05,671 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:24:05,671 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:24:05,675 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:24:05,683 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:24:05,686 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 14
22:24:08,488 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:08,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7877852639649063. input_tokens=2337, output_tokens=499
22:24:09,522 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:09,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8247224770020694. input_tokens=2583, output_tokens=663
22:24:09,534 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:24:09,686 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:24:09,686 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:24:09,696 datashaper.workflow.workflow INFO executing verb create_final_documents
22:24:09,701 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:24:09,718 graphrag.index.cli INFO All workflows completed successfully.
22:24:36,294 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:24:36,296 graphrag.index.cli INFO Starting pipeline run for: 20241101-222436, dryrun=False
22:24:36,296 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:24:36,298 graphrag.index.create_pipeline_config INFO skipping workflows 
22:24:36,298 graphrag.index.run.run INFO Running pipeline
22:24:36,298 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:24:36,299 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:24:36,299 graphrag.index.input.load_input INFO using file storage for input
22:24:36,300 graphrag.index.input.csv INFO Loading csv files from input_eval
22:24:36,300 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:24:36,304 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:24:36,304 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:24:36,305 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:24:36,305 graphrag.index.run.run INFO Final # of rows loaded: 1
22:24:36,418 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:24:36,421 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:24:36,777 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:24:36,903 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:24:36,903 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:24:36,910 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:24:36,910 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:24:36,949 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:24:36,949 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:24:39,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:39,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.692569733131677. input_tokens=2087, output_tokens=527
22:24:46,667 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:46,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.0255169400479645. input_tokens=32, output_tokens=2292
22:24:47,336 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:47,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6661778939887881. input_tokens=34, output_tokens=1
22:24:47,379 root INFO Starting preprocessing of transition probabilities on graph with 15 nodes and 18 edges
22:24:47,379 root INFO Starting at time 1730496287.3792288
22:24:47,379 root INFO Beginning preprocessing of transition probabilities for 15 vertices
22:24:47,379 root INFO Completed 1 / 15 vertices
22:24:47,379 root INFO Completed 2 / 15 vertices
22:24:47,380 root INFO Completed 3 / 15 vertices
22:24:47,381 root INFO Completed 4 / 15 vertices
22:24:47,381 root INFO Completed 5 / 15 vertices
22:24:47,382 root INFO Completed 6 / 15 vertices
22:24:47,382 root INFO Completed 7 / 15 vertices
22:24:47,383 root INFO Completed 8 / 15 vertices
22:24:47,383 root INFO Completed 9 / 15 vertices
22:24:47,383 root INFO Completed 10 / 15 vertices
22:24:47,384 root INFO Completed 11 / 15 vertices
22:24:47,385 root INFO Completed 12 / 15 vertices
22:24:47,385 root INFO Completed 13 / 15 vertices
22:24:47,386 root INFO Completed 14 / 15 vertices
22:24:47,386 root INFO Completed 15 / 15 vertices
22:24:47,386 root INFO Completed preprocessing of transition probabilities for vertices
22:24:47,386 root INFO Beginning preprocessing of transition probabilities for 18 edges
22:24:47,387 root INFO Completed 1 / 18 edges
22:24:47,387 root INFO Completed 2 / 18 edges
22:24:47,388 root INFO Completed 3 / 18 edges
22:24:47,388 root INFO Completed 4 / 18 edges
22:24:47,389 root INFO Completed 5 / 18 edges
22:24:47,389 root INFO Completed 6 / 18 edges
22:24:47,390 root INFO Completed 7 / 18 edges
22:24:47,390 root INFO Completed 8 / 18 edges
22:24:47,390 root INFO Completed 9 / 18 edges
22:24:47,390 root INFO Completed 10 / 18 edges
22:24:47,391 root INFO Completed 11 / 18 edges
22:24:47,391 root INFO Completed 12 / 18 edges
22:24:47,391 root INFO Completed 13 / 18 edges
22:24:47,391 root INFO Completed 14 / 18 edges
22:24:47,391 root INFO Completed 15 / 18 edges
22:24:47,392 root INFO Completed 16 / 18 edges
22:24:47,392 root INFO Completed 17 / 18 edges
22:24:47,392 root INFO Completed 18 / 18 edges
22:24:47,393 root INFO Completed preprocessing of transition probabilities for edges
22:24:47,393 root INFO Simulating walks on graph at time 1730496287.3932052
22:24:47,393 root INFO Walk iteration: 1/10
22:24:47,395 root INFO Walk iteration: 2/10
22:24:47,395 root INFO Walk iteration: 3/10
22:24:47,396 root INFO Walk iteration: 4/10
22:24:47,397 root INFO Walk iteration: 5/10
22:24:47,398 root INFO Walk iteration: 6/10
22:24:47,398 root INFO Walk iteration: 7/10
22:24:47,399 root INFO Walk iteration: 8/10
22:24:47,400 root INFO Walk iteration: 9/10
22:24:47,400 root INFO Walk iteration: 10/10
22:24:47,401 root INFO Learning embeddings at time 1730496287.4011154
22:24:47,401 gensim.models.word2vec INFO collecting all words and their counts
22:24:47,401 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:24:47,401 gensim.models.word2vec INFO collected 15 word types from a corpus of 2520 raw words and 150 sentences
22:24:47,402 gensim.models.word2vec INFO Creating a fresh vocabulary
22:24:47,403 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 15 unique words (100.00% of original 15, drops 0)', 'datetime': '2024-11-01T22:24:47.402989', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:47,403 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2520 word corpus (100.00% of original 2520, drops 0)', 'datetime': '2024-11-01T22:24:47.403288', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:47,403 gensim.models.word2vec INFO deleting the raw counts dictionary of 15 items
22:24:47,403 gensim.models.word2vec INFO sample=0.001 downsamples 15 most-common words
22:24:47,403 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 317.4676849368212 word corpus (12.6%% of prior 2520)', 'datetime': '2024-11-01T22:24:47.403769', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:24:47,404 gensim.models.word2vec INFO estimated required memory for 15 words and 1536 dimensions: 191820 bytes
22:24:47,404 gensim.models.word2vec INFO resetting layer weights
22:24:47,405 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:24:47.405652', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:24:47,405 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 15 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:24:47.405962', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:24:47,410 gensim.models.word2vec INFO EPOCH 0: training on 2520 raw words (317 effective words) took 0.0s, 204553 effective words/s
22:24:47,413 gensim.models.word2vec INFO EPOCH 1: training on 2520 raw words (305 effective words) took 0.0s, 166502 effective words/s
22:24:47,417 gensim.models.word2vec INFO EPOCH 2: training on 2520 raw words (324 effective words) took 0.0s, 151265 effective words/s
22:24:47,417 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 7560 raw words (946 effective words) took 0.0s, 86792 effective words/s', 'datetime': '2024-11-01T22:24:47.417476', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:24:47,418 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:24:47.418155', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:24:47,418 root INFO Completed. Ending time is 1730496287.418932 Elapsed time is -0.0397031307220459
22:24:47,447 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:24:47,596 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:24:47,596 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:47,603 datashaper.workflow.workflow INFO executing verb create_final_entities
22:24:47,606 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:24:47,645 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:24:47,645 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:24:47,645 graphrag.index.operations.embed_text.strategies.openai INFO embedding 15 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:24:48,858 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:24:48,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2418995159678161. input_tokens=448, output_tokens=0
22:24:48,893 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:24:49,35 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:24:49,36 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:49,43 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:24:51,447 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:24:51,612 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:24:51,613 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:51,620 datashaper.workflow.workflow INFO executing verb create_final_communities
22:24:51,631 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:24:51,765 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:24:51,765 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:24:51,769 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:24:51,777 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:24:51,783 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:24:51,933 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:24:51,933 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:24:51,936 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:24:51,938 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:24:51,945 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:24:51,955 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:24:52,99 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:24:52,99 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:24:52,102 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:24:52,111 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:24:52,114 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 15
22:24:54,482 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:54,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3491097460500896. input_tokens=2110, output_tokens=390
22:24:55,178 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:55,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0499854010995477. input_tokens=2548, output_tokens=605
22:24:55,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:55,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1838023809250444. input_tokens=2247, output_tokens=494
22:24:55,322 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:24:55,463 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:24:55,467 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:24:55,476 datashaper.workflow.workflow INFO executing verb create_final_documents
22:24:55,482 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:24:55,499 graphrag.index.cli INFO All workflows completed successfully.
22:25:30,803 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:25:30,805 graphrag.index.cli INFO Starting pipeline run for: 20241101-222530, dryrun=False
22:25:30,805 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:25:30,807 graphrag.index.create_pipeline_config INFO skipping workflows 
22:25:30,807 graphrag.index.run.run INFO Running pipeline
22:25:30,808 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:25:30,809 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:25:30,809 graphrag.index.input.load_input INFO using file storage for input
22:25:30,811 graphrag.index.input.csv INFO Loading csv files from input_eval
22:25:30,812 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:25:30,816 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:25:30,817 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:25:30,818 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:25:30,818 graphrag.index.run.run INFO Final # of rows loaded: 1
22:25:30,933 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:25:30,936 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:25:31,440 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:25:31,568 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:25:31,568 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:25:31,575 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:25:31,576 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:25:31,615 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:25:31,615 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:25:34,345 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:34,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7380471089854836. input_tokens=2087, output_tokens=527
22:25:40,651 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:40,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.29743814910762. input_tokens=33, output_tokens=2065
22:25:41,262 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:41,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6070813699625432. input_tokens=34, output_tokens=1
22:25:41,289 root INFO Starting preprocessing of transition probabilities on graph with 14 nodes and 17 edges
22:25:41,290 root INFO Starting at time 1730496341.2900167
22:25:41,290 root INFO Beginning preprocessing of transition probabilities for 14 vertices
22:25:41,290 root INFO Completed 1 / 14 vertices
22:25:41,290 root INFO Completed 2 / 14 vertices
22:25:41,290 root INFO Completed 3 / 14 vertices
22:25:41,290 root INFO Completed 4 / 14 vertices
22:25:41,290 root INFO Completed 5 / 14 vertices
22:25:41,290 root INFO Completed 6 / 14 vertices
22:25:41,290 root INFO Completed 7 / 14 vertices
22:25:41,290 root INFO Completed 8 / 14 vertices
22:25:41,290 root INFO Completed 9 / 14 vertices
22:25:41,290 root INFO Completed 10 / 14 vertices
22:25:41,290 root INFO Completed 11 / 14 vertices
22:25:41,290 root INFO Completed 12 / 14 vertices
22:25:41,290 root INFO Completed 13 / 14 vertices
22:25:41,291 root INFO Completed 14 / 14 vertices
22:25:41,291 root INFO Completed preprocessing of transition probabilities for vertices
22:25:41,291 root INFO Beginning preprocessing of transition probabilities for 17 edges
22:25:41,291 root INFO Completed 1 / 17 edges
22:25:41,291 root INFO Completed 2 / 17 edges
22:25:41,291 root INFO Completed 3 / 17 edges
22:25:41,291 root INFO Completed 4 / 17 edges
22:25:41,292 root INFO Completed 5 / 17 edges
22:25:41,292 root INFO Completed 6 / 17 edges
22:25:41,292 root INFO Completed 7 / 17 edges
22:25:41,292 root INFO Completed 8 / 17 edges
22:25:41,292 root INFO Completed 9 / 17 edges
22:25:41,292 root INFO Completed 10 / 17 edges
22:25:41,292 root INFO Completed 11 / 17 edges
22:25:41,292 root INFO Completed 12 / 17 edges
22:25:41,292 root INFO Completed 13 / 17 edges
22:25:41,292 root INFO Completed 14 / 17 edges
22:25:41,293 root INFO Completed 15 / 17 edges
22:25:41,293 root INFO Completed 16 / 17 edges
22:25:41,293 root INFO Completed 17 / 17 edges
22:25:41,293 root INFO Completed preprocessing of transition probabilities for edges
22:25:41,293 root INFO Simulating walks on graph at time 1730496341.2938027
22:25:41,293 root INFO Walk iteration: 1/10
22:25:41,294 root INFO Walk iteration: 2/10
22:25:41,295 root INFO Walk iteration: 3/10
22:25:41,295 root INFO Walk iteration: 4/10
22:25:41,296 root INFO Walk iteration: 5/10
22:25:41,297 root INFO Walk iteration: 6/10
22:25:41,297 root INFO Walk iteration: 7/10
22:25:41,298 root INFO Walk iteration: 8/10
22:25:41,299 root INFO Walk iteration: 9/10
22:25:41,299 root INFO Walk iteration: 10/10
22:25:41,300 root INFO Learning embeddings at time 1730496341.3002217
22:25:41,300 gensim.models.word2vec INFO collecting all words and their counts
22:25:41,300 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:25:41,301 gensim.models.word2vec INFO collected 14 word types from a corpus of 2720 raw words and 140 sentences
22:25:41,301 gensim.models.word2vec INFO Creating a fresh vocabulary
22:25:41,302 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 14 unique words (100.00% of original 14, drops 0)', 'datetime': '2024-11-01T22:25:41.302067', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:25:41,302 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2720 word corpus (100.00% of original 2720, drops 0)', 'datetime': '2024-11-01T22:25:41.302516', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:25:41,302 gensim.models.word2vec INFO deleting the raw counts dictionary of 14 items
22:25:41,303 gensim.models.word2vec INFO sample=0.001 downsamples 14 most-common words
22:25:41,303 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 332.6633469193555 word corpus (12.2%% of prior 2720)', 'datetime': '2024-11-01T22:25:41.303124', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:25:41,303 gensim.models.word2vec INFO estimated required memory for 14 words and 1536 dimensions: 179032 bytes
22:25:41,304 gensim.models.word2vec INFO resetting layer weights
22:25:41,304 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:25:41.304556', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:25:41,304 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 14 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:25:41.304785', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:25:41,310 gensim.models.word2vec INFO EPOCH 0: training on 2720 raw words (328 effective words) took 0.0s, 141970 effective words/s
22:25:41,318 gensim.models.word2vec INFO EPOCH 1: training on 2720 raw words (364 effective words) took 0.0s, 53852 effective words/s
22:25:41,326 gensim.models.word2vec INFO EPOCH 2: training on 2720 raw words (324 effective words) took 0.0s, 74031 effective words/s
22:25:41,326 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 8160 raw words (1016 effective words) took 0.0s, 48242 effective words/s', 'datetime': '2024-11-01T22:25:41.326344', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:25:41,327 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=14, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:25:41.327182', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:25:41,327 root INFO Completed. Ending time is 1730496341.3272994 Elapsed time is -0.037282705307006836
22:25:41,357 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:25:41,493 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:25:41,494 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:25:41,502 datashaper.workflow.workflow INFO executing verb create_final_entities
22:25:41,506 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:25:41,544 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:25:41,544 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:25:41,545 graphrag.index.operations.embed_text.strategies.openai INFO embedding 14 inputs via 14 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:25:42,656 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:25:42,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1474540040362626. input_tokens=403, output_tokens=0
22:25:42,697 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:25:42,838 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:25:42,839 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:25:42,846 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:25:45,214 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:25:45,380 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:25:45,381 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:25:45,388 datashaper.workflow.workflow INFO executing verb create_final_communities
22:25:45,399 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:25:45,534 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:25:45,534 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:25:45,538 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:25:45,545 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:25:45,551 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:25:45,690 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:25:45,691 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:25:45,694 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:25:45,696 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:25:45,706 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:25:45,715 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:25:45,852 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:25:45,853 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:25:45,856 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:25:45,864 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:25:45,867 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 14
22:25:48,459 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:48,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5736482180655003. input_tokens=2094, output_tokens=317
22:25:48,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:48,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.659624806139618. input_tokens=2169, output_tokens=382
22:25:48,879 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:48,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.998966885963455. input_tokens=2530, output_tokens=591
22:25:48,891 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:25:49,40 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:25:49,40 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:25:49,49 datashaper.workflow.workflow INFO executing verb create_final_documents
22:25:49,55 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:25:49,73 graphrag.index.cli INFO All workflows completed successfully.
22:28:21,342 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:28:21,343 graphrag.index.cli INFO Starting pipeline run for: 20241101-222821, dryrun=False
22:28:21,343 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:28:21,346 graphrag.index.create_pipeline_config INFO skipping workflows 
22:28:21,346 graphrag.index.run.run INFO Running pipeline
22:28:21,346 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:28:21,347 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:28:21,347 graphrag.index.input.load_input INFO using file storage for input
22:28:21,349 graphrag.index.input.csv INFO Loading csv files from input_eval
22:28:21,349 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:28:21,352 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:28:21,353 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:28:21,353 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:28:21,354 graphrag.index.run.run INFO Final # of rows loaded: 1
22:28:21,467 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:28:21,469 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:28:21,950 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:28:22,78 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:28:22,78 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:28:22,85 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:28:22,86 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:28:22,125 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:28:22,125 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:28:24,883 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:24,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7667601769790053. input_tokens=2087, output_tokens=527
22:28:32,257 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:32,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.365343501092866. input_tokens=33, output_tokens=2403
22:28:32,812 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:32,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5503058850299567. input_tokens=30, output_tokens=1
22:28:40,939 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:40,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 8.126607245998457. input_tokens=33, output_tokens=2752
22:28:41,684 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:41,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.7397022990044206. input_tokens=30, output_tokens=1
22:28:53,638 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:53,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 11.952952223131433. input_tokens=33, output_tokens=4180
22:28:54,536 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:54,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.890569088049233. input_tokens=30, output_tokens=1
22:29:07,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:07,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 13.20660086791031. input_tokens=33, output_tokens=4651
22:29:08,710 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:08,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.959282340016216. input_tokens=30, output_tokens=1
22:29:22,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:22,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 13.37615046207793. input_tokens=33, output_tokens=4654
22:29:23,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:23,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.1336456760764122. input_tokens=30, output_tokens=1
22:29:36,452 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:36,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 13.220827861921862. input_tokens=33, output_tokens=4660
22:29:37,648 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:37,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.187208066927269. input_tokens=30, output_tokens=1
22:29:50,822 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:50,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 13.172121756942943. input_tokens=33, output_tokens=4714
22:29:51,352 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:51,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5072128248866647. input_tokens=145, output_tokens=23
22:29:51,889 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:51,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0424474168103188. input_tokens=189, output_tokens=14
22:29:52,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:52,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3513994328677654. input_tokens=189, output_tokens=185
22:29:52,228 root INFO Starting preprocessing of transition probabilities on graph with 35 nodes and 63 edges
22:29:52,228 root INFO Starting at time 1730496592.2284708
22:29:52,228 root INFO Beginning preprocessing of transition probabilities for 35 vertices
22:29:52,228 root INFO Completed 1 / 35 vertices
22:29:52,228 root INFO Completed 4 / 35 vertices
22:29:52,228 root INFO Completed 7 / 35 vertices
22:29:52,229 root INFO Completed 10 / 35 vertices
22:29:52,229 root INFO Completed 13 / 35 vertices
22:29:52,230 root INFO Completed 16 / 35 vertices
22:29:52,230 root INFO Completed 19 / 35 vertices
22:29:52,230 root INFO Completed 22 / 35 vertices
22:29:52,231 root INFO Completed 25 / 35 vertices
22:29:52,232 root INFO Completed 28 / 35 vertices
22:29:52,232 root INFO Completed 31 / 35 vertices
22:29:52,233 root INFO Completed 34 / 35 vertices
22:29:52,233 root INFO Completed preprocessing of transition probabilities for vertices
22:29:52,233 root INFO Beginning preprocessing of transition probabilities for 63 edges
22:29:52,233 root INFO Completed 1 / 63 edges
22:29:52,233 root INFO Completed 7 / 63 edges
22:29:52,234 root INFO Completed 13 / 63 edges
22:29:52,235 root INFO Completed 19 / 63 edges
22:29:52,235 root INFO Completed 25 / 63 edges
22:29:52,236 root INFO Completed 31 / 63 edges
22:29:52,237 root INFO Completed 37 / 63 edges
22:29:52,237 root INFO Completed 43 / 63 edges
22:29:52,238 root INFO Completed 49 / 63 edges
22:29:52,239 root INFO Completed 55 / 63 edges
22:29:52,240 root INFO Completed 61 / 63 edges
22:29:52,241 root INFO Completed preprocessing of transition probabilities for edges
22:29:52,241 root INFO Simulating walks on graph at time 1730496592.2416327
22:29:52,241 root INFO Walk iteration: 1/10
22:29:52,244 root INFO Walk iteration: 2/10
22:29:52,245 root INFO Walk iteration: 3/10
22:29:52,246 root INFO Walk iteration: 4/10
22:29:52,247 root INFO Walk iteration: 5/10
22:29:52,248 root INFO Walk iteration: 6/10
22:29:52,249 root INFO Walk iteration: 7/10
22:29:52,250 root INFO Walk iteration: 8/10
22:29:52,251 root INFO Walk iteration: 9/10
22:29:52,252 root INFO Walk iteration: 10/10
22:29:52,253 root INFO Learning embeddings at time 1730496592.253643
22:29:52,253 gensim.models.word2vec INFO collecting all words and their counts
22:29:52,254 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:29:52,255 gensim.models.word2vec INFO collected 35 word types from a corpus of 7320 raw words and 350 sentences
22:29:52,255 gensim.models.word2vec INFO Creating a fresh vocabulary
22:29:52,255 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 35 unique words (100.00% of original 35, drops 0)', 'datetime': '2024-11-01T22:29:52.255454', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,255 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7320 word corpus (100.00% of original 7320, drops 0)', 'datetime': '2024-11-01T22:29:52.255929', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,256 gensim.models.word2vec INFO deleting the raw counts dictionary of 35 items
22:29:52,256 gensim.models.word2vec INFO sample=0.001 downsamples 35 most-common words
22:29:52,256 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1484.2269082768037 word corpus (20.3%% of prior 7320)', 'datetime': '2024-11-01T22:29:52.256765', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,257 gensim.models.word2vec INFO estimated required memory for 35 words and 1536 dimensions: 447580 bytes
22:29:52,257 gensim.models.word2vec INFO resetting layer weights
22:29:52,258 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:29:52.258592', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:29:52,259 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 35 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:29:52.259032', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:29:52,265 gensim.models.word2vec INFO EPOCH 0: training on 7320 raw words (1522 effective words) took 0.0s, 336628 effective words/s
22:29:52,274 gensim.models.word2vec INFO EPOCH 1: training on 7320 raw words (1432 effective words) took 0.0s, 187114 effective words/s
22:29:52,285 gensim.models.word2vec INFO EPOCH 2: training on 7320 raw words (1514 effective words) took 0.0s, 148892 effective words/s
22:29:52,285 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 21960 raw words (4468 effective words) took 0.0s, 174393 effective words/s', 'datetime': '2024-11-01T22:29:52.285170', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:29:52,285 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=35, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:29:52.285793', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:29:52,285 root INFO Completed. Ending time is 1730496592.2858381 Elapsed time is -0.05736732482910156
22:29:52,291 root INFO Starting preprocessing of transition probabilities on graph with 35 nodes and 63 edges
22:29:52,291 root INFO Starting at time 1730496592.2911875
22:29:52,291 root INFO Beginning preprocessing of transition probabilities for 35 vertices
22:29:52,291 root INFO Completed 1 / 35 vertices
22:29:52,291 root INFO Completed 4 / 35 vertices
22:29:52,292 root INFO Completed 7 / 35 vertices
22:29:52,293 root INFO Completed 10 / 35 vertices
22:29:52,293 root INFO Completed 13 / 35 vertices
22:29:52,294 root INFO Completed 16 / 35 vertices
22:29:52,294 root INFO Completed 19 / 35 vertices
22:29:52,294 root INFO Completed 22 / 35 vertices
22:29:52,295 root INFO Completed 25 / 35 vertices
22:29:52,295 root INFO Completed 28 / 35 vertices
22:29:52,295 root INFO Completed 31 / 35 vertices
22:29:52,296 root INFO Completed 34 / 35 vertices
22:29:52,296 root INFO Completed preprocessing of transition probabilities for vertices
22:29:52,296 root INFO Beginning preprocessing of transition probabilities for 63 edges
22:29:52,296 root INFO Completed 1 / 63 edges
22:29:52,297 root INFO Completed 7 / 63 edges
22:29:52,297 root INFO Completed 13 / 63 edges
22:29:52,298 root INFO Completed 19 / 63 edges
22:29:52,299 root INFO Completed 25 / 63 edges
22:29:52,299 root INFO Completed 31 / 63 edges
22:29:52,300 root INFO Completed 37 / 63 edges
22:29:52,300 root INFO Completed 43 / 63 edges
22:29:52,301 root INFO Completed 49 / 63 edges
22:29:52,301 root INFO Completed 55 / 63 edges
22:29:52,302 root INFO Completed 61 / 63 edges
22:29:52,303 root INFO Completed preprocessing of transition probabilities for edges
22:29:52,303 root INFO Simulating walks on graph at time 1730496592.3036108
22:29:52,304 root INFO Walk iteration: 1/10
22:29:52,306 root INFO Walk iteration: 2/10
22:29:52,309 root INFO Walk iteration: 3/10
22:29:52,310 root INFO Walk iteration: 4/10
22:29:52,312 root INFO Walk iteration: 5/10
22:29:52,313 root INFO Walk iteration: 6/10
22:29:52,318 root INFO Walk iteration: 7/10
22:29:52,319 root INFO Walk iteration: 8/10
22:29:52,321 root INFO Walk iteration: 9/10
22:29:52,322 root INFO Walk iteration: 10/10
22:29:52,323 root INFO Learning embeddings at time 1730496592.3230538
22:29:52,323 gensim.models.word2vec INFO collecting all words and their counts
22:29:52,323 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:29:52,324 gensim.models.word2vec INFO collected 35 word types from a corpus of 7320 raw words and 350 sentences
22:29:52,326 gensim.models.word2vec INFO Creating a fresh vocabulary
22:29:52,327 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 35 unique words (100.00% of original 35, drops 0)', 'datetime': '2024-11-01T22:29:52.327466', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,328 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7320 word corpus (100.00% of original 7320, drops 0)', 'datetime': '2024-11-01T22:29:52.328137', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,328 gensim.models.word2vec INFO deleting the raw counts dictionary of 35 items
22:29:52,335 gensim.models.word2vec INFO sample=0.001 downsamples 35 most-common words
22:29:52,335 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1484.2269082768037 word corpus (20.3%% of prior 7320)', 'datetime': '2024-11-01T22:29:52.335573', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:29:52,342 gensim.models.word2vec INFO estimated required memory for 35 words and 1536 dimensions: 447580 bytes
22:29:52,342 gensim.models.word2vec INFO resetting layer weights
22:29:52,343 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:29:52.343505', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:29:52,343 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 35 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:29:52.343823', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:29:52,367 gensim.models.word2vec INFO EPOCH 0: training on 7320 raw words (1522 effective words) took 0.0s, 75349 effective words/s
22:29:52,384 gensim.models.word2vec INFO EPOCH 1: training on 7320 raw words (1432 effective words) took 0.0s, 89833 effective words/s
22:29:52,409 gensim.models.word2vec INFO EPOCH 2: training on 7320 raw words (1514 effective words) took 0.0s, 71906 effective words/s
22:29:52,409 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 21960 raw words (4468 effective words) took 0.1s, 68244 effective words/s', 'datetime': '2024-11-01T22:29:52.409364', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:29:52,409 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=35, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:29:52.409428', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:29:52,409 root INFO Completed. Ending time is 1730496592.4094925 Elapsed time is -0.11830496788024902
22:29:52,460 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:29:52,616 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:29:52,616 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:52,629 datashaper.workflow.workflow INFO executing verb create_final_entities
22:29:52,636 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:29:52,674 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:29:52,674 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:29:52,675 graphrag.index.operations.embed_text.strategies.openai INFO embedding 41 inputs via 41 snippets using 3 batches. max_batch_size=16, max_tokens=8191
22:29:53,883 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:29:53,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2229406600818038. input_tokens=250, output_tokens=0
22:29:54,71 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:29:54,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4193254390265793. input_tokens=448, output_tokens=0
22:29:54,97 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:29:54,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4444454449694604. input_tokens=351, output_tokens=0
22:29:54,124 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:29:54,290 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:29:54,290 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:54,299 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:29:57,109 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:29:57,275 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:29:57,275 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:57,284 datashaper.workflow.workflow INFO executing verb create_final_communities
22:29:57,299 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:29:57,436 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:29:57,437 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:29:57,442 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:29:57,455 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:29:57,464 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:29:57,602 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
22:29:57,603 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:29:57,606 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:29:57,609 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:29:57,616 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:29:57,626 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:29:57,766 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:29:57,766 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:29:57,770 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:29:57,780 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:29:57,785 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 29
22:29:57,801 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 77
22:30:00,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:00,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.562813752098009. input_tokens=2042, output_tokens=285
22:30:00,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:00,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.915019562933594. input_tokens=2424, output_tokens=541
22:30:01,35 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:01,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2092538960278034. input_tokens=2244, output_tokens=482
22:30:01,397 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:01,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.576026305789128. input_tokens=2378, output_tokens=505
22:30:01,804 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:01,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9810839721467346. input_tokens=2536, output_tokens=583
22:30:04,204 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:04,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.37786996900104. input_tokens=2202, output_tokens=428
22:30:04,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:04,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.010212766006589. input_tokens=2864, output_tokens=530
22:30:05,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:05,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2033237779978663. input_tokens=3107, output_tokens=572
22:30:05,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:05,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9240410740021616. input_tokens=2751, output_tokens=715
22:30:05,764 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:30:05,909 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:30:05,909 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:30:05,918 datashaper.workflow.workflow INFO executing verb create_final_documents
22:30:05,925 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:30:05,945 graphrag.index.cli INFO All workflows completed successfully.
22:35:34,402 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:35:34,403 graphrag.index.cli INFO Starting pipeline run for: 20241101-223534, dryrun=False
22:35:34,403 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:35:34,405 graphrag.index.create_pipeline_config INFO skipping workflows 
22:35:34,405 graphrag.index.run.run INFO Running pipeline
22:35:34,405 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:35:34,406 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:35:34,406 graphrag.index.input.load_input INFO using file storage for input
22:35:34,408 graphrag.index.input.csv INFO Loading csv files from input_eval
22:35:34,408 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:35:34,411 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:35:34,411 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:35:34,412 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:35:34,412 graphrag.index.run.run INFO Final # of rows loaded: 1
22:35:34,525 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:35:34,527 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:35:34,989 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:35:35,117 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:35:35,117 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:35:35,123 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:35:35,124 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:35:35,163 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:35:35,163 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:35:37,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:37,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.755745369940996. input_tokens=2087, output_tokens=527
22:35:42,734 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:42,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.815503556048498. input_tokens=46, output_tokens=1213
22:35:43,246 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:43,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5084221290890127. input_tokens=30, output_tokens=1
22:35:45,801 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:45,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.5541401309892535. input_tokens=46, output_tokens=538
22:35:46,437 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:46,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6339547578245401. input_tokens=30, output_tokens=1
22:35:51,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:51,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 5.105370665900409. input_tokens=46, output_tokens=1241
22:35:52,216 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:52,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.670159013941884. input_tokens=30, output_tokens=1
22:35:56,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:56,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 4.34349368698895. input_tokens=46, output_tokens=1088
22:35:59,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:59,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 2.688447932014242. input_tokens=30, output_tokens=1
22:36:19,986 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:19,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 20.733166211051866. input_tokens=46, output_tokens=1320
22:36:20,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:20,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6016953859943897. input_tokens=30, output_tokens=1
22:36:26,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:26,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 5.484593362081796. input_tokens=46, output_tokens=1461
22:36:27,445 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:27,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.3636115181725472. input_tokens=30, output_tokens=1
22:36:35,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:35,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 7.895548747153953. input_tokens=46, output_tokens=2332
22:36:35,380 root INFO Starting preprocessing of transition probabilities on graph with 61 nodes and 93 edges
22:36:35,380 root INFO Starting at time 1730496995.380147
22:36:35,380 root INFO Beginning preprocessing of transition probabilities for 61 vertices
22:36:35,380 root INFO Completed 1 / 61 vertices
22:36:35,381 root INFO Completed 7 / 61 vertices
22:36:35,381 root INFO Completed 13 / 61 vertices
22:36:35,382 root INFO Completed 19 / 61 vertices
22:36:35,382 root INFO Completed 25 / 61 vertices
22:36:35,383 root INFO Completed 31 / 61 vertices
22:36:35,383 root INFO Completed 37 / 61 vertices
22:36:35,384 root INFO Completed 43 / 61 vertices
22:36:35,385 root INFO Completed 49 / 61 vertices
22:36:35,385 root INFO Completed 55 / 61 vertices
22:36:35,386 root INFO Completed 61 / 61 vertices
22:36:35,386 root INFO Completed preprocessing of transition probabilities for vertices
22:36:35,386 root INFO Beginning preprocessing of transition probabilities for 93 edges
22:36:35,386 root INFO Completed 1 / 93 edges
22:36:35,387 root INFO Completed 10 / 93 edges
22:36:35,387 root INFO Completed 19 / 93 edges
22:36:35,388 root INFO Completed 28 / 93 edges
22:36:35,388 root INFO Completed 37 / 93 edges
22:36:35,389 root INFO Completed 46 / 93 edges
22:36:35,389 root INFO Completed 55 / 93 edges
22:36:35,390 root INFO Completed 64 / 93 edges
22:36:35,390 root INFO Completed 73 / 93 edges
22:36:35,391 root INFO Completed 82 / 93 edges
22:36:35,391 root INFO Completed 91 / 93 edges
22:36:35,392 root INFO Completed preprocessing of transition probabilities for edges
22:36:35,393 root INFO Simulating walks on graph at time 1730496995.393193
22:36:35,393 root INFO Walk iteration: 1/10
22:36:35,395 root INFO Walk iteration: 2/10
22:36:35,397 root INFO Walk iteration: 3/10
22:36:35,398 root INFO Walk iteration: 4/10
22:36:35,400 root INFO Walk iteration: 5/10
22:36:35,401 root INFO Walk iteration: 6/10
22:36:35,403 root INFO Walk iteration: 7/10
22:36:35,404 root INFO Walk iteration: 8/10
22:36:35,406 root INFO Walk iteration: 9/10
22:36:35,407 root INFO Walk iteration: 10/10
22:36:35,409 root INFO Learning embeddings at time 1730496995.409421
22:36:35,409 gensim.models.word2vec INFO collecting all words and their counts
22:36:35,409 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:36:35,410 gensim.models.word2vec INFO collected 61 word types from a corpus of 12200 raw words and 610 sentences
22:36:35,410 gensim.models.word2vec INFO Creating a fresh vocabulary
22:36:35,410 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 61 unique words (100.00% of original 61, drops 0)', 'datetime': '2024-11-01T22:36:35.410561', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,411 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 12200 word corpus (100.00% of original 12200, drops 0)', 'datetime': '2024-11-01T22:36:35.411048', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,411 gensim.models.word2vec INFO deleting the raw counts dictionary of 61 items
22:36:35,411 gensim.models.word2vec INFO sample=0.001 downsamples 61 most-common words
22:36:35,412 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3473.7172414708307 word corpus (28.5%% of prior 12200)', 'datetime': '2024-11-01T22:36:35.412476', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,413 gensim.models.word2vec INFO estimated required memory for 61 words and 1536 dimensions: 780068 bytes
22:36:35,413 gensim.models.word2vec INFO resetting layer weights
22:36:35,413 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:36:35.413733', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:36:35,413 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 61 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:36:35.413896', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:36:35,427 gensim.models.word2vec INFO EPOCH 0: training on 12200 raw words (3476 effective words) took 0.0s, 300228 effective words/s
22:36:35,440 gensim.models.word2vec INFO EPOCH 1: training on 12200 raw words (3495 effective words) took 0.0s, 293614 effective words/s
22:36:35,454 gensim.models.word2vec INFO EPOCH 2: training on 12200 raw words (3534 effective words) took 0.0s, 252528 effective words/s
22:36:35,455 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 36600 raw words (10505 effective words) took 0.0s, 259699 effective words/s', 'datetime': '2024-11-01T22:36:35.454994', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:36:35,455 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=61, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:36:35.455041', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:36:35,455 root INFO Completed. Ending time is 1730496995.4556186 Elapsed time is -0.07547163963317871
22:36:35,463 root INFO Starting preprocessing of transition probabilities on graph with 61 nodes and 93 edges
22:36:35,463 root INFO Starting at time 1730496995.4639437
22:36:35,463 root INFO Beginning preprocessing of transition probabilities for 61 vertices
22:36:35,463 root INFO Completed 1 / 61 vertices
22:36:35,464 root INFO Completed 7 / 61 vertices
22:36:35,464 root INFO Completed 13 / 61 vertices
22:36:35,465 root INFO Completed 19 / 61 vertices
22:36:35,465 root INFO Completed 25 / 61 vertices
22:36:35,466 root INFO Completed 31 / 61 vertices
22:36:35,466 root INFO Completed 37 / 61 vertices
22:36:35,466 root INFO Completed 43 / 61 vertices
22:36:35,467 root INFO Completed 49 / 61 vertices
22:36:35,468 root INFO Completed 55 / 61 vertices
22:36:35,468 root INFO Completed 61 / 61 vertices
22:36:35,468 root INFO Completed preprocessing of transition probabilities for vertices
22:36:35,469 root INFO Beginning preprocessing of transition probabilities for 93 edges
22:36:35,469 root INFO Completed 1 / 93 edges
22:36:35,470 root INFO Completed 10 / 93 edges
22:36:35,471 root INFO Completed 19 / 93 edges
22:36:35,472 root INFO Completed 28 / 93 edges
22:36:35,472 root INFO Completed 37 / 93 edges
22:36:35,473 root INFO Completed 46 / 93 edges
22:36:35,473 root INFO Completed 55 / 93 edges
22:36:35,474 root INFO Completed 64 / 93 edges
22:36:35,474 root INFO Completed 73 / 93 edges
22:36:35,475 root INFO Completed 82 / 93 edges
22:36:35,476 root INFO Completed 91 / 93 edges
22:36:35,476 root INFO Completed preprocessing of transition probabilities for edges
22:36:35,476 root INFO Simulating walks on graph at time 1730496995.4769654
22:36:35,477 root INFO Walk iteration: 1/10
22:36:35,482 root INFO Walk iteration: 2/10
22:36:35,486 root INFO Walk iteration: 3/10
22:36:35,489 root INFO Walk iteration: 4/10
22:36:35,491 root INFO Walk iteration: 5/10
22:36:35,493 root INFO Walk iteration: 6/10
22:36:35,494 root INFO Walk iteration: 7/10
22:36:35,496 root INFO Walk iteration: 8/10
22:36:35,497 root INFO Walk iteration: 9/10
22:36:35,499 root INFO Walk iteration: 10/10
22:36:35,501 root INFO Learning embeddings at time 1730496995.5010436
22:36:35,501 gensim.models.word2vec INFO collecting all words and their counts
22:36:35,501 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:36:35,502 gensim.models.word2vec INFO collected 61 word types from a corpus of 12200 raw words and 610 sentences
22:36:35,502 gensim.models.word2vec INFO Creating a fresh vocabulary
22:36:35,503 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 61 unique words (100.00% of original 61, drops 0)', 'datetime': '2024-11-01T22:36:35.503624', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,504 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 12200 word corpus (100.00% of original 12200, drops 0)', 'datetime': '2024-11-01T22:36:35.504408', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,505 gensim.models.word2vec INFO deleting the raw counts dictionary of 61 items
22:36:35,505 gensim.models.word2vec INFO sample=0.001 downsamples 61 most-common words
22:36:35,506 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3473.7172414708307 word corpus (28.5%% of prior 12200)', 'datetime': '2024-11-01T22:36:35.506410', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:36:35,507 gensim.models.word2vec INFO estimated required memory for 61 words and 1536 dimensions: 780068 bytes
22:36:35,507 gensim.models.word2vec INFO resetting layer weights
22:36:35,507 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:36:35.507931', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:36:35,508 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 61 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:36:35.508187', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:36:35,522 gensim.models.word2vec INFO EPOCH 0: training on 12200 raw words (3476 effective words) took 0.0s, 269019 effective words/s
22:36:35,536 gensim.models.word2vec INFO EPOCH 1: training on 12200 raw words (3495 effective words) took 0.0s, 258819 effective words/s
22:36:35,549 gensim.models.word2vec INFO EPOCH 2: training on 12200 raw words (3534 effective words) took 0.0s, 309430 effective words/s
22:36:35,549 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 36600 raw words (10505 effective words) took 0.0s, 257150 effective words/s', 'datetime': '2024-11-01T22:36:35.549055', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:36:35,549 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=61, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:36:35.549086', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:36:35,549 root INFO Completed. Ending time is 1730496995.549123 Elapsed time is -0.08517932891845703
22:36:35,593 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:36:35,795 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:36:35,795 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:36:35,820 datashaper.workflow.workflow INFO executing verb create_final_entities
22:36:35,827 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:36:35,865 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:36:35,865 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:36:35,867 graphrag.index.operations.embed_text.strategies.openai INFO embedding 64 inputs via 58 snippets using 4 batches. max_batch_size=16, max_tokens=8191
22:36:37,200 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:36:37,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3472288239281625. input_tokens=433, output_tokens=0
22:36:37,239 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:36:37,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3935211529023945. input_tokens=579, output_tokens=0
22:36:37,356 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:36:37,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5103264041244984. input_tokens=691, output_tokens=0
22:36:37,382 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:36:37,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5381985041312873. input_tokens=571, output_tokens=0
22:36:37,410 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:36:37,596 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:36:37,596 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:36:37,607 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:36:40,476 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:36:40,671 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:36:40,671 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:36:40,683 datashaper.workflow.workflow INFO executing verb create_final_communities
22:36:40,703 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:36:40,842 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:36:40,842 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:36:40,852 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:36:40,862 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:36:40,872 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:36:41,12 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
22:36:41,12 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:36:41,17 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:36:41,20 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:36:41,27 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:36:41,37 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:36:41,174 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:36:41,174 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:36:41,181 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:36:41,189 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:36:41,196 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 35
22:36:41,214 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 151
22:36:43,823 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:43,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.570949852000922. input_tokens=2378, output_tokens=404
22:36:44,567 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:44,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3219125859905034. input_tokens=2446, output_tokens=528
22:36:44,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:44,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.542547646909952. input_tokens=2808, output_tokens=672
22:36:44,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:44,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.610935599077493. input_tokens=2557, output_tokens=630
22:36:45,624 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:45,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.376514537027106. input_tokens=2968, output_tokens=721
22:36:46,203 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:46,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.363370391074568. input_tokens=2419, output_tokens=460
22:36:49,48 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:49,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8181261750869453. input_tokens=2495, output_tokens=504
22:36:49,117 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:49,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.893538655946031. input_tokens=2622, output_tokens=497
22:36:49,410 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:49,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1842767680063844. input_tokens=2482, output_tokens=493
22:36:49,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:49,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7450359149370342. input_tokens=4164, output_tokens=643
22:36:50,973 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:50,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8549002578947693. input_tokens=2088, output_tokens=292
22:36:51,605 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:51,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5416833967901766. input_tokens=2269, output_tokens=442
22:36:51,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:51,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2541448490228504. input_tokens=2177, output_tokens=413
22:36:52,972 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:52,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.74401967599988. input_tokens=3761, output_tokens=665
22:36:52,998 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:36:53,146 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:36:53,147 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:36:53,156 datashaper.workflow.workflow INFO executing verb create_final_documents
22:36:53,162 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:36:53,179 graphrag.index.cli INFO All workflows completed successfully.
22:43:43,803 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:43:43,804 graphrag.index.cli INFO Starting pipeline run for: 20241101-224343, dryrun=False
22:43:43,804 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:43:43,806 graphrag.index.create_pipeline_config INFO skipping workflows 
22:43:43,807 graphrag.index.run.run INFO Running pipeline
22:43:43,807 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:43:43,808 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:43:43,808 graphrag.index.input.load_input INFO using file storage for input
22:43:43,810 graphrag.index.input.csv INFO Loading csv files from input_eval
22:43:43,810 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:43:43,813 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:43:43,813 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:43:43,814 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:43:43,814 graphrag.index.run.run INFO Final # of rows loaded: 1
22:43:43,927 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:43:43,930 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:43:44,312 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:43:44,442 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:43:44,442 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:43:44,449 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:43:44,450 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:43:44,488 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:43:44,488 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:43:47,268 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:47,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.781452269060537. input_tokens=2087, output_tokens=527
22:43:50,501 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:50,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.2319196627940983. input_tokens=47, output_tokens=740
22:43:51,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:51,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5136660500429571. input_tokens=30, output_tokens=1
22:43:53,255 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:53,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.2364869290031493. input_tokens=47, output_tokens=492
22:43:53,764 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:53,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5075390879064798. input_tokens=30, output_tokens=1
22:43:55,707 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:55,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 1.941418922971934. input_tokens=47, output_tokens=368
22:43:56,300 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:56,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.592028294922784. input_tokens=30, output_tokens=1
22:43:58,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:58,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 1.823807792039588. input_tokens=47, output_tokens=366
22:43:58,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:58,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6137580450158566. input_tokens=30, output_tokens=1
22:44:00,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:00,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 1.2732207670342177. input_tokens=47, output_tokens=207
22:44:00,631 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:00,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6141424600500613. input_tokens=30, output_tokens=1
22:44:01,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:01,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 1.3183621889911592. input_tokens=47, output_tokens=189
22:44:02,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:02,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.5446596860419959. input_tokens=30, output_tokens=1
22:44:03,559 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:03,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 1.0616462498437613. input_tokens=47, output_tokens=130
22:44:03,582 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 24 edges
22:44:03,582 root INFO Starting at time 1730497443.5820863
22:44:03,582 root INFO Beginning preprocessing of transition probabilities for 22 vertices
22:44:03,582 root INFO Completed 1 / 22 vertices
22:44:03,582 root INFO Completed 3 / 22 vertices
22:44:03,582 root INFO Completed 5 / 22 vertices
22:44:03,582 root INFO Completed 7 / 22 vertices
22:44:03,582 root INFO Completed 9 / 22 vertices
22:44:03,583 root INFO Completed 11 / 22 vertices
22:44:03,584 root INFO Completed 13 / 22 vertices
22:44:03,584 root INFO Completed 15 / 22 vertices
22:44:03,584 root INFO Completed 17 / 22 vertices
22:44:03,584 root INFO Completed 19 / 22 vertices
22:44:03,584 root INFO Completed 21 / 22 vertices
22:44:03,584 root INFO Completed preprocessing of transition probabilities for vertices
22:44:03,585 root INFO Beginning preprocessing of transition probabilities for 24 edges
22:44:03,585 root INFO Completed 1 / 24 edges
22:44:03,585 root INFO Completed 3 / 24 edges
22:44:03,586 root INFO Completed 5 / 24 edges
22:44:03,587 root INFO Completed 7 / 24 edges
22:44:03,587 root INFO Completed 9 / 24 edges
22:44:03,587 root INFO Completed 11 / 24 edges
22:44:03,587 root INFO Completed 13 / 24 edges
22:44:03,587 root INFO Completed 15 / 24 edges
22:44:03,587 root INFO Completed 17 / 24 edges
22:44:03,588 root INFO Completed 19 / 24 edges
22:44:03,588 root INFO Completed 21 / 24 edges
22:44:03,589 root INFO Completed 23 / 24 edges
22:44:03,589 root INFO Completed preprocessing of transition probabilities for edges
22:44:03,589 root INFO Simulating walks on graph at time 1730497443.589749
22:44:03,589 root INFO Walk iteration: 1/10
22:44:03,591 root INFO Walk iteration: 2/10
22:44:03,591 root INFO Walk iteration: 3/10
22:44:03,592 root INFO Walk iteration: 4/10
22:44:03,593 root INFO Walk iteration: 5/10
22:44:03,593 root INFO Walk iteration: 6/10
22:44:03,594 root INFO Walk iteration: 7/10
22:44:03,594 root INFO Walk iteration: 8/10
22:44:03,595 root INFO Walk iteration: 9/10
22:44:03,595 root INFO Walk iteration: 10/10
22:44:03,596 root INFO Learning embeddings at time 1730497443.5963254
22:44:03,596 gensim.models.word2vec INFO collecting all words and their counts
22:44:03,596 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:44:03,597 gensim.models.word2vec INFO collected 22 word types from a corpus of 4080 raw words and 220 sentences
22:44:03,597 gensim.models.word2vec INFO Creating a fresh vocabulary
22:44:03,597 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-01T22:44:03.597488', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:44:03,597 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4080 word corpus (100.00% of original 4080, drops 0)', 'datetime': '2024-11-01T22:44:03.597986', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:44:03,598 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
22:44:03,598 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
22:44:03,598 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 654.9927560866299 word corpus (16.1%% of prior 4080)', 'datetime': '2024-11-01T22:44:03.598461', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:44:03,598 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
22:44:03,599 gensim.models.word2vec INFO resetting layer weights
22:44:03,599 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:44:03.599247', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:44:03,599 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:44:03.599673', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:44:03,603 gensim.models.word2vec INFO EPOCH 0: training on 4080 raw words (647 effective words) took 0.0s, 289155 effective words/s
22:44:03,607 gensim.models.word2vec INFO EPOCH 1: training on 4080 raw words (662 effective words) took 0.0s, 216919 effective words/s
22:44:03,612 gensim.models.word2vec INFO EPOCH 2: training on 4080 raw words (621 effective words) took 0.0s, 177611 effective words/s
22:44:03,612 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 12240 raw words (1930 effective words) took 0.0s, 154759 effective words/s', 'datetime': '2024-11-01T22:44:03.612638', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:44:03,612 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:44:03.612732', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:44:03,613 root INFO Completed. Ending time is 1730497443.6134808 Elapsed time is -0.03139448165893555
22:44:03,649 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:44:03,791 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:44:03,791 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:03,798 datashaper.workflow.workflow INFO executing verb create_final_entities
22:44:03,802 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:44:03,839 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:44:03,839 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:44:03,840 graphrag.index.operations.embed_text.strategies.openai INFO embedding 28 inputs via 26 snippets using 2 batches. max_batch_size=16, max_tokens=8191
22:44:04,618 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:44:04,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8015444111078978. input_tokens=454, output_tokens=0
22:44:05,2 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:44:05,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.180418340023607. input_tokens=268, output_tokens=0
22:44:05,27 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:44:05,177 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:44:05,177 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:05,185 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:44:07,549 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:44:07,721 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:44:07,722 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:07,730 datashaper.workflow.workflow INFO executing verb create_final_communities
22:44:07,741 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:44:07,877 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:44:07,878 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:44:07,882 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:44:07,890 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:44:07,896 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:44:08,40 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
22:44:08,40 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:44:08,44 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:44:08,47 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:44:08,53 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:44:08,62 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:44:08,201 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:44:08,201 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:44:08,204 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:44:08,212 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:44:08,216 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 28
22:44:10,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:10,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.33906828192994. input_tokens=2147, output_tokens=443
22:44:10,864 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:10,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6260385690256953. input_tokens=2053, output_tokens=335
22:44:11,180 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:11,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9436911321245134. input_tokens=2114, output_tokens=421
22:44:11,502 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:11,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2614614518824965. input_tokens=2554, output_tokens=586
22:44:11,942 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:11,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7076108471956104. input_tokens=2297, output_tokens=541
22:44:11,965 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:44:12,110 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:44:12,110 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:44:12,122 datashaper.workflow.workflow INFO executing verb create_final_documents
22:44:12,128 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:44:12,144 graphrag.index.cli INFO All workflows completed successfully.
22:46:48,995 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:46:48,996 graphrag.index.cli INFO Starting pipeline run for: 20241101-224648, dryrun=False
22:46:48,996 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:46:48,998 graphrag.index.create_pipeline_config INFO skipping workflows 
22:46:48,999 graphrag.index.run.run INFO Running pipeline
22:46:48,999 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:46:49,1 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:46:49,1 graphrag.index.input.load_input INFO using file storage for input
22:46:49,2 graphrag.index.input.csv INFO Loading csv files from input_eval
22:46:49,3 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:46:49,6 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:46:49,7 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:46:49,8 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:46:49,8 graphrag.index.run.run INFO Final # of rows loaded: 1
22:46:49,128 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:46:49,131 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:46:49,719 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:46:49,851 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:46:49,852 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:46:49,858 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:46:49,859 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:46:49,897 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:46:49,897 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:46:52,664 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:52,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.77175089600496. input_tokens=2087, output_tokens=527
22:46:55,616 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:55,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.946821958059445. input_tokens=47, output_tokens=663
22:46:56,91 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:56,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.47407945594750345. input_tokens=30, output_tokens=1
22:46:59,680 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:59,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.5877065549138933. input_tokens=47, output_tokens=342
22:47:00,208 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:00,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5251864378806204. input_tokens=30, output_tokens=1
22:47:02,896 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:02,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 2.6876076529733837. input_tokens=47, output_tokens=296
22:47:03,522 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:03,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6224983271677047. input_tokens=30, output_tokens=1
22:47:08,100 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:08,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 4.5793132989201695. input_tokens=47, output_tokens=346
22:47:09,296 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:09,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 1.1942909779027104. input_tokens=30, output_tokens=1
22:47:15,568 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 500 Internal Server Error"
22:47:15,571 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'What entities must be added to infer all the knowledge in the text? For each entity1: For each entity2: if there is any causal relationship between entity1 and entity2 add it. Add them below using the same format:\n'}
22:47:18,473 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:18,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 1 retries took 1.4168975839857012. input_tokens=47, output_tokens=245
22:47:19,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:19,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5900514218956232. input_tokens=30, output_tokens=1
22:47:20,553 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:20,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 1.4871407002210617. input_tokens=47, output_tokens=201
22:47:21,191 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:21,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.6355874058790505. input_tokens=30, output_tokens=1
22:47:22,688 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:22,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 1.4977773348800838. input_tokens=47, output_tokens=194
22:47:22,713 root INFO Starting preprocessing of transition probabilities on graph with 21 nodes and 23 edges
22:47:22,713 root INFO Starting at time 1730497642.7134392
22:47:22,713 root INFO Beginning preprocessing of transition probabilities for 21 vertices
22:47:22,713 root INFO Completed 1 / 21 vertices
22:47:22,713 root INFO Completed 3 / 21 vertices
22:47:22,713 root INFO Completed 5 / 21 vertices
22:47:22,713 root INFO Completed 7 / 21 vertices
22:47:22,714 root INFO Completed 9 / 21 vertices
22:47:22,714 root INFO Completed 11 / 21 vertices
22:47:22,714 root INFO Completed 13 / 21 vertices
22:47:22,715 root INFO Completed 15 / 21 vertices
22:47:22,715 root INFO Completed 17 / 21 vertices
22:47:22,715 root INFO Completed 19 / 21 vertices
22:47:22,715 root INFO Completed 21 / 21 vertices
22:47:22,716 root INFO Completed preprocessing of transition probabilities for vertices
22:47:22,716 root INFO Beginning preprocessing of transition probabilities for 23 edges
22:47:22,716 root INFO Completed 1 / 23 edges
22:47:22,716 root INFO Completed 3 / 23 edges
22:47:22,717 root INFO Completed 5 / 23 edges
22:47:22,717 root INFO Completed 7 / 23 edges
22:47:22,718 root INFO Completed 9 / 23 edges
22:47:22,719 root INFO Completed 11 / 23 edges
22:47:22,719 root INFO Completed 13 / 23 edges
22:47:22,720 root INFO Completed 15 / 23 edges
22:47:22,720 root INFO Completed 17 / 23 edges
22:47:22,721 root INFO Completed 19 / 23 edges
22:47:22,722 root INFO Completed 21 / 23 edges
22:47:22,722 root INFO Completed 23 / 23 edges
22:47:22,723 root INFO Completed preprocessing of transition probabilities for edges
22:47:22,723 root INFO Simulating walks on graph at time 1730497642.7233226
22:47:22,724 root INFO Walk iteration: 1/10
22:47:22,725 root INFO Walk iteration: 2/10
22:47:22,726 root INFO Walk iteration: 3/10
22:47:22,727 root INFO Walk iteration: 4/10
22:47:22,727 root INFO Walk iteration: 5/10
22:47:22,728 root INFO Walk iteration: 6/10
22:47:22,728 root INFO Walk iteration: 7/10
22:47:22,729 root INFO Walk iteration: 8/10
22:47:22,730 root INFO Walk iteration: 9/10
22:47:22,730 root INFO Walk iteration: 10/10
22:47:22,731 root INFO Learning embeddings at time 1730497642.7315671
22:47:22,731 gensim.models.word2vec INFO collecting all words and their counts
22:47:22,732 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:47:22,732 gensim.models.word2vec INFO collected 21 word types from a corpus of 3920 raw words and 210 sentences
22:47:22,732 gensim.models.word2vec INFO Creating a fresh vocabulary
22:47:22,732 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 21 unique words (100.00% of original 21, drops 0)', 'datetime': '2024-11-01T22:47:22.732834', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:47:22,733 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3920 word corpus (100.00% of original 3920, drops 0)', 'datetime': '2024-11-01T22:47:22.733329', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:47:22,734 gensim.models.word2vec INFO deleting the raw counts dictionary of 21 items
22:47:22,734 gensim.models.word2vec INFO sample=0.001 downsamples 21 most-common words
22:47:22,734 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 624.4302159326179 word corpus (15.9%% of prior 3920)', 'datetime': '2024-11-01T22:47:22.734746', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:47:22,734 gensim.models.word2vec INFO estimated required memory for 21 words and 1536 dimensions: 268548 bytes
22:47:22,735 gensim.models.word2vec INFO resetting layer weights
22:47:22,735 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:47:22.735320', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:47:22,735 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 21 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:47:22.735751', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:47:22,739 gensim.models.word2vec INFO EPOCH 0: training on 3920 raw words (642 effective words) took 0.0s, 256285 effective words/s
22:47:22,743 gensim.models.word2vec INFO EPOCH 1: training on 3920 raw words (633 effective words) took 0.0s, 272941 effective words/s
22:47:22,746 gensim.models.word2vec INFO EPOCH 2: training on 3920 raw words (629 effective words) took 0.0s, 214162 effective words/s
22:47:22,747 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 11760 raw words (1904 effective words) took 0.0s, 177120 effective words/s', 'datetime': '2024-11-01T22:47:22.747004', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:47:22,747 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:47:22.747756', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:47:22,748 root INFO Completed. Ending time is 1730497642.748392 Elapsed time is -0.03495287895202637
22:47:22,773 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:47:22,918 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:47:22,918 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:47:22,926 datashaper.workflow.workflow INFO executing verb create_final_entities
22:47:22,930 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:47:22,967 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:47:22,967 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:47:22,968 graphrag.index.operations.embed_text.strategies.openai INFO embedding 28 inputs via 22 snippets using 2 batches. max_batch_size=16, max_tokens=8191
22:47:24,45 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:47:24,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0872738400939852. input_tokens=182, output_tokens=0
22:47:24,175 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:47:24,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2328920380678028. input_tokens=467, output_tokens=0
22:47:24,207 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:47:24,361 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:47:24,362 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:47:24,369 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:47:26,773 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:47:26,947 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:47:26,948 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:47:26,956 datashaper.workflow.workflow INFO executing verb create_final_communities
22:47:26,967 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:47:27,103 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:47:27,104 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:47:27,108 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:47:27,116 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:47:27,122 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:47:27,264 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:47:27,265 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:47:27,268 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:47:27,270 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:47:27,278 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:47:27,287 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:47:27,426 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:47:27,426 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:47:27,431 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:47:27,438 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:47:27,442 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 28
22:47:30,803 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:30,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3405806790106. input_tokens=2187, output_tokens=417
22:47:32,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:32,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.721442259149626. input_tokens=2100, output_tokens=381
22:47:32,326 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:32,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.867521330947056. input_tokens=2262, output_tokens=415
22:47:33,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:33,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.59167490596883. input_tokens=2254, output_tokens=480
22:47:34,518 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:34,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.053553398000076. input_tokens=2191, output_tokens=687
22:47:35,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:35,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.777219047071412. input_tokens=2043, output_tokens=307
22:47:35,263 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:47:35,419 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:47:35,419 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:47:35,429 datashaper.workflow.workflow INFO executing verb create_final_documents
22:47:35,435 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:47:35,452 graphrag.index.cli INFO All workflows completed successfully.
22:48:44,637 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:48:44,638 graphrag.index.cli INFO Starting pipeline run for: 20241101-224844, dryrun=False
22:48:44,638 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:48:44,640 graphrag.index.create_pipeline_config INFO skipping workflows 
22:48:44,640 graphrag.index.run.run INFO Running pipeline
22:48:44,640 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:48:44,641 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:48:44,641 graphrag.index.input.load_input INFO using file storage for input
22:48:44,643 graphrag.index.input.csv INFO Loading csv files from input_eval
22:48:44,643 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:48:44,647 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:48:44,647 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:48:44,648 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:48:44,648 graphrag.index.run.run INFO Final # of rows loaded: 1
22:48:44,766 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:48:44,768 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:48:45,83 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:48:45,220 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:48:45,220 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:48:45,227 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:48:45,228 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:48:45,267 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:48:45,267 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:48:48,4 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:48,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7396540089976043. input_tokens=2087, output_tokens=527
22:48:50,959 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:50,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.9522584341466427. input_tokens=47, output_tokens=663
22:48:51,419 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:51,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4584961060900241. input_tokens=2, output_tokens=1
22:48:52,205 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:52,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.7847770999651402. input_tokens=47, output_tokens=58
22:48:52,668 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:52,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.46156704192981124. input_tokens=2, output_tokens=1
22:48:53,371 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:53,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.7014891430735588. input_tokens=47, output_tokens=58
22:48:53,884 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:53,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.5119067020714283. input_tokens=2, output_tokens=1
22:48:54,531 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:54,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.6460856639314443. input_tokens=47, output_tokens=58
22:48:55,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:55,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.517793504986912. input_tokens=2, output_tokens=1
22:48:55,687 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:55,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.6357963839545846. input_tokens=47, output_tokens=58
22:48:56,150 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:56,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.4624332149978727. input_tokens=2, output_tokens=1
22:48:56,811 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:56,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.6600609889719635. input_tokens=47, output_tokens=58
22:48:57,296 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:57,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.48255791887640953. input_tokens=2, output_tokens=1
22:48:58,14 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:58,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.7182026319205761. input_tokens=47, output_tokens=58
22:48:58,37 root INFO Starting preprocessing of transition probabilities on graph with 7 nodes and 6 edges
22:48:58,37 root INFO Starting at time 1730497738.037555
22:48:58,37 root INFO Beginning preprocessing of transition probabilities for 7 vertices
22:48:58,37 root INFO Completed 1 / 7 vertices
22:48:58,37 root INFO Completed 2 / 7 vertices
22:48:58,37 root INFO Completed 3 / 7 vertices
22:48:58,37 root INFO Completed 4 / 7 vertices
22:48:58,37 root INFO Completed 5 / 7 vertices
22:48:58,37 root INFO Completed 6 / 7 vertices
22:48:58,38 root INFO Completed 7 / 7 vertices
22:48:58,39 root INFO Completed preprocessing of transition probabilities for vertices
22:48:58,40 root INFO Beginning preprocessing of transition probabilities for 6 edges
22:48:58,41 root INFO Completed 1 / 6 edges
22:48:58,42 root INFO Completed 2 / 6 edges
22:48:58,42 root INFO Completed 3 / 6 edges
22:48:58,43 root INFO Completed 4 / 6 edges
22:48:58,44 root INFO Completed 5 / 6 edges
22:48:58,44 root INFO Completed 6 / 6 edges
22:48:58,45 root INFO Completed preprocessing of transition probabilities for edges
22:48:58,46 root INFO Simulating walks on graph at time 1730497738.046129
22:48:58,46 root INFO Walk iteration: 1/10
22:48:58,47 root INFO Walk iteration: 2/10
22:48:58,48 root INFO Walk iteration: 3/10
22:48:58,49 root INFO Walk iteration: 4/10
22:48:58,50 root INFO Walk iteration: 5/10
22:48:58,51 root INFO Walk iteration: 6/10
22:48:58,52 root INFO Walk iteration: 7/10
22:48:58,52 root INFO Walk iteration: 8/10
22:48:58,53 root INFO Walk iteration: 9/10
22:48:58,54 root INFO Walk iteration: 10/10
22:48:58,54 root INFO Learning embeddings at time 1730497738.054365
22:48:58,54 gensim.models.word2vec INFO collecting all words and their counts
22:48:58,55 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:48:58,55 gensim.models.word2vec INFO collected 7 word types from a corpus of 880 raw words and 70 sentences
22:48:58,56 gensim.models.word2vec INFO Creating a fresh vocabulary
22:48:58,56 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 7 unique words (100.00% of original 7, drops 0)', 'datetime': '2024-11-01T22:48:58.056240', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:58,56 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 880 word corpus (100.00% of original 880, drops 0)', 'datetime': '2024-11-01T22:48:58.056900', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:58,57 gensim.models.word2vec INFO deleting the raw counts dictionary of 7 items
22:48:58,57 gensim.models.word2vec INFO sample=0.001 downsamples 7 most-common words
22:48:58,57 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 72.83403182672615 word corpus (8.3%% of prior 880)', 'datetime': '2024-11-01T22:48:58.057746', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:48:58,58 gensim.models.word2vec INFO estimated required memory for 7 words and 1536 dimensions: 89516 bytes
22:48:58,59 gensim.models.word2vec INFO resetting layer weights
22:48:58,59 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:48:58.059409', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:48:58,59 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 7 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:48:58.059869', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:48:58,64 gensim.models.word2vec INFO EPOCH 0: training on 880 raw words (83 effective words) took 0.0s, 205677 effective words/s
22:48:58,67 gensim.models.word2vec INFO EPOCH 1: training on 880 raw words (67 effective words) took 0.0s, 70776 effective words/s
22:48:58,71 gensim.models.word2vec INFO EPOCH 2: training on 880 raw words (78 effective words) took 0.0s, 88806 effective words/s
22:48:58,71 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2640 raw words (228 effective words) took 0.0s, 20123 effective words/s', 'datetime': '2024-11-01T22:48:58.071282', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:48:58,72 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:48:58.072096', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:48:58,72 root INFO Completed. Ending time is 1730497738.0721946 Elapsed time is -0.034639596939086914
22:48:58,99 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:48:58,247 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:48:58,247 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:58,255 datashaper.workflow.workflow INFO executing verb create_final_entities
22:48:58,259 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:48:58,296 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:48:58,296 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:48:58,296 graphrag.index.operations.embed_text.strategies.openai INFO embedding 15 inputs via 14 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:48:59,502 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:48:59,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2265206100419164. input_tokens=416, output_tokens=0
22:48:59,530 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:48:59,665 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:48:59,665 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:48:59,673 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:49:01,942 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:49:02,104 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:49:02,104 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:49:02,113 datashaper.workflow.workflow INFO executing verb create_final_communities
22:49:02,123 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:49:02,255 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:49:02,256 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:49:02,260 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:49:02,267 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:49:02,273 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:49:02,409 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
22:49:02,409 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:49:02,414 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:49:02,416 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:49:02,423 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:49:02,432 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:49:02,566 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:49:02,567 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:49:02,571 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:49:02,579 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:49:02,582 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 15
22:49:04,793 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:04,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2014064609538764. input_tokens=2329, output_tokens=420
22:49:04,805 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:49:04,957 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:49:04,957 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:49:04,968 datashaper.workflow.workflow INFO executing verb create_final_documents
22:49:04,975 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:49:04,991 graphrag.index.cli INFO All workflows completed successfully.
22:50:14,358 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:50:14,360 graphrag.index.cli INFO Starting pipeline run for: 20241101-225014, dryrun=False
22:50:14,361 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:50:14,363 graphrag.index.create_pipeline_config INFO skipping workflows 
22:50:14,363 graphrag.index.run.run INFO Running pipeline
22:50:14,363 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:50:14,364 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:50:14,365 graphrag.index.input.load_input INFO using file storage for input
22:50:14,367 graphrag.index.input.csv INFO Loading csv files from input_eval
22:50:14,367 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:50:14,371 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:50:14,371 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:50:14,372 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:50:14,373 graphrag.index.run.run INFO Final # of rows loaded: 1
22:50:14,487 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:50:14,490 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:50:14,895 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:50:15,20 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:50:15,21 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:50:15,28 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:50:15,29 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:50:15,67 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:50:15,67 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:50:17,799 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:17,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.735695391893387. input_tokens=2087, output_tokens=527
22:50:21,807 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:21,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.004831014201045. input_tokens=43, output_tokens=927
22:50:22,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:22,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.48383403196930885. input_tokens=2, output_tokens=1
22:50:23,54 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:23,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.7590139789972454. input_tokens=43, output_tokens=52
22:50:23,537 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:23,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.4817517208866775. input_tokens=2, output_tokens=1
22:50:24,198 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:24,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.6596324020065367. input_tokens=43, output_tokens=34
22:50:24,656 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:24,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.4575429807882756. input_tokens=2, output_tokens=1
22:50:25,334 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:25,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.6769728660583496. input_tokens=43, output_tokens=34
22:50:25,779 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:25,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.44333048607222736. input_tokens=2, output_tokens=1
22:50:26,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:26,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.5623792430851609. input_tokens=43, output_tokens=34
22:50:26,858 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:26,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5151821100153029. input_tokens=2, output_tokens=1
22:50:27,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:27,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.5666960240341723. input_tokens=43, output_tokens=34
22:50:27,964 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:27,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.5371108129620552. input_tokens=2, output_tokens=1
22:50:28,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:28,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.5650347860064358. input_tokens=43, output_tokens=34
22:50:28,557 root INFO Starting preprocessing of transition probabilities on graph with 6 nodes and 5 edges
22:50:28,557 root INFO Starting at time 1730497828.5575135
22:50:28,557 root INFO Beginning preprocessing of transition probabilities for 6 vertices
22:50:28,557 root INFO Completed 1 / 6 vertices
22:50:28,557 root INFO Completed 2 / 6 vertices
22:50:28,558 root INFO Completed 3 / 6 vertices
22:50:28,558 root INFO Completed 4 / 6 vertices
22:50:28,558 root INFO Completed 5 / 6 vertices
22:50:28,558 root INFO Completed 6 / 6 vertices
22:50:28,558 root INFO Completed preprocessing of transition probabilities for vertices
22:50:28,559 root INFO Beginning preprocessing of transition probabilities for 5 edges
22:50:28,559 root INFO Completed 1 / 5 edges
22:50:28,559 root INFO Completed 2 / 5 edges
22:50:28,559 root INFO Completed 3 / 5 edges
22:50:28,559 root INFO Completed 4 / 5 edges
22:50:28,559 root INFO Completed 5 / 5 edges
22:50:28,559 root INFO Completed preprocessing of transition probabilities for edges
22:50:28,559 root INFO Simulating walks on graph at time 1730497828.5598943
22:50:28,560 root INFO Walk iteration: 1/10
22:50:28,560 root INFO Walk iteration: 2/10
22:50:28,561 root INFO Walk iteration: 3/10
22:50:28,561 root INFO Walk iteration: 4/10
22:50:28,562 root INFO Walk iteration: 5/10
22:50:28,563 root INFO Walk iteration: 6/10
22:50:28,563 root INFO Walk iteration: 7/10
22:50:28,564 root INFO Walk iteration: 8/10
22:50:28,565 root INFO Walk iteration: 9/10
22:50:28,565 root INFO Walk iteration: 10/10
22:50:28,566 root INFO Learning embeddings at time 1730497828.5665562
22:50:28,567 gensim.models.word2vec INFO collecting all words and their counts
22:50:28,567 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:50:28,567 gensim.models.word2vec INFO collected 6 word types from a corpus of 800 raw words and 60 sentences
22:50:28,567 gensim.models.word2vec INFO Creating a fresh vocabulary
22:50:28,567 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 6 unique words (100.00% of original 6, drops 0)', 'datetime': '2024-11-01T22:50:28.567940', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:50:28,568 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 800 word corpus (100.00% of original 800, drops 0)', 'datetime': '2024-11-01T22:50:28.568457', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:50:28,568 gensim.models.word2vec INFO deleting the raw counts dictionary of 6 items
22:50:28,569 gensim.models.word2vec INFO sample=0.001 downsamples 6 most-common words
22:50:28,569 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 61.97767105626599 word corpus (7.7%% of prior 800)', 'datetime': '2024-11-01T22:50:28.569080', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:50:28,569 gensim.models.word2vec INFO estimated required memory for 6 words and 1536 dimensions: 76728 bytes
22:50:28,570 gensim.models.word2vec INFO resetting layer weights
22:50:28,570 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:50:28.570497', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:50:28,570 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 6 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:50:28.570816', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:50:28,574 gensim.models.word2vec INFO EPOCH 0: training on 800 raw words (77 effective words) took 0.0s, 149999 effective words/s
22:50:28,576 gensim.models.word2vec INFO EPOCH 1: training on 800 raw words (56 effective words) took 0.0s, 84776 effective words/s
22:50:28,579 gensim.models.word2vec INFO EPOCH 2: training on 800 raw words (62 effective words) took 0.0s, 89069 effective words/s
22:50:28,579 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2400 raw words (195 effective words) took 0.0s, 23376 effective words/s', 'datetime': '2024-11-01T22:50:28.579224', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:50:28,579 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=6, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:50:28.579867', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:50:28,579 root INFO Completed. Ending time is 1730497828.579948 Elapsed time is -0.022434473037719727
22:50:28,601 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:50:28,744 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:50:28,744 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:50:28,751 datashaper.workflow.workflow INFO executing verb create_final_entities
22:50:28,755 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:50:28,792 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:50:28,792 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:50:28,792 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:50:29,797 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:50:29,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0310423080809414. input_tokens=214, output_tokens=0
22:50:29,828 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:50:29,965 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:50:29,965 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:50:29,973 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:50:32,522 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:50:32,667 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:50:32,668 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:50:32,675 datashaper.workflow.workflow INFO executing verb create_final_communities
22:50:32,686 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:50:32,821 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
22:50:32,821 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:50:32,825 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:50:32,832 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:50:32,838 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:50:32,974 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
22:50:32,975 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:50:32,978 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:50:32,980 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:50:32,987 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:50:32,996 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:50:33,136 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:50:33,136 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:50:33,140 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:50:33,148 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:50:33,150 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
22:50:35,702 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:35,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.541010773042217. input_tokens=2281, output_tokens=470
22:50:35,721 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:50:35,874 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:50:35,875 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:50:35,884 datashaper.workflow.workflow INFO executing verb create_final_documents
22:50:35,890 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:50:35,907 graphrag.index.cli INFO All workflows completed successfully.
22:51:28,133 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:51:28,135 graphrag.index.cli INFO Starting pipeline run for: 20241101-225128, dryrun=False
22:51:28,135 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:51:28,137 graphrag.index.create_pipeline_config INFO skipping workflows 
22:51:28,137 graphrag.index.run.run INFO Running pipeline
22:51:28,137 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:51:28,139 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:51:28,139 graphrag.index.input.load_input INFO using file storage for input
22:51:28,140 graphrag.index.input.csv INFO Loading csv files from input_eval
22:51:28,140 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:51:28,143 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:51:28,143 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:51:28,144 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:51:28,144 graphrag.index.run.run INFO Final # of rows loaded: 1
22:51:28,258 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:51:28,261 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:51:28,742 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:51:28,870 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:51:28,871 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:51:28,877 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:51:28,878 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:51:28,917 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:51:28,917 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:51:31,615 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:31,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.706036994000897. input_tokens=2087, output_tokens=527
22:51:35,218 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:35,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5942076339852065. input_tokens=20, output_tokens=908
22:51:35,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:35,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5308988969773054. input_tokens=2, output_tokens=1
22:51:39,451 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:39,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.6987070250324905. input_tokens=20, output_tokens=934
22:51:40,28 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:40,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5735975280404091. input_tokens=2, output_tokens=1
22:51:44,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:44,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 4.397141416091472. input_tokens=20, output_tokens=1109
22:51:45,73 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:45,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6478735699784011. input_tokens=2, output_tokens=1
22:51:49,616 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:49,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 4.541642091004178. input_tokens=20, output_tokens=1186
22:51:50,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:50,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6467213779687881. input_tokens=2, output_tokens=1
22:52:07,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:07,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 16.93040390405804. input_tokens=20, output_tokens=0
22:52:07,830 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:07,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6291855608578771. input_tokens=2, output_tokens=1
22:52:14,75 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:14,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 6.243879638146609. input_tokens=20, output_tokens=1599
22:52:14,718 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:14,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.6395284389145672. input_tokens=2, output_tokens=1
22:52:22,1 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:22,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 7.281407443108037. input_tokens=20, output_tokens=2028
22:52:22,468 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:22,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4433246629778296. input_tokens=158, output_tokens=22
22:52:22,500 root INFO Starting preprocessing of transition probabilities on graph with 38 nodes and 37 edges
22:52:22,500 root INFO Starting at time 1730497942.5007005
22:52:22,500 root INFO Beginning preprocessing of transition probabilities for 38 vertices
22:52:22,500 root INFO Completed 1 / 38 vertices
22:52:22,500 root INFO Completed 4 / 38 vertices
22:52:22,501 root INFO Completed 7 / 38 vertices
22:52:22,502 root INFO Completed 10 / 38 vertices
22:52:22,502 root INFO Completed 13 / 38 vertices
22:52:22,502 root INFO Completed 16 / 38 vertices
22:52:22,503 root INFO Completed 19 / 38 vertices
22:52:22,503 root INFO Completed 22 / 38 vertices
22:52:22,504 root INFO Completed 25 / 38 vertices
22:52:22,504 root INFO Completed 28 / 38 vertices
22:52:22,504 root INFO Completed 31 / 38 vertices
22:52:22,504 root INFO Completed 34 / 38 vertices
22:52:22,505 root INFO Completed 37 / 38 vertices
22:52:22,505 root INFO Completed preprocessing of transition probabilities for vertices
22:52:22,505 root INFO Beginning preprocessing of transition probabilities for 37 edges
22:52:22,505 root INFO Completed 1 / 37 edges
22:52:22,505 root INFO Completed 4 / 37 edges
22:52:22,506 root INFO Completed 7 / 37 edges
22:52:22,506 root INFO Completed 10 / 37 edges
22:52:22,507 root INFO Completed 13 / 37 edges
22:52:22,508 root INFO Completed 16 / 37 edges
22:52:22,508 root INFO Completed 19 / 37 edges
22:52:22,509 root INFO Completed 22 / 37 edges
22:52:22,509 root INFO Completed 25 / 37 edges
22:52:22,510 root INFO Completed 28 / 37 edges
22:52:22,510 root INFO Completed 31 / 37 edges
22:52:22,511 root INFO Completed 34 / 37 edges
22:52:22,511 root INFO Completed 37 / 37 edges
22:52:22,513 root INFO Completed preprocessing of transition probabilities for edges
22:52:22,513 root INFO Simulating walks on graph at time 1730497942.5136507
22:52:22,513 root INFO Walk iteration: 1/10
22:52:22,515 root INFO Walk iteration: 2/10
22:52:22,516 root INFO Walk iteration: 3/10
22:52:22,518 root INFO Walk iteration: 4/10
22:52:22,520 root INFO Walk iteration: 5/10
22:52:22,522 root INFO Walk iteration: 6/10
22:52:22,524 root INFO Walk iteration: 7/10
22:52:22,525 root INFO Walk iteration: 8/10
22:52:22,527 root INFO Walk iteration: 9/10
22:52:22,528 root INFO Walk iteration: 10/10
22:52:22,529 root INFO Learning embeddings at time 1730497942.5297823
22:52:22,530 gensim.models.word2vec INFO collecting all words and their counts
22:52:22,530 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:52:22,531 gensim.models.word2vec INFO collected 38 word types from a corpus of 6800 raw words and 380 sentences
22:52:22,531 gensim.models.word2vec INFO Creating a fresh vocabulary
22:52:22,532 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 38 unique words (100.00% of original 38, drops 0)', 'datetime': '2024-11-01T22:52:22.532811', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:52:22,533 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6800 word corpus (100.00% of original 6800, drops 0)', 'datetime': '2024-11-01T22:52:22.533429', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:52:22,534 gensim.models.word2vec INFO deleting the raw counts dictionary of 38 items
22:52:22,534 gensim.models.word2vec INFO sample=0.001 downsamples 38 most-common words
22:52:22,535 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1497.4466417031672 word corpus (22.0%% of prior 6800)', 'datetime': '2024-11-01T22:52:22.535537', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:52:22,536 gensim.models.word2vec INFO estimated required memory for 38 words and 1536 dimensions: 485944 bytes
22:52:22,537 gensim.models.word2vec INFO resetting layer weights
22:52:22,538 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:52:22.538506', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:52:22,538 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 38 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:52:22.538663', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:52:22,551 gensim.models.word2vec INFO EPOCH 0: training on 6800 raw words (1554 effective words) took 0.0s, 139334 effective words/s
22:52:22,568 gensim.models.word2vec INFO EPOCH 1: training on 6800 raw words (1525 effective words) took 0.0s, 97758 effective words/s
22:52:22,586 gensim.models.word2vec INFO EPOCH 2: training on 6800 raw words (1486 effective words) took 0.0s, 96286 effective words/s
22:52:22,586 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 20400 raw words (4565 effective words) took 0.0s, 96219 effective words/s', 'datetime': '2024-11-01T22:52:22.586165', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:52:22,586 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=38, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:52:22.586907', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:52:22,587 root INFO Completed. Ending time is 1730497942.5876467 Elapsed time is -0.08694624900817871
22:52:22,637 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:52:22,794 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:52:22,795 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:22,803 datashaper.workflow.workflow INFO executing verb create_final_entities
22:52:22,809 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:52:22,847 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:52:22,847 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:52:22,849 graphrag.index.operations.embed_text.strategies.openai INFO embedding 108 inputs via 101 snippets using 7 batches. max_batch_size=16, max_tokens=8191
22:52:23,814 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:23,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9891716570127755. input_tokens=312, output_tokens=0
22:52:23,986 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:24,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1611066870391369. input_tokens=367, output_tokens=0
22:52:24,309 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:24,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4827550908084959. input_tokens=289, output_tokens=0
22:52:24,336 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:24,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5106439869850874. input_tokens=437, output_tokens=0
22:52:24,412 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:24,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5861764601431787. input_tokens=364, output_tokens=0
22:52:24,440 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:24,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43403184111230075. input_tokens=97, output_tokens=0
22:52:25,58 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:25,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2399421350564808. input_tokens=374, output_tokens=0
22:52:25,88 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:52:25,246 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:52:25,246 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:25,255 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:52:27,832 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:52:27,991 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:52:27,991 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:28,1 datashaper.workflow.workflow INFO executing verb create_final_communities
22:52:28,15 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:52:28,149 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:52:28,149 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:52:28,155 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:52:28,163 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:52:28,170 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:52:28,310 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
22:52:28,310 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:52:28,313 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:52:28,315 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:52:28,325 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:52:28,335 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:52:28,471 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:52:28,472 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:52:28,475 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:52:28,484 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:52:28,488 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 108
22:52:31,2 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:31,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.488084982847795. input_tokens=2106, output_tokens=409
22:52:31,250 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:31,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.742380427895114. input_tokens=2114, output_tokens=348
22:52:31,428 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:31,429 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9140319251455367. input_tokens=2319, output_tokens=416
22:52:31,708 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:31,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.191611137939617. input_tokens=2265, output_tokens=439
22:52:31,849 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:31,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.339187412057072. input_tokens=2328, output_tokens=482
22:52:33,593 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:52:33,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5744059139396995. input_tokens=2205, output_tokens=485
22:52:33,613 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:52:33,763 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:52:33,766 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:52:33,774 datashaper.workflow.workflow INFO executing verb create_final_documents
22:52:33,780 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:52:33,798 graphrag.index.cli INFO All workflows completed successfully.
22:55:39,939 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:55:39,941 graphrag.index.cli INFO Starting pipeline run for: 20241101-225539, dryrun=False
22:55:39,941 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:55:39,943 graphrag.index.create_pipeline_config INFO skipping workflows 
22:55:39,943 graphrag.index.run.run INFO Running pipeline
22:55:39,943 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:55:39,945 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:55:39,945 graphrag.index.input.load_input INFO using file storage for input
22:55:39,946 graphrag.index.input.csv INFO Loading csv files from input_eval
22:55:39,946 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:55:39,951 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:55:39,951 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:55:39,952 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:55:39,952 graphrag.index.run.run INFO Final # of rows loaded: 1
22:55:40,73 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:55:40,76 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:55:40,450 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:55:40,579 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:55:40,579 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:55:40,586 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:55:40,587 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:55:40,626 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:55:40,626 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:55:42,860 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:42,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2435662450734526. input_tokens=2087, output_tokens=527
22:55:46,329 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:46,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.4594940419774503. input_tokens=20, output_tokens=908
22:55:46,939 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:46,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6066769319586456. input_tokens=2, output_tokens=1
22:55:50,481 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:50,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.5412913961336017. input_tokens=20, output_tokens=867
22:55:51,102 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:55:51,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6182539630681276. input_tokens=2, output_tokens=1
22:56:04,238 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:04,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 13.135641447035596. input_tokens=20, output_tokens=4067
22:56:05,60 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:05,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.814722266048193. input_tokens=2, output_tokens=1
22:56:19,403 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:19,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 14.34078364004381. input_tokens=20, output_tokens=4338
22:56:20,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:20,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8556900448165834. input_tokens=2, output_tokens=1
22:56:32,341 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:32,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.078852297039703. input_tokens=20, output_tokens=4338
22:56:33,290 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:33,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9491108369547874. input_tokens=2, output_tokens=1
22:56:45,702 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:45,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.410275741945952. input_tokens=20, output_tokens=4338
22:56:46,815 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:46,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.1088222600519657. input_tokens=2, output_tokens=1
22:56:59,336 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:59,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.52000609319657. input_tokens=20, output_tokens=4338
22:56:59,870 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:56:59,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5085655930452049. input_tokens=161, output_tokens=33
22:57:00,362 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:00,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4875275909435004. input_tokens=140, output_tokens=24
22:57:00,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:00,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.133322207024321. input_tokens=147, output_tokens=18
22:57:00,501 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:00,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1317283420357853. input_tokens=140, output_tokens=14
22:57:00,520 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:00,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1556201870553195. input_tokens=151, output_tokens=20
22:57:02,526 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:02,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1598952589556575. input_tokens=156, output_tokens=39
22:57:02,555 root INFO Starting preprocessing of transition probabilities on graph with 78 nodes and 78 edges
22:57:02,555 root INFO Starting at time 1730498222.555198
22:57:02,555 root INFO Beginning preprocessing of transition probabilities for 78 vertices
22:57:02,555 root INFO Completed 1 / 78 vertices
22:57:02,555 root INFO Completed 8 / 78 vertices
22:57:02,556 root INFO Completed 15 / 78 vertices
22:57:02,556 root INFO Completed 22 / 78 vertices
22:57:02,557 root INFO Completed 29 / 78 vertices
22:57:02,557 root INFO Completed 36 / 78 vertices
22:57:02,557 root INFO Completed 43 / 78 vertices
22:57:02,557 root INFO Completed 50 / 78 vertices
22:57:02,557 root INFO Completed 57 / 78 vertices
22:57:02,558 root INFO Completed 64 / 78 vertices
22:57:02,558 root INFO Completed 71 / 78 vertices
22:57:02,558 root INFO Completed 78 / 78 vertices
22:57:02,558 root INFO Completed preprocessing of transition probabilities for vertices
22:57:02,558 root INFO Beginning preprocessing of transition probabilities for 78 edges
22:57:02,558 root INFO Completed 1 / 78 edges
22:57:02,558 root INFO Completed 8 / 78 edges
22:57:02,559 root INFO Completed 15 / 78 edges
22:57:02,560 root INFO Completed 22 / 78 edges
22:57:02,561 root INFO Completed 29 / 78 edges
22:57:02,561 root INFO Completed 36 / 78 edges
22:57:02,562 root INFO Completed 43 / 78 edges
22:57:02,563 root INFO Completed 50 / 78 edges
22:57:02,564 root INFO Completed 57 / 78 edges
22:57:02,564 root INFO Completed 64 / 78 edges
22:57:02,565 root INFO Completed 71 / 78 edges
22:57:02,565 root INFO Completed 78 / 78 edges
22:57:02,566 root INFO Completed preprocessing of transition probabilities for edges
22:57:02,566 root INFO Simulating walks on graph at time 1730498222.566391
22:57:02,567 root INFO Walk iteration: 1/10
22:57:02,569 root INFO Walk iteration: 2/10
22:57:02,572 root INFO Walk iteration: 3/10
22:57:02,575 root INFO Walk iteration: 4/10
22:57:02,577 root INFO Walk iteration: 5/10
22:57:02,579 root INFO Walk iteration: 6/10
22:57:02,580 root INFO Walk iteration: 7/10
22:57:02,582 root INFO Walk iteration: 8/10
22:57:02,583 root INFO Walk iteration: 9/10
22:57:02,584 root INFO Walk iteration: 10/10
22:57:02,586 root INFO Learning embeddings at time 1730498222.5862913
22:57:02,586 gensim.models.word2vec INFO collecting all words and their counts
22:57:02,586 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:57:02,587 gensim.models.word2vec INFO collected 78 word types from a corpus of 9440 raw words and 780 sentences
22:57:02,587 gensim.models.word2vec INFO Creating a fresh vocabulary
22:57:02,587 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 78 unique words (100.00% of original 78, drops 0)', 'datetime': '2024-11-01T22:57:02.587700', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:57:02,588 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 9440 word corpus (100.00% of original 9440, drops 0)', 'datetime': '2024-11-01T22:57:02.588202', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:57:02,588 gensim.models.word2vec INFO deleting the raw counts dictionary of 78 items
22:57:02,588 gensim.models.word2vec INFO sample=0.001 downsamples 77 most-common words
22:57:02,589 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2902.969889728804 word corpus (30.8%% of prior 9440)', 'datetime': '2024-11-01T22:57:02.589012', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:57:02,589 gensim.models.word2vec INFO estimated required memory for 78 words and 1536 dimensions: 997464 bytes
22:57:02,589 gensim.models.word2vec INFO resetting layer weights
22:57:02,590 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T22:57:02.589995', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:57:02,590 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 78 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T22:57:02.590190', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:57:02,602 gensim.models.word2vec INFO EPOCH 0: training on 9440 raw words (2954 effective words) took 0.0s, 267809 effective words/s
22:57:02,616 gensim.models.word2vec INFO EPOCH 1: training on 9440 raw words (2903 effective words) took 0.0s, 233520 effective words/s
22:57:02,639 gensim.models.word2vec INFO EPOCH 2: training on 9440 raw words (2977 effective words) took 0.0s, 137958 effective words/s
22:57:02,639 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 28320 raw words (8834 effective words) took 0.0s, 178224 effective words/s', 'datetime': '2024-11-01T22:57:02.639780', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:57:02,640 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=78, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T22:57:02.640602', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:57:02,641 root INFO Completed. Ending time is 1730498222.641309 Elapsed time is -0.08611106872558594
22:57:02,698 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:57:02,864 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:57:02,865 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:57:02,875 datashaper.workflow.workflow INFO executing verb create_final_entities
22:57:02,883 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:57:02,920 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:57:02,920 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:57:02,924 graphrag.index.operations.embed_text.strategies.openai INFO embedding 167 inputs via 166 snippets using 11 batches. max_batch_size=16, max_tokens=8191
22:57:03,936 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:03,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0340944619383663. input_tokens=347, output_tokens=0
22:57:04,370 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:04,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4669388190377504. input_tokens=305, output_tokens=0
22:57:04,493 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:04,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5911481599323452. input_tokens=343, output_tokens=0
22:57:04,519 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:04,520 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:04,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.617225302848965. input_tokens=260, output_tokens=0
22:57:04,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6422112679574639. input_tokens=437, output_tokens=0
22:57:04,569 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:04,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6293162908405066. input_tokens=263, output_tokens=0
22:57:05,553 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:05,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.181005859049037. input_tokens=363, output_tokens=0
22:57:05,654 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:05,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1587543711066246. input_tokens=413, output_tokens=0
22:57:05,798 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:05,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2534425880294293. input_tokens=322, output_tokens=0
22:57:05,890 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:05,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3582271789200604. input_tokens=340, output_tokens=0
22:57:11,170 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:11,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5863032590132207. input_tokens=172, output_tokens=0
22:57:11,186 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:57:11,365 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:57:11,365 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:57:11,376 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:57:13,842 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:57:14,9 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:57:14,9 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:57:14,22 datashaper.workflow.workflow INFO executing verb create_final_communities
22:57:14,38 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:57:14,173 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:57:14,174 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:57:14,182 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:57:14,193 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:57:14,202 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:57:14,340 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
22:57:14,340 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:57:14,345 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:57:14,347 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:57:14,391 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:57:14,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:57:14,540 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
22:57:14,540 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:57:14,545 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:57:14,556 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:57:14,560 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 167
22:57:16,893 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:16,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.303911089198664. input_tokens=2055, output_tokens=267
22:57:17,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:17,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5573202699888498. input_tokens=2071, output_tokens=343
22:57:17,260 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:17,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6740118709858507. input_tokens=2147, output_tokens=369
22:57:18,513 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:18,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.930174175882712. input_tokens=2394, output_tokens=656
22:57:19,112 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:19,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.203516632085666. input_tokens=2041, output_tokens=384
22:57:19,581 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:57:19,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.999984838068485. input_tokens=5000, output_tokens=844
22:57:19,595 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:57:19,738 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:57:19,739 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:57:19,750 datashaper.workflow.workflow INFO executing verb create_final_documents
22:57:19,756 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:57:19,774 graphrag.index.cli INFO All workflows completed successfully.
23:00:03,588 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:00:03,589 graphrag.index.cli INFO Starting pipeline run for: 20241101-230003, dryrun=False
23:00:03,590 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:00:03,592 graphrag.index.create_pipeline_config INFO skipping workflows 
23:00:03,592 graphrag.index.run.run INFO Running pipeline
23:00:03,592 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:00:03,593 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:00:03,593 graphrag.index.input.load_input INFO using file storage for input
23:00:03,594 graphrag.index.input.csv INFO Loading csv files from input_eval
23:00:03,594 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:00:03,598 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:00:03,598 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:00:03,599 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:00:03,599 graphrag.index.run.run INFO Final # of rows loaded: 1
23:00:03,722 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:00:03,724 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:00:04,253 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:00:04,380 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:00:04,381 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:00:04,387 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:00:04,389 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:00:04,428 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:00:04,428 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:00:07,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:07,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.670766678871587. input_tokens=2087, output_tokens=523
23:00:14,33 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:14,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.935299962060526. input_tokens=39, output_tokens=2315
23:00:14,675 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:14,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6376212909817696. input_tokens=2, output_tokens=1
23:00:20,990 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:20,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.314930447842926. input_tokens=39, output_tokens=2315
23:00:21,711 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:21,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.7182556400075555. input_tokens=2, output_tokens=1
23:00:28,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:28,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.412024833029136. input_tokens=39, output_tokens=2315
23:00:28,866 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:28,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7372474849689752. input_tokens=2, output_tokens=1
23:00:35,470 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:35,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 6.602506464114413. input_tokens=39, output_tokens=2315
23:00:36,295 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:36,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.819330234080553. input_tokens=2, output_tokens=1
23:00:45,257 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:45,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 8.962474728934467. input_tokens=39, output_tokens=2315
23:00:46,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:46,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.0101926890201867. input_tokens=2, output_tokens=1
23:00:52,732 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:52,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 6.458687060978264. input_tokens=39, output_tokens=2315
23:00:53,718 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:00:53,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.9812561399303377. input_tokens=2, output_tokens=1
23:01:00,318 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:01:00,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 6.598132940009236. input_tokens=39, output_tokens=2315
23:01:00,352 root INFO Starting preprocessing of transition probabilities on graph with 15 nodes and 19 edges
23:01:00,352 root INFO Starting at time 1730498460.3521988
23:01:00,352 root INFO Beginning preprocessing of transition probabilities for 15 vertices
23:01:00,352 root INFO Completed 1 / 15 vertices
23:01:00,352 root INFO Completed 2 / 15 vertices
23:01:00,353 root INFO Completed 3 / 15 vertices
23:01:00,354 root INFO Completed 4 / 15 vertices
23:01:00,355 root INFO Completed 5 / 15 vertices
23:01:00,355 root INFO Completed 6 / 15 vertices
23:01:00,355 root INFO Completed 7 / 15 vertices
23:01:00,355 root INFO Completed 8 / 15 vertices
23:01:00,356 root INFO Completed 9 / 15 vertices
23:01:00,356 root INFO Completed 10 / 15 vertices
23:01:00,356 root INFO Completed 11 / 15 vertices
23:01:00,356 root INFO Completed 12 / 15 vertices
23:01:00,357 root INFO Completed 13 / 15 vertices
23:01:00,357 root INFO Completed 14 / 15 vertices
23:01:00,357 root INFO Completed 15 / 15 vertices
23:01:00,357 root INFO Completed preprocessing of transition probabilities for vertices
23:01:00,357 root INFO Beginning preprocessing of transition probabilities for 19 edges
23:01:00,358 root INFO Completed 1 / 19 edges
23:01:00,358 root INFO Completed 2 / 19 edges
23:01:00,359 root INFO Completed 3 / 19 edges
23:01:00,359 root INFO Completed 4 / 19 edges
23:01:00,360 root INFO Completed 5 / 19 edges
23:01:00,360 root INFO Completed 6 / 19 edges
23:01:00,360 root INFO Completed 7 / 19 edges
23:01:00,361 root INFO Completed 8 / 19 edges
23:01:00,361 root INFO Completed 9 / 19 edges
23:01:00,362 root INFO Completed 10 / 19 edges
23:01:00,362 root INFO Completed 11 / 19 edges
23:01:00,362 root INFO Completed 12 / 19 edges
23:01:00,363 root INFO Completed 13 / 19 edges
23:01:00,363 root INFO Completed 14 / 19 edges
23:01:00,363 root INFO Completed 15 / 19 edges
23:01:00,364 root INFO Completed 16 / 19 edges
23:01:00,364 root INFO Completed 17 / 19 edges
23:01:00,365 root INFO Completed 18 / 19 edges
23:01:00,365 root INFO Completed 19 / 19 edges
23:01:00,365 root INFO Completed preprocessing of transition probabilities for edges
23:01:00,366 root INFO Simulating walks on graph at time 1730498460.3662167
23:01:00,366 root INFO Walk iteration: 1/10
23:01:00,368 root INFO Walk iteration: 2/10
23:01:00,370 root INFO Walk iteration: 3/10
23:01:00,371 root INFO Walk iteration: 4/10
23:01:00,372 root INFO Walk iteration: 5/10
23:01:00,373 root INFO Walk iteration: 6/10
23:01:00,374 root INFO Walk iteration: 7/10
23:01:00,374 root INFO Walk iteration: 8/10
23:01:00,375 root INFO Walk iteration: 9/10
23:01:00,376 root INFO Walk iteration: 10/10
23:01:00,377 root INFO Learning embeddings at time 1730498460.3769965
23:01:00,377 gensim.models.word2vec INFO collecting all words and their counts
23:01:00,377 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:01:00,377 gensim.models.word2vec INFO collected 15 word types from a corpus of 2760 raw words and 150 sentences
23:01:00,378 gensim.models.word2vec INFO Creating a fresh vocabulary
23:01:00,379 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 15 unique words (100.00% of original 15, drops 0)', 'datetime': '2024-11-01T23:01:00.379453', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:01:00,380 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2760 word corpus (100.00% of original 2760, drops 0)', 'datetime': '2024-11-01T23:01:00.380020', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:01:00,380 gensim.models.word2vec INFO deleting the raw counts dictionary of 15 items
23:01:00,381 gensim.models.word2vec INFO sample=0.001 downsamples 15 most-common words
23:01:00,381 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 343.4902616174611 word corpus (12.4%% of prior 2760)', 'datetime': '2024-11-01T23:01:00.381313', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:01:00,382 gensim.models.word2vec INFO estimated required memory for 15 words and 1536 dimensions: 191820 bytes
23:01:00,382 gensim.models.word2vec INFO resetting layer weights
23:01:00,382 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:01:00.382806', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:01:00,383 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 15 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:01:00.383417', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:01:00,388 gensim.models.word2vec INFO EPOCH 0: training on 2760 raw words (350 effective words) took 0.0s, 207040 effective words/s
23:01:00,394 gensim.models.word2vec INFO EPOCH 1: training on 2760 raw words (346 effective words) took 0.0s, 104655 effective words/s
23:01:00,401 gensim.models.word2vec INFO EPOCH 2: training on 2760 raw words (359 effective words) took 0.0s, 71726 effective words/s
23:01:00,401 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 8280 raw words (1055 effective words) took 0.0s, 59715 effective words/s', 'datetime': '2024-11-01T23:01:00.401754', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:01:00,402 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:01:00.402906', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:01:00,403 root INFO Completed. Ending time is 1730498460.4030216 Elapsed time is -0.05082273483276367
23:01:00,437 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:01:00,575 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:01:00,575 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:01:00,582 datashaper.workflow.workflow INFO executing verb create_final_entities
23:01:00,586 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:01:00,623 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:01:00,623 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:01:00,624 graphrag.index.operations.embed_text.strategies.openai INFO embedding 15 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
23:01:01,827 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:01,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2274699651170522. input_tokens=434, output_tokens=0
23:01:01,857 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:01:01,997 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:01:01,998 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:01:02,5 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:01:04,392 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:01:04,560 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:01:04,560 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:01:04,568 datashaper.workflow.workflow INFO executing verb create_final_communities
23:01:04,578 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:01:04,711 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:01:04,712 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:01:04,716 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:01:04,723 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:01:04,729 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:01:04,869 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
23:01:04,870 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:01:04,872 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:01:04,875 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:01:04,882 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:01:04,891 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:01:05,27 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
23:01:05,30 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:01:05,34 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:01:05,41 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:01:05,44 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 15
23:01:08,47 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:01:08,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9872951458673924. input_tokens=2180, output_tokens=590
23:01:08,761 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:01:08,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.696787011809647. input_tokens=2551, output_tokens=603
23:01:15,994 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:01:15,997 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 10.93233218207024. input_tokens=2285, output_tokens=525
23:01:16,14 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:01:16,161 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:01:16,161 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:01:16,171 datashaper.workflow.workflow INFO executing verb create_final_documents
23:01:16,177 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:01:16,192 graphrag.index.cli INFO All workflows completed successfully.
23:04:22,933 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:04:22,934 graphrag.index.cli INFO Starting pipeline run for: 20241101-230422, dryrun=False
23:04:22,934 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:04:22,937 graphrag.index.create_pipeline_config INFO skipping workflows 
23:04:22,937 graphrag.index.run.run INFO Running pipeline
23:04:22,937 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:04:22,938 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:04:22,938 graphrag.index.input.load_input INFO using file storage for input
23:04:22,938 graphrag.index.input.csv INFO Loading csv files from input_eval
23:04:22,938 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:04:22,941 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:04:22,942 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:04:22,942 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:04:22,943 graphrag.index.run.run INFO Final # of rows loaded: 1
23:04:23,54 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:04:23,57 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:04:23,438 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:04:23,567 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:04:23,567 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:04:23,575 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:04:23,576 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:04:23,614 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:04:23,614 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:04:28,469 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:28,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.864004543051124. input_tokens=2087, output_tokens=527
23:04:44,479 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:44,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.00052091991529. input_tokens=32, output_tokens=1622
23:04:46,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:46,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.6945041639264673. input_tokens=2, output_tokens=1
23:04:52,166 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:52,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.988341349875554. input_tokens=32, output_tokens=2065
23:04:52,854 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:52,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6831303949002177. input_tokens=2, output_tokens=1
23:04:59,602 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:04:59,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.7464994178153574. input_tokens=32, output_tokens=2353
23:05:00,312 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:00,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7058242179919034. input_tokens=2, output_tokens=1
23:05:07,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:07,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 7.141539625125006. input_tokens=32, output_tokens=2525
23:05:08,269 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:08,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8106190559919924. input_tokens=2, output_tokens=1
23:05:18,785 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:18,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 10.514690340030938. input_tokens=32, output_tokens=3600
23:05:19,628 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:19,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.8351086140610278. input_tokens=2, output_tokens=1
23:05:31,993 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:32,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.363256081938744. input_tokens=32, output_tokens=4392
23:05:33,35 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:33,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.0353649011813104. input_tokens=2, output_tokens=1
23:05:45,304 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:45,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.267000674968585. input_tokens=32, output_tokens=4392
23:05:45,845 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:45,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5283874459564686. input_tokens=154, output_tokens=36
23:05:46,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7798052220605314. input_tokens=156, output_tokens=43
23:05:46,287 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43746702116914093. input_tokens=170, output_tokens=20
23:05:46,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2556282088626176. input_tokens=157, output_tokens=41
23:05:46,581 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2553124988917261. input_tokens=159, output_tokens=55
23:05:46,589 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.266417107079178. input_tokens=160, output_tokens=44
23:05:46,668 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:46,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5630497860256582. input_tokens=175, output_tokens=34
23:05:46,699 root INFO Starting preprocessing of transition probabilities on graph with 39 nodes and 41 edges
23:05:46,699 root INFO Starting at time 1730498746.6997445
23:05:46,699 root INFO Beginning preprocessing of transition probabilities for 39 vertices
23:05:46,699 root INFO Completed 1 / 39 vertices
23:05:46,699 root INFO Completed 4 / 39 vertices
23:05:46,699 root INFO Completed 7 / 39 vertices
23:05:46,699 root INFO Completed 10 / 39 vertices
23:05:46,701 root INFO Completed 13 / 39 vertices
23:05:46,702 root INFO Completed 16 / 39 vertices
23:05:46,703 root INFO Completed 19 / 39 vertices
23:05:46,703 root INFO Completed 22 / 39 vertices
23:05:46,704 root INFO Completed 25 / 39 vertices
23:05:46,705 root INFO Completed 28 / 39 vertices
23:05:46,705 root INFO Completed 31 / 39 vertices
23:05:46,705 root INFO Completed 34 / 39 vertices
23:05:46,706 root INFO Completed 37 / 39 vertices
23:05:46,706 root INFO Completed preprocessing of transition probabilities for vertices
23:05:46,706 root INFO Beginning preprocessing of transition probabilities for 41 edges
23:05:46,707 root INFO Completed 1 / 41 edges
23:05:46,707 root INFO Completed 5 / 41 edges
23:05:46,708 root INFO Completed 9 / 41 edges
23:05:46,709 root INFO Completed 13 / 41 edges
23:05:46,709 root INFO Completed 17 / 41 edges
23:05:46,710 root INFO Completed 21 / 41 edges
23:05:46,711 root INFO Completed 25 / 41 edges
23:05:46,712 root INFO Completed 29 / 41 edges
23:05:46,712 root INFO Completed 33 / 41 edges
23:05:46,713 root INFO Completed 37 / 41 edges
23:05:46,713 root INFO Completed 41 / 41 edges
23:05:46,714 root INFO Completed preprocessing of transition probabilities for edges
23:05:46,715 root INFO Simulating walks on graph at time 1730498746.7153
23:05:46,716 root INFO Walk iteration: 1/10
23:05:46,719 root INFO Walk iteration: 2/10
23:05:46,723 root INFO Walk iteration: 3/10
23:05:46,727 root INFO Walk iteration: 4/10
23:05:46,730 root INFO Walk iteration: 5/10
23:05:46,733 root INFO Walk iteration: 6/10
23:05:46,735 root INFO Walk iteration: 7/10
23:05:46,736 root INFO Walk iteration: 8/10
23:05:46,737 root INFO Walk iteration: 9/10
23:05:46,738 root INFO Walk iteration: 10/10
23:05:46,739 root INFO Learning embeddings at time 1730498746.7396748
23:05:46,739 gensim.models.word2vec INFO collecting all words and their counts
23:05:46,740 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:05:46,741 gensim.models.word2vec INFO collected 39 word types from a corpus of 6440 raw words and 390 sentences
23:05:46,741 gensim.models.word2vec INFO Creating a fresh vocabulary
23:05:46,742 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 39 unique words (100.00% of original 39, drops 0)', 'datetime': '2024-11-01T23:05:46.742655', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,743 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6440 word corpus (100.00% of original 6440, drops 0)', 'datetime': '2024-11-01T23:05:46.743246', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,743 gensim.models.word2vec INFO deleting the raw counts dictionary of 39 items
23:05:46,744 gensim.models.word2vec INFO sample=0.001 downsamples 38 most-common words
23:05:46,744 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1341.18887035818 word corpus (20.8%% of prior 6440)', 'datetime': '2024-11-01T23:05:46.744160', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,744 gensim.models.word2vec INFO estimated required memory for 39 words and 1536 dimensions: 498732 bytes
23:05:46,745 gensim.models.word2vec INFO resetting layer weights
23:05:46,745 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:05:46.745586', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:05:46,745 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 39 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:05:46.745880', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:05:46,754 gensim.models.word2vec INFO EPOCH 0: training on 6440 raw words (1368 effective words) took 0.0s, 202560 effective words/s
23:05:46,766 gensim.models.word2vec INFO EPOCH 1: training on 6440 raw words (1366 effective words) took 0.0s, 140256 effective words/s
23:05:46,775 gensim.models.word2vec INFO EPOCH 2: training on 6440 raw words (1330 effective words) took 0.0s, 163961 effective words/s
23:05:46,775 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 19320 raw words (4064 effective words) took 0.0s, 136474 effective words/s', 'datetime': '2024-11-01T23:05:46.775696', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:05:46,775 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:05:46.775799', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:05:46,775 root INFO Completed. Ending time is 1730498746.7758949 Elapsed time is -0.07615041732788086
23:05:46,785 root INFO Starting preprocessing of transition probabilities on graph with 39 nodes and 41 edges
23:05:46,786 root INFO Starting at time 1730498746.7860332
23:05:46,786 root INFO Beginning preprocessing of transition probabilities for 39 vertices
23:05:46,786 root INFO Completed 1 / 39 vertices
23:05:46,786 root INFO Completed 4 / 39 vertices
23:05:46,787 root INFO Completed 7 / 39 vertices
23:05:46,787 root INFO Completed 10 / 39 vertices
23:05:46,787 root INFO Completed 13 / 39 vertices
23:05:46,787 root INFO Completed 16 / 39 vertices
23:05:46,788 root INFO Completed 19 / 39 vertices
23:05:46,788 root INFO Completed 22 / 39 vertices
23:05:46,788 root INFO Completed 25 / 39 vertices
23:05:46,789 root INFO Completed 28 / 39 vertices
23:05:46,789 root INFO Completed 31 / 39 vertices
23:05:46,789 root INFO Completed 34 / 39 vertices
23:05:46,789 root INFO Completed 37 / 39 vertices
23:05:46,789 root INFO Completed preprocessing of transition probabilities for vertices
23:05:46,789 root INFO Beginning preprocessing of transition probabilities for 41 edges
23:05:46,790 root INFO Completed 1 / 41 edges
23:05:46,790 root INFO Completed 5 / 41 edges
23:05:46,791 root INFO Completed 9 / 41 edges
23:05:46,791 root INFO Completed 13 / 41 edges
23:05:46,792 root INFO Completed 17 / 41 edges
23:05:46,792 root INFO Completed 21 / 41 edges
23:05:46,792 root INFO Completed 25 / 41 edges
23:05:46,793 root INFO Completed 29 / 41 edges
23:05:46,793 root INFO Completed 33 / 41 edges
23:05:46,793 root INFO Completed 37 / 41 edges
23:05:46,793 root INFO Completed 41 / 41 edges
23:05:46,793 root INFO Completed preprocessing of transition probabilities for edges
23:05:46,794 root INFO Simulating walks on graph at time 1730498746.7945788
23:05:46,795 root INFO Walk iteration: 1/10
23:05:46,798 root INFO Walk iteration: 2/10
23:05:46,800 root INFO Walk iteration: 3/10
23:05:46,801 root INFO Walk iteration: 4/10
23:05:46,802 root INFO Walk iteration: 5/10
23:05:46,803 root INFO Walk iteration: 6/10
23:05:46,804 root INFO Walk iteration: 7/10
23:05:46,805 root INFO Walk iteration: 8/10
23:05:46,806 root INFO Walk iteration: 9/10
23:05:46,807 root INFO Walk iteration: 10/10
23:05:46,808 root INFO Learning embeddings at time 1730498746.8080657
23:05:46,808 gensim.models.word2vec INFO collecting all words and their counts
23:05:46,808 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:05:46,808 gensim.models.word2vec INFO collected 39 word types from a corpus of 6440 raw words and 390 sentences
23:05:46,809 gensim.models.word2vec INFO Creating a fresh vocabulary
23:05:46,809 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 39 unique words (100.00% of original 39, drops 0)', 'datetime': '2024-11-01T23:05:46.809313', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,809 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6440 word corpus (100.00% of original 6440, drops 0)', 'datetime': '2024-11-01T23:05:46.809791', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,809 gensim.models.word2vec INFO deleting the raw counts dictionary of 39 items
23:05:46,810 gensim.models.word2vec INFO sample=0.001 downsamples 38 most-common words
23:05:46,810 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1341.18887035818 word corpus (20.8%% of prior 6440)', 'datetime': '2024-11-01T23:05:46.810395', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:05:46,810 gensim.models.word2vec INFO estimated required memory for 39 words and 1536 dimensions: 498732 bytes
23:05:46,810 gensim.models.word2vec INFO resetting layer weights
23:05:46,811 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:05:46.811122', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:05:46,811 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 39 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:05:46.811530', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:05:46,816 gensim.models.word2vec INFO EPOCH 0: training on 6440 raw words (1368 effective words) took 0.0s, 374044 effective words/s
23:05:46,826 gensim.models.word2vec INFO EPOCH 1: training on 6440 raw words (1366 effective words) took 0.0s, 159300 effective words/s
23:05:46,843 gensim.models.word2vec INFO EPOCH 2: training on 6440 raw words (1330 effective words) took 0.0s, 91468 effective words/s
23:05:46,843 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 19320 raw words (4064 effective words) took 0.0s, 127684 effective words/s', 'datetime': '2024-11-01T23:05:46.843383', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:05:46,843 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=39, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:05:46.843446', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:05:46,843 root INFO Completed. Ending time is 1730498746.843503 Elapsed time is -0.057469844818115234
23:05:46,900 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:05:47,59 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:05:47,60 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:05:47,70 datashaper.workflow.workflow INFO executing verb create_final_entities
23:05:47,76 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:05:47,113 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:05:47,113 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:05:47,114 graphrag.index.operations.embed_text.strategies.openai INFO embedding 49 inputs via 48 snippets using 3 batches. max_batch_size=16, max_tokens=8191
23:05:47,986 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:48,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9018452370073646. input_tokens=457, output_tokens=0
23:05:48,407 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:48,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.313788534142077. input_tokens=367, output_tokens=0
23:05:48,432 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:48,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3402062270324677. input_tokens=466, output_tokens=0
23:05:48,460 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:05:48,631 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:05:48,631 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:05:48,642 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:05:51,354 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:05:51,523 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:05:51,525 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:05:51,534 datashaper.workflow.workflow INFO executing verb create_final_communities
23:05:51,549 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:05:51,684 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
23:05:51,685 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:05:51,692 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:05:51,702 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:05:51,710 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:05:51,847 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
23:05:51,848 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:05:51,852 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:05:51,855 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:05:51,862 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:05:51,872 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:05:52,7 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
23:05:52,7 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:05:52,13 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:05:52,21 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:05:52,27 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 27
23:05:52,40 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 115
23:05:54,655 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:54,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.590890657156706. input_tokens=2077, output_tokens=322
23:05:55,8 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:55,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9420853802002966. input_tokens=2086, output_tokens=437
23:05:55,605 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:55,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.542642790125683. input_tokens=2750, output_tokens=640
23:05:57,458 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:57,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.836069923825562. input_tokens=2045, output_tokens=313
23:05:58,498 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:58,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.873085269005969. input_tokens=2213, output_tokens=505
23:05:58,681 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:58,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0548694911412895. input_tokens=2442, output_tokens=452
23:05:59,67 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:59,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.446809547021985. input_tokens=3125, output_tokens=641
23:05:59,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:59,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4724147990345955. input_tokens=2492, output_tokens=519
23:05:59,112 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:05:59,264 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:05:59,264 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:05:59,274 datashaper.workflow.workflow INFO executing verb create_final_documents
23:05:59,281 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:05:59,298 graphrag.index.cli INFO All workflows completed successfully.
23:09:50,726 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:09:50,727 graphrag.index.cli INFO Starting pipeline run for: 20241101-230950, dryrun=False
23:09:50,727 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:09:50,729 graphrag.index.create_pipeline_config INFO skipping workflows 
23:09:50,730 graphrag.index.run.run INFO Running pipeline
23:09:50,730 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:09:50,730 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:09:50,730 graphrag.index.input.load_input INFO using file storage for input
23:09:50,731 graphrag.index.input.csv INFO Loading csv files from input_eval
23:09:50,731 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:09:50,734 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:09:50,734 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:09:50,734 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:09:50,734 graphrag.index.run.run INFO Final # of rows loaded: 1
23:09:50,846 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:09:50,849 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:09:51,173 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:09:51,300 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:09:51,300 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:09:51,307 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:09:51,308 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:09:51,347 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:09:51,347 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:09:53,468 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:09:53,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1247272670734674. input_tokens=2087, output_tokens=527
23:09:59,779 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:09:59,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.3064347768668085. input_tokens=33, output_tokens=2065
23:10:00,440 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:00,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6614430819172412. input_tokens=2, output_tokens=1
23:10:06,45 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:06,50 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.603722437983379. input_tokens=33, output_tokens=2065
23:10:06,693 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:06,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6444933509919792. input_tokens=2, output_tokens=1
23:10:16,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:16,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 10.132187203969806. input_tokens=33, output_tokens=3529
23:10:17,646 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:17,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8169374568387866. input_tokens=2, output_tokens=1
23:10:27,929 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:27,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 10.282130309846252. input_tokens=33, output_tokens=3845
23:10:28,814 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:28,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8770169590134174. input_tokens=2, output_tokens=1
23:10:40,203 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:40,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 11.387792300898582. input_tokens=33, output_tokens=4277
23:10:41,153 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:41,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9456982458941638. input_tokens=2, output_tokens=1
23:10:53,590 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:53,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.435745017137378. input_tokens=33, output_tokens=4567
23:10:54,686 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:54,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.0877394720446318. input_tokens=2, output_tokens=1
23:11:07,347 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:07,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.659653183072805. input_tokens=33, output_tokens=4661
23:11:07,382 root INFO Starting preprocessing of transition probabilities on graph with 30 nodes and 34 edges
23:11:07,382 root INFO Starting at time 1730499067.3827887
23:11:07,382 root INFO Beginning preprocessing of transition probabilities for 30 vertices
23:11:07,382 root INFO Completed 1 / 30 vertices
23:11:07,382 root INFO Completed 4 / 30 vertices
23:11:07,383 root INFO Completed 7 / 30 vertices
23:11:07,384 root INFO Completed 10 / 30 vertices
23:11:07,385 root INFO Completed 13 / 30 vertices
23:11:07,386 root INFO Completed 16 / 30 vertices
23:11:07,386 root INFO Completed 19 / 30 vertices
23:11:07,386 root INFO Completed 22 / 30 vertices
23:11:07,386 root INFO Completed 25 / 30 vertices
23:11:07,386 root INFO Completed 28 / 30 vertices
23:11:07,386 root INFO Completed preprocessing of transition probabilities for vertices
23:11:07,387 root INFO Beginning preprocessing of transition probabilities for 34 edges
23:11:07,387 root INFO Completed 1 / 34 edges
23:11:07,387 root INFO Completed 4 / 34 edges
23:11:07,388 root INFO Completed 7 / 34 edges
23:11:07,388 root INFO Completed 10 / 34 edges
23:11:07,389 root INFO Completed 13 / 34 edges
23:11:07,390 root INFO Completed 16 / 34 edges
23:11:07,390 root INFO Completed 19 / 34 edges
23:11:07,390 root INFO Completed 22 / 34 edges
23:11:07,391 root INFO Completed 25 / 34 edges
23:11:07,391 root INFO Completed 28 / 34 edges
23:11:07,392 root INFO Completed 31 / 34 edges
23:11:07,392 root INFO Completed 34 / 34 edges
23:11:07,392 root INFO Completed preprocessing of transition probabilities for edges
23:11:07,392 root INFO Simulating walks on graph at time 1730499067.3927894
23:11:07,393 root INFO Walk iteration: 1/10
23:11:07,395 root INFO Walk iteration: 2/10
23:11:07,397 root INFO Walk iteration: 3/10
23:11:07,399 root INFO Walk iteration: 4/10
23:11:07,401 root INFO Walk iteration: 5/10
23:11:07,403 root INFO Walk iteration: 6/10
23:11:07,405 root INFO Walk iteration: 7/10
23:11:07,407 root INFO Walk iteration: 8/10
23:11:07,408 root INFO Walk iteration: 9/10
23:11:07,409 root INFO Walk iteration: 10/10
23:11:07,410 root INFO Learning embeddings at time 1730499067.4106705
23:11:07,411 gensim.models.word2vec INFO collecting all words and their counts
23:11:07,411 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:11:07,412 gensim.models.word2vec INFO collected 30 word types from a corpus of 5440 raw words and 300 sentences
23:11:07,412 gensim.models.word2vec INFO Creating a fresh vocabulary
23:11:07,413 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 30 unique words (100.00% of original 30, drops 0)', 'datetime': '2024-11-01T23:11:07.413031', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,413 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5440 word corpus (100.00% of original 5440, drops 0)', 'datetime': '2024-11-01T23:11:07.413456', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,413 gensim.models.word2vec INFO deleting the raw counts dictionary of 30 items
23:11:07,414 gensim.models.word2vec INFO sample=0.001 downsamples 29 most-common words
23:11:07,414 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 982.4297951123381 word corpus (18.1%% of prior 5440)', 'datetime': '2024-11-01T23:11:07.414136', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,414 gensim.models.word2vec INFO estimated required memory for 30 words and 1536 dimensions: 383640 bytes
23:11:07,415 gensim.models.word2vec INFO resetting layer weights
23:11:07,415 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:11:07.415765', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:11:07,416 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 30 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:11:07.416242', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:11:07,426 gensim.models.word2vec INFO EPOCH 0: training on 5440 raw words (1044 effective words) took 0.0s, 131255 effective words/s
23:11:07,437 gensim.models.word2vec INFO EPOCH 1: training on 5440 raw words (991 effective words) took 0.0s, 114152 effective words/s
23:11:07,445 gensim.models.word2vec INFO EPOCH 2: training on 5440 raw words (948 effective words) took 0.0s, 143065 effective words/s
23:11:07,445 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16320 raw words (2983 effective words) took 0.0s, 104716 effective words/s', 'datetime': '2024-11-01T23:11:07.445394', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:11:07,445 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=30, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:11:07.445431', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:11:07,446 root INFO Completed. Ending time is 1730499067.4464557 Elapsed time is -0.06366705894470215
23:11:07,455 root INFO Starting preprocessing of transition probabilities on graph with 30 nodes and 34 edges
23:11:07,456 root INFO Starting at time 1730499067.456107
23:11:07,456 root INFO Beginning preprocessing of transition probabilities for 30 vertices
23:11:07,457 root INFO Completed 1 / 30 vertices
23:11:07,457 root INFO Completed 4 / 30 vertices
23:11:07,457 root INFO Completed 7 / 30 vertices
23:11:07,458 root INFO Completed 10 / 30 vertices
23:11:07,458 root INFO Completed 13 / 30 vertices
23:11:07,458 root INFO Completed 16 / 30 vertices
23:11:07,459 root INFO Completed 19 / 30 vertices
23:11:07,459 root INFO Completed 22 / 30 vertices
23:11:07,460 root INFO Completed 25 / 30 vertices
23:11:07,460 root INFO Completed 28 / 30 vertices
23:11:07,460 root INFO Completed preprocessing of transition probabilities for vertices
23:11:07,460 root INFO Beginning preprocessing of transition probabilities for 34 edges
23:11:07,461 root INFO Completed 1 / 34 edges
23:11:07,461 root INFO Completed 4 / 34 edges
23:11:07,462 root INFO Completed 7 / 34 edges
23:11:07,463 root INFO Completed 10 / 34 edges
23:11:07,464 root INFO Completed 13 / 34 edges
23:11:07,464 root INFO Completed 16 / 34 edges
23:11:07,465 root INFO Completed 19 / 34 edges
23:11:07,466 root INFO Completed 22 / 34 edges
23:11:07,467 root INFO Completed 25 / 34 edges
23:11:07,467 root INFO Completed 28 / 34 edges
23:11:07,468 root INFO Completed 31 / 34 edges
23:11:07,469 root INFO Completed 34 / 34 edges
23:11:07,469 root INFO Completed preprocessing of transition probabilities for edges
23:11:07,470 root INFO Simulating walks on graph at time 1730499067.4703338
23:11:07,470 root INFO Walk iteration: 1/10
23:11:07,474 root INFO Walk iteration: 2/10
23:11:07,477 root INFO Walk iteration: 3/10
23:11:07,480 root INFO Walk iteration: 4/10
23:11:07,483 root INFO Walk iteration: 5/10
23:11:07,484 root INFO Walk iteration: 6/10
23:11:07,486 root INFO Walk iteration: 7/10
23:11:07,487 root INFO Walk iteration: 8/10
23:11:07,488 root INFO Walk iteration: 9/10
23:11:07,489 root INFO Walk iteration: 10/10
23:11:07,490 root INFO Learning embeddings at time 1730499067.4902666
23:11:07,490 gensim.models.word2vec INFO collecting all words and their counts
23:11:07,490 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:11:07,491 gensim.models.word2vec INFO collected 30 word types from a corpus of 5440 raw words and 300 sentences
23:11:07,492 gensim.models.word2vec INFO Creating a fresh vocabulary
23:11:07,492 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 30 unique words (100.00% of original 30, drops 0)', 'datetime': '2024-11-01T23:11:07.492266', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,492 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5440 word corpus (100.00% of original 5440, drops 0)', 'datetime': '2024-11-01T23:11:07.492820', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,492 gensim.models.word2vec INFO deleting the raw counts dictionary of 30 items
23:11:07,493 gensim.models.word2vec INFO sample=0.001 downsamples 29 most-common words
23:11:07,493 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 982.4297951123381 word corpus (18.1%% of prior 5440)', 'datetime': '2024-11-01T23:11:07.493455', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:11:07,494 gensim.models.word2vec INFO estimated required memory for 30 words and 1536 dimensions: 383640 bytes
23:11:07,494 gensim.models.word2vec INFO resetting layer weights
23:11:07,494 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:11:07.494883', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:11:07,495 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 30 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:11:07.495406', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:11:07,504 gensim.models.word2vec INFO EPOCH 0: training on 5440 raw words (1044 effective words) took 0.0s, 147124 effective words/s
23:11:07,520 gensim.models.word2vec INFO EPOCH 1: training on 5440 raw words (991 effective words) took 0.0s, 80695 effective words/s
23:11:07,537 gensim.models.word2vec INFO EPOCH 2: training on 5440 raw words (948 effective words) took 0.0s, 78258 effective words/s
23:11:07,537 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16320 raw words (2983 effective words) took 0.0s, 71851 effective words/s', 'datetime': '2024-11-01T23:11:07.537426', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:11:07,538 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=30, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:11:07.538263', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:11:07,538 root INFO Completed. Ending time is 1730499067.538872 Elapsed time is -0.08276510238647461
23:11:07,589 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:11:07,742 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:11:07,742 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:11:07,750 datashaper.workflow.workflow INFO executing verb create_final_entities
23:11:07,755 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:11:07,792 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:11:07,792 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:11:07,793 graphrag.index.operations.embed_text.strategies.openai INFO embedding 34 inputs via 34 snippets using 3 batches. max_batch_size=16, max_tokens=8191
23:11:08,856 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:11:08,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.068514043930918. input_tokens=37, output_tokens=0
23:11:09,70 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:11:09,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3002359920646995. input_tokens=358, output_tokens=0
23:11:09,96 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:11:09,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3258846199605614. input_tokens=451, output_tokens=0
23:11:09,123 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:11:09,279 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:11:09,281 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:11:09,290 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:11:11,954 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:11:12,120 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:11:12,121 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:11:12,131 datashaper.workflow.workflow INFO executing verb create_final_communities
23:11:12,145 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:11:12,280 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:11:12,280 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:11:12,287 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:11:12,302 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:11:12,313 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:11:12,451 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
23:11:12,452 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:11:12,463 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:11:12,465 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:11:12,472 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:11:12,483 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:11:12,621 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:11:12,621 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:11:12,625 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:11:12,634 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:11:12,640 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 18
23:11:12,655 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 82
23:11:14,678 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:14,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9984502401202917. input_tokens=2047, output_tokens=344
23:11:15,115 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:15,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.437959582079202. input_tokens=2115, output_tokens=329
23:11:16,238 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:16,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.563557570800185. input_tokens=2487, output_tokens=638
23:11:18,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:18,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0883206019643694. input_tokens=2103, output_tokens=352
23:11:18,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:18,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.316903148079291. input_tokens=2181, output_tokens=387
23:11:18,654 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:18,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.397662819828838. input_tokens=2071, output_tokens=307
23:11:19,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:19,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0417079720646143. input_tokens=2362, output_tokens=566
23:11:20,497 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:20,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.23736466281116. input_tokens=2873, output_tokens=677
23:11:20,505 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:11:20,649 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:11:20,650 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:11:20,660 datashaper.workflow.workflow INFO executing verb create_final_documents
23:11:20,666 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:11:20,682 graphrag.index.cli INFO All workflows completed successfully.
23:13:48,611 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:13:48,612 graphrag.index.cli INFO Starting pipeline run for: 20241101-231348, dryrun=False
23:13:48,612 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:13:48,614 graphrag.index.create_pipeline_config INFO skipping workflows 
23:13:48,614 graphrag.index.run.run INFO Running pipeline
23:13:48,614 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:13:48,615 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:13:48,615 graphrag.index.input.load_input INFO using file storage for input
23:13:48,616 graphrag.index.input.csv INFO Loading csv files from input_eval
23:13:48,616 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:13:48,619 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:13:48,619 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:13:48,620 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:13:48,621 graphrag.index.run.run INFO Final # of rows loaded: 1
23:13:48,733 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:13:48,735 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:13:49,139 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:13:49,265 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:13:49,266 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:13:49,273 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:13:49,274 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:13:49,313 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:13:49,313 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:13:52,84 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:13:52,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.775635328143835. input_tokens=2087, output_tokens=527
23:13:58,331 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:13:58,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.243766101077199. input_tokens=36, output_tokens=1718
23:13:58,960 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:13:58,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6258179990109056. input_tokens=2, output_tokens=1
23:14:06,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:06,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.453923627967015. input_tokens=36, output_tokens=2297
23:14:07,31 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:07,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6118006098549813. input_tokens=2, output_tokens=1
23:14:17,47 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:17,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 10.014823894016445. input_tokens=36, output_tokens=3249
23:14:17,920 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:17,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.866785692051053. input_tokens=2, output_tokens=1
23:14:41,157 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:41,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 23.2360356322024. input_tokens=36, output_tokens=4024
23:14:42,55 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:42,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8921958340797573. input_tokens=2, output_tokens=1
23:14:55,69 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:55,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 13.010927214985713. input_tokens=36, output_tokens=4594
23:14:56,155 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:56,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.0806300700642169. input_tokens=2, output_tokens=1
23:15:09,718 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:15:09,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 13.562118076952174. input_tokens=36, output_tokens=4664
23:15:10,868 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:15:10,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.1451968920882791. input_tokens=2, output_tokens=1
23:15:24,169 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:15:24,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 13.299026219174266. input_tokens=36, output_tokens=4667
23:15:24,189 root INFO Starting preprocessing of transition probabilities on graph with 6 nodes and 5 edges
23:15:24,189 root INFO Starting at time 1730499324.1898804
23:15:24,189 root INFO Beginning preprocessing of transition probabilities for 6 vertices
23:15:24,189 root INFO Completed 1 / 6 vertices
23:15:24,189 root INFO Completed 2 / 6 vertices
23:15:24,189 root INFO Completed 3 / 6 vertices
23:15:24,190 root INFO Completed 4 / 6 vertices
23:15:24,190 root INFO Completed 5 / 6 vertices
23:15:24,190 root INFO Completed 6 / 6 vertices
23:15:24,191 root INFO Completed preprocessing of transition probabilities for vertices
23:15:24,191 root INFO Beginning preprocessing of transition probabilities for 5 edges
23:15:24,192 root INFO Completed 1 / 5 edges
23:15:24,192 root INFO Completed 2 / 5 edges
23:15:24,193 root INFO Completed 3 / 5 edges
23:15:24,194 root INFO Completed 4 / 5 edges
23:15:24,194 root INFO Completed 5 / 5 edges
23:15:24,194 root INFO Completed preprocessing of transition probabilities for edges
23:15:24,194 root INFO Simulating walks on graph at time 1730499324.1946623
23:15:24,195 root INFO Walk iteration: 1/10
23:15:24,196 root INFO Walk iteration: 2/10
23:15:24,196 root INFO Walk iteration: 3/10
23:15:24,197 root INFO Walk iteration: 4/10
23:15:24,198 root INFO Walk iteration: 5/10
23:15:24,198 root INFO Walk iteration: 6/10
23:15:24,199 root INFO Walk iteration: 7/10
23:15:24,200 root INFO Walk iteration: 8/10
23:15:24,201 root INFO Walk iteration: 9/10
23:15:24,201 root INFO Walk iteration: 10/10
23:15:24,202 root INFO Learning embeddings at time 1730499324.202183
23:15:24,202 gensim.models.word2vec INFO collecting all words and their counts
23:15:24,203 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:15:24,203 gensim.models.word2vec INFO collected 6 word types from a corpus of 800 raw words and 60 sentences
23:15:24,203 gensim.models.word2vec INFO Creating a fresh vocabulary
23:15:24,204 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 6 unique words (100.00% of original 6, drops 0)', 'datetime': '2024-11-01T23:15:24.204472', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:15:24,205 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 800 word corpus (100.00% of original 800, drops 0)', 'datetime': '2024-11-01T23:15:24.205073', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:15:24,205 gensim.models.word2vec INFO deleting the raw counts dictionary of 6 items
23:15:24,205 gensim.models.word2vec INFO sample=0.001 downsamples 6 most-common words
23:15:24,205 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 61.97767105626599 word corpus (7.7%% of prior 800)', 'datetime': '2024-11-01T23:15:24.205905', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:15:24,206 gensim.models.word2vec INFO estimated required memory for 6 words and 1536 dimensions: 76728 bytes
23:15:24,207 gensim.models.word2vec INFO resetting layer weights
23:15:24,207 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:15:24.207533', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:15:24,208 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 6 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:15:24.208355', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:15:24,212 gensim.models.word2vec INFO EPOCH 0: training on 800 raw words (77 effective words) took 0.0s, 471787 effective words/s
23:15:24,215 gensim.models.word2vec INFO EPOCH 1: training on 800 raw words (56 effective words) took 0.0s, 64898 effective words/s
23:15:24,218 gensim.models.word2vec INFO EPOCH 2: training on 800 raw words (62 effective words) took 0.0s, 75822 effective words/s
23:15:24,219 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 2400 raw words (195 effective words) took 0.0s, 19894 effective words/s', 'datetime': '2024-11-01T23:15:24.218968', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:15:24,219 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=6, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:15:24.219923', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:15:24,220 root INFO Completed. Ending time is 1730499324.2208135 Elapsed time is -0.030933141708374023
23:15:24,242 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:15:24,381 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:15:24,381 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:15:24,389 datashaper.workflow.workflow INFO executing verb create_final_entities
23:15:24,393 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:15:24,430 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:15:24,430 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:15:24,430 graphrag.index.operations.embed_text.strategies.openai INFO embedding 6 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
23:15:25,436 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:15:25,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0241141449660063. input_tokens=214, output_tokens=0
23:15:25,461 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:15:25,597 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:15:25,598 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:15:25,605 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:15:27,859 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:15:28,17 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:15:28,18 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:15:28,26 datashaper.workflow.workflow INFO executing verb create_final_communities
23:15:28,36 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:15:28,168 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
23:15:28,168 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:15:28,174 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:15:28,180 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:15:28,186 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:15:28,321 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
23:15:28,322 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:15:28,326 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:15:28,328 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:15:28,335 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:15:28,344 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:15:28,479 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:15:28,480 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:15:28,484 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:15:28,491 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:15:28,494 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 6
23:15:30,901 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:15:30,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.396159363212064. input_tokens=2281, output_tokens=448
23:15:30,922 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:15:31,63 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:15:31,63 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:15:31,74 datashaper.workflow.workflow INFO executing verb create_final_documents
23:15:31,80 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:15:31,103 graphrag.index.cli INFO All workflows completed successfully.
23:17:43,605 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:17:43,607 graphrag.index.cli INFO Starting pipeline run for: 20241101-231743, dryrun=False
23:17:43,607 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:17:43,609 graphrag.index.create_pipeline_config INFO skipping workflows 
23:17:43,609 graphrag.index.run.run INFO Running pipeline
23:17:43,609 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:17:43,611 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:17:43,611 graphrag.index.input.load_input INFO using file storage for input
23:17:43,612 graphrag.index.input.csv INFO Loading csv files from input_eval
23:17:43,612 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:17:43,615 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:17:43,615 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:17:43,616 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:17:43,617 graphrag.index.run.run INFO Final # of rows loaded: 1
23:17:43,728 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:17:43,730 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:17:44,61 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:17:44,187 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:17:44,188 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:17:44,195 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:17:44,196 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:17:44,234 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:17:44,234 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:17:49,406 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:49,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.176216102903709. input_tokens=2087, output_tokens=527
23:17:53,775 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:53,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.364524509990588. input_tokens=34, output_tokens=1212
23:17:54,246 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:54,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4690931490622461. input_tokens=2, output_tokens=1
23:17:58,427 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:58,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.1802271120250225. input_tokens=34, output_tokens=380
23:17:59,2 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:59,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5730426970403641. input_tokens=2, output_tokens=1
23:18:01,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:01,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 2.1225226670503616. input_tokens=34, output_tokens=183
23:18:01,711 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:01,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.584462049882859. input_tokens=2, output_tokens=1
23:18:02,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:02,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.8971948600374162. input_tokens=34, output_tokens=71
23:18:03,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:03,203 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.5903783289249986. input_tokens=2, output_tokens=1
23:18:04,41 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:04,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.8385318878572434. input_tokens=34, output_tokens=78
23:18:04,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:04,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5316960881464183. input_tokens=2, output_tokens=1
23:18:05,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:05,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.9855641860049218. input_tokens=34, output_tokens=77
23:18:06,157 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:06,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.5952609390951693. input_tokens=2, output_tokens=1
23:18:06,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:06,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.8247927639167756. input_tokens=34, output_tokens=83
23:18:07,7 root INFO Starting preprocessing of transition probabilities on graph with 25 nodes and 24 edges
23:18:07,7 root INFO Starting at time 1730499487.0079048
23:18:07,7 root INFO Beginning preprocessing of transition probabilities for 25 vertices
23:18:07,7 root INFO Completed 1 / 25 vertices
23:18:07,8 root INFO Completed 3 / 25 vertices
23:18:07,9 root INFO Completed 5 / 25 vertices
23:18:07,10 root INFO Completed 7 / 25 vertices
23:18:07,10 root INFO Completed 9 / 25 vertices
23:18:07,11 root INFO Completed 11 / 25 vertices
23:18:07,12 root INFO Completed 13 / 25 vertices
23:18:07,13 root INFO Completed 15 / 25 vertices
23:18:07,13 root INFO Completed 17 / 25 vertices
23:18:07,14 root INFO Completed 19 / 25 vertices
23:18:07,14 root INFO Completed 21 / 25 vertices
23:18:07,15 root INFO Completed 23 / 25 vertices
23:18:07,15 root INFO Completed 25 / 25 vertices
23:18:07,16 root INFO Completed preprocessing of transition probabilities for vertices
23:18:07,16 root INFO Beginning preprocessing of transition probabilities for 24 edges
23:18:07,16 root INFO Completed 1 / 24 edges
23:18:07,16 root INFO Completed 3 / 24 edges
23:18:07,17 root INFO Completed 5 / 24 edges
23:18:07,17 root INFO Completed 7 / 24 edges
23:18:07,18 root INFO Completed 9 / 24 edges
23:18:07,19 root INFO Completed 11 / 24 edges
23:18:07,19 root INFO Completed 13 / 24 edges
23:18:07,20 root INFO Completed 15 / 24 edges
23:18:07,20 root INFO Completed 17 / 24 edges
23:18:07,21 root INFO Completed 19 / 24 edges
23:18:07,21 root INFO Completed 21 / 24 edges
23:18:07,22 root INFO Completed 23 / 24 edges
23:18:07,23 root INFO Completed preprocessing of transition probabilities for edges
23:18:07,23 root INFO Simulating walks on graph at time 1730499487.0235834
23:18:07,23 root INFO Walk iteration: 1/10
23:18:07,25 root INFO Walk iteration: 2/10
23:18:07,26 root INFO Walk iteration: 3/10
23:18:07,27 root INFO Walk iteration: 4/10
23:18:07,28 root INFO Walk iteration: 5/10
23:18:07,28 root INFO Walk iteration: 6/10
23:18:07,29 root INFO Walk iteration: 7/10
23:18:07,30 root INFO Walk iteration: 8/10
23:18:07,30 root INFO Walk iteration: 9/10
23:18:07,31 root INFO Walk iteration: 10/10
23:18:07,31 root INFO Learning embeddings at time 1730499487.0319307
23:18:07,32 gensim.models.word2vec INFO collecting all words and their counts
23:18:07,32 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:18:07,32 gensim.models.word2vec INFO collected 25 word types from a corpus of 3760 raw words and 250 sentences
23:18:07,33 gensim.models.word2vec INFO Creating a fresh vocabulary
23:18:07,33 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 25 unique words (100.00% of original 25, drops 0)', 'datetime': '2024-11-01T23:18:07.033692', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:18:07,34 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3760 word corpus (100.00% of original 3760, drops 0)', 'datetime': '2024-11-01T23:18:07.034152', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:18:07,34 gensim.models.word2vec INFO deleting the raw counts dictionary of 25 items
23:18:07,35 gensim.models.word2vec INFO sample=0.001 downsamples 25 most-common words
23:18:07,35 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 624.4938915355715 word corpus (16.6%% of prior 3760)', 'datetime': '2024-11-01T23:18:07.035112', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:18:07,35 gensim.models.word2vec INFO estimated required memory for 25 words and 1536 dimensions: 319700 bytes
23:18:07,36 gensim.models.word2vec INFO resetting layer weights
23:18:07,36 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:18:07.036814', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:18:07,37 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 25 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:18:07.037048', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:18:07,41 gensim.models.word2vec INFO EPOCH 0: training on 3760 raw words (632 effective words) took 0.0s, 291075 effective words/s
23:18:07,46 gensim.models.word2vec INFO EPOCH 1: training on 3760 raw words (621 effective words) took 0.0s, 212635 effective words/s
23:18:07,49 gensim.models.word2vec INFO EPOCH 2: training on 3760 raw words (595 effective words) took 0.0s, 212420 effective words/s
23:18:07,49 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 11280 raw words (1848 effective words) took 0.0s, 143686 effective words/s', 'datetime': '2024-11-01T23:18:07.049943', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:18:07,50 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=25, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:18:07.050636', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:18:07,50 root INFO Completed. Ending time is 1730499487.0506911 Elapsed time is -0.042786359786987305
23:18:07,75 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:18:07,228 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:18:07,228 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:18:07,239 datashaper.workflow.workflow INFO executing verb create_final_entities
23:18:07,243 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:18:07,281 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:18:07,281 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:18:07,282 graphrag.index.operations.embed_text.strategies.openai INFO embedding 35 inputs via 28 snippets using 2 batches. max_batch_size=16, max_tokens=8191
23:18:08,445 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:18:08,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1807294159661978. input_tokens=277, output_tokens=0
23:18:08,632 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:18:08,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.372882538009435. input_tokens=450, output_tokens=0
23:18:08,660 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:18:08,810 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:18:08,810 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:18:08,819 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:18:11,67 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:18:11,242 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:18:11,242 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:18:11,251 datashaper.workflow.workflow INFO executing verb create_final_communities
23:18:11,263 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:18:11,397 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
23:18:11,397 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:18:11,403 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:18:11,411 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:18:11,418 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:18:11,558 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
23:18:11,558 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:18:11,564 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:18:11,566 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:18:11,573 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:18:11,582 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:18:11,716 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
23:18:11,716 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:18:11,721 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:18:11,729 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:18:11,733 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 35
23:18:14,424 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:14,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.67509254720062. input_tokens=2276, output_tokens=385
23:18:14,703 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:14,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.951582054840401. input_tokens=2169, output_tokens=432
23:18:14,710 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:14,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9548190471250564. input_tokens=2114, output_tokens=420
23:18:14,767 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:14,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.019905081950128. input_tokens=2528, output_tokens=504
23:18:14,778 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:18:14,929 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:18:14,933 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:18:14,942 datashaper.workflow.workflow INFO executing verb create_final_documents
23:18:14,949 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:18:14,969 graphrag.index.cli INFO All workflows completed successfully.
23:22:12,510 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:22:12,512 graphrag.index.cli INFO Starting pipeline run for: 20241101-232212, dryrun=False
23:22:12,512 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:22:12,514 graphrag.index.create_pipeline_config INFO skipping workflows 
23:22:12,514 graphrag.index.run.run INFO Running pipeline
23:22:12,514 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:22:12,516 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:22:12,516 graphrag.index.input.load_input INFO using file storage for input
23:22:12,517 graphrag.index.input.csv INFO Loading csv files from input_eval
23:22:12,517 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:22:12,521 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:22:12,521 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:22:12,522 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:22:12,523 graphrag.index.run.run INFO Final # of rows loaded: 1
23:22:12,640 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:22:12,642 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:22:13,48 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:22:13,178 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:22:13,178 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:22:13,185 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:22:13,187 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:22:13,226 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:22:13,226 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:22:15,927 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:15,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.704779279883951. input_tokens=2087, output_tokens=527
23:22:19,897 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:19,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.967061899136752. input_tokens=43, output_tokens=961
23:22:20,373 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:20,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.473459605127573. input_tokens=2, output_tokens=1
23:22:23,299 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:23,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.9244563370011747. input_tokens=43, output_tokens=961
23:22:23,959 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:23,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6583006149157882. input_tokens=2, output_tokens=1
23:22:27,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:27,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 3.7185818371362984. input_tokens=43, output_tokens=1132
23:22:28,340 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:28,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.657983188983053. input_tokens=2, output_tokens=1
23:22:32,449 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:32,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 4.107594604836777. input_tokens=43, output_tokens=1312
23:22:33,129 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:33,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6776290431153029. input_tokens=2, output_tokens=1
23:22:37,777 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:37,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 4.645977468928322. input_tokens=43, output_tokens=1477
23:22:38,509 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:38,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.7292123457882553. input_tokens=2, output_tokens=1
23:22:43,254 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:43,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 4.743657708866522. input_tokens=43, output_tokens=1543
23:22:43,997 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:43,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.7393617010675371. input_tokens=2, output_tokens=1
23:22:49,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:49,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 5.027446410851553. input_tokens=43, output_tokens=1614
23:22:49,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:49,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5562717339489609. input_tokens=202, output_tokens=35
23:22:50,171 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:50,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.121914351126179. input_tokens=181, output_tokens=25
23:22:50,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:50,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1346694079693407. input_tokens=184, output_tokens=28
23:22:50,216 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:50,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1722691641189158. input_tokens=199, output_tokens=34
23:22:50,223 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:50,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1726197560783476. input_tokens=156, output_tokens=16
23:22:50,243 root INFO Starting preprocessing of transition probabilities on graph with 20 nodes and 22 edges
23:22:50,243 root INFO Starting at time 1730499770.243939
23:22:50,243 root INFO Beginning preprocessing of transition probabilities for 20 vertices
23:22:50,243 root INFO Completed 1 / 20 vertices
23:22:50,244 root INFO Completed 3 / 20 vertices
23:22:50,244 root INFO Completed 5 / 20 vertices
23:22:50,244 root INFO Completed 7 / 20 vertices
23:22:50,245 root INFO Completed 9 / 20 vertices
23:22:50,246 root INFO Completed 11 / 20 vertices
23:22:50,247 root INFO Completed 13 / 20 vertices
23:22:50,247 root INFO Completed 15 / 20 vertices
23:22:50,248 root INFO Completed 17 / 20 vertices
23:22:50,248 root INFO Completed 19 / 20 vertices
23:22:50,249 root INFO Completed preprocessing of transition probabilities for vertices
23:22:50,249 root INFO Beginning preprocessing of transition probabilities for 22 edges
23:22:50,250 root INFO Completed 1 / 22 edges
23:22:50,251 root INFO Completed 3 / 22 edges
23:22:50,251 root INFO Completed 5 / 22 edges
23:22:50,252 root INFO Completed 7 / 22 edges
23:22:50,253 root INFO Completed 9 / 22 edges
23:22:50,253 root INFO Completed 11 / 22 edges
23:22:50,254 root INFO Completed 13 / 22 edges
23:22:50,254 root INFO Completed 15 / 22 edges
23:22:50,254 root INFO Completed 17 / 22 edges
23:22:50,255 root INFO Completed 19 / 22 edges
23:22:50,255 root INFO Completed 21 / 22 edges
23:22:50,256 root INFO Completed preprocessing of transition probabilities for edges
23:22:50,257 root INFO Simulating walks on graph at time 1730499770.2571664
23:22:50,257 root INFO Walk iteration: 1/10
23:22:50,258 root INFO Walk iteration: 2/10
23:22:50,259 root INFO Walk iteration: 3/10
23:22:50,260 root INFO Walk iteration: 4/10
23:22:50,261 root INFO Walk iteration: 5/10
23:22:50,261 root INFO Walk iteration: 6/10
23:22:50,262 root INFO Walk iteration: 7/10
23:22:50,262 root INFO Walk iteration: 8/10
23:22:50,263 root INFO Walk iteration: 9/10
23:22:50,263 root INFO Walk iteration: 10/10
23:22:50,264 root INFO Learning embeddings at time 1730499770.2644508
23:22:50,264 gensim.models.word2vec INFO collecting all words and their counts
23:22:50,265 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:22:50,265 gensim.models.word2vec INFO collected 20 word types from a corpus of 3360 raw words and 200 sentences
23:22:50,265 gensim.models.word2vec INFO Creating a fresh vocabulary
23:22:50,265 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 20 unique words (100.00% of original 20, drops 0)', 'datetime': '2024-11-01T23:22:50.265969', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,266 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3360 word corpus (100.00% of original 3360, drops 0)', 'datetime': '2024-11-01T23:22:50.266418', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,267 gensim.models.word2vec INFO deleting the raw counts dictionary of 20 items
23:22:50,267 gensim.models.word2vec INFO sample=0.001 downsamples 20 most-common words
23:22:50,268 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 462.18245336966845 word corpus (13.8%% of prior 3360)', 'datetime': '2024-11-01T23:22:50.268411', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,269 gensim.models.word2vec INFO estimated required memory for 20 words and 1536 dimensions: 255760 bytes
23:22:50,269 gensim.models.word2vec INFO resetting layer weights
23:22:50,270 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:22:50.270038', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:22:50,270 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 20 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:22:50.270492', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:22:50,275 gensim.models.word2vec INFO EPOCH 0: training on 3360 raw words (472 effective words) took 0.0s, 191083 effective words/s
23:22:50,278 gensim.models.word2vec INFO EPOCH 1: training on 3360 raw words (446 effective words) took 0.0s, 191236 effective words/s
23:22:50,282 gensim.models.word2vec INFO EPOCH 2: training on 3360 raw words (445 effective words) took 0.0s, 159381 effective words/s
23:22:50,282 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10080 raw words (1363 effective words) took 0.0s, 116747 effective words/s', 'datetime': '2024-11-01T23:22:50.282797', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:22:50,283 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:22:50.283494', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:22:50,284 root INFO Completed. Ending time is 1730499770.2842035 Elapsed time is -0.04026460647583008
23:22:50,296 root INFO Starting preprocessing of transition probabilities on graph with 20 nodes and 22 edges
23:22:50,296 root INFO Starting at time 1730499770.2968216
23:22:50,296 root INFO Beginning preprocessing of transition probabilities for 20 vertices
23:22:50,297 root INFO Completed 1 / 20 vertices
23:22:50,298 root INFO Completed 3 / 20 vertices
23:22:50,299 root INFO Completed 5 / 20 vertices
23:22:50,299 root INFO Completed 7 / 20 vertices
23:22:50,299 root INFO Completed 9 / 20 vertices
23:22:50,300 root INFO Completed 11 / 20 vertices
23:22:50,300 root INFO Completed 13 / 20 vertices
23:22:50,300 root INFO Completed 15 / 20 vertices
23:22:50,300 root INFO Completed 17 / 20 vertices
23:22:50,300 root INFO Completed 19 / 20 vertices
23:22:50,300 root INFO Completed preprocessing of transition probabilities for vertices
23:22:50,301 root INFO Beginning preprocessing of transition probabilities for 22 edges
23:22:50,301 root INFO Completed 1 / 22 edges
23:22:50,301 root INFO Completed 3 / 22 edges
23:22:50,302 root INFO Completed 5 / 22 edges
23:22:50,303 root INFO Completed 7 / 22 edges
23:22:50,303 root INFO Completed 9 / 22 edges
23:22:50,304 root INFO Completed 11 / 22 edges
23:22:50,305 root INFO Completed 13 / 22 edges
23:22:50,306 root INFO Completed 15 / 22 edges
23:22:50,306 root INFO Completed 17 / 22 edges
23:22:50,307 root INFO Completed 19 / 22 edges
23:22:50,308 root INFO Completed 21 / 22 edges
23:22:50,309 root INFO Completed preprocessing of transition probabilities for edges
23:22:50,309 root INFO Simulating walks on graph at time 1730499770.3096402
23:22:50,310 root INFO Walk iteration: 1/10
23:22:50,312 root INFO Walk iteration: 2/10
23:22:50,314 root INFO Walk iteration: 3/10
23:22:50,316 root INFO Walk iteration: 4/10
23:22:50,318 root INFO Walk iteration: 5/10
23:22:50,320 root INFO Walk iteration: 6/10
23:22:50,321 root INFO Walk iteration: 7/10
23:22:50,322 root INFO Walk iteration: 8/10
23:22:50,323 root INFO Walk iteration: 9/10
23:22:50,324 root INFO Walk iteration: 10/10
23:22:50,325 root INFO Learning embeddings at time 1730499770.3251555
23:22:50,325 gensim.models.word2vec INFO collecting all words and their counts
23:22:50,325 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:22:50,326 gensim.models.word2vec INFO collected 20 word types from a corpus of 3360 raw words and 200 sentences
23:22:50,327 gensim.models.word2vec INFO Creating a fresh vocabulary
23:22:50,328 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 20 unique words (100.00% of original 20, drops 0)', 'datetime': '2024-11-01T23:22:50.328381', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,329 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3360 word corpus (100.00% of original 3360, drops 0)', 'datetime': '2024-11-01T23:22:50.329127', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,330 gensim.models.word2vec INFO deleting the raw counts dictionary of 20 items
23:22:50,330 gensim.models.word2vec INFO sample=0.001 downsamples 20 most-common words
23:22:50,331 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 462.18245336966845 word corpus (13.8%% of prior 3360)', 'datetime': '2024-11-01T23:22:50.331726', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:22:50,332 gensim.models.word2vec INFO estimated required memory for 20 words and 1536 dimensions: 255760 bytes
23:22:50,333 gensim.models.word2vec INFO resetting layer weights
23:22:50,334 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:22:50.334049', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:22:50,334 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 20 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:22:50.334660', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:22:50,341 gensim.models.word2vec INFO EPOCH 0: training on 3360 raw words (472 effective words) took 0.0s, 119405 effective words/s
23:22:50,349 gensim.models.word2vec INFO EPOCH 1: training on 3360 raw words (446 effective words) took 0.0s, 111077 effective words/s
23:22:50,357 gensim.models.word2vec INFO EPOCH 2: training on 3360 raw words (445 effective words) took 0.0s, 81901 effective words/s
23:22:50,357 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 10080 raw words (1363 effective words) took 0.0s, 60341 effective words/s', 'datetime': '2024-11-01T23:22:50.357953', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:22:50,358 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:22:50.358839', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:22:50,358 root INFO Completed. Ending time is 1730499770.358948 Elapsed time is -0.06212639808654785
23:22:50,415 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:22:50,556 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:22:50,556 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:22:50,575 datashaper.workflow.workflow INFO executing verb create_final_entities
23:22:50,580 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:22:50,617 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:22:50,617 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:22:50,618 graphrag.index.operations.embed_text.strategies.openai INFO embedding 20 inputs via 20 snippets using 2 batches. max_batch_size=16, max_tokens=8191
23:22:51,619 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:22:51,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0101432709489018. input_tokens=102, output_tokens=0
23:22:51,909 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:22:51,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3138587391003966. input_tokens=570, output_tokens=0
23:22:51,939 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:22:52,84 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:22:52,84 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:22:52,93 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:22:54,759 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:22:54,929 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:22:54,929 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:22:54,939 datashaper.workflow.workflow INFO executing verb create_final_communities
23:22:54,951 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:22:55,84 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
23:22:55,84 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:22:55,91 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:22:55,98 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:22:55,105 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:22:55,243 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
23:22:55,243 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:22:55,247 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:22:55,253 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:22:55,260 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:22:55,270 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:22:55,405 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:22:55,405 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:22:55,409 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:22:55,417 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:22:55,421 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 14
23:22:55,432 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 38
23:22:57,223 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:57,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7777086258865893. input_tokens=2060, output_tokens=256
23:22:58,997 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:59,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5490865730680525. input_tokens=2725, output_tokens=670
23:23:02,588 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:23:02,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.575324939098209. input_tokens=2189, output_tokens=552
23:23:05,878 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:23:05,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.8673906130716205. input_tokens=2955, output_tokens=656
23:23:10,838 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:23:10,839 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 11.822132613975555. input_tokens=2164, output_tokens=346
23:23:10,845 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:23:10,995 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:23:10,995 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:23:11,4 datashaper.workflow.workflow INFO executing verb create_final_documents
23:23:11,10 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:23:11,28 graphrag.index.cli INFO All workflows completed successfully.
23:25:21,614 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:25:21,615 graphrag.index.cli INFO Starting pipeline run for: 20241101-232521, dryrun=False
23:25:21,615 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:25:21,617 graphrag.index.create_pipeline_config INFO skipping workflows 
23:25:21,617 graphrag.index.run.run INFO Running pipeline
23:25:21,618 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:25:21,619 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:25:21,620 graphrag.index.input.load_input INFO using file storage for input
23:25:21,621 graphrag.index.input.csv INFO Loading csv files from input_eval
23:25:21,621 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:25:21,625 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:25:21,625 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:25:21,626 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:25:21,627 graphrag.index.run.run INFO Final # of rows loaded: 1
23:25:21,738 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:25:21,740 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:25:22,156 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:25:22,285 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:25:22,286 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:25:22,293 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:25:22,295 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:25:22,333 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:25:22,333 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:25:25,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:25,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.76657833205536. input_tokens=2087, output_tokens=527
23:25:29,144 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:29,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.044984339969233. input_tokens=46, output_tokens=954
23:25:29,658 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:29,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.510576002066955. input_tokens=2, output_tokens=1
23:25:32,545 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:32,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.8865407230332494. input_tokens=46, output_tokens=954
23:25:33,175 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:33,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6267275209538639. input_tokens=2, output_tokens=1
23:25:36,136 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:36,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 2.9603413799777627. input_tokens=46, output_tokens=714
23:25:36,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:36,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6787685309536755. input_tokens=2, output_tokens=1
23:25:40,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:40,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 3.460455466993153. input_tokens=46, output_tokens=990
23:25:40,906 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:40,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.6255449561867863. input_tokens=2, output_tokens=1
23:25:44,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:44,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 3.8482434030156583. input_tokens=46, output_tokens=1220
23:25:45,417 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:45,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6576629630289972. input_tokens=2, output_tokens=1
23:25:50,216 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:50,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 4.797912260051817. input_tokens=46, output_tokens=1488
23:25:50,927 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:50,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.7079280260950327. input_tokens=2, output_tokens=1
23:25:55,901 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:55,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 4.972551480866969. input_tokens=46, output_tokens=1658
23:25:56,361 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:56,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44141117203980684. input_tokens=192, output_tokens=33
23:25:56,558 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:56,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6347094080410898. input_tokens=197, output_tokens=34
23:25:56,568 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:56,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6416304861195385. input_tokens=185, output_tokens=27
23:25:57,67 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:57,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.145142253022641. input_tokens=176, output_tokens=25
23:25:57,74 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:57,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1456577701028436. input_tokens=205, output_tokens=38
23:25:57,88 root INFO Starting preprocessing of transition probabilities on graph with 27 nodes and 33 edges
23:25:57,88 root INFO Starting at time 1730499957.0883677
23:25:57,88 root INFO Beginning preprocessing of transition probabilities for 27 vertices
23:25:57,88 root INFO Completed 1 / 27 vertices
23:25:57,88 root INFO Completed 3 / 27 vertices
23:25:57,89 root INFO Completed 5 / 27 vertices
23:25:57,89 root INFO Completed 7 / 27 vertices
23:25:57,89 root INFO Completed 9 / 27 vertices
23:25:57,90 root INFO Completed 11 / 27 vertices
23:25:57,91 root INFO Completed 13 / 27 vertices
23:25:57,92 root INFO Completed 15 / 27 vertices
23:25:57,93 root INFO Completed 17 / 27 vertices
23:25:57,93 root INFO Completed 19 / 27 vertices
23:25:57,94 root INFO Completed 21 / 27 vertices
23:25:57,95 root INFO Completed 23 / 27 vertices
23:25:57,96 root INFO Completed 25 / 27 vertices
23:25:57,97 root INFO Completed 27 / 27 vertices
23:25:57,98 root INFO Completed preprocessing of transition probabilities for vertices
23:25:57,98 root INFO Beginning preprocessing of transition probabilities for 33 edges
23:25:57,98 root INFO Completed 1 / 33 edges
23:25:57,98 root INFO Completed 4 / 33 edges
23:25:57,99 root INFO Completed 7 / 33 edges
23:25:57,100 root INFO Completed 10 / 33 edges
23:25:57,100 root INFO Completed 13 / 33 edges
23:25:57,101 root INFO Completed 16 / 33 edges
23:25:57,102 root INFO Completed 19 / 33 edges
23:25:57,102 root INFO Completed 22 / 33 edges
23:25:57,103 root INFO Completed 25 / 33 edges
23:25:57,104 root INFO Completed 28 / 33 edges
23:25:57,105 root INFO Completed 31 / 33 edges
23:25:57,106 root INFO Completed preprocessing of transition probabilities for edges
23:25:57,106 root INFO Simulating walks on graph at time 1730499957.1066408
23:25:57,107 root INFO Walk iteration: 1/10
23:25:57,110 root INFO Walk iteration: 2/10
23:25:57,114 root INFO Walk iteration: 3/10
23:25:57,117 root INFO Walk iteration: 4/10
23:25:57,119 root INFO Walk iteration: 5/10
23:25:57,121 root INFO Walk iteration: 6/10
23:25:57,123 root INFO Walk iteration: 7/10
23:25:57,124 root INFO Walk iteration: 8/10
23:25:57,125 root INFO Walk iteration: 9/10
23:25:57,126 root INFO Walk iteration: 10/10
23:25:57,127 root INFO Learning embeddings at time 1730499957.1276557
23:25:57,128 gensim.models.word2vec INFO collecting all words and their counts
23:25:57,128 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:25:57,129 gensim.models.word2vec INFO collected 27 word types from a corpus of 5160 raw words and 270 sentences
23:25:57,130 gensim.models.word2vec INFO Creating a fresh vocabulary
23:25:57,131 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 27 unique words (100.00% of original 27, drops 0)', 'datetime': '2024-11-01T23:25:57.131196', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:25:57,131 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5160 word corpus (100.00% of original 5160, drops 0)', 'datetime': '2024-11-01T23:25:57.131944', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:25:57,132 gensim.models.word2vec INFO deleting the raw counts dictionary of 27 items
23:25:57,133 gensim.models.word2vec INFO sample=0.001 downsamples 27 most-common words
23:25:57,133 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 919.3140552707863 word corpus (17.8%% of prior 5160)', 'datetime': '2024-11-01T23:25:57.133659', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:25:57,134 gensim.models.word2vec INFO estimated required memory for 27 words and 1536 dimensions: 345276 bytes
23:25:57,135 gensim.models.word2vec INFO resetting layer weights
23:25:57,135 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:25:57.135888', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:25:57,136 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 27 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:25:57.136282', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:25:57,146 gensim.models.word2vec INFO EPOCH 0: training on 5160 raw words (940 effective words) took 0.0s, 112871 effective words/s
23:25:57,162 gensim.models.word2vec INFO EPOCH 1: training on 5160 raw words (919 effective words) took 0.0s, 72897 effective words/s
23:25:57,177 gensim.models.word2vec INFO EPOCH 2: training on 5160 raw words (942 effective words) took 0.0s, 72543 effective words/s
23:25:57,177 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 15480 raw words (2801 effective words) took 0.0s, 69529 effective words/s', 'datetime': '2024-11-01T23:25:57.177200', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:25:57,178 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=27, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:25:57.178261', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:25:57,179 root INFO Completed. Ending time is 1730499957.1792288 Elapsed time is -0.09086108207702637
23:25:57,225 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:25:57,367 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:25:57,367 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:25:57,374 datashaper.workflow.workflow INFO executing verb create_final_entities
23:25:57,378 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:25:57,415 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:25:57,415 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:25:57,417 graphrag.index.operations.embed_text.strategies.openai INFO embedding 29 inputs via 28 snippets using 2 batches. max_batch_size=16, max_tokens=8191
23:25:58,581 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:25:58,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1818382309284061. input_tokens=408, output_tokens=0
23:25:58,700 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:25:58,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3063095631077886. input_tokens=574, output_tokens=0
23:25:58,729 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:25:58,885 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:25:58,885 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:25:58,894 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:26:01,148 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:26:01,326 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:26:01,326 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:26:01,335 datashaper.workflow.workflow INFO executing verb create_final_communities
23:26:01,346 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:26:01,480 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:26:01,481 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:26:01,486 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:26:01,495 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:26:01,501 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:26:01,642 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
23:26:01,643 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:26:01,648 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:26:01,650 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:26:01,657 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:26:01,666 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:26:01,801 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:26:01,801 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:26:01,805 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:26:01,814 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:26:01,817 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 29
23:26:04,153 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:04,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3167781259398907. input_tokens=2094, output_tokens=343
23:26:04,447 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:04,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.608508609002456. input_tokens=2297, output_tokens=461
23:26:04,764 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:04,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.921618739143014. input_tokens=2263, output_tokens=430
23:26:04,790 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:04,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9498124348465353. input_tokens=2102, output_tokens=380
23:26:05,876 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:05,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.0424170850310475. input_tokens=2765, output_tokens=757
23:26:06,993 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:06,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8329029681626707. input_tokens=2420, output_tokens=518
23:26:07,21 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:26:07,166 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:26:07,166 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:26:07,177 datashaper.workflow.workflow INFO executing verb create_final_documents
23:26:07,183 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:26:07,200 graphrag.index.cli INFO All workflows completed successfully.
23:27:11,196 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:27:11,197 graphrag.index.cli INFO Starting pipeline run for: 20241101-232711, dryrun=False
23:27:11,197 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:27:11,199 graphrag.index.create_pipeline_config INFO skipping workflows 
23:27:11,199 graphrag.index.run.run INFO Running pipeline
23:27:11,200 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:27:11,201 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:27:11,201 graphrag.index.input.load_input INFO using file storage for input
23:27:11,202 graphrag.index.input.csv INFO Loading csv files from input_eval
23:27:11,202 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:27:11,206 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:27:11,206 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:27:11,207 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:27:11,207 graphrag.index.run.run INFO Final # of rows loaded: 1
23:27:11,318 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:27:11,321 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:27:11,651 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:27:11,777 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:27:11,778 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:27:11,785 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:27:11,786 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:27:11,826 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:27:11,826 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:27:14,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:14,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7250392381101847. input_tokens=2087, output_tokens=527
23:27:18,811 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:18,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.259527249028906. input_tokens=32, output_tokens=1258
23:27:19,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:19,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4963913010433316. input_tokens=2, output_tokens=1
23:27:25,375 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:25,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.064745167037472. input_tokens=32, output_tokens=1923
23:27:26,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:26,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6380909520667046. input_tokens=2, output_tokens=1
23:27:31,737 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:31,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 5.719697086140513. input_tokens=32, output_tokens=2009
23:27:32,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:32,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.7149111859034747. input_tokens=2, output_tokens=1
23:27:39,195 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:39,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 6.738127087941393. input_tokens=32, output_tokens=2238
23:27:39,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:39,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.751019046176225. input_tokens=2, output_tokens=1
23:27:49,194 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:49,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 9.242417578818277. input_tokens=32, output_tokens=3112
23:27:50,63 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:27:50,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.8632791370619088. input_tokens=2, output_tokens=1
23:28:02,40 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:02,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 11.976542562013492. input_tokens=32, output_tokens=4058
23:28:03,67 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:03,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.0228620739653707. input_tokens=2, output_tokens=1
23:28:15,350 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:15,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.28259957791306. input_tokens=32, output_tokens=4383
23:28:15,848 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:15,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48239285591989756. input_tokens=178, output_tokens=37
23:28:16,467 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:16,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0956298329401761. input_tokens=172, output_tokens=21
23:28:16,535 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:16,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1622343419585377. input_tokens=165, output_tokens=28
23:28:16,654 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:16,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2847127609420568. input_tokens=160, output_tokens=44
23:28:16,681 root INFO Starting preprocessing of transition probabilities on graph with 49 nodes and 48 edges
23:28:16,681 root INFO Starting at time 1730500096.6813653
23:28:16,681 root INFO Beginning preprocessing of transition probabilities for 49 vertices
23:28:16,681 root INFO Completed 1 / 49 vertices
23:28:16,681 root INFO Completed 5 / 49 vertices
23:28:16,681 root INFO Completed 9 / 49 vertices
23:28:16,682 root INFO Completed 13 / 49 vertices
23:28:16,683 root INFO Completed 17 / 49 vertices
23:28:16,684 root INFO Completed 21 / 49 vertices
23:28:16,684 root INFO Completed 25 / 49 vertices
23:28:16,685 root INFO Completed 29 / 49 vertices
23:28:16,686 root INFO Completed 33 / 49 vertices
23:28:16,687 root INFO Completed 37 / 49 vertices
23:28:16,688 root INFO Completed 41 / 49 vertices
23:28:16,689 root INFO Completed 45 / 49 vertices
23:28:16,689 root INFO Completed 49 / 49 vertices
23:28:16,689 root INFO Completed preprocessing of transition probabilities for vertices
23:28:16,689 root INFO Beginning preprocessing of transition probabilities for 48 edges
23:28:16,689 root INFO Completed 1 / 48 edges
23:28:16,689 root INFO Completed 5 / 48 edges
23:28:16,691 root INFO Completed 9 / 48 edges
23:28:16,691 root INFO Completed 13 / 48 edges
23:28:16,692 root INFO Completed 17 / 48 edges
23:28:16,693 root INFO Completed 21 / 48 edges
23:28:16,693 root INFO Completed 25 / 48 edges
23:28:16,694 root INFO Completed 29 / 48 edges
23:28:16,695 root INFO Completed 33 / 48 edges
23:28:16,695 root INFO Completed 37 / 48 edges
23:28:16,696 root INFO Completed 41 / 48 edges
23:28:16,697 root INFO Completed 45 / 48 edges
23:28:16,697 root INFO Completed preprocessing of transition probabilities for edges
23:28:16,698 root INFO Simulating walks on graph at time 1730500096.6983252
23:28:16,698 root INFO Walk iteration: 1/10
23:28:16,700 root INFO Walk iteration: 2/10
23:28:16,702 root INFO Walk iteration: 3/10
23:28:16,704 root INFO Walk iteration: 4/10
23:28:16,705 root INFO Walk iteration: 5/10
23:28:16,706 root INFO Walk iteration: 6/10
23:28:16,707 root INFO Walk iteration: 7/10
23:28:16,708 root INFO Walk iteration: 8/10
23:28:16,709 root INFO Walk iteration: 9/10
23:28:16,710 root INFO Walk iteration: 10/10
23:28:16,711 root INFO Learning embeddings at time 1730500096.711084
23:28:16,711 gensim.models.word2vec INFO collecting all words and their counts
23:28:16,711 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:28:16,712 gensim.models.word2vec INFO collected 49 word types from a corpus of 8000 raw words and 490 sentences
23:28:16,712 gensim.models.word2vec INFO Creating a fresh vocabulary
23:28:16,712 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 49 unique words (100.00% of original 49, drops 0)', 'datetime': '2024-11-01T23:28:16.712461', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,713 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 8000 word corpus (100.00% of original 8000, drops 0)', 'datetime': '2024-11-01T23:28:16.713050', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,713 gensim.models.word2vec INFO deleting the raw counts dictionary of 49 items
23:28:16,713 gensim.models.word2vec INFO sample=0.001 downsamples 49 most-common words
23:28:16,713 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1923.6963226049052 word corpus (24.0%% of prior 8000)', 'datetime': '2024-11-01T23:28:16.713790', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,714 gensim.models.word2vec INFO estimated required memory for 49 words and 1536 dimensions: 626612 bytes
23:28:16,715 gensim.models.word2vec INFO resetting layer weights
23:28:16,715 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:28:16.715358', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:28:16,715 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 49 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:28:16.715784', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:28:16,724 gensim.models.word2vec INFO EPOCH 0: training on 8000 raw words (1946 effective words) took 0.0s, 273621 effective words/s
23:28:16,734 gensim.models.word2vec INFO EPOCH 1: training on 8000 raw words (1950 effective words) took 0.0s, 249567 effective words/s
23:28:16,744 gensim.models.word2vec INFO EPOCH 2: training on 8000 raw words (1913 effective words) took 0.0s, 205488 effective words/s
23:28:16,744 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 24000 raw words (5809 effective words) took 0.0s, 203787 effective words/s', 'datetime': '2024-11-01T23:28:16.744882', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:28:16,745 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:28:16.745588', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:28:16,745 root INFO Completed. Ending time is 1730500096.7456388 Elapsed time is -0.06427359580993652
23:28:16,753 root INFO Starting preprocessing of transition probabilities on graph with 49 nodes and 48 edges
23:28:16,754 root INFO Starting at time 1730500096.7540538
23:28:16,754 root INFO Beginning preprocessing of transition probabilities for 49 vertices
23:28:16,754 root INFO Completed 1 / 49 vertices
23:28:16,754 root INFO Completed 5 / 49 vertices
23:28:16,755 root INFO Completed 9 / 49 vertices
23:28:16,756 root INFO Completed 13 / 49 vertices
23:28:16,756 root INFO Completed 17 / 49 vertices
23:28:16,756 root INFO Completed 21 / 49 vertices
23:28:16,757 root INFO Completed 25 / 49 vertices
23:28:16,757 root INFO Completed 29 / 49 vertices
23:28:16,758 root INFO Completed 33 / 49 vertices
23:28:16,758 root INFO Completed 37 / 49 vertices
23:28:16,758 root INFO Completed 41 / 49 vertices
23:28:16,758 root INFO Completed 45 / 49 vertices
23:28:16,759 root INFO Completed 49 / 49 vertices
23:28:16,759 root INFO Completed preprocessing of transition probabilities for vertices
23:28:16,760 root INFO Beginning preprocessing of transition probabilities for 48 edges
23:28:16,760 root INFO Completed 1 / 48 edges
23:28:16,760 root INFO Completed 5 / 48 edges
23:28:16,761 root INFO Completed 9 / 48 edges
23:28:16,762 root INFO Completed 13 / 48 edges
23:28:16,763 root INFO Completed 17 / 48 edges
23:28:16,763 root INFO Completed 21 / 48 edges
23:28:16,764 root INFO Completed 25 / 48 edges
23:28:16,764 root INFO Completed 29 / 48 edges
23:28:16,765 root INFO Completed 33 / 48 edges
23:28:16,766 root INFO Completed 37 / 48 edges
23:28:16,766 root INFO Completed 41 / 48 edges
23:28:16,767 root INFO Completed 45 / 48 edges
23:28:16,768 root INFO Completed preprocessing of transition probabilities for edges
23:28:16,768 root INFO Simulating walks on graph at time 1730500096.7687008
23:28:16,769 root INFO Walk iteration: 1/10
23:28:16,773 root INFO Walk iteration: 2/10
23:28:16,775 root INFO Walk iteration: 3/10
23:28:16,777 root INFO Walk iteration: 4/10
23:28:16,779 root INFO Walk iteration: 5/10
23:28:16,780 root INFO Walk iteration: 6/10
23:28:16,781 root INFO Walk iteration: 7/10
23:28:16,782 root INFO Walk iteration: 8/10
23:28:16,783 root INFO Walk iteration: 9/10
23:28:16,784 root INFO Walk iteration: 10/10
23:28:16,785 root INFO Learning embeddings at time 1730500096.7859032
23:28:16,786 gensim.models.word2vec INFO collecting all words and their counts
23:28:16,786 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:28:16,786 gensim.models.word2vec INFO collected 49 word types from a corpus of 8000 raw words and 490 sentences
23:28:16,787 gensim.models.word2vec INFO Creating a fresh vocabulary
23:28:16,787 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 49 unique words (100.00% of original 49, drops 0)', 'datetime': '2024-11-01T23:28:16.787368', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,787 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 8000 word corpus (100.00% of original 8000, drops 0)', 'datetime': '2024-11-01T23:28:16.787847', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,787 gensim.models.word2vec INFO deleting the raw counts dictionary of 49 items
23:28:16,788 gensim.models.word2vec INFO sample=0.001 downsamples 49 most-common words
23:28:16,788 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1923.6963226049052 word corpus (24.0%% of prior 8000)', 'datetime': '2024-11-01T23:28:16.788814', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:28:16,789 gensim.models.word2vec INFO estimated required memory for 49 words and 1536 dimensions: 626612 bytes
23:28:16,790 gensim.models.word2vec INFO resetting layer weights
23:28:16,790 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:28:16.790400', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:28:16,790 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 49 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:28:16.790786', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:28:16,800 gensim.models.word2vec INFO EPOCH 0: training on 8000 raw words (1946 effective words) took 0.0s, 252248 effective words/s
23:28:16,819 gensim.models.word2vec INFO EPOCH 1: training on 8000 raw words (1950 effective words) took 0.0s, 104791 effective words/s
23:28:16,835 gensim.models.word2vec INFO EPOCH 2: training on 8000 raw words (1913 effective words) took 0.0s, 147999 effective words/s
23:28:16,835 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 24000 raw words (5809 effective words) took 0.0s, 131048 effective words/s', 'datetime': '2024-11-01T23:28:16.835726', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:28:16,835 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=49, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:28:16.835763', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:28:16,836 root INFO Completed. Ending time is 1730500096.8365824 Elapsed time is -0.08252859115600586
23:28:16,897 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:28:17,67 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:28:17,68 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:28:17,78 datashaper.workflow.workflow INFO executing verb create_final_entities
23:28:17,84 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:28:17,121 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:28:17,121 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:28:17,123 graphrag.index.operations.embed_text.strategies.openai INFO embedding 54 inputs via 54 snippets using 4 batches. max_batch_size=16, max_tokens=8191
23:28:17,970 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:28:17,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8553899810649455. input_tokens=139, output_tokens=0
23:28:17,982 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:28:18,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8828301439061761. input_tokens=472, output_tokens=0
23:28:18,713 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:28:18,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6116749159991741. input_tokens=344, output_tokens=0
23:28:18,738 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:28:18,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6375011301133782. input_tokens=360, output_tokens=0
23:28:18,766 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:28:18,942 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:28:18,943 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:28:18,953 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:28:21,651 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:28:21,828 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:28:21,828 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:28:21,840 datashaper.workflow.workflow INFO executing verb create_final_communities
23:28:21,856 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:28:21,992 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:28:21,992 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:28:22,0 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:28:22,11 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:28:22,19 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:28:22,157 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
23:28:22,157 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:28:22,164 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:28:22,166 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:28:22,173 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:28:22,183 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:28:22,318 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
23:28:22,319 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:28:22,326 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:28:22,334 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:28:22,341 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 20
23:28:22,355 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 156
23:28:24,544 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:24,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.158580594928935. input_tokens=2053, output_tokens=244
23:28:24,671 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:24,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.286750679835677. input_tokens=2136, output_tokens=294
23:28:26,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:26,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7880048418883234. input_tokens=2590, output_tokens=643
23:28:28,149 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:28,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.962483205832541. input_tokens=2043, output_tokens=302
23:28:28,795 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:28,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.608034464996308. input_tokens=2309, output_tokens=415
23:28:29,119 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:29,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9276171838864684. input_tokens=2317, output_tokens=428
23:28:29,597 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:29,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.4433885149192065. input_tokens=2117, output_tokens=214
23:28:29,837 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:29,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.654043046059087. input_tokens=3094, output_tokens=703
23:28:31,307 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:31,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.497449839953333. input_tokens=2326, output_tokens=439
23:28:31,458 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:31,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.268983011832461. input_tokens=2228, output_tokens=424
23:28:31,913 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:31,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.784194917883724. input_tokens=2133, output_tokens=392
23:28:31,924 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:28:32,71 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:28:32,72 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:28:32,82 datashaper.workflow.workflow INFO executing verb create_final_documents
23:28:32,89 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:28:32,105 graphrag.index.cli INFO All workflows completed successfully.
23:29:11,142 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:29:11,143 graphrag.index.cli INFO Starting pipeline run for: 20241101-232911, dryrun=False
23:29:11,143 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:29:11,145 graphrag.index.create_pipeline_config INFO skipping workflows 
23:29:11,145 graphrag.index.run.run INFO Running pipeline
23:29:11,145 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:29:11,146 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:29:11,146 graphrag.index.input.load_input INFO using file storage for input
23:29:11,148 graphrag.index.input.csv INFO Loading csv files from input_eval
23:29:11,148 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:29:11,151 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:29:11,151 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:29:11,152 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:29:11,152 graphrag.index.run.run INFO Final # of rows loaded: 1
23:29:11,264 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:29:11,266 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:29:11,771 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:29:11,897 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:29:11,898 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:29:11,905 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:29:11,906 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:29:11,944 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:29:11,944 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:29:14,665 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:14,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.724980544997379. input_tokens=2087, output_tokens=527
23:29:18,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:18,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.287446049042046. input_tokens=32, output_tokens=1258
23:29:19,428 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:19,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.46937832701951265. input_tokens=2, output_tokens=1
23:29:25,406 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:25,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.976676892954856. input_tokens=32, output_tokens=1919
23:29:26,60 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:26,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.6509827140253037. input_tokens=2, output_tokens=1
23:29:32,305 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:32,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.244383976096287. input_tokens=32, output_tokens=2152
23:29:33,3 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:33,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6927224989049137. input_tokens=2, output_tokens=1
23:29:44,751 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:44,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 11.746960366144776. input_tokens=32, output_tokens=3930
23:29:45,661 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:45,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.9069188009016216. input_tokens=2, output_tokens=1
23:30:14,831 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:14,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 29.168966277036816. input_tokens=32, output_tokens=4326
23:30:15,758 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:15,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9212585580535233. input_tokens=2, output_tokens=1
23:30:28,61 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:28,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.302428488153964. input_tokens=32, output_tokens=4326
23:30:32,24 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:32,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 3.9535764260217547. input_tokens=2, output_tokens=1
23:30:44,372 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:44,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.346801287028939. input_tokens=32, output_tokens=4326
23:30:44,884 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:44,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4902262771502137. input_tokens=177, output_tokens=36
23:30:45,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.587674753041938. input_tokens=162, output_tokens=44
23:30:45,669 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2719095218926668. input_tokens=168, output_tokens=42
23:30:45,677 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2762663778848946. input_tokens=172, output_tokens=47
23:30:45,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3657457020599395. input_tokens=160, output_tokens=44
23:30:45,813 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4155402919277549. input_tokens=184, output_tokens=85
23:30:45,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4974812709260732. input_tokens=159, output_tokens=35
23:30:46,128 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45419535087421536. input_tokens=170, output_tokens=20
23:30:46,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5174852379132062. input_tokens=159, output_tokens=38
23:30:46,375 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5616763238795102. input_tokens=179, output_tokens=47
23:30:46,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4858097250107676. input_tokens=156, output_tokens=29
23:30:46,536 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7662728049326688. input_tokens=164, output_tokens=67
23:30:46,851 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6552225060295314. input_tokens=159, output_tokens=43
23:30:46,912 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:46,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7740883480291814. input_tokens=157, output_tokens=48
23:30:47,32 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:47,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.650388632202521. input_tokens=155, output_tokens=39
23:30:48,957 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:48,957 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:49,361 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:49,361 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:50,473 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:50,473 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:50,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:50,638 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:50,639 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:50,640 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:50,668 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:50,668 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:51,345 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:51,346 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:51,979 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:51,980 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:52,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:52,461 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:52,822 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:52,823 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:52,849 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:52,850 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:53,56 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:53,57 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:54,58 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:54,60 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:55,179 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:55,180 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:55,376 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:55,377 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:57,456 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:57,457 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:57,721 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:57,722 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:58,431 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:58,431 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:30:59,736 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:30:59,737 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:00,341 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:00,342 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:06,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:06,245 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:06,669 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:06,670 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:06,824 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:06,825 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:08,105 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:08,106 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:08,685 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:08,686 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:16,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:16,271 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:16,695 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:16,696 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:16,842 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:16,842 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:18,129 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:18,130 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:18,698 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:18,698 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:26,296 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:26,296 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:26,724 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:26,724 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:26,862 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:26,862 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:28,149 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:28,150 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:28,724 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:28,725 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:36,328 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:36,328 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:36,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:36,751 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:36,886 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:36,887 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:38,175 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:38,176 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:38,752 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:38,753 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:46,363 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
23:31:46,364 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
23:31:47,782 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:47,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 1.0315353891346604. input_tokens=179, output_tokens=25
23:31:47,903 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:47,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 1.015588924055919. input_tokens=176, output_tokens=25
23:31:48,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:48,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4561926079913974. input_tokens=167, output_tokens=23
23:31:48,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:48,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6063629810232669. input_tokens=163, output_tokens=39
23:31:48,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:48,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 0.6421125170309097. input_tokens=165, output_tokens=23
23:31:48,867 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:48,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6204679259099066. input_tokens=157, output_tokens=37
23:31:49,21 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:49,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.503223760984838. input_tokens=157, output_tokens=26
23:31:49,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:49,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6332151021342725. input_tokens=165, output_tokens=50
23:31:49,987 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:49,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 8 retries took 1.2336415038444102. input_tokens=165, output_tokens=39
23:31:57,709 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:31:57,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 9 retries took 1.3388103949837387. input_tokens=172, output_tokens=84
23:31:57,735 root INFO Starting preprocessing of transition probabilities on graph with 47 nodes and 47 edges
23:31:57,735 root INFO Starting at time 1730500317.7354198
23:31:57,735 root INFO Beginning preprocessing of transition probabilities for 47 vertices
23:31:57,735 root INFO Completed 1 / 47 vertices
23:31:57,735 root INFO Completed 5 / 47 vertices
23:31:57,736 root INFO Completed 9 / 47 vertices
23:31:57,736 root INFO Completed 13 / 47 vertices
23:31:57,736 root INFO Completed 17 / 47 vertices
23:31:57,737 root INFO Completed 21 / 47 vertices
23:31:57,738 root INFO Completed 25 / 47 vertices
23:31:57,738 root INFO Completed 29 / 47 vertices
23:31:57,739 root INFO Completed 33 / 47 vertices
23:31:57,740 root INFO Completed 37 / 47 vertices
23:31:57,740 root INFO Completed 41 / 47 vertices
23:31:57,741 root INFO Completed 45 / 47 vertices
23:31:57,741 root INFO Completed preprocessing of transition probabilities for vertices
23:31:57,742 root INFO Beginning preprocessing of transition probabilities for 47 edges
23:31:57,742 root INFO Completed 1 / 47 edges
23:31:57,742 root INFO Completed 5 / 47 edges
23:31:57,743 root INFO Completed 9 / 47 edges
23:31:57,744 root INFO Completed 13 / 47 edges
23:31:57,745 root INFO Completed 17 / 47 edges
23:31:57,745 root INFO Completed 21 / 47 edges
23:31:57,746 root INFO Completed 25 / 47 edges
23:31:57,746 root INFO Completed 29 / 47 edges
23:31:57,747 root INFO Completed 33 / 47 edges
23:31:57,747 root INFO Completed 37 / 47 edges
23:31:57,748 root INFO Completed 41 / 47 edges
23:31:57,749 root INFO Completed 45 / 47 edges
23:31:57,751 root INFO Completed preprocessing of transition probabilities for edges
23:31:57,751 root INFO Simulating walks on graph at time 1730500317.7516754
23:31:57,751 root INFO Walk iteration: 1/10
23:31:57,753 root INFO Walk iteration: 2/10
23:31:57,755 root INFO Walk iteration: 3/10
23:31:57,756 root INFO Walk iteration: 4/10
23:31:57,757 root INFO Walk iteration: 5/10
23:31:57,758 root INFO Walk iteration: 6/10
23:31:57,759 root INFO Walk iteration: 7/10
23:31:57,760 root INFO Walk iteration: 8/10
23:31:57,761 root INFO Walk iteration: 9/10
23:31:57,762 root INFO Walk iteration: 10/10
23:31:57,763 root INFO Learning embeddings at time 1730500317.7632306
23:31:57,763 gensim.models.word2vec INFO collecting all words and their counts
23:31:57,763 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:31:57,764 gensim.models.word2vec INFO collected 47 word types from a corpus of 7680 raw words and 470 sentences
23:31:57,765 gensim.models.word2vec INFO Creating a fresh vocabulary
23:31:57,765 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 47 unique words (100.00% of original 47, drops 0)', 'datetime': '2024-11-01T23:31:57.765144', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,765 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7680 word corpus (100.00% of original 7680, drops 0)', 'datetime': '2024-11-01T23:31:57.765665', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,766 gensim.models.word2vec INFO deleting the raw counts dictionary of 47 items
23:31:57,766 gensim.models.word2vec INFO sample=0.001 downsamples 46 most-common words
23:31:57,766 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1778.3690932311317 word corpus (23.2%% of prior 7680)', 'datetime': '2024-11-01T23:31:57.766776', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,767 gensim.models.word2vec INFO estimated required memory for 47 words and 1536 dimensions: 601036 bytes
23:31:57,768 gensim.models.word2vec INFO resetting layer weights
23:31:57,768 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:31:57.768471', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:31:57,768 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 47 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:31:57.768787', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:31:57,777 gensim.models.word2vec INFO EPOCH 0: training on 7680 raw words (1771 effective words) took 0.0s, 263342 effective words/s
23:31:57,792 gensim.models.word2vec INFO EPOCH 1: training on 7680 raw words (1800 effective words) took 0.0s, 132807 effective words/s
23:31:57,807 gensim.models.word2vec INFO EPOCH 2: training on 7680 raw words (1761 effective words) took 0.0s, 130568 effective words/s
23:31:57,807 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 23040 raw words (5332 effective words) took 0.0s, 140393 effective words/s', 'datetime': '2024-11-01T23:31:57.807294', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:31:57,807 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=47, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:31:57.807346', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:31:57,808 root INFO Completed. Ending time is 1730500317.8080873 Elapsed time is -0.07266759872436523
23:31:57,814 root INFO Starting preprocessing of transition probabilities on graph with 47 nodes and 47 edges
23:31:57,814 root INFO Starting at time 1730500317.8145218
23:31:57,814 root INFO Beginning preprocessing of transition probabilities for 47 vertices
23:31:57,814 root INFO Completed 1 / 47 vertices
23:31:57,814 root INFO Completed 5 / 47 vertices
23:31:57,815 root INFO Completed 9 / 47 vertices
23:31:57,815 root INFO Completed 13 / 47 vertices
23:31:57,815 root INFO Completed 17 / 47 vertices
23:31:57,815 root INFO Completed 21 / 47 vertices
23:31:57,816 root INFO Completed 25 / 47 vertices
23:31:57,816 root INFO Completed 29 / 47 vertices
23:31:57,817 root INFO Completed 33 / 47 vertices
23:31:57,818 root INFO Completed 37 / 47 vertices
23:31:57,818 root INFO Completed 41 / 47 vertices
23:31:57,818 root INFO Completed 45 / 47 vertices
23:31:57,818 root INFO Completed preprocessing of transition probabilities for vertices
23:31:57,819 root INFO Beginning preprocessing of transition probabilities for 47 edges
23:31:57,819 root INFO Completed 1 / 47 edges
23:31:57,820 root INFO Completed 5 / 47 edges
23:31:57,821 root INFO Completed 9 / 47 edges
23:31:57,822 root INFO Completed 13 / 47 edges
23:31:57,822 root INFO Completed 17 / 47 edges
23:31:57,823 root INFO Completed 21 / 47 edges
23:31:57,824 root INFO Completed 25 / 47 edges
23:31:57,824 root INFO Completed 29 / 47 edges
23:31:57,825 root INFO Completed 33 / 47 edges
23:31:57,826 root INFO Completed 37 / 47 edges
23:31:57,827 root INFO Completed 41 / 47 edges
23:31:57,827 root INFO Completed 45 / 47 edges
23:31:57,828 root INFO Completed preprocessing of transition probabilities for edges
23:31:57,828 root INFO Simulating walks on graph at time 1730500317.8289104
23:31:57,829 root INFO Walk iteration: 1/10
23:31:57,834 root INFO Walk iteration: 2/10
23:31:57,837 root INFO Walk iteration: 3/10
23:31:57,842 root INFO Walk iteration: 4/10
23:31:57,847 root INFO Walk iteration: 5/10
23:31:57,850 root INFO Walk iteration: 6/10
23:31:57,853 root INFO Walk iteration: 7/10
23:31:57,855 root INFO Walk iteration: 8/10
23:31:57,857 root INFO Walk iteration: 9/10
23:31:57,858 root INFO Walk iteration: 10/10
23:31:57,859 root INFO Learning embeddings at time 1730500317.8592768
23:31:57,859 gensim.models.word2vec INFO collecting all words and their counts
23:31:57,859 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:31:57,860 gensim.models.word2vec INFO collected 47 word types from a corpus of 7680 raw words and 470 sentences
23:31:57,860 gensim.models.word2vec INFO Creating a fresh vocabulary
23:31:57,860 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 47 unique words (100.00% of original 47, drops 0)', 'datetime': '2024-11-01T23:31:57.860349', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,860 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 7680 word corpus (100.00% of original 7680, drops 0)', 'datetime': '2024-11-01T23:31:57.860850', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,860 gensim.models.word2vec INFO deleting the raw counts dictionary of 47 items
23:31:57,861 gensim.models.word2vec INFO sample=0.001 downsamples 46 most-common words
23:31:57,861 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1778.3690932311317 word corpus (23.2%% of prior 7680)', 'datetime': '2024-11-01T23:31:57.861551', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:31:57,862 gensim.models.word2vec INFO estimated required memory for 47 words and 1536 dimensions: 601036 bytes
23:31:57,862 gensim.models.word2vec INFO resetting layer weights
23:31:57,863 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:31:57.863772', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:31:57,864 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 47 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:31:57.864098', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:31:57,876 gensim.models.word2vec INFO EPOCH 0: training on 7680 raw words (1771 effective words) took 0.0s, 163537 effective words/s
23:31:57,897 gensim.models.word2vec INFO EPOCH 1: training on 7680 raw words (1800 effective words) took 0.0s, 93727 effective words/s
23:31:57,915 gensim.models.word2vec INFO EPOCH 2: training on 7680 raw words (1761 effective words) took 0.0s, 105269 effective words/s
23:31:57,915 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 23040 raw words (5332 effective words) took 0.1s, 102998 effective words/s', 'datetime': '2024-11-01T23:31:57.915897', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:31:57,916 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=47, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:31:57.916636', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:31:57,916 root INFO Completed. Ending time is 1730500317.9166882 Elapsed time is -0.10216641426086426
23:31:57,967 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:31:58,131 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:31:58,131 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:31:58,146 datashaper.workflow.workflow INFO executing verb create_final_entities
23:31:58,153 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:31:58,190 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:31:58,190 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:31:58,192 graphrag.index.operations.embed_text.strategies.openai INFO embedding 72 inputs via 72 snippets using 5 batches. max_batch_size=16, max_tokens=8191
23:31:59,559 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:31:59,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.378357701934874. input_tokens=225, output_tokens=0
23:31:59,753 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:31:59,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5834510310087353. input_tokens=566, output_tokens=0
23:31:59,845 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:31:59,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6752125800121576. input_tokens=539, output_tokens=0
23:31:59,869 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:31:59,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.698100629961118. input_tokens=604, output_tokens=0
23:31:59,946 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:31:59,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.775183746125549. input_tokens=378, output_tokens=0
23:31:59,974 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:32:00,135 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:32:00,136 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:32:00,146 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:32:02,862 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:32:03,36 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:32:03,36 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:32:03,48 datashaper.workflow.workflow INFO executing verb create_final_communities
23:32:03,65 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:32:03,202 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
23:32:03,202 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:32:03,208 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:32:03,219 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:32:03,228 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:32:03,369 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
23:32:03,369 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:32:03,376 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:32:03,378 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:32:03,385 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:32:03,395 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:32:03,531 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
23:32:03,531 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:32:03,539 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:32:03,547 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:32:03,553 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 42
23:32:03,567 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 162
23:32:06,66 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:06,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.463200150988996. input_tokens=2052, output_tokens=303
23:32:07,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:07,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.227079144911841. input_tokens=2051, output_tokens=392
23:32:09,783 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:09,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.1830171179026365. input_tokens=2666, output_tokens=454
23:32:11,928 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:11,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.324131808010861. input_tokens=2248, output_tokens=436
23:32:13,510 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:13,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.5685534321237355. input_tokens=2077, output_tokens=237
23:32:14,380 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:14,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4398910629097372. input_tokens=2043, output_tokens=313
23:32:14,721 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:14,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.783032309031114. input_tokens=2564, output_tokens=485
23:32:14,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:14,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7965022390708327. input_tokens=2309, output_tokens=499
23:32:15,361 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:15,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.423440085956827. input_tokens=3393, output_tokens=636
23:32:16,294 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:16,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7771749841049314. input_tokens=2498, output_tokens=522
23:32:16,468 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:16,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.073852799832821. input_tokens=2106, output_tokens=383
23:32:16,748 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:16,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0095365929882973. input_tokens=2133, output_tokens=352
23:32:16,761 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:32:16,910 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:32:16,910 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:32:16,921 datashaper.workflow.workflow INFO executing verb create_final_documents
23:32:16,927 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:32:16,944 graphrag.index.cli INFO All workflows completed successfully.
23:36:41,83 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
23:36:41,84 graphrag.index.cli INFO Starting pipeline run for: 20241101-233641, dryrun=False
23:36:41,84 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
23:36:41,87 graphrag.index.create_pipeline_config INFO skipping workflows 
23:36:41,87 graphrag.index.run.run INFO Running pipeline
23:36:41,87 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
23:36:41,88 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
23:36:41,88 graphrag.index.input.load_input INFO using file storage for input
23:36:41,90 graphrag.index.input.csv INFO Loading csv files from input_eval
23:36:41,90 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
23:36:41,93 graphrag.index.input.csv INFO Found 1 csv files, loading 1
23:36:41,93 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
23:36:41,94 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
23:36:41,94 graphrag.index.run.run INFO Final # of rows loaded: 1
23:36:41,208 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:36:41,210 datashaper.workflow.workflow INFO executing verb create_base_text_units
23:36:41,544 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:36:41,671 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
23:36:41,671 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:36:41,679 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
23:36:41,680 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:36:41,718 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
23:36:41,718 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:36:44,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:36:44,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6608130619861186. input_tokens=2087, output_tokens=527
23:36:58,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:36:58,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.033874301007017. input_tokens=24, output_tokens=2545
23:36:59,51 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:36:59,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.633839727146551. input_tokens=2, output_tokens=1
23:37:10,152 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:10,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 11.100894621107727. input_tokens=24, output_tokens=3851
23:37:10,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:10,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.8240517310332507. input_tokens=2, output_tokens=1
23:37:23,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:23,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 12.766397482948378. input_tokens=24, output_tokens=4549
23:37:24,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:24,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.9019932670053095. input_tokens=2, output_tokens=1
23:37:36,913 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:36,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 12.251845345832407. input_tokens=24, output_tokens=4530
23:37:37,926 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:37,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 1.008020673179999. input_tokens=2, output_tokens=1
23:37:50,434 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:50,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.508444109931588. input_tokens=24, output_tokens=4515
23:37:51,569 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:51,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.1307558668777347. input_tokens=2, output_tokens=1
23:38:04,317 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:04,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.746448636986315. input_tokens=24, output_tokens=4515
23:38:05,655 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:05,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.3327388549223542. input_tokens=2, output_tokens=1
23:38:18,431 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:18,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.774809357943013. input_tokens=24, output_tokens=4501
23:38:19,137 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:19,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.677739922888577. input_tokens=150, output_tokens=58
23:38:19,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:19,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8158532660454512. input_tokens=168, output_tokens=75
23:38:19,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:19,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0304907660465688. input_tokens=152, output_tokens=13
23:38:19,529 root INFO Starting preprocessing of transition probabilities on graph with 26 nodes and 92 edges
23:38:19,529 root INFO Starting at time 1730500699.529157
23:38:19,529 root INFO Beginning preprocessing of transition probabilities for 26 vertices
23:38:19,529 root INFO Completed 1 / 26 vertices
23:38:19,529 root INFO Completed 3 / 26 vertices
23:38:19,530 root INFO Completed 5 / 26 vertices
23:38:19,530 root INFO Completed 7 / 26 vertices
23:38:19,531 root INFO Completed 9 / 26 vertices
23:38:19,532 root INFO Completed 11 / 26 vertices
23:38:19,533 root INFO Completed 13 / 26 vertices
23:38:19,533 root INFO Completed 15 / 26 vertices
23:38:19,533 root INFO Completed 17 / 26 vertices
23:38:19,533 root INFO Completed 19 / 26 vertices
23:38:19,533 root INFO Completed 21 / 26 vertices
23:38:19,533 root INFO Completed 23 / 26 vertices
23:38:19,534 root INFO Completed 25 / 26 vertices
23:38:19,535 root INFO Completed preprocessing of transition probabilities for vertices
23:38:19,535 root INFO Beginning preprocessing of transition probabilities for 92 edges
23:38:19,536 root INFO Completed 1 / 92 edges
23:38:19,537 root INFO Completed 10 / 92 edges
23:38:19,537 root INFO Completed 19 / 92 edges
23:38:19,538 root INFO Completed 28 / 92 edges
23:38:19,539 root INFO Completed 37 / 92 edges
23:38:19,539 root INFO Completed 46 / 92 edges
23:38:19,540 root INFO Completed 55 / 92 edges
23:38:19,540 root INFO Completed 64 / 92 edges
23:38:19,541 root INFO Completed 73 / 92 edges
23:38:19,541 root INFO Completed 82 / 92 edges
23:38:19,542 root INFO Completed 91 / 92 edges
23:38:19,542 root INFO Completed preprocessing of transition probabilities for edges
23:38:19,543 root INFO Simulating walks on graph at time 1730500699.5435
23:38:19,543 root INFO Walk iteration: 1/10
23:38:19,545 root INFO Walk iteration: 2/10
23:38:19,547 root INFO Walk iteration: 3/10
23:38:19,549 root INFO Walk iteration: 4/10
23:38:19,551 root INFO Walk iteration: 5/10
23:38:19,553 root INFO Walk iteration: 6/10
23:38:19,554 root INFO Walk iteration: 7/10
23:38:19,555 root INFO Walk iteration: 8/10
23:38:19,557 root INFO Walk iteration: 9/10
23:38:19,557 root INFO Walk iteration: 10/10
23:38:19,558 root INFO Learning embeddings at time 1730500699.5587533
23:38:19,559 gensim.models.word2vec INFO collecting all words and their counts
23:38:19,559 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:38:19,559 gensim.models.word2vec INFO collected 26 word types from a corpus of 5560 raw words and 260 sentences
23:38:19,559 gensim.models.word2vec INFO Creating a fresh vocabulary
23:38:19,559 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 26 unique words (100.00% of original 26, drops 0)', 'datetime': '2024-11-01T23:38:19.559830', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,560 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5560 word corpus (100.00% of original 5560, drops 0)', 'datetime': '2024-11-01T23:38:19.560269', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,560 gensim.models.word2vec INFO deleting the raw counts dictionary of 26 items
23:38:19,560 gensim.models.word2vec INFO sample=0.001 downsamples 24 most-common words
23:38:19,560 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 963.4653989888578 word corpus (17.3%% of prior 5560)', 'datetime': '2024-11-01T23:38:19.560756', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,560 gensim.models.word2vec INFO estimated required memory for 26 words and 1536 dimensions: 332488 bytes
23:38:19,561 gensim.models.word2vec INFO resetting layer weights
23:38:19,561 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:38:19.561432', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:38:19,561 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 26 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:38:19.561811', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:38:19,568 gensim.models.word2vec INFO EPOCH 0: training on 5560 raw words (994 effective words) took 0.0s, 214770 effective words/s
23:38:19,579 gensim.models.word2vec INFO EPOCH 1: training on 5560 raw words (959 effective words) took 0.0s, 106725 effective words/s
23:38:19,592 gensim.models.word2vec INFO EPOCH 2: training on 5560 raw words (940 effective words) took 0.0s, 96368 effective words/s
23:38:19,592 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16680 raw words (2893 effective words) took 0.0s, 96435 effective words/s', 'datetime': '2024-11-01T23:38:19.592540', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:38:19,592 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=26, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:38:19.592620', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:38:19,593 root INFO Completed. Ending time is 1730500699.593411 Elapsed time is -0.0642540454864502
23:38:19,602 root INFO Starting preprocessing of transition probabilities on graph with 26 nodes and 92 edges
23:38:19,602 root INFO Starting at time 1730500699.602667
23:38:19,602 root INFO Beginning preprocessing of transition probabilities for 26 vertices
23:38:19,602 root INFO Completed 1 / 26 vertices
23:38:19,602 root INFO Completed 3 / 26 vertices
23:38:19,603 root INFO Completed 5 / 26 vertices
23:38:19,604 root INFO Completed 7 / 26 vertices
23:38:19,605 root INFO Completed 9 / 26 vertices
23:38:19,605 root INFO Completed 11 / 26 vertices
23:38:19,606 root INFO Completed 13 / 26 vertices
23:38:19,607 root INFO Completed 15 / 26 vertices
23:38:19,607 root INFO Completed 17 / 26 vertices
23:38:19,607 root INFO Completed 19 / 26 vertices
23:38:19,608 root INFO Completed 21 / 26 vertices
23:38:19,608 root INFO Completed 23 / 26 vertices
23:38:19,609 root INFO Completed 25 / 26 vertices
23:38:19,609 root INFO Completed preprocessing of transition probabilities for vertices
23:38:19,610 root INFO Beginning preprocessing of transition probabilities for 92 edges
23:38:19,610 root INFO Completed 1 / 92 edges
23:38:19,611 root INFO Completed 10 / 92 edges
23:38:19,612 root INFO Completed 19 / 92 edges
23:38:19,613 root INFO Completed 28 / 92 edges
23:38:19,614 root INFO Completed 37 / 92 edges
23:38:19,615 root INFO Completed 46 / 92 edges
23:38:19,615 root INFO Completed 55 / 92 edges
23:38:19,616 root INFO Completed 64 / 92 edges
23:38:19,617 root INFO Completed 73 / 92 edges
23:38:19,617 root INFO Completed 82 / 92 edges
23:38:19,618 root INFO Completed 91 / 92 edges
23:38:19,619 root INFO Completed preprocessing of transition probabilities for edges
23:38:19,620 root INFO Simulating walks on graph at time 1730500699.6199832
23:38:19,620 root INFO Walk iteration: 1/10
23:38:19,622 root INFO Walk iteration: 2/10
23:38:19,624 root INFO Walk iteration: 3/10
23:38:19,625 root INFO Walk iteration: 4/10
23:38:19,626 root INFO Walk iteration: 5/10
23:38:19,627 root INFO Walk iteration: 6/10
23:38:19,628 root INFO Walk iteration: 7/10
23:38:19,629 root INFO Walk iteration: 8/10
23:38:19,630 root INFO Walk iteration: 9/10
23:38:19,630 root INFO Walk iteration: 10/10
23:38:19,631 root INFO Learning embeddings at time 1730500699.6316762
23:38:19,631 gensim.models.word2vec INFO collecting all words and their counts
23:38:19,632 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
23:38:19,632 gensim.models.word2vec INFO collected 26 word types from a corpus of 5560 raw words and 260 sentences
23:38:19,633 gensim.models.word2vec INFO Creating a fresh vocabulary
23:38:19,633 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 26 unique words (100.00% of original 26, drops 0)', 'datetime': '2024-11-01T23:38:19.633572', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,634 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 5560 word corpus (100.00% of original 5560, drops 0)', 'datetime': '2024-11-01T23:38:19.634227', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,634 gensim.models.word2vec INFO deleting the raw counts dictionary of 26 items
23:38:19,634 gensim.models.word2vec INFO sample=0.001 downsamples 24 most-common words
23:38:19,634 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 963.4653989888578 word corpus (17.3%% of prior 5560)', 'datetime': '2024-11-01T23:38:19.634900', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
23:38:19,635 gensim.models.word2vec INFO estimated required memory for 26 words and 1536 dimensions: 332488 bytes
23:38:19,636 gensim.models.word2vec INFO resetting layer weights
23:38:19,636 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-01T23:38:19.636353', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
23:38:19,636 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 26 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-01T23:38:19.636831', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:38:19,641 gensim.models.word2vec INFO EPOCH 0: training on 5560 raw words (994 effective words) took 0.0s, 267512 effective words/s
23:38:19,648 gensim.models.word2vec INFO EPOCH 1: training on 5560 raw words (959 effective words) took 0.0s, 172524 effective words/s
23:38:19,656 gensim.models.word2vec INFO EPOCH 2: training on 5560 raw words (940 effective words) took 0.0s, 142439 effective words/s
23:38:19,656 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 16680 raw words (2893 effective words) took 0.0s, 145730 effective words/s', 'datetime': '2024-11-01T23:38:19.656696', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
23:38:19,656 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=26, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-01T23:38:19.656726', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
23:38:19,656 root INFO Completed. Ending time is 1730500699.6567514 Elapsed time is -0.05408430099487305
23:38:19,710 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
23:38:19,864 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
23:38:19,864 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:38:19,873 datashaper.workflow.workflow INFO executing verb create_final_entities
23:38:19,879 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:38:19,916 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
23:38:19,916 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
23:38:19,918 graphrag.index.operations.embed_text.strategies.openai INFO embedding 48 inputs via 46 snippets using 3 batches. max_batch_size=16, max_tokens=8191
23:38:20,684 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:38:20,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7888903019484133. input_tokens=422, output_tokens=0
23:38:21,182 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:38:21,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2881917799822986. input_tokens=359, output_tokens=0
23:38:21,210 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:38:21,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3152304869145155. input_tokens=439, output_tokens=0
23:38:21,238 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:38:21,402 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:38:21,402 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:38:21,411 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:38:24,102 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
23:38:24,264 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
23:38:24,264 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:38:24,274 datashaper.workflow.workflow INFO executing verb create_final_communities
23:38:24,291 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
23:38:24,429 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
23:38:24,429 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:38:24,435 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:38:24,444 datashaper.workflow.workflow INFO executing verb create_final_relationships
23:38:24,453 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
23:38:24,591 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
23:38:24,591 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
23:38:24,597 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:38:24,599 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:38:24,607 datashaper.workflow.workflow INFO executing verb create_final_text_units
23:38:24,617 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
23:38:24,751 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
23:38:24,751 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
23:38:24,757 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
23:38:24,769 datashaper.workflow.workflow INFO executing verb create_final_community_reports
23:38:24,778 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 32
23:38:24,791 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 96
23:38:27,605 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:27,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7810459500178695. input_tokens=2267, output_tokens=504
23:38:28,41 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:28,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.215301292948425. input_tokens=2184, output_tokens=414
23:38:28,123 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:28,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2948094320017844. input_tokens=2226, output_tokens=501
23:38:31,923 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:31,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.78161567193456. input_tokens=3119, output_tokens=628
23:38:32,402 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:32,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.257830728078261. input_tokens=2766, output_tokens=685
23:38:32,449 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:32,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.3155271171126515. input_tokens=3304, output_tokens=778
23:38:32,518 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:32,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.379018553066999. input_tokens=3242, output_tokens=863
23:38:32,529 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
23:38:32,683 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
23:38:32,683 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
23:38:32,694 datashaper.workflow.workflow INFO executing verb create_final_documents
23:38:32,701 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
23:38:32,718 graphrag.index.cli INFO All workflows completed successfully.
22:30:24,267 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
22:30:24,268 graphrag.index.cli INFO Starting pipeline run for: 20241102-223024, dryrun=False
22:30:24,268 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:30:24,270 graphrag.index.create_pipeline_config INFO skipping workflows 
22:30:24,270 graphrag.index.run.run INFO Running pipeline
22:30:24,271 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
22:30:24,272 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
22:30:24,272 graphrag.index.input.load_input INFO using file storage for input
22:30:24,273 graphrag.index.input.csv INFO Loading csv files from input_eval
22:30:24,273 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
22:30:24,276 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:30:24,276 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
22:30:24,277 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:30:24,277 graphrag.index.run.run INFO Final # of rows loaded: 1
22:30:24,389 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:30:24,391 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:30:24,751 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:30:24,877 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:30:24,878 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:30:24,886 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:30:24,887 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:30:24,926 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:30:24,926 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:30:27,734 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:27,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8119738770183176. input_tokens=2087, output_tokens=527
22:30:31,386 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:31,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6497131609357893. input_tokens=18, output_tokens=1131
22:30:31,875 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:31,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4850466279312968. input_tokens=3, output_tokens=1
22:30:44,251 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:44,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.37558210385032. input_tokens=18, output_tokens=4668
22:30:45,0 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:45,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.7416093030478805. input_tokens=3, output_tokens=1
22:30:57,48 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:57,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 12.047139356844127. input_tokens=18, output_tokens=4667
22:30:57,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:57,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.8483519770670682. input_tokens=3, output_tokens=1
22:31:10,106 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:10,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 12.200227069901302. input_tokens=18, output_tokens=4667
22:31:11,164 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:11,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 1.0495979229453951. input_tokens=3, output_tokens=1
22:31:23,356 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:23,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.190529530867934. input_tokens=18, output_tokens=4667
22:31:24,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:24,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 1.1545325650367886. input_tokens=3, output_tokens=1
22:31:36,927 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:36,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.411692115943879. input_tokens=18, output_tokens=4667
22:31:38,134 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:38,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.2024575220420957. input_tokens=3, output_tokens=1
22:31:50,796 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:50,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.660246615065262. input_tokens=18, output_tokens=4667
22:31:51,309 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:51,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49103330890648067. input_tokens=174, output_tokens=20
22:31:51,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:51,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6557343869935721. input_tokens=179, output_tokens=24
22:31:51,862 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:51,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5426520318724215. input_tokens=165, output_tokens=37
22:31:51,951 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:51,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.129021393833682. input_tokens=175, output_tokens=28
22:31:51,975 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:51,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1513619420584291. input_tokens=290, output_tokens=17
22:31:51,998 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:52,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1778037529438734. input_tokens=176, output_tokens=26
22:31:52,23 root INFO Starting preprocessing of transition probabilities on graph with 48 nodes and 52 edges
22:31:52,23 root INFO Starting at time 1730583112.0237443
22:31:52,23 root INFO Beginning preprocessing of transition probabilities for 48 vertices
22:31:52,23 root INFO Completed 1 / 48 vertices
22:31:52,23 root INFO Completed 5 / 48 vertices
22:31:52,24 root INFO Completed 9 / 48 vertices
22:31:52,25 root INFO Completed 13 / 48 vertices
22:31:52,26 root INFO Completed 17 / 48 vertices
22:31:52,26 root INFO Completed 21 / 48 vertices
22:31:52,26 root INFO Completed 25 / 48 vertices
22:31:52,27 root INFO Completed 29 / 48 vertices
22:31:52,27 root INFO Completed 33 / 48 vertices
22:31:52,28 root INFO Completed 37 / 48 vertices
22:31:52,28 root INFO Completed 41 / 48 vertices
22:31:52,29 root INFO Completed 45 / 48 vertices
22:31:52,29 root INFO Completed preprocessing of transition probabilities for vertices
22:31:52,30 root INFO Beginning preprocessing of transition probabilities for 52 edges
22:31:52,31 root INFO Completed 1 / 52 edges
22:31:52,31 root INFO Completed 6 / 52 edges
22:31:52,32 root INFO Completed 11 / 52 edges
22:31:52,32 root INFO Completed 16 / 52 edges
22:31:52,33 root INFO Completed 21 / 52 edges
22:31:52,34 root INFO Completed 26 / 52 edges
22:31:52,34 root INFO Completed 31 / 52 edges
22:31:52,35 root INFO Completed 36 / 52 edges
22:31:52,36 root INFO Completed 41 / 52 edges
22:31:52,36 root INFO Completed 46 / 52 edges
22:31:52,37 root INFO Completed 51 / 52 edges
22:31:52,37 root INFO Completed preprocessing of transition probabilities for edges
22:31:52,38 root INFO Simulating walks on graph at time 1730583112.038473
22:31:52,38 root INFO Walk iteration: 1/10
22:31:52,40 root INFO Walk iteration: 2/10
22:31:52,42 root INFO Walk iteration: 3/10
22:31:52,43 root INFO Walk iteration: 4/10
22:31:52,44 root INFO Walk iteration: 5/10
22:31:52,45 root INFO Walk iteration: 6/10
22:31:52,46 root INFO Walk iteration: 7/10
22:31:52,47 root INFO Walk iteration: 8/10
22:31:52,48 root INFO Walk iteration: 9/10
22:31:52,49 root INFO Walk iteration: 10/10
22:31:52,50 root INFO Learning embeddings at time 1730583112.0501406
22:31:52,50 gensim.models.word2vec INFO collecting all words and their counts
22:31:52,50 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
22:31:52,51 gensim.models.word2vec INFO collected 48 word types from a corpus of 6720 raw words and 480 sentences
22:31:52,52 gensim.models.word2vec INFO Creating a fresh vocabulary
22:31:52,52 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 48 unique words (100.00% of original 48, drops 0)', 'datetime': '2024-11-02T22:31:52.052902', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:31:52,53 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 6720 word corpus (100.00% of original 6720, drops 0)', 'datetime': '2024-11-02T22:31:52.053443', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:31:52,53 gensim.models.word2vec INFO deleting the raw counts dictionary of 48 items
22:31:52,54 gensim.models.word2vec INFO sample=0.001 downsamples 48 most-common words
22:31:52,54 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1535.2007055189215 word corpus (22.8%% of prior 6720)', 'datetime': '2024-11-02T22:31:52.054195', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
22:31:52,54 gensim.models.word2vec INFO estimated required memory for 48 words and 1536 dimensions: 613824 bytes
22:31:52,55 gensim.models.word2vec INFO resetting layer weights
22:31:52,55 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-02T22:31:52.055734', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
22:31:52,56 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 48 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-02T22:31:52.056193', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:31:52,63 gensim.models.word2vec INFO EPOCH 0: training on 6720 raw words (1555 effective words) took 0.0s, 277056 effective words/s
22:31:52,75 gensim.models.word2vec INFO EPOCH 1: training on 6720 raw words (1490 effective words) took 0.0s, 143126 effective words/s
22:31:52,98 gensim.models.word2vec INFO EPOCH 2: training on 6720 raw words (1539 effective words) took 0.0s, 74123 effective words/s
22:31:52,98 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 20160 raw words (4584 effective words) took 0.0s, 109717 effective words/s', 'datetime': '2024-11-02T22:31:52.098607', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
22:31:52,99 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-02T22:31:52.099372', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
22:31:52,99 root INFO Completed. Ending time is 1730583112.0994687 Elapsed time is -0.07572436332702637
22:31:52,149 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:31:52,299 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:31:52,299 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:31:52,309 datashaper.workflow.workflow INFO executing verb create_final_entities
22:31:52,313 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:31:52,351 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:31:52,351 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:31:52,352 graphrag.index.operations.embed_text.strategies.openai INFO embedding 48 inputs via 6 snippets using 1 batches. max_batch_size=16, max_tokens=8191
22:31:52,851 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:31:52,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5153650150168687. input_tokens=214, output_tokens=0
22:31:52,873 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
22:31:53,8 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
22:31:53,9 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:31:53,18 datashaper.workflow.workflow INFO executing verb create_final_nodes
22:31:55,455 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
22:31:55,628 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
22:31:55,629 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:31:55,639 datashaper.workflow.workflow INFO executing verb create_final_communities
22:31:55,651 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
22:31:55,783 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
22:31:55,783 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:31:55,790 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:31:55,804 datashaper.workflow.workflow INFO executing verb create_final_relationships
22:31:55,812 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
22:31:55,960 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
22:31:55,962 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
22:31:55,966 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:31:55,968 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:31:55,975 datashaper.workflow.workflow INFO executing verb create_final_text_units
22:31:55,985 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
22:31:56,120 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
22:31:56,121 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
22:31:56,126 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
22:31:56,134 datashaper.workflow.workflow INFO executing verb create_final_community_reports
22:31:56,137 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 48
22:31:59,285 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:59,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1268162729684263. input_tokens=2292, output_tokens=598
22:31:59,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:59,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.261671139160171. input_tokens=2349, output_tokens=684
22:31:59,551 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:59,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3944781629834324. input_tokens=3759, output_tokens=646
22:31:59,562 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
22:31:59,707 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
22:31:59,707 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
22:31:59,717 datashaper.workflow.workflow INFO executing verb create_final_documents
22:31:59,723 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
22:31:59,742 graphrag.index.cli INFO All workflows completed successfully.
00:59:21,765 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
00:59:21,767 graphrag.index.cli INFO Starting pipeline run for: 20241103-005921, dryrun=False
00:59:21,767 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:59:21,769 graphrag.index.create_pipeline_config INFO skipping workflows 
00:59:21,769 graphrag.index.run.run INFO Running pipeline
00:59:21,769 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
00:59:21,770 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
00:59:21,770 graphrag.index.input.load_input INFO using file storage for input
00:59:21,771 graphrag.index.input.csv INFO Loading csv files from input_eval
00:59:21,771 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
00:59:21,774 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:59:21,774 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
00:59:21,774 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:59:21,775 graphrag.index.run.run INFO Final # of rows loaded: 1
00:59:21,886 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:59:21,888 datashaper.workflow.workflow INFO executing verb create_base_text_units
00:59:22,294 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:59:22,421 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
00:59:22,422 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:59:22,429 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
00:59:22,430 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
00:59:22,469 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
00:59:22,469 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
00:59:24,770 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:24,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.308951214887202. input_tokens=2087, output_tokens=523
00:59:29,596 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:29,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.8177383108995855. input_tokens=19, output_tokens=1397
00:59:30,121 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:30,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5214443691074848. input_tokens=3, output_tokens=1
00:59:35,516 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:35,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.393939499044791. input_tokens=19, output_tokens=1765
00:59:36,389 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:36,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.8701189060229808. input_tokens=3, output_tokens=1
00:59:43,358 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:43,364 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 6.968429764965549. input_tokens=19, output_tokens=2362
00:59:44,55 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:44,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.6925443359650671. input_tokens=3, output_tokens=1
00:59:56,91 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:56,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 12.03608654299751. input_tokens=19, output_tokens=4422
00:59:56,990 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:56,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.8941144039854407. input_tokens=3, output_tokens=1
01:00:09,215 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:09,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 12.22447637678124. input_tokens=19, output_tokens=4421
01:00:10,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:10,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.9761903621256351. input_tokens=3, output_tokens=1
01:00:22,383 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:22,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 12.184853876940906. input_tokens=19, output_tokens=4421
01:00:23,474 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:23,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 1.0842656139284372. input_tokens=3, output_tokens=1
01:00:35,776 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:35,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 12.302079112036154. input_tokens=19, output_tokens=4421
01:00:36,212 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:36,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.420337094925344. input_tokens=162, output_tokens=17
01:00:36,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:36,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.514819652074948. input_tokens=192, output_tokens=15
01:00:37,150 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:37,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3532536020502448. input_tokens=171, output_tokens=45
01:00:37,172 root INFO Starting preprocessing of transition probabilities on graph with 30 nodes and 30 edges
01:00:37,172 root INFO Starting at time 1730592037.1729364
01:00:37,172 root INFO Beginning preprocessing of transition probabilities for 30 vertices
01:00:37,172 root INFO Completed 1 / 30 vertices
01:00:37,173 root INFO Completed 4 / 30 vertices
01:00:37,173 root INFO Completed 7 / 30 vertices
01:00:37,173 root INFO Completed 10 / 30 vertices
01:00:37,173 root INFO Completed 13 / 30 vertices
01:00:37,173 root INFO Completed 16 / 30 vertices
01:00:37,173 root INFO Completed 19 / 30 vertices
01:00:37,174 root INFO Completed 22 / 30 vertices
01:00:37,174 root INFO Completed 25 / 30 vertices
01:00:37,174 root INFO Completed 28 / 30 vertices
01:00:37,174 root INFO Completed preprocessing of transition probabilities for vertices
01:00:37,174 root INFO Beginning preprocessing of transition probabilities for 30 edges
01:00:37,174 root INFO Completed 1 / 30 edges
01:00:37,174 root INFO Completed 4 / 30 edges
01:00:37,175 root INFO Completed 7 / 30 edges
01:00:37,175 root INFO Completed 10 / 30 edges
01:00:37,175 root INFO Completed 13 / 30 edges
01:00:37,176 root INFO Completed 16 / 30 edges
01:00:37,176 root INFO Completed 19 / 30 edges
01:00:37,177 root INFO Completed 22 / 30 edges
01:00:37,178 root INFO Completed 25 / 30 edges
01:00:37,179 root INFO Completed 28 / 30 edges
01:00:37,179 root INFO Completed preprocessing of transition probabilities for edges
01:00:37,179 root INFO Simulating walks on graph at time 1730592037.1799555
01:00:37,180 root INFO Walk iteration: 1/10
01:00:37,181 root INFO Walk iteration: 2/10
01:00:37,182 root INFO Walk iteration: 3/10
01:00:37,183 root INFO Walk iteration: 4/10
01:00:37,185 root INFO Walk iteration: 5/10
01:00:37,186 root INFO Walk iteration: 6/10
01:00:37,188 root INFO Walk iteration: 7/10
01:00:37,190 root INFO Walk iteration: 8/10
01:00:37,191 root INFO Walk iteration: 9/10
01:00:37,193 root INFO Walk iteration: 10/10
01:00:37,194 root INFO Learning embeddings at time 1730592037.1944714
01:00:37,195 gensim.models.word2vec INFO collecting all words and their counts
01:00:37,195 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:00:37,195 gensim.models.word2vec INFO collected 30 word types from a corpus of 4880 raw words and 300 sentences
01:00:37,196 gensim.models.word2vec INFO Creating a fresh vocabulary
01:00:37,196 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 30 unique words (100.00% of original 30, drops 0)', 'datetime': '2024-11-03T01:00:37.196180', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:00:37,196 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4880 word corpus (100.00% of original 4880, drops 0)', 'datetime': '2024-11-03T01:00:37.196808', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:00:37,197 gensim.models.word2vec INFO deleting the raw counts dictionary of 30 items
01:00:37,198 gensim.models.word2vec INFO sample=0.001 downsamples 30 most-common words
01:00:37,198 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 889.2358715599669 word corpus (18.2%% of prior 4880)', 'datetime': '2024-11-03T01:00:37.198536', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:00:37,199 gensim.models.word2vec INFO estimated required memory for 30 words and 1536 dimensions: 383640 bytes
01:00:37,200 gensim.models.word2vec INFO resetting layer weights
01:00:37,200 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:00:37.200499', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:00:37,201 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 30 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:00:37.200989', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:00:37,213 gensim.models.word2vec INFO EPOCH 0: training on 4880 raw words (910 effective words) took 0.0s, 116665 effective words/s
01:00:37,225 gensim.models.word2vec INFO EPOCH 1: training on 4880 raw words (912 effective words) took 0.0s, 78722 effective words/s
01:00:37,234 gensim.models.word2vec INFO EPOCH 2: training on 4880 raw words (932 effective words) took 0.0s, 157787 effective words/s
01:00:37,234 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 14640 raw words (2754 effective words) took 0.0s, 81175 effective words/s', 'datetime': '2024-11-03T01:00:37.234964', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:00:37,235 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=30, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:00:37.235027', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:00:37,235 root INFO Completed. Ending time is 1730592037.2350686 Elapsed time is -0.06213212013244629
01:00:37,285 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:00:37,428 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:00:37,428 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:00:37,440 datashaper.workflow.workflow INFO executing verb create_final_entities
01:00:37,445 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:00:37,483 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:00:37,483 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:00:37,484 graphrag.index.operations.embed_text.strategies.openai INFO embedding 34 inputs via 34 snippets using 3 batches. max_batch_size=16, max_tokens=8191
01:00:38,40 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:00:38,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5705311310011894. input_tokens=37, output_tokens=0
01:00:38,772 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:00:38,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3115087589249015. input_tokens=332, output_tokens=0
01:00:38,885 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:00:38,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4270168419461697. input_tokens=420, output_tokens=0
01:00:38,918 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:00:39,74 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:00:39,74 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:00:39,83 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:00:41,480 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:00:41,794 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:00:41,794 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:00:41,803 datashaper.workflow.workflow INFO executing verb create_final_communities
01:00:41,816 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:00:41,951 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:00:41,951 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:00:41,957 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:00:41,965 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:00:41,971 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:00:42,117 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
01:00:42,117 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:00:42,121 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:00:42,125 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:00:42,131 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:00:42,141 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:00:42,275 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:00:42,276 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:00:42,280 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:00:42,289 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:00:42,292 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 34
01:00:44,505 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:44,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.189619362121448. input_tokens=2057, output_tokens=241
01:00:44,705 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:44,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3858997710049152. input_tokens=2035, output_tokens=311
01:00:45,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:45,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.234923840034753. input_tokens=2310, output_tokens=504
01:00:45,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:45,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2748566190712154. input_tokens=2240, output_tokens=501
01:00:46,135 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:46,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6154305420350283. input_tokens=2011, output_tokens=264
01:00:46,183 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:46,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8702723600436. input_tokens=2802, output_tokens=715
01:00:46,194 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:00:46,348 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:00:46,351 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:00:46,360 datashaper.workflow.workflow INFO executing verb create_final_documents
01:00:46,367 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:00:46,385 graphrag.index.cli INFO All workflows completed successfully.
01:04:59,254 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:04:59,255 graphrag.index.cli INFO Starting pipeline run for: 20241103-010459, dryrun=False
01:04:59,256 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:04:59,257 graphrag.index.create_pipeline_config INFO skipping workflows 
01:04:59,257 graphrag.index.run.run INFO Running pipeline
01:04:59,258 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:04:59,259 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:04:59,259 graphrag.index.input.load_input INFO using file storage for input
01:04:59,261 graphrag.index.input.csv INFO Loading csv files from input_eval
01:04:59,261 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:04:59,264 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:04:59,264 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:04:59,265 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:04:59,265 graphrag.index.run.run INFO Final # of rows loaded: 1
01:04:59,376 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:04:59,378 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:04:59,806 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:04:59,935 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:04:59,935 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:04:59,943 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:04:59,944 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:04:59,982 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:04:59,983 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:05:02,731 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:02,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7522617601789534. input_tokens=2087, output_tokens=527
01:05:05,754 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:05,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.020656842039898. input_tokens=34, output_tokens=854
01:05:06,260 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:06,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5032687960192561. input_tokens=3, output_tokens=1
01:05:07,170 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:07,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.9071386649738997. input_tokens=34, output_tokens=126
01:05:08,788 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:08,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 1.6179953929968178. input_tokens=3, output_tokens=1
01:05:09,686 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:09,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.8967168410308659. input_tokens=34, output_tokens=105
01:05:10,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:10,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.5020870498847216. input_tokens=3, output_tokens=1
01:05:10,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:10,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.7618359108455479. input_tokens=34, output_tokens=72
01:05:11,449 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:11,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.4959784271195531. input_tokens=3, output_tokens=1
01:05:12,222 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:12,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.7721025669015944. input_tokens=34, output_tokens=74
01:05:12,831 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:12,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.6075685529503971. input_tokens=3, output_tokens=1
01:05:13,591 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:13,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.7587252280209213. input_tokens=34, output_tokens=90
01:05:14,62 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:14,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.4700221598614007. input_tokens=3, output_tokens=1
01:05:14,812 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:14,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 0.7454828168265522. input_tokens=34, output_tokens=95
01:05:14,833 root INFO Starting preprocessing of transition probabilities on graph with 22 nodes and 23 edges
01:05:14,833 root INFO Starting at time 1730592314.833949
01:05:14,833 root INFO Beginning preprocessing of transition probabilities for 22 vertices
01:05:14,833 root INFO Completed 1 / 22 vertices
01:05:14,834 root INFO Completed 3 / 22 vertices
01:05:14,834 root INFO Completed 5 / 22 vertices
01:05:14,834 root INFO Completed 7 / 22 vertices
01:05:14,835 root INFO Completed 9 / 22 vertices
01:05:14,835 root INFO Completed 11 / 22 vertices
01:05:14,835 root INFO Completed 13 / 22 vertices
01:05:14,835 root INFO Completed 15 / 22 vertices
01:05:14,835 root INFO Completed 17 / 22 vertices
01:05:14,835 root INFO Completed 19 / 22 vertices
01:05:14,835 root INFO Completed 21 / 22 vertices
01:05:14,835 root INFO Completed preprocessing of transition probabilities for vertices
01:05:14,836 root INFO Beginning preprocessing of transition probabilities for 23 edges
01:05:14,836 root INFO Completed 1 / 23 edges
01:05:14,836 root INFO Completed 3 / 23 edges
01:05:14,836 root INFO Completed 5 / 23 edges
01:05:14,836 root INFO Completed 7 / 23 edges
01:05:14,837 root INFO Completed 9 / 23 edges
01:05:14,838 root INFO Completed 11 / 23 edges
01:05:14,844 root INFO Completed 13 / 23 edges
01:05:14,845 root INFO Completed 15 / 23 edges
01:05:14,845 root INFO Completed 17 / 23 edges
01:05:14,846 root INFO Completed 19 / 23 edges
01:05:14,847 root INFO Completed 21 / 23 edges
01:05:14,848 root INFO Completed 23 / 23 edges
01:05:14,848 root INFO Completed preprocessing of transition probabilities for edges
01:05:14,848 root INFO Simulating walks on graph at time 1730592314.8487868
01:05:14,849 root INFO Walk iteration: 1/10
01:05:14,850 root INFO Walk iteration: 2/10
01:05:14,851 root INFO Walk iteration: 3/10
01:05:14,852 root INFO Walk iteration: 4/10
01:05:14,853 root INFO Walk iteration: 5/10
01:05:14,854 root INFO Walk iteration: 6/10
01:05:14,855 root INFO Walk iteration: 7/10
01:05:14,856 root INFO Walk iteration: 8/10
01:05:14,856 root INFO Walk iteration: 9/10
01:05:14,857 root INFO Walk iteration: 10/10
01:05:14,857 root INFO Learning embeddings at time 1730592314.8578699
01:05:14,858 gensim.models.word2vec INFO collecting all words and their counts
01:05:14,858 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:05:14,858 gensim.models.word2vec INFO collected 22 word types from a corpus of 3920 raw words and 220 sentences
01:05:14,858 gensim.models.word2vec INFO Creating a fresh vocabulary
01:05:14,859 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 22 unique words (100.00% of original 22, drops 0)', 'datetime': '2024-11-03T01:05:14.859622', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:05:14,860 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3920 word corpus (100.00% of original 3920, drops 0)', 'datetime': '2024-11-03T01:05:14.860079', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:05:14,860 gensim.models.word2vec INFO deleting the raw counts dictionary of 22 items
01:05:14,861 gensim.models.word2vec INFO sample=0.001 downsamples 22 most-common words
01:05:14,861 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 578.9365124555525 word corpus (14.8%% of prior 3920)', 'datetime': '2024-11-03T01:05:14.861958', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:05:14,862 gensim.models.word2vec INFO estimated required memory for 22 words and 1536 dimensions: 281336 bytes
01:05:14,863 gensim.models.word2vec INFO resetting layer weights
01:05:14,863 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:05:14.863361', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:05:14,863 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:05:14.863738', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:05:14,868 gensim.models.word2vec INFO EPOCH 0: training on 3920 raw words (582 effective words) took 0.0s, 238103 effective words/s
01:05:14,872 gensim.models.word2vec INFO EPOCH 1: training on 3920 raw words (549 effective words) took 0.0s, 199323 effective words/s
01:05:14,877 gensim.models.word2vec INFO EPOCH 2: training on 3920 raw words (585 effective words) took 0.0s, 148436 effective words/s
01:05:14,877 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 11760 raw words (1716 effective words) took 0.0s, 127435 effective words/s', 'datetime': '2024-11-03T01:05:14.877794', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:05:14,878 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=22, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:05:14.878498', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:05:14,879 root INFO Completed. Ending time is 1730592314.8791842 Elapsed time is -0.04523515701293945
01:05:14,918 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:05:15,59 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:05:15,59 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:05:15,68 datashaper.workflow.workflow INFO executing verb create_final_entities
01:05:15,72 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:05:15,109 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:05:15,109 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:05:15,110 graphrag.index.operations.embed_text.strategies.openai INFO embedding 22 inputs via 15 snippets using 1 batches. max_batch_size=16, max_tokens=8191
01:05:16,229 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:05:16,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1406126827932894. input_tokens=430, output_tokens=0
01:05:16,257 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:05:16,399 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:05:16,400 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:05:16,407 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:05:18,828 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:05:18,981 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:05:18,982 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:05:18,991 datashaper.workflow.workflow INFO executing verb create_final_communities
01:05:19,2 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:05:19,135 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:05:19,135 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:05:19,140 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:05:19,148 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:05:19,153 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:05:19,291 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
01:05:19,292 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:05:19,296 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:05:19,298 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:05:19,305 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:05:19,314 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:05:19,448 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:05:19,451 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:05:19,454 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:05:19,462 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:05:19,465 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 22
01:05:21,783 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:21,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.298223759047687. input_tokens=2034, output_tokens=300
01:05:22,31 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:22,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.546937343897298. input_tokens=2082, output_tokens=337
01:05:22,185 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:22,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7034929238725454. input_tokens=2105, output_tokens=369
01:05:23,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:23,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.431411832105368. input_tokens=2873, output_tokens=883
01:05:23,920 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:05:24,73 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:05:24,74 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:05:24,85 datashaper.workflow.workflow INFO executing verb create_final_documents
01:05:24,91 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:05:24,109 graphrag.index.cli INFO All workflows completed successfully.
01:05:51,292 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:05:51,293 graphrag.index.cli INFO Starting pipeline run for: 20241103-010551, dryrun=False
01:05:51,294 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 7,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:05:51,295 graphrag.index.create_pipeline_config INFO skipping workflows 
01:05:51,296 graphrag.index.run.run INFO Running pipeline
01:05:51,296 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:05:51,296 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:05:51,296 graphrag.index.input.load_input INFO using file storage for input
01:05:51,297 graphrag.index.input.csv INFO Loading csv files from input_eval
01:05:51,297 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:05:51,300 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:05:51,300 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:05:51,301 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:05:51,301 graphrag.index.run.run INFO Final # of rows loaded: 1
01:05:51,412 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:05:51,415 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:05:51,706 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:05:51,832 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:05:51,832 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:05:51,840 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:05:51,841 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:05:51,879 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:05:51,879 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:05:54,574 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:54,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6989566339179873. input_tokens=2087, output_tokens=527
01:05:57,479 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:57,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.900993274990469. input_tokens=34, output_tokens=802
01:05:57,929 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:57,931 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4478447230067104. input_tokens=3, output_tokens=1
01:05:58,820 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:58,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.890257780207321. input_tokens=34, output_tokens=120
01:05:59,325 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:59,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-1" with 0 retries took 0.5039832780603319. input_tokens=3, output_tokens=1
01:06:00,210 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:00,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-2" with 0 retries took 0.8834555898793042. input_tokens=34, output_tokens=134
01:06:00,681 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:00,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-2" with 0 retries took 0.4701169559266418. input_tokens=3, output_tokens=1
01:06:01,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:01,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-3" with 0 retries took 0.8146656809840351. input_tokens=34, output_tokens=72
01:06:02,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:02,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-3" with 0 retries took 0.5194216968957335. input_tokens=3, output_tokens=1
01:06:02,694 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:02,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-4" with 0 retries took 0.6757867820560932. input_tokens=34, output_tokens=74
01:06:03,203 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:03,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-4" with 0 retries took 0.5075713600963354. input_tokens=3, output_tokens=1
01:06:04,28 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:04,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-5" with 0 retries took 0.8241885139141232. input_tokens=34, output_tokens=108
01:06:04,510 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:04,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-5" with 0 retries took 0.4808121940586716. input_tokens=3, output_tokens=1
01:06:06,54 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:06,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-6" with 0 retries took 1.5428107499610633. input_tokens=34, output_tokens=369
01:06:06,86 root INFO Starting preprocessing of transition probabilities on graph with 23 nodes and 30 edges
01:06:06,86 root INFO Starting at time 1730592366.0861437
01:06:06,86 root INFO Beginning preprocessing of transition probabilities for 23 vertices
01:06:06,86 root INFO Completed 1 / 23 vertices
01:06:06,86 root INFO Completed 3 / 23 vertices
01:06:06,86 root INFO Completed 5 / 23 vertices
01:06:06,86 root INFO Completed 7 / 23 vertices
01:06:06,87 root INFO Completed 9 / 23 vertices
01:06:06,88 root INFO Completed 11 / 23 vertices
01:06:06,88 root INFO Completed 13 / 23 vertices
01:06:06,88 root INFO Completed 15 / 23 vertices
01:06:06,88 root INFO Completed 17 / 23 vertices
01:06:06,88 root INFO Completed 19 / 23 vertices
01:06:06,88 root INFO Completed 21 / 23 vertices
01:06:06,89 root INFO Completed 23 / 23 vertices
01:06:06,89 root INFO Completed preprocessing of transition probabilities for vertices
01:06:06,89 root INFO Beginning preprocessing of transition probabilities for 30 edges
01:06:06,89 root INFO Completed 1 / 30 edges
01:06:06,90 root INFO Completed 4 / 30 edges
01:06:06,91 root INFO Completed 7 / 30 edges
01:06:06,91 root INFO Completed 10 / 30 edges
01:06:06,92 root INFO Completed 13 / 30 edges
01:06:06,92 root INFO Completed 16 / 30 edges
01:06:06,93 root INFO Completed 19 / 30 edges
01:06:06,93 root INFO Completed 22 / 30 edges
01:06:06,94 root INFO Completed 25 / 30 edges
01:06:06,94 root INFO Completed 28 / 30 edges
01:06:06,95 root INFO Completed preprocessing of transition probabilities for edges
01:06:06,95 root INFO Simulating walks on graph at time 1730592366.0954957
01:06:06,95 root INFO Walk iteration: 1/10
01:06:06,97 root INFO Walk iteration: 2/10
01:06:06,98 root INFO Walk iteration: 3/10
01:06:06,99 root INFO Walk iteration: 4/10
01:06:06,100 root INFO Walk iteration: 5/10
01:06:06,100 root INFO Walk iteration: 6/10
01:06:06,101 root INFO Walk iteration: 7/10
01:06:06,102 root INFO Walk iteration: 8/10
01:06:06,102 root INFO Walk iteration: 9/10
01:06:06,103 root INFO Walk iteration: 10/10
01:06:06,103 root INFO Learning embeddings at time 1730592366.1038644
01:06:06,104 gensim.models.word2vec INFO collecting all words and their counts
01:06:06,104 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:06:06,104 gensim.models.word2vec INFO collected 23 word types from a corpus of 4400 raw words and 230 sentences
01:06:06,104 gensim.models.word2vec INFO Creating a fresh vocabulary
01:06:06,105 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 23 unique words (100.00% of original 23, drops 0)', 'datetime': '2024-11-03T01:06:06.105058', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:06:06,105 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4400 word corpus (100.00% of original 4400, drops 0)', 'datetime': '2024-11-03T01:06:06.105594', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:06:06,105 gensim.models.word2vec INFO deleting the raw counts dictionary of 23 items
01:06:06,106 gensim.models.word2vec INFO sample=0.001 downsamples 23 most-common words
01:06:06,106 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 685.065841036032 word corpus (15.6%% of prior 4400)', 'datetime': '2024-11-03T01:06:06.106206', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:06:06,106 gensim.models.word2vec INFO estimated required memory for 23 words and 1536 dimensions: 294124 bytes
01:06:06,107 gensim.models.word2vec INFO resetting layer weights
01:06:06,107 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:06:06.107502', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:06:06,107 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 23 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:06:06.107973', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:06:06,112 gensim.models.word2vec INFO EPOCH 0: training on 4400 raw words (683 effective words) took 0.0s, 261999 effective words/s
01:06:06,116 gensim.models.word2vec INFO EPOCH 1: training on 4400 raw words (667 effective words) took 0.0s, 232018 effective words/s
01:06:06,120 gensim.models.word2vec INFO EPOCH 2: training on 4400 raw words (661 effective words) took 0.0s, 202881 effective words/s
01:06:06,120 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 13200 raw words (2011 effective words) took 0.0s, 167072 effective words/s', 'datetime': '2024-11-03T01:06:06.120486', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:06:06,121 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:06:06.121216', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:06:06,121 root INFO Completed. Ending time is 1730592366.1218903 Elapsed time is -0.03574657440185547
01:06:06,152 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:06:06,300 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:06:06,300 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:06:06,312 datashaper.workflow.workflow INFO executing verb create_final_entities
01:06:06,316 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:06:06,353 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:06:06,353 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:06:06,353 graphrag.index.operations.embed_text.strategies.openai INFO embedding 23 inputs via 16 snippets using 1 batches. max_batch_size=16, max_tokens=8191
01:06:07,684 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:06:07,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3566904009785503. input_tokens=462, output_tokens=0
01:06:07,716 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:06:07,856 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:06:07,857 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:06:07,865 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:06:10,140 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:06:10,309 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:06:10,310 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:06:10,319 datashaper.workflow.workflow INFO executing verb create_final_communities
01:06:10,330 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:06:10,462 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:06:10,465 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:06:10,470 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:06:10,477 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:06:10,483 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:06:10,627 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
01:06:10,627 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:06:10,632 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:06:10,634 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:06:10,641 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:06:10,650 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:06:10,783 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
01:06:10,784 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:06:10,789 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:06:10,797 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:06:10,800 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 23
01:06:13,174 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:13,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.358198727015406. input_tokens=2096, output_tokens=399
01:06:14,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:14,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1996206890325993. input_tokens=2527, output_tokens=583
01:06:14,372 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:14,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.558640298899263. input_tokens=2811, output_tokens=665
01:06:14,383 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:06:14,538 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:06:14,538 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:06:14,548 datashaper.workflow.workflow INFO executing verb create_final_documents
01:06:14,555 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:06:14,573 graphrag.index.cli INFO All workflows completed successfully.
01:07:43,915 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:07:43,916 graphrag.index.cli INFO Starting pipeline run for: 20241103-010743, dryrun=False
01:07:43,916 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:07:43,918 graphrag.index.create_pipeline_config INFO skipping workflows 
01:07:43,919 graphrag.index.run.run INFO Running pipeline
01:07:43,919 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:07:43,920 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:07:43,920 graphrag.index.input.load_input INFO using file storage for input
01:07:43,922 graphrag.index.input.csv INFO Loading csv files from input_eval
01:07:43,923 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:07:43,927 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:07:43,927 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:07:43,928 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:07:43,928 graphrag.index.run.run INFO Final # of rows loaded: 1
01:07:44,43 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:07:44,45 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:07:44,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:07:44,528 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:07:44,528 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:07:44,536 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:07:44,537 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:07:44,576 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:07:44,576 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:07:47,303 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:47,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7279574431013316. input_tokens=2087, output_tokens=527
01:07:50,771 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:50,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.4669878769200295. input_tokens=34, output_tokens=945
01:07:50,793 root INFO Starting preprocessing of transition probabilities on graph with 18 nodes and 18 edges
01:07:50,793 root INFO Starting at time 1730592470.7937508
01:07:50,793 root INFO Beginning preprocessing of transition probabilities for 18 vertices
01:07:50,793 root INFO Completed 1 / 18 vertices
01:07:50,793 root INFO Completed 2 / 18 vertices
01:07:50,793 root INFO Completed 3 / 18 vertices
01:07:50,793 root INFO Completed 4 / 18 vertices
01:07:50,793 root INFO Completed 5 / 18 vertices
01:07:50,794 root INFO Completed 6 / 18 vertices
01:07:50,794 root INFO Completed 7 / 18 vertices
01:07:50,794 root INFO Completed 8 / 18 vertices
01:07:50,794 root INFO Completed 9 / 18 vertices
01:07:50,794 root INFO Completed 10 / 18 vertices
01:07:50,794 root INFO Completed 11 / 18 vertices
01:07:50,794 root INFO Completed 12 / 18 vertices
01:07:50,794 root INFO Completed 13 / 18 vertices
01:07:50,794 root INFO Completed 14 / 18 vertices
01:07:50,794 root INFO Completed 15 / 18 vertices
01:07:50,794 root INFO Completed 16 / 18 vertices
01:07:50,795 root INFO Completed 17 / 18 vertices
01:07:50,795 root INFO Completed 18 / 18 vertices
01:07:50,795 root INFO Completed preprocessing of transition probabilities for vertices
01:07:50,795 root INFO Beginning preprocessing of transition probabilities for 18 edges
01:07:50,795 root INFO Completed 1 / 18 edges
01:07:50,795 root INFO Completed 2 / 18 edges
01:07:50,795 root INFO Completed 3 / 18 edges
01:07:50,796 root INFO Completed 4 / 18 edges
01:07:50,796 root INFO Completed 5 / 18 edges
01:07:50,796 root INFO Completed 6 / 18 edges
01:07:50,797 root INFO Completed 7 / 18 edges
01:07:50,797 root INFO Completed 8 / 18 edges
01:07:50,797 root INFO Completed 9 / 18 edges
01:07:50,797 root INFO Completed 10 / 18 edges
01:07:50,797 root INFO Completed 11 / 18 edges
01:07:50,798 root INFO Completed 12 / 18 edges
01:07:50,798 root INFO Completed 13 / 18 edges
01:07:50,798 root INFO Completed 14 / 18 edges
01:07:50,798 root INFO Completed 15 / 18 edges
01:07:50,798 root INFO Completed 16 / 18 edges
01:07:50,798 root INFO Completed 17 / 18 edges
01:07:50,798 root INFO Completed 18 / 18 edges
01:07:50,798 root INFO Completed preprocessing of transition probabilities for edges
01:07:50,798 root INFO Simulating walks on graph at time 1730592470.7988226
01:07:50,798 root INFO Walk iteration: 1/10
01:07:50,800 root INFO Walk iteration: 2/10
01:07:50,800 root INFO Walk iteration: 3/10
01:07:50,801 root INFO Walk iteration: 4/10
01:07:50,801 root INFO Walk iteration: 5/10
01:07:50,802 root INFO Walk iteration: 6/10
01:07:50,803 root INFO Walk iteration: 7/10
01:07:50,804 root INFO Walk iteration: 8/10
01:07:50,804 root INFO Walk iteration: 9/10
01:07:50,805 root INFO Walk iteration: 10/10
01:07:50,806 root INFO Learning embeddings at time 1730592470.8062377
01:07:50,806 gensim.models.word2vec INFO collecting all words and their counts
01:07:50,807 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:07:50,807 gensim.models.word2vec INFO collected 18 word types from a corpus of 3200 raw words and 180 sentences
01:07:50,808 gensim.models.word2vec INFO Creating a fresh vocabulary
01:07:50,808 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 18 unique words (100.00% of original 18, drops 0)', 'datetime': '2024-11-03T01:07:50.808290', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,808 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3200 word corpus (100.00% of original 3200, drops 0)', 'datetime': '2024-11-03T01:07:50.808936', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,809 gensim.models.word2vec INFO deleting the raw counts dictionary of 18 items
01:07:50,809 gensim.models.word2vec INFO sample=0.001 downsamples 18 most-common words
01:07:50,809 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 445.83618105397017 word corpus (13.9%% of prior 3200)', 'datetime': '2024-11-03T01:07:50.809679', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,809 gensim.models.word2vec INFO estimated required memory for 18 words and 1536 dimensions: 230184 bytes
01:07:50,810 gensim.models.word2vec INFO resetting layer weights
01:07:50,810 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:07:50.810469', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:07:50,810 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 18 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:07:50.810947', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:07:50,814 gensim.models.word2vec INFO EPOCH 0: training on 3200 raw words (454 effective words) took 0.0s, 436090 effective words/s
01:07:50,817 gensim.models.word2vec INFO EPOCH 1: training on 3200 raw words (431 effective words) took 0.0s, 251083 effective words/s
01:07:50,819 gensim.models.word2vec INFO EPOCH 2: training on 3200 raw words (471 effective words) took 0.0s, 268477 effective words/s
01:07:50,819 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 9600 raw words (1356 effective words) took 0.0s, 162853 effective words/s', 'datetime': '2024-11-03T01:07:50.819885', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:07:50,820 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=18, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:07:50.820737', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:07:50,821 root INFO Completed. Ending time is 1730592470.8217394 Elapsed time is -0.027988672256469727
01:07:50,824 root INFO Starting preprocessing of transition probabilities on graph with 18 nodes and 18 edges
01:07:50,824 root INFO Starting at time 1730592470.8246303
01:07:50,824 root INFO Beginning preprocessing of transition probabilities for 18 vertices
01:07:50,824 root INFO Completed 1 / 18 vertices
01:07:50,824 root INFO Completed 2 / 18 vertices
01:07:50,825 root INFO Completed 3 / 18 vertices
01:07:50,825 root INFO Completed 4 / 18 vertices
01:07:50,825 root INFO Completed 5 / 18 vertices
01:07:50,826 root INFO Completed 6 / 18 vertices
01:07:50,826 root INFO Completed 7 / 18 vertices
01:07:50,826 root INFO Completed 8 / 18 vertices
01:07:50,827 root INFO Completed 9 / 18 vertices
01:07:50,827 root INFO Completed 10 / 18 vertices
01:07:50,827 root INFO Completed 11 / 18 vertices
01:07:50,827 root INFO Completed 12 / 18 vertices
01:07:50,827 root INFO Completed 13 / 18 vertices
01:07:50,828 root INFO Completed 14 / 18 vertices
01:07:50,828 root INFO Completed 15 / 18 vertices
01:07:50,828 root INFO Completed 16 / 18 vertices
01:07:50,828 root INFO Completed 17 / 18 vertices
01:07:50,828 root INFO Completed 18 / 18 vertices
01:07:50,828 root INFO Completed preprocessing of transition probabilities for vertices
01:07:50,829 root INFO Beginning preprocessing of transition probabilities for 18 edges
01:07:50,829 root INFO Completed 1 / 18 edges
01:07:50,830 root INFO Completed 2 / 18 edges
01:07:50,830 root INFO Completed 3 / 18 edges
01:07:50,831 root INFO Completed 4 / 18 edges
01:07:50,831 root INFO Completed 5 / 18 edges
01:07:50,832 root INFO Completed 6 / 18 edges
01:07:50,833 root INFO Completed 7 / 18 edges
01:07:50,833 root INFO Completed 8 / 18 edges
01:07:50,834 root INFO Completed 9 / 18 edges
01:07:50,834 root INFO Completed 10 / 18 edges
01:07:50,835 root INFO Completed 11 / 18 edges
01:07:50,835 root INFO Completed 12 / 18 edges
01:07:50,836 root INFO Completed 13 / 18 edges
01:07:50,836 root INFO Completed 14 / 18 edges
01:07:50,836 root INFO Completed 15 / 18 edges
01:07:50,836 root INFO Completed 16 / 18 edges
01:07:50,837 root INFO Completed 17 / 18 edges
01:07:50,837 root INFO Completed 18 / 18 edges
01:07:50,838 root INFO Completed preprocessing of transition probabilities for edges
01:07:50,838 root INFO Simulating walks on graph at time 1730592470.8384423
01:07:50,839 root INFO Walk iteration: 1/10
01:07:50,841 root INFO Walk iteration: 2/10
01:07:50,842 root INFO Walk iteration: 3/10
01:07:50,843 root INFO Walk iteration: 4/10
01:07:50,844 root INFO Walk iteration: 5/10
01:07:50,845 root INFO Walk iteration: 6/10
01:07:50,846 root INFO Walk iteration: 7/10
01:07:50,846 root INFO Walk iteration: 8/10
01:07:50,847 root INFO Walk iteration: 9/10
01:07:50,848 root INFO Walk iteration: 10/10
01:07:50,848 root INFO Learning embeddings at time 1730592470.8488553
01:07:50,849 gensim.models.word2vec INFO collecting all words and their counts
01:07:50,849 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:07:50,849 gensim.models.word2vec INFO collected 18 word types from a corpus of 3200 raw words and 180 sentences
01:07:50,850 gensim.models.word2vec INFO Creating a fresh vocabulary
01:07:50,850 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 18 unique words (100.00% of original 18, drops 0)', 'datetime': '2024-11-03T01:07:50.850281', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,850 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3200 word corpus (100.00% of original 3200, drops 0)', 'datetime': '2024-11-03T01:07:50.850886', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,850 gensim.models.word2vec INFO deleting the raw counts dictionary of 18 items
01:07:50,851 gensim.models.word2vec INFO sample=0.001 downsamples 18 most-common words
01:07:50,851 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 445.83618105397017 word corpus (13.9%% of prior 3200)', 'datetime': '2024-11-03T01:07:50.851710', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:07:50,852 gensim.models.word2vec INFO estimated required memory for 18 words and 1536 dimensions: 230184 bytes
01:07:50,852 gensim.models.word2vec INFO resetting layer weights
01:07:50,853 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:07:50.853098', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:07:50,853 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 18 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:07:50.853520', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:07:50,858 gensim.models.word2vec INFO EPOCH 0: training on 3200 raw words (454 effective words) took 0.0s, 217789 effective words/s
01:07:50,862 gensim.models.word2vec INFO EPOCH 1: training on 3200 raw words (431 effective words) took 0.0s, 153205 effective words/s
01:07:50,867 gensim.models.word2vec INFO EPOCH 2: training on 3200 raw words (471 effective words) took 0.0s, 131002 effective words/s
01:07:50,867 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 9600 raw words (1356 effective words) took 0.0s, 101742 effective words/s', 'datetime': '2024-11-03T01:07:50.867432', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:07:50,868 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=18, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:07:50.868225', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:07:50,868 root INFO Completed. Ending time is 1730592470.8682837 Elapsed time is -0.04365348815917969
01:07:50,915 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:07:51,75 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:07:51,76 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:07:51,84 datashaper.workflow.workflow INFO executing verb create_final_entities
01:07:51,88 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:07:51,125 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:07:51,125 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:07:51,126 graphrag.index.operations.embed_text.strategies.openai INFO embedding 18 inputs via 17 snippets using 2 batches. max_batch_size=16, max_tokens=8191
01:07:52,108 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:07:52,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9862550050020218. input_tokens=21, output_tokens=0
01:07:52,440 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:07:52,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3365269028581679. input_tokens=440, output_tokens=0
01:07:52,470 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:07:52,615 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:07:52,615 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:07:52,624 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:07:55,302 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:07:55,472 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:07:55,472 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:07:55,482 datashaper.workflow.workflow INFO executing verb create_final_communities
01:07:55,494 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:07:55,629 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:07:55,630 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:07:55,635 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:07:55,644 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:07:55,650 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:07:55,790 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
01:07:55,791 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:07:55,794 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:07:55,795 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:07:55,806 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:07:55,816 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:07:55,952 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
01:07:55,952 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:07:55,958 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:07:55,966 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:07:55,970 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 10
01:07:55,983 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 42
01:07:58,618 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:58,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.618644940899685. input_tokens=2047, output_tokens=378
01:07:59,678 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:59,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6813435389194638. input_tokens=2396, output_tokens=576
01:08:01,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:01,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8821869699750096. input_tokens=2143, output_tokens=308
01:08:02,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:02,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8582057398743927. input_tokens=2202, output_tokens=441
01:08:02,786 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:02,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.099490631138906. input_tokens=2564, output_tokens=633
01:08:02,799 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:08:02,949 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:08:02,949 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:08:02,960 datashaper.workflow.workflow INFO executing verb create_final_documents
01:08:02,966 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:08:02,982 graphrag.index.cli INFO All workflows completed successfully.
01:08:47,898 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:08:47,900 graphrag.index.cli INFO Starting pipeline run for: 20241103-010847, dryrun=False
01:08:47,900 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:08:47,901 graphrag.index.create_pipeline_config INFO skipping workflows 
01:08:47,902 graphrag.index.run.run INFO Running pipeline
01:08:47,902 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:08:47,903 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:08:47,903 graphrag.index.input.load_input INFO using file storage for input
01:08:47,904 graphrag.index.input.csv INFO Loading csv files from input_eval
01:08:47,904 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:08:47,907 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:08:47,907 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:08:47,908 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:08:47,909 graphrag.index.run.run INFO Final # of rows loaded: 1
01:08:48,19 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:08:48,22 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:08:48,544 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:08:48,675 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:08:48,675 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:08:48,683 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:08:48,684 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:08:48,722 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:08:48,722 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:08:51,479 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:51,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7618981490377337. input_tokens=2087, output_tokens=527
01:08:58,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:58,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.906570036895573. input_tokens=39, output_tokens=2206
01:08:58,410 root INFO Starting preprocessing of transition probabilities on graph with 12 nodes and 18 edges
01:08:58,410 root INFO Starting at time 1730592538.4101677
01:08:58,410 root INFO Beginning preprocessing of transition probabilities for 12 vertices
01:08:58,410 root INFO Completed 1 / 12 vertices
01:08:58,410 root INFO Completed 2 / 12 vertices
01:08:58,410 root INFO Completed 3 / 12 vertices
01:08:58,410 root INFO Completed 4 / 12 vertices
01:08:58,410 root INFO Completed 5 / 12 vertices
01:08:58,410 root INFO Completed 6 / 12 vertices
01:08:58,410 root INFO Completed 7 / 12 vertices
01:08:58,410 root INFO Completed 8 / 12 vertices
01:08:58,411 root INFO Completed 9 / 12 vertices
01:08:58,411 root INFO Completed 10 / 12 vertices
01:08:58,411 root INFO Completed 11 / 12 vertices
01:08:58,411 root INFO Completed 12 / 12 vertices
01:08:58,411 root INFO Completed preprocessing of transition probabilities for vertices
01:08:58,411 root INFO Beginning preprocessing of transition probabilities for 18 edges
01:08:58,411 root INFO Completed 1 / 18 edges
01:08:58,411 root INFO Completed 2 / 18 edges
01:08:58,411 root INFO Completed 3 / 18 edges
01:08:58,412 root INFO Completed 4 / 18 edges
01:08:58,412 root INFO Completed 5 / 18 edges
01:08:58,412 root INFO Completed 6 / 18 edges
01:08:58,412 root INFO Completed 7 / 18 edges
01:08:58,412 root INFO Completed 8 / 18 edges
01:08:58,413 root INFO Completed 9 / 18 edges
01:08:58,413 root INFO Completed 10 / 18 edges
01:08:58,413 root INFO Completed 11 / 18 edges
01:08:58,413 root INFO Completed 12 / 18 edges
01:08:58,413 root INFO Completed 13 / 18 edges
01:08:58,413 root INFO Completed 14 / 18 edges
01:08:58,413 root INFO Completed 15 / 18 edges
01:08:58,414 root INFO Completed 16 / 18 edges
01:08:58,414 root INFO Completed 17 / 18 edges
01:08:58,414 root INFO Completed 18 / 18 edges
01:08:58,414 root INFO Completed preprocessing of transition probabilities for edges
01:08:58,414 root INFO Simulating walks on graph at time 1730592538.4145997
01:08:58,415 root INFO Walk iteration: 1/10
01:08:58,416 root INFO Walk iteration: 2/10
01:08:58,417 root INFO Walk iteration: 3/10
01:08:58,417 root INFO Walk iteration: 4/10
01:08:58,418 root INFO Walk iteration: 5/10
01:08:58,419 root INFO Walk iteration: 6/10
01:08:58,419 root INFO Walk iteration: 7/10
01:08:58,420 root INFO Walk iteration: 8/10
01:08:58,420 root INFO Walk iteration: 9/10
01:08:58,421 root INFO Walk iteration: 10/10
01:08:58,422 root INFO Learning embeddings at time 1730592538.4221191
01:08:58,422 gensim.models.word2vec INFO collecting all words and their counts
01:08:58,423 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:08:58,423 gensim.models.word2vec INFO collected 12 word types from a corpus of 1780 raw words and 120 sentences
01:08:58,423 gensim.models.word2vec INFO Creating a fresh vocabulary
01:08:58,423 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 12 unique words (100.00% of original 12, drops 0)', 'datetime': '2024-11-03T01:08:58.423843', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:08:58,424 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 1780 word corpus (100.00% of original 1780, drops 0)', 'datetime': '2024-11-03T01:08:58.424377', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:08:58,424 gensim.models.word2vec INFO deleting the raw counts dictionary of 12 items
01:08:58,425 gensim.models.word2vec INFO sample=0.001 downsamples 12 most-common words
01:08:58,425 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 202.72796696538276 word corpus (11.4%% of prior 1780)', 'datetime': '2024-11-03T01:08:58.425037', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:08:58,425 gensim.models.word2vec INFO estimated required memory for 12 words and 1536 dimensions: 153456 bytes
01:08:58,426 gensim.models.word2vec INFO resetting layer weights
01:08:58,426 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:08:58.426651', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:08:58,427 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 12 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:08:58.427010', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:08:58,430 gensim.models.word2vec INFO EPOCH 0: training on 1780 raw words (200 effective words) took 0.0s, 180804 effective words/s
01:08:58,433 gensim.models.word2vec INFO EPOCH 1: training on 1780 raw words (186 effective words) took 0.0s, 148659 effective words/s
01:08:58,436 gensim.models.word2vec INFO EPOCH 2: training on 1780 raw words (202 effective words) took 0.0s, 215845 effective words/s
01:08:58,436 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 5340 raw words (588 effective words) took 0.0s, 66681 effective words/s', 'datetime': '2024-11-03T01:08:58.436446', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:08:58,437 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:08:58.437164', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:08:58,437 root INFO Completed. Ending time is 1730592538.437884 Elapsed time is -0.027716398239135742
01:08:58,467 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:08:58,608 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:08:58,608 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:08:58,616 datashaper.workflow.workflow INFO executing verb create_final_entities
01:08:58,619 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:08:58,657 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:08:58,657 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:08:58,658 graphrag.index.operations.embed_text.strategies.openai INFO embedding 12 inputs via 12 snippets using 1 batches. max_batch_size=16, max_tokens=8191
01:08:59,310 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:08:59,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6711648697964847. input_tokens=450, output_tokens=0
01:08:59,336 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:08:59,474 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:08:59,474 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:08:59,483 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:09:01,891 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:09:02,43 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:09:02,44 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:09:02,53 datashaper.workflow.workflow INFO executing verb create_final_communities
01:09:02,63 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:09:02,199 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:09:02,199 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:09:02,205 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:09:02,213 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:09:02,218 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:09:02,362 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
01:09:02,362 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:09:02,366 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:09:02,368 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:09:02,376 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:09:02,385 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:09:02,527 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
01:09:02,528 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:09:02,533 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:09:02,540 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:09:02,543 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 12
01:09:05,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:05,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4590658301021904. input_tokens=2220, output_tokens=462
01:09:05,233 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:05,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.672095588874072. input_tokens=2172, output_tokens=462
01:09:06,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:06,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6299262079410255. input_tokens=2571, output_tokens=600
01:09:06,210 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:09:06,361 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:09:06,361 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:09:06,371 datashaper.workflow.workflow INFO executing verb create_final_documents
01:09:06,377 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:09:06,394 graphrag.index.cli INFO All workflows completed successfully.
01:12:21,832 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:12:21,833 graphrag.index.cli INFO Starting pipeline run for: 20241103-011221, dryrun=False
01:12:21,833 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:12:21,835 graphrag.index.create_pipeline_config INFO skipping workflows 
01:12:21,835 graphrag.index.run.run INFO Running pipeline
01:12:21,835 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:12:21,837 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:12:21,837 graphrag.index.input.load_input INFO using file storage for input
01:12:21,838 graphrag.index.input.csv INFO Loading csv files from input_eval
01:12:21,838 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:12:21,842 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:12:21,842 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:12:21,843 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:12:21,843 graphrag.index.run.run INFO Final # of rows loaded: 1
01:12:21,959 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:12:21,961 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:12:22,411 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:12:22,541 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:12:22,541 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:12:22,549 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:12:22,550 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:12:22,590 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:12:22,590 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:12:24,755 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:24,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.172799658961594. input_tokens=2087, output_tokens=527
01:12:29,943 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:29,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.178988207131624. input_tokens=27, output_tokens=1536
01:12:30,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:30,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43988491711206734. input_tokens=165, output_tokens=20
01:12:30,409 root INFO Starting preprocessing of transition probabilities on graph with 21 nodes and 20 edges
01:12:30,409 root INFO Starting at time 1730592750.4094498
01:12:30,409 root INFO Beginning preprocessing of transition probabilities for 21 vertices
01:12:30,409 root INFO Completed 1 / 21 vertices
01:12:30,409 root INFO Completed 3 / 21 vertices
01:12:30,409 root INFO Completed 5 / 21 vertices
01:12:30,410 root INFO Completed 7 / 21 vertices
01:12:30,410 root INFO Completed 9 / 21 vertices
01:12:30,410 root INFO Completed 11 / 21 vertices
01:12:30,410 root INFO Completed 13 / 21 vertices
01:12:30,410 root INFO Completed 15 / 21 vertices
01:12:30,410 root INFO Completed 17 / 21 vertices
01:12:30,411 root INFO Completed 19 / 21 vertices
01:12:30,411 root INFO Completed 21 / 21 vertices
01:12:30,411 root INFO Completed preprocessing of transition probabilities for vertices
01:12:30,411 root INFO Beginning preprocessing of transition probabilities for 20 edges
01:12:30,411 root INFO Completed 1 / 20 edges
01:12:30,411 root INFO Completed 3 / 20 edges
01:12:30,412 root INFO Completed 5 / 20 edges
01:12:30,412 root INFO Completed 7 / 20 edges
01:12:30,412 root INFO Completed 9 / 20 edges
01:12:30,413 root INFO Completed 11 / 20 edges
01:12:30,413 root INFO Completed 13 / 20 edges
01:12:30,414 root INFO Completed 15 / 20 edges
01:12:30,415 root INFO Completed 17 / 20 edges
01:12:30,416 root INFO Completed 19 / 20 edges
01:12:30,417 root INFO Completed preprocessing of transition probabilities for edges
01:12:30,417 root INFO Simulating walks on graph at time 1730592750.4177992
01:12:30,418 root INFO Walk iteration: 1/10
01:12:30,420 root INFO Walk iteration: 2/10
01:12:30,420 root INFO Walk iteration: 3/10
01:12:30,421 root INFO Walk iteration: 4/10
01:12:30,422 root INFO Walk iteration: 5/10
01:12:30,423 root INFO Walk iteration: 6/10
01:12:30,423 root INFO Walk iteration: 7/10
01:12:30,424 root INFO Walk iteration: 8/10
01:12:30,424 root INFO Walk iteration: 9/10
01:12:30,425 root INFO Walk iteration: 10/10
01:12:30,426 root INFO Learning embeddings at time 1730592750.4263768
01:12:30,426 gensim.models.word2vec INFO collecting all words and their counts
01:12:30,427 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:12:30,427 gensim.models.word2vec INFO collected 21 word types from a corpus of 3040 raw words and 210 sentences
01:12:30,428 gensim.models.word2vec INFO Creating a fresh vocabulary
01:12:30,428 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 21 unique words (100.00% of original 21, drops 0)', 'datetime': '2024-11-03T01:12:30.428239', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:12:30,428 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3040 word corpus (100.00% of original 3040, drops 0)', 'datetime': '2024-11-03T01:12:30.428830', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:12:30,428 gensim.models.word2vec INFO deleting the raw counts dictionary of 21 items
01:12:30,429 gensim.models.word2vec INFO sample=0.001 downsamples 21 most-common words
01:12:30,429 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 460.1882102039943 word corpus (15.1%% of prior 3040)', 'datetime': '2024-11-03T01:12:30.429628', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:12:30,430 gensim.models.word2vec INFO estimated required memory for 21 words and 1536 dimensions: 268548 bytes
01:12:30,431 gensim.models.word2vec INFO resetting layer weights
01:12:30,431 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:12:30.431450', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:12:30,431 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 21 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:12:30.431788', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:12:30,435 gensim.models.word2vec INFO EPOCH 0: training on 3040 raw words (448 effective words) took 0.0s, 553903 effective words/s
01:12:30,437 gensim.models.word2vec INFO EPOCH 1: training on 3040 raw words (464 effective words) took 0.0s, 440969 effective words/s
01:12:30,440 gensim.models.word2vec INFO EPOCH 2: training on 3040 raw words (493 effective words) took 0.0s, 221490 effective words/s
01:12:30,440 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 9120 raw words (1405 effective words) took 0.0s, 158896 effective words/s', 'datetime': '2024-11-03T01:12:30.440659', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:12:30,441 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=21, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:12:30.441399', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:12:30,441 root INFO Completed. Ending time is 1730592750.4414456 Elapsed time is -0.03199577331542969
01:12:30,473 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:12:30,627 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:12:30,627 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:12:30,636 datashaper.workflow.workflow INFO executing verb create_final_entities
01:12:30,639 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:12:30,677 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:12:30,677 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:12:30,677 graphrag.index.operations.embed_text.strategies.openai INFO embedding 21 inputs via 21 snippets using 2 batches. max_batch_size=16, max_tokens=8191
01:12:31,187 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:12:31,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5184109599795192. input_tokens=81, output_tokens=0
01:12:31,368 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:12:31,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7172950489912182. input_tokens=405, output_tokens=0
01:12:31,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:12:31,546 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:12:31,547 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:12:31,555 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:12:33,970 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:12:34,120 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:12:34,121 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:12:34,130 datashaper.workflow.workflow INFO executing verb create_final_communities
01:12:34,142 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:12:34,276 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:12:34,277 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:12:34,282 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:12:34,290 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:12:34,297 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:12:34,440 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
01:12:34,440 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:12:34,444 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:12:34,446 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:12:34,454 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:12:34,463 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:12:34,605 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:12:34,605 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:12:34,609 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:12:34,618 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:12:34,621 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 21
01:12:36,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:36,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1028624870814383. input_tokens=2038, output_tokens=314
01:12:37,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:37,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.530285883927718. input_tokens=2011, output_tokens=312
01:12:37,268 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:37,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6248579248785973. input_tokens=2119, output_tokens=322
01:12:37,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:37,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6589340169448406. input_tokens=2029, output_tokens=323
01:12:37,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:37,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1327415590640157. input_tokens=2646, output_tokens=613
01:12:37,781 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:12:37,927 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:12:37,928 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:12:37,938 datashaper.workflow.workflow INFO executing verb create_final_documents
01:12:37,948 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:12:37,965 graphrag.index.cli INFO All workflows completed successfully.
01:13:34,266 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:13:34,268 graphrag.index.cli INFO Starting pipeline run for: 20241103-011334, dryrun=False
01:13:34,268 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:13:34,270 graphrag.index.create_pipeline_config INFO skipping workflows 
01:13:34,270 graphrag.index.run.run INFO Running pipeline
01:13:34,270 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:13:34,270 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:13:34,270 graphrag.index.input.load_input INFO using file storage for input
01:13:34,271 graphrag.index.input.csv INFO Loading csv files from input_eval
01:13:34,271 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:13:34,274 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:13:34,274 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:13:34,274 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:13:34,275 graphrag.index.run.run INFO Final # of rows loaded: 1
01:13:34,386 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:13:34,389 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:13:34,807 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:13:34,933 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:13:34,934 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:13:34,941 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:13:34,942 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:13:34,980 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:13:34,980 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:13:37,759 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:37,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7862262099515647. input_tokens=2087, output_tokens=527
01:13:43,722 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:43,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.955043998081237. input_tokens=27, output_tokens=1750
01:13:44,638 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:44,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.9140175019856542. input_tokens=3, output_tokens=1
01:13:49,400 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:49,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.760176898911595. input_tokens=27, output_tokens=1750
01:13:49,845 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:49,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4313786698039621. input_tokens=165, output_tokens=20
01:13:49,867 root INFO Starting preprocessing of transition probabilities on graph with 23 nodes and 23 edges
01:13:49,867 root INFO Starting at time 1730592829.8676207
01:13:49,867 root INFO Beginning preprocessing of transition probabilities for 23 vertices
01:13:49,867 root INFO Completed 1 / 23 vertices
01:13:49,867 root INFO Completed 3 / 23 vertices
01:13:49,867 root INFO Completed 5 / 23 vertices
01:13:49,867 root INFO Completed 7 / 23 vertices
01:13:49,867 root INFO Completed 9 / 23 vertices
01:13:49,868 root INFO Completed 11 / 23 vertices
01:13:49,868 root INFO Completed 13 / 23 vertices
01:13:49,868 root INFO Completed 15 / 23 vertices
01:13:49,868 root INFO Completed 17 / 23 vertices
01:13:49,868 root INFO Completed 19 / 23 vertices
01:13:49,868 root INFO Completed 21 / 23 vertices
01:13:49,868 root INFO Completed 23 / 23 vertices
01:13:49,868 root INFO Completed preprocessing of transition probabilities for vertices
01:13:49,868 root INFO Beginning preprocessing of transition probabilities for 23 edges
01:13:49,868 root INFO Completed 1 / 23 edges
01:13:49,869 root INFO Completed 3 / 23 edges
01:13:49,870 root INFO Completed 5 / 23 edges
01:13:49,871 root INFO Completed 7 / 23 edges
01:13:49,872 root INFO Completed 9 / 23 edges
01:13:49,873 root INFO Completed 11 / 23 edges
01:13:49,874 root INFO Completed 13 / 23 edges
01:13:49,874 root INFO Completed 15 / 23 edges
01:13:49,876 root INFO Completed 17 / 23 edges
01:13:49,876 root INFO Completed 19 / 23 edges
01:13:49,877 root INFO Completed 21 / 23 edges
01:13:49,877 root INFO Completed 23 / 23 edges
01:13:49,877 root INFO Completed preprocessing of transition probabilities for edges
01:13:49,877 root INFO Simulating walks on graph at time 1730592829.8771784
01:13:49,878 root INFO Walk iteration: 1/10
01:13:49,880 root INFO Walk iteration: 2/10
01:13:49,881 root INFO Walk iteration: 3/10
01:13:49,883 root INFO Walk iteration: 4/10
01:13:49,885 root INFO Walk iteration: 5/10
01:13:49,887 root INFO Walk iteration: 6/10
01:13:49,889 root INFO Walk iteration: 7/10
01:13:49,890 root INFO Walk iteration: 8/10
01:13:49,892 root INFO Walk iteration: 9/10
01:13:49,893 root INFO Walk iteration: 10/10
01:13:49,894 root INFO Learning embeddings at time 1730592829.8943865
01:13:49,894 gensim.models.word2vec INFO collecting all words and their counts
01:13:49,895 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:13:49,896 gensim.models.word2vec INFO collected 23 word types from a corpus of 4120 raw words and 230 sentences
01:13:49,896 gensim.models.word2vec INFO Creating a fresh vocabulary
01:13:49,896 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 23 unique words (100.00% of original 23, drops 0)', 'datetime': '2024-11-03T01:13:49.896856', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:13:49,897 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 4120 word corpus (100.00% of original 4120, drops 0)', 'datetime': '2024-11-03T01:13:49.897670', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:13:49,898 gensim.models.word2vec INFO deleting the raw counts dictionary of 23 items
01:13:49,899 gensim.models.word2vec INFO sample=0.001 downsamples 23 most-common words
01:13:49,899 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 672.8123766441885 word corpus (16.3%% of prior 4120)', 'datetime': '2024-11-03T01:13:49.899440', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:13:49,899 gensim.models.word2vec INFO estimated required memory for 23 words and 1536 dimensions: 294124 bytes
01:13:49,900 gensim.models.word2vec INFO resetting layer weights
01:13:49,900 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:13:49.900563', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:13:49,900 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 23 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:13:49.900867', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:13:49,908 gensim.models.word2vec INFO EPOCH 0: training on 4120 raw words (680 effective words) took 0.0s, 161133 effective words/s
01:13:49,919 gensim.models.word2vec INFO EPOCH 1: training on 4120 raw words (657 effective words) took 0.0s, 75224 effective words/s
01:13:49,930 gensim.models.word2vec INFO EPOCH 2: training on 4120 raw words (686 effective words) took 0.0s, 79432 effective words/s
01:13:49,930 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 12360 raw words (2023 effective words) took 0.0s, 69043 effective words/s', 'datetime': '2024-11-03T01:13:49.930235', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:13:49,931 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:13:49.931126', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:13:49,932 root INFO Completed. Ending time is 1730592829.9321525 Elapsed time is -0.06453180313110352
01:13:49,969 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:13:50,106 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:13:50,107 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:13:50,115 datashaper.workflow.workflow INFO executing verb create_final_entities
01:13:50,119 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:13:50,156 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:13:50,156 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:13:50,157 graphrag.index.operations.embed_text.strategies.openai INFO embedding 23 inputs via 23 snippets using 2 batches. max_batch_size=16, max_tokens=8191
01:13:50,870 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:13:50,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7263986049219966. input_tokens=111, output_tokens=0
01:13:51,477 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:13:51,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3425666328985244. input_tokens=417, output_tokens=0
01:13:51,505 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:13:51,656 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:13:51,656 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:13:51,665 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:13:53,944 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:13:54,114 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:13:54,114 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:13:54,124 datashaper.workflow.workflow INFO executing verb create_final_communities
01:13:54,136 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:13:54,270 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:13:54,270 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:13:54,278 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:13:54,286 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:13:54,292 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:13:54,431 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
01:13:54,434 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:13:54,437 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:13:54,439 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:13:54,447 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:13:54,456 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:13:54,589 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
01:13:54,590 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:13:54,595 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:13:54,603 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:13:54,606 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 23
01:13:57,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:57,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6741604320704937. input_tokens=2033, output_tokens=359
01:13:57,749 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:57,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.128578691976145. input_tokens=2487, output_tokens=639
01:13:57,760 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:57,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1355873569846153. input_tokens=2258, output_tokens=511
01:13:58,343 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:58,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.716884315945208. input_tokens=2273, output_tokens=583
01:13:58,365 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:13:58,509 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:13:58,510 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:13:58,522 datashaper.workflow.workflow INFO executing verb create_final_documents
01:13:58,528 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:13:58,545 graphrag.index.cli INFO All workflows completed successfully.
01:34:51,44 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:34:51,46 graphrag.index.cli INFO Starting pipeline run for: 20241103-013451, dryrun=False
01:34:51,46 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:34:51,48 graphrag.index.create_pipeline_config INFO skipping workflows 
01:34:51,48 graphrag.index.run.run INFO Running pipeline
01:34:51,48 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:34:51,49 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:34:51,49 graphrag.index.input.load_input INFO using file storage for input
01:34:51,49 graphrag.index.input.csv INFO Loading csv files from input_eval
01:34:51,49 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:34:51,53 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:34:51,53 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:34:51,54 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:34:51,54 graphrag.index.run.run INFO Final # of rows loaded: 1
01:34:51,166 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:34:51,168 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:34:51,587 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:34:51,716 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:34:51,716 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:34:51,724 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:34:51,725 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:34:51,763 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:34:51,763 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:34:54,450 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:34:54,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.693418702110648. input_tokens=2087, output_tokens=527
01:35:03,324 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:03,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.866876090876758. input_tokens=36, output_tokens=2735
01:35:03,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:03,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6388066839426756. input_tokens=30, output_tokens=1
01:35:13,1 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:13,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 9.034671650035307. input_tokens=36, output_tokens=2847
01:35:13,24 root INFO Starting preprocessing of transition probabilities on graph with 17 nodes and 25 edges
01:35:13,24 root INFO Starting at time 1730594113.0246987
01:35:13,24 root INFO Beginning preprocessing of transition probabilities for 17 vertices
01:35:13,24 root INFO Completed 1 / 17 vertices
01:35:13,24 root INFO Completed 2 / 17 vertices
01:35:13,24 root INFO Completed 3 / 17 vertices
01:35:13,24 root INFO Completed 4 / 17 vertices
01:35:13,25 root INFO Completed 5 / 17 vertices
01:35:13,25 root INFO Completed 6 / 17 vertices
01:35:13,25 root INFO Completed 7 / 17 vertices
01:35:13,25 root INFO Completed 8 / 17 vertices
01:35:13,25 root INFO Completed 9 / 17 vertices
01:35:13,26 root INFO Completed 10 / 17 vertices
01:35:13,27 root INFO Completed 11 / 17 vertices
01:35:13,28 root INFO Completed 12 / 17 vertices
01:35:13,29 root INFO Completed 13 / 17 vertices
01:35:13,29 root INFO Completed 14 / 17 vertices
01:35:13,29 root INFO Completed 15 / 17 vertices
01:35:13,29 root INFO Completed 16 / 17 vertices
01:35:13,29 root INFO Completed 17 / 17 vertices
01:35:13,30 root INFO Completed preprocessing of transition probabilities for vertices
01:35:13,31 root INFO Beginning preprocessing of transition probabilities for 25 edges
01:35:13,31 root INFO Completed 1 / 25 edges
01:35:13,31 root INFO Completed 3 / 25 edges
01:35:13,32 root INFO Completed 5 / 25 edges
01:35:13,33 root INFO Completed 7 / 25 edges
01:35:13,34 root INFO Completed 9 / 25 edges
01:35:13,34 root INFO Completed 11 / 25 edges
01:35:13,35 root INFO Completed 13 / 25 edges
01:35:13,35 root INFO Completed 15 / 25 edges
01:35:13,36 root INFO Completed 17 / 25 edges
01:35:13,37 root INFO Completed 19 / 25 edges
01:35:13,37 root INFO Completed 21 / 25 edges
01:35:13,38 root INFO Completed 23 / 25 edges
01:35:13,39 root INFO Completed 25 / 25 edges
01:35:13,39 root INFO Completed preprocessing of transition probabilities for edges
01:35:13,39 root INFO Simulating walks on graph at time 1730594113.0398188
01:35:13,40 root INFO Walk iteration: 1/10
01:35:13,42 root INFO Walk iteration: 2/10
01:35:13,43 root INFO Walk iteration: 3/10
01:35:13,44 root INFO Walk iteration: 4/10
01:35:13,45 root INFO Walk iteration: 5/10
01:35:13,46 root INFO Walk iteration: 6/10
01:35:13,47 root INFO Walk iteration: 7/10
01:35:13,47 root INFO Walk iteration: 8/10
01:35:13,48 root INFO Walk iteration: 9/10
01:35:13,50 root INFO Walk iteration: 10/10
01:35:13,51 root INFO Learning embeddings at time 1730594113.0514944
01:35:13,52 gensim.models.word2vec INFO collecting all words and their counts
01:35:13,52 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:35:13,52 gensim.models.word2vec INFO collected 17 word types from a corpus of 3200 raw words and 170 sentences
01:35:13,52 gensim.models.word2vec INFO Creating a fresh vocabulary
01:35:13,53 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 17 unique words (100.00% of original 17, drops 0)', 'datetime': '2024-11-03T01:35:13.052996', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:35:13,53 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3200 word corpus (100.00% of original 3200, drops 0)', 'datetime': '2024-11-03T01:35:13.053452', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:35:13,54 gensim.models.word2vec INFO deleting the raw counts dictionary of 17 items
01:35:13,54 gensim.models.word2vec INFO sample=0.001 downsamples 17 most-common words
01:35:13,55 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 439.9262275826231 word corpus (13.7%% of prior 3200)', 'datetime': '2024-11-03T01:35:13.055292', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:35:13,56 gensim.models.word2vec INFO estimated required memory for 17 words and 1536 dimensions: 217396 bytes
01:35:13,56 gensim.models.word2vec INFO resetting layer weights
01:35:13,57 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:35:13.057029', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:35:13,57 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 17 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:35:13.057162', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:35:13,64 gensim.models.word2vec INFO EPOCH 0: training on 3200 raw words (442 effective words) took 0.0s, 91334 effective words/s
01:35:13,73 gensim.models.word2vec INFO EPOCH 1: training on 3200 raw words (455 effective words) took 0.0s, 109271 effective words/s
01:35:13,80 gensim.models.word2vec INFO EPOCH 2: training on 3200 raw words (456 effective words) took 0.0s, 75567 effective words/s
01:35:13,80 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 9600 raw words (1353 effective words) took 0.0s, 58571 effective words/s', 'datetime': '2024-11-03T01:35:13.080343', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:35:13,81 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=17, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:35:13.081208', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:35:13,81 root INFO Completed. Ending time is 1730594113.081954 Elapsed time is -0.05725526809692383
01:35:13,114 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:35:13,253 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:35:13,254 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:35:13,261 datashaper.workflow.workflow INFO executing verb create_final_entities
01:35:13,264 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:35:13,302 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:35:13,302 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:35:13,302 graphrag.index.operations.embed_text.strategies.openai INFO embedding 17 inputs via 17 snippets using 2 batches. max_batch_size=16, max_tokens=8191
01:35:14,278 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:14,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9837868991307914. input_tokens=27, output_tokens=0
01:35:14,612 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:14,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3328817770816386. input_tokens=599, output_tokens=0
01:35:14,642 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:35:14,788 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:35:14,788 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:35:14,797 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:35:17,202 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:35:17,352 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:35:17,354 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:35:17,362 datashaper.workflow.workflow INFO executing verb create_final_communities
01:35:17,373 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:35:17,506 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:35:17,507 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:35:17,512 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:35:17,520 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:35:17,526 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:35:17,664 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
01:35:17,665 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:35:17,673 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:35:17,675 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:35:17,682 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:35:17,691 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:35:17,827 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:35:17,828 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:35:17,832 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:35:17,840 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:35:17,844 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 17
01:35:20,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:20,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4383823431562632. input_tokens=2193, output_tokens=460
01:35:20,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:20,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7064769288990647. input_tokens=2082, output_tokens=373
01:35:21,0 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:21,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.138057669159025. input_tokens=2482, output_tokens=571
01:35:21,321 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:21,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4603702679742128. input_tokens=2640, output_tokens=665
01:35:21,333 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:35:21,479 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:35:21,483 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:35:21,492 datashaper.workflow.workflow INFO executing verb create_final_documents
01:35:21,498 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:35:21,514 graphrag.index.cli INFO All workflows completed successfully.
01:35:55,345 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval/indexing-engine.log
01:35:55,346 graphrag.index.cli INFO Starting pipeline run for: 20241103-013555, dryrun=False
01:35:55,347 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "none",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input_eval",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:35:55,348 graphrag.index.create_pipeline_config INFO skipping workflows 
01:35:55,348 graphrag.index.run.run INFO Running pipeline
01:35:55,349 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output_eval
01:35:55,351 graphrag.index.input.load_input INFO loading input from root_dir=input_eval
01:35:55,351 graphrag.index.input.load_input INFO using file storage for input
01:35:55,352 graphrag.index.input.csv INFO Loading csv files from input_eval
01:35:55,352 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input_eval for files matching .*\.csv$
01:35:55,355 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:35:55,355 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 1
01:35:55,356 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:35:55,357 graphrag.index.run.run INFO Final # of rows loaded: 1
01:35:55,468 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:35:55,470 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:35:55,913 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:35:56,39 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:35:56,40 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:35:56,47 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:35:56,48 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:35:56,86 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:35:56,86 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:35:58,756 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:58,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6781455751042813. input_tokens=2087, output_tokens=527
01:36:09,205 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:09,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.440321568865329. input_tokens=36, output_tokens=3329
01:36:09,833 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:09,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6214622149709612. input_tokens=30, output_tokens=1
01:36:19,427 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:19,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 9.593794168904424. input_tokens=36, output_tokens=3266
01:36:19,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:19,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49244123697280884. input_tokens=158, output_tokens=22
01:36:20,589 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:20,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6437156749889255. input_tokens=162, output_tokens=47
01:36:20,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:20,661 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:20,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2091795729938895. input_tokens=164, output_tokens=27
01:36:20,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2059608611743897. input_tokens=159, output_tokens=35
01:36:20,665 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:20,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2097422189544886. input_tokens=167, output_tokens=37
01:36:20,707 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:20,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2531244498677552. input_tokens=170, output_tokens=44
01:36:21,121 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:21,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5268920310772955. input_tokens=168, output_tokens=37
01:36:21,414 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:21,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7523259478621185. input_tokens=161, output_tokens=70
01:36:21,434 root INFO Starting preprocessing of transition probabilities on graph with 20 nodes and 29 edges
01:36:21,434 root INFO Starting at time 1730594181.4345393
01:36:21,434 root INFO Beginning preprocessing of transition probabilities for 20 vertices
01:36:21,434 root INFO Completed 1 / 20 vertices
01:36:21,434 root INFO Completed 3 / 20 vertices
01:36:21,435 root INFO Completed 5 / 20 vertices
01:36:21,436 root INFO Completed 7 / 20 vertices
01:36:21,437 root INFO Completed 9 / 20 vertices
01:36:21,438 root INFO Completed 11 / 20 vertices
01:36:21,438 root INFO Completed 13 / 20 vertices
01:36:21,439 root INFO Completed 15 / 20 vertices
01:36:21,439 root INFO Completed 17 / 20 vertices
01:36:21,439 root INFO Completed 19 / 20 vertices
01:36:21,439 root INFO Completed preprocessing of transition probabilities for vertices
01:36:21,439 root INFO Beginning preprocessing of transition probabilities for 29 edges
01:36:21,439 root INFO Completed 1 / 29 edges
01:36:21,440 root INFO Completed 3 / 29 edges
01:36:21,440 root INFO Completed 5 / 29 edges
01:36:21,440 root INFO Completed 7 / 29 edges
01:36:21,441 root INFO Completed 9 / 29 edges
01:36:21,442 root INFO Completed 11 / 29 edges
01:36:21,443 root INFO Completed 13 / 29 edges
01:36:21,444 root INFO Completed 15 / 29 edges
01:36:21,444 root INFO Completed 17 / 29 edges
01:36:21,445 root INFO Completed 19 / 29 edges
01:36:21,446 root INFO Completed 21 / 29 edges
01:36:21,447 root INFO Completed 23 / 29 edges
01:36:21,448 root INFO Completed 25 / 29 edges
01:36:21,448 root INFO Completed 27 / 29 edges
01:36:21,449 root INFO Completed 29 / 29 edges
01:36:21,450 root INFO Completed preprocessing of transition probabilities for edges
01:36:21,451 root INFO Simulating walks on graph at time 1730594181.4512062
01:36:21,452 root INFO Walk iteration: 1/10
01:36:21,455 root INFO Walk iteration: 2/10
01:36:21,457 root INFO Walk iteration: 3/10
01:36:21,460 root INFO Walk iteration: 4/10
01:36:21,462 root INFO Walk iteration: 5/10
01:36:21,464 root INFO Walk iteration: 6/10
01:36:21,466 root INFO Walk iteration: 7/10
01:36:21,467 root INFO Walk iteration: 8/10
01:36:21,469 root INFO Walk iteration: 9/10
01:36:21,470 root INFO Walk iteration: 10/10
01:36:21,471 root INFO Learning embeddings at time 1730594181.471102
01:36:21,471 gensim.models.word2vec INFO collecting all words and their counts
01:36:21,471 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:36:21,472 gensim.models.word2vec INFO collected 20 word types from a corpus of 3680 raw words and 200 sentences
01:36:21,473 gensim.models.word2vec INFO Creating a fresh vocabulary
01:36:21,474 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 20 unique words (100.00% of original 20, drops 0)', 'datetime': '2024-11-03T01:36:21.474316', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:36:21,474 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 3680 word corpus (100.00% of original 3680, drops 0)', 'datetime': '2024-11-03T01:36:21.474960', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:36:21,475 gensim.models.word2vec INFO deleting the raw counts dictionary of 20 items
01:36:21,476 gensim.models.word2vec INFO sample=0.001 downsamples 20 most-common words
01:36:21,477 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 559.4031681972845 word corpus (15.2%% of prior 3680)', 'datetime': '2024-11-03T01:36:21.477270', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:36:21,478 gensim.models.word2vec INFO estimated required memory for 20 words and 1536 dimensions: 255760 bytes
01:36:21,478 gensim.models.word2vec INFO resetting layer weights
01:36:21,480 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-11-03T01:36:21.480015', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:36:21,480 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 20 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-11-03T01:36:21.480354', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:36:21,486 gensim.models.word2vec INFO EPOCH 0: training on 3680 raw words (570 effective words) took 0.0s, 152662 effective words/s
01:36:21,492 gensim.models.word2vec INFO EPOCH 1: training on 3680 raw words (550 effective words) took 0.0s, 148494 effective words/s
01:36:21,501 gensim.models.word2vec INFO EPOCH 2: training on 3680 raw words (577 effective words) took 0.0s, 82893 effective words/s
01:36:21,501 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 11040 raw words (1697 effective words) took 0.0s, 81949 effective words/s', 'datetime': '2024-11-03T01:36:21.501775', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:36:21,502 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=20, vector_size=1536, alpha=0.025>', 'datetime': '2024-11-03T01:36:21.502746', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:36:21,503 root INFO Completed. Ending time is 1730594181.5034957 Elapsed time is -0.06895637512207031
01:36:21,547 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:36:21,695 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:36:21,696 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:36:21,704 datashaper.workflow.workflow INFO executing verb create_final_entities
01:36:21,708 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:36:21,745 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:36:21,745 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:36:21,746 graphrag.index.operations.embed_text.strategies.openai INFO embedding 20 inputs via 20 snippets using 2 batches. max_batch_size=16, max_tokens=8191
01:36:22,430 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:22,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7089713078457862. input_tokens=462, output_tokens=0
01:36:22,768 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:22,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0308283739723265. input_tokens=91, output_tokens=0
01:36:22,784 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:36:22,932 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:36:22,932 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:36:22,940 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:36:25,355 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:36:25,507 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:36:25,508 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:36:25,517 datashaper.workflow.workflow INFO executing verb create_final_communities
01:36:25,532 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:36:25,665 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:36:25,665 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:36:25,670 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:36:25,678 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:36:25,684 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:36:25,823 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
01:36:25,824 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:36:25,833 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:36:25,835 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:36:25,842 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:36:25,852 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:36:25,986 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:36:25,986 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:36:25,992 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:36:26,0 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:36:26,3 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 20
01:36:28,505 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:28,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.47567998804152. input_tokens=2096, output_tokens=362
01:36:28,602 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:28,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.573736065067351. input_tokens=2139, output_tokens=350
01:36:28,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:28,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.774100855924189. input_tokens=2323, output_tokens=501
01:36:29,151 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:29,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.128093324834481. input_tokens=2292, output_tokens=511
01:36:29,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:29,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.415044064866379. input_tokens=2574, output_tokens=596
01:36:31,7 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:36:31,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.489512715023011. input_tokens=2183, output_tokens=478
01:36:31,20 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
01:36:31,175 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
01:36:31,175 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
01:36:31,185 datashaper.workflow.workflow INFO executing verb create_final_documents
01:36:31,192 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
01:36:31,209 graphrag.index.cli INFO All workflows completed successfully.
