{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb?ref=blog.langchain.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2304.12244v2.pdf',\n",
       " '2402.05930v1.pdf',\n",
       " '2405.16506v1.pdf',\n",
       " '2102.00151.pdf',\n",
       " '2310.06825v1.pdf',\n",
       " '2112.07916v2.pdf',\n",
       " '2401.13919v4.pdf',\n",
       " '2312.12423.pdf',\n",
       " '2305.10601v2.pdf',\n",
       " '2406.05085v1.pdf',\n",
       " '2409.00149v1.pdf',\n",
       " '2212.10423v1.pdf',\n",
       " '2311.07587v2.pdf',\n",
       " '2409.04109v1.pdf',\n",
       " '2310.12931v2.pdf',\n",
       " '2309.14521.pdf',\n",
       " '2408.06292v3.pdf',\n",
       " '2402.15301v2.pdf',\n",
       " '2108.10447v1.pdf',\n",
       " '2406.12430v1.pdf',\n",
       " '2311.18751v2.pdf',\n",
       " '2402.03146v1.pdf',\n",
       " '2312.10997v5.pdf',\n",
       " '2404.00610v1.pdf',\n",
       " '2203.05794v1.pdf',\n",
       " '2303.18223v13.pdf',\n",
       " '2409.03284v1.pdf',\n",
       " '2308.02357v1.pdf',\n",
       " '2209.11755v1.pdf',\n",
       " '2309.15698v1.pdf',\n",
       " '2408.06292v1.pdf',\n",
       " '2305.05084v6.pdf',\n",
       " '2405.10292v2.pdf',\n",
       " '2402.18041v1.pdf',\n",
       " '2310.08184v1.pdf',\n",
       " '2312.10029v2.pdf',\n",
       " '2305.13453v2.pdf',\n",
       " '2310.11511v1.pdf',\n",
       " '2406.14550v1.pdf',\n",
       " 'P10-1031.pdf',\n",
       " '2409.04004v2.pdf',\n",
       " '2109.05679v2.pdf',\n",
       " '2102.01187.pdf',\n",
       " '2312.15713v1.pdf',\n",
       " '2408.03010v1.pdf',\n",
       " '2305.17888v1.pdf',\n",
       " '2303.11366v4.pdf',\n",
       " '2307.03109v9.pdf',\n",
       " '2408.08921v2.pdf',\n",
       " '2106.14807v1.pdf',\n",
       " '2304.12210.pdf',\n",
       " '2307.12856v4.pdf',\n",
       " '2404.16130.pdf',\n",
       " '2409.00786v1.pdf',\n",
       " '2403.08345v1.pdf',\n",
       " '2312.14238.pdf',\n",
       " '2402.03216v4.pdf',\n",
       " '2403.14403v2.pdf',\n",
       " '2209.11000v1.pdf',\n",
       " '2307.08621v4.pdf',\n",
       " '2406.11736v1.pdf',\n",
       " '2206.08896v1.pdf',\n",
       " '2210.03945v2.pdf',\n",
       " '2210.11416v5.pdf',\n",
       " '2112.08778v1.pdf']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "path = 'documents/'\n",
    "files = os.listdir(path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "# Get elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "        type: str\n",
    "        text: Any\n",
    "def get_text_and_table(filename):\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "    filename=path + filename,\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path,\n",
    "    )\n",
    "    # Create a dictionary to store counts of each type\n",
    "    category_counts = {}\n",
    "\n",
    "    for element in raw_pdf_elements:\n",
    "        category = str(type(element))\n",
    "        if category in category_counts:\n",
    "            category_counts[category] += 1\n",
    "        else:\n",
    "            category_counts[category] = 1\n",
    "\n",
    "    # Unique_categories will have unique elements\n",
    "    unique_categories = set(category_counts.keys())\n",
    "    category_counts\n",
    "\n",
    "\n",
    "\n",
    "    # Categorize by type\n",
    "    categorized_elements = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "    # Tables\n",
    "    table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "    print(len(table_elements))\n",
    "\n",
    "    # Text\n",
    "    text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
    "    print(len(text_elements))\n",
    "    \n",
    "    return text_elements,table_elements\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ['OPENAI_API_KEY']='any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "base_url = \"http://localhost:8080/\"\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\",base_url=base_url)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_sleep(batch,sleep=2):\n",
    "    new_list = []\n",
    "    for t in batch:\n",
    "        s = summarize_chain.invoke(t)\n",
    "        new_list.append(s)\n",
    "        time.sleep(sleep)\n",
    "    \n",
    "    return new_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2402.05930v1.pdf\n",
      "32\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient('./chroma_docs')\n",
    "collection = client.get_or_create_collection('documents_summary')\n",
    "collection_raw= client.get_or_create_collection('documents_raw')\n",
    "#collection_table = client.get_or_create_collection('table_summary')\n",
    "\n",
    "for i,filename in enumerate(files[1:]):\n",
    "    \n",
    "    print(i,filename)\n",
    "    text_elements,table_elements = get_text_and_table(filename)\n",
    "    # Apply to text\n",
    "    texts = [i.text for i in text_elements]\n",
    "    tables = [i.text for i in table_elements]\n",
    "    \n",
    "    ids = [filename+'_'+ str(i) for i in range(len(text_summaries))]\n",
    "    ids = [filename+'_table_'+ str(i) for i in range(len(tables))]\n",
    "    \n",
    "    #RAW\n",
    "    collection_raw.add(ids=ids,tables=tables)\n",
    "    collection_raw.add(ids=ids,documents=texts)\n",
    "    \n",
    "    #LLM\n",
    "    text_summaries = batch_sleep(texts)\n",
    "    collection.add(ids=ids,documents=text_summaries)\n",
    "    \n",
    "    table_summaries = batch_sleep(tables)\n",
    "    collection.add(ids=ids,documents=table_summaries)\n",
    "    \n",
    "    print(collection.count())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
