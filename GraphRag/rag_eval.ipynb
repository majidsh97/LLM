{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"anything\"\n",
    "from ragas.testset.generator import TestsetGenerator,RunConfig\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.integrations.llama_index import evaluate \n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex,Document,Settings\n",
    "from llama_index.core import VectorStoreIndex,Document,Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "\n",
    "\n",
    "# generator with openai models\n",
    "model_name = \"gpt-4o\"\n",
    "base_url = \"http://localhost:8080/\"\n",
    "max_tokens = 1024*8\n",
    "generator_llm = ChatOpenAI(model=model_name, base_url=base_url,max_tokens=max_tokens)\n",
    "critic_llm = ChatOpenAI(model=model_name, base_url=base_url,max_tokens=max_tokens)\n",
    "embeddings = OpenAIEmbeddings(model='text', base_url=base_url)\n",
    "runconfig = RunConfig(max_workers=1,max_retries=1)\n",
    "\n",
    "unit_text_path = '/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/create_base_text_units.parquet'\n",
    "\n",
    "texts = pd.read_parquet(unit_text_path)\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Settings.embed_model = embeddings #OpenAIEmbedding(model='text',api_base=base_url,max_retries=1)\n",
    "Settings.llm=OpenAI(model=model_name,api_base=base_url,max_tokens=max_tokens,max_retries=1)\n",
    "Settings.chunk_size=1200\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store( vector_store )\n",
    "retriever = index.as_retriever()\n",
    "rag_pipline = index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "rag_pipline.query(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new datqa\n",
    "#d = texts['chunk'].tolist()\n",
    "#e = embeddings.embed_documents(d)\n",
    "#chroma_collection.add(ids=texts['id'].tolist(),documents=d,embeddings=e)\n",
    "#chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = chroma_collection.get(include=['documents','embeddings'])\n",
    "docs = data['documents']\n",
    "docs_llama = list(map(lambda x:Document(text=x ),docs))\n",
    "docs_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=docs_llama[:20],\n",
    "    num_questions_per_chunk=1,\n",
    "    #text_question_template=DEFAULT_QUESTION_GENERATION_PROMPT,\n",
    "    #question_gen_query=DEFAULT_TEXT_QA_PROMPT,\n",
    "    show_progress=True,\n",
    "    workers=1\n",
    "    \n",
    ")\n",
    "\n",
    "len(dataset_generator.nodes) # 1314\n",
    "\n",
    "rag_dataset =  dataset_generator.generate_dataset_from_nodes()\n",
    "df = rag_dataset.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragchecker.integrations.llama_index import response_to_rag_results\n",
    "\n",
    "\n",
    "rag_result_list = []\n",
    "for i,x in df.iterrows():\n",
    "    response_object = rag_pipline.query(x['query'])\n",
    "    rag_result = response_to_rag_results(\n",
    "    query=x['query'],\n",
    "    gt_answer=x['reference_answer'],\n",
    "    response_object=response_object,\n",
    "    )\n",
    "    rag_result_list.append(rag_result)\n",
    "\n",
    "rag_results = RAGResults.from_dict({\"results\": rag_result_list})\n",
    "print(rag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragchecker import RAGResults, RAGChecker\n",
    "from ragchecker.metrics import all_metrics\n",
    "\n",
    "\n",
    "\n",
    "# Create RAGResults object\n",
    "\n",
    "# initialize ragresults from json/dict\n",
    "\n",
    "# set-up the evaluator\n",
    "evaluator = RAGChecker(\n",
    "    extractor_name=model_name,\n",
    "    checker_name=model_name,\n",
    "    batch_size_extractor=1,\n",
    "    batch_size_checker=1,\n",
    "    #custom_llm_api_func=generator_llm.invoke,\n",
    "    checker_api_base=base_url,\n",
    "    extractor_api_base=base_url,\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# evaluate results with selected metrics or certain groups, e.g., retriever_metrics, generator_metrics, all_metrics\n",
    "evaluator.evaluate(rag_results, all_metrics)\n",
    "print(rag_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
