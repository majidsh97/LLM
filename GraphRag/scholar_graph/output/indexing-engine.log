22:22:44,985 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
22:22:44,986 graphrag.index.cli INFO Starting pipeline run for: 20241017-222244, dryrun=False
22:22:44,986 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
22:22:44,988 graphrag.index.create_pipeline_config INFO skipping workflows 
22:22:44,988 graphrag.index.run.run INFO Running pipeline
22:22:44,988 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
22:22:44,990 graphrag.index.input.load_input INFO loading input from root_dir=input
22:22:44,990 graphrag.index.input.load_input INFO using file storage for input
22:22:44,991 graphrag.index.input.csv INFO Loading csv files from input
22:22:44,991 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
22:22:44,996 graphrag.index.input.csv INFO Found 1 csv files, loading 1
22:22:44,996 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
22:22:44,997 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
22:22:44,997 graphrag.index.run.run INFO Final # of rows loaded: 100
22:22:45,106 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
22:22:45,109 datashaper.workflow.workflow INFO executing verb create_base_text_units
22:22:45,435 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
22:22:45,566 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
22:22:45,566 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
22:22:45,574 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
22:22:45,578 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:22:45,617 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
22:22:45,617 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
22:22:48,532 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:48,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.9069269850006094. input_tokens=28, output_tokens=615
22:22:49,235 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:49,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.609342078998452. input_tokens=28, output_tokens=687
22:22:49,749 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:49,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.130535559001146. input_tokens=2342, output_tokens=876
22:22:50,332 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:50,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.702703035000013. input_tokens=28, output_tokens=377
22:22:50,359 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:50,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8092897149908822. input_tokens=1915, output_tokens=384
22:22:53,133 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:53,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.7711337130022002. input_tokens=28, output_tokens=637
22:22:53,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:53,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.7105179599893745. input_tokens=28, output_tokens=1815
22:22:53,509 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:53,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.259970754996175. input_tokens=1941, output_tokens=883
22:22:54,980 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:54,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.2227056859992445. input_tokens=28, output_tokens=1298
22:22:55,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:55,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7252387560001807. input_tokens=2457, output_tokens=664
22:22:58,343 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:58,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.342109449993586. input_tokens=2688, output_tokens=821
22:22:58,612 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:58,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.250098553005955. input_tokens=2345, output_tokens=1600
22:22:58,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:22:58,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.389273245004006. input_tokens=28, output_tokens=1341
22:23:00,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:00,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9915557609929238. input_tokens=28, output_tokens=397
22:23:02,792 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:02,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.906609515004675. input_tokens=28, output_tokens=1911
22:23:02,910 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:02,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.55622800999845. input_tokens=2412, output_tokens=4200
22:23:04,471 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:04,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.850869204994524. input_tokens=28, output_tokens=1563
22:23:11,363 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:11,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.434182097000303. input_tokens=2893, output_tokens=4202
22:23:12,0 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:12,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.509279163001338. input_tokens=1859, output_tokens=2196
22:23:12,6 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:12,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.645628184007364. input_tokens=2055, output_tokens=3626
22:23:13,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:13,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5780015740019735. input_tokens=28, output_tokens=151
22:23:14,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:14,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.998803460999625. input_tokens=28, output_tokens=3845
22:23:15,477 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:15,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8622315270040417. input_tokens=2250, output_tokens=374
22:23:15,816 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:15,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.007038919007755. input_tokens=2894, output_tokens=4066
22:23:16,598 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:16,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.574940449005226. input_tokens=28, output_tokens=993
22:23:20,355 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:20,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.871455168991815. input_tokens=28, output_tokens=1164
22:23:23,93 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:23,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.471781839005416. input_tokens=1968, output_tokens=1987
22:23:25,124 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:25,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.022394768995582. input_tokens=28, output_tokens=353
22:23:26,680 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:26,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.308079724010895. input_tokens=28, output_tokens=4491
22:23:28,140 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:28,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.996792136997101. input_tokens=1911, output_tokens=713
22:23:28,980 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:28,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.040613297998789. input_tokens=2894, output_tokens=4150
22:23:30,673 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:30,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.847710855989135. input_tokens=28, output_tokens=4032
22:23:32,596 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:32,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.901568060988211. input_tokens=1841, output_tokens=387
22:23:34,231 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:34,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.852715123008238. input_tokens=2893, output_tokens=4167
22:23:36,297 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:36,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6938272319966927. input_tokens=28, output_tokens=863
22:23:39,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:39,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.087719814007869. input_tokens=2894, output_tokens=4091
22:23:43,327 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:43,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.338520238001365. input_tokens=28, output_tokens=4316
22:23:44,560 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:44,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.2119309120025719. input_tokens=1720, output_tokens=221
22:23:45,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:45,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.330168800006504. input_tokens=28, output_tokens=3342
22:23:45,864 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:45,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.71588752999378. input_tokens=28, output_tokens=4481
22:23:46,5 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:46,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4390658580086892. input_tokens=28, output_tokens=255
22:23:49,367 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:49,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.563666869013105. input_tokens=28, output_tokens=2812
22:23:50,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:50,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.808747432995006. input_tokens=2820, output_tokens=4185
22:23:50,767 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:50,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.3799102490011137. input_tokens=2129, output_tokens=263
22:23:51,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:51,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.367214710000553. input_tokens=2010, output_tokens=1505
22:23:52,300 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:52,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5264237990049878. input_tokens=28, output_tokens=246
22:23:53,381 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:53,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.1215557029936463. input_tokens=28, output_tokens=405
22:23:54,106 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:54,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.710923102000379. input_tokens=1749, output_tokens=40
22:23:54,123 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:54,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8033131849952042. input_tokens=2218, output_tokens=383
22:23:54,629 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:54,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.492231672993512. input_tokens=28, output_tokens=1206
22:23:55,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:55,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.5353004489879822. input_tokens=1713, output_tokens=37
22:23:55,355 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:55,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.2430513150029583. input_tokens=28, output_tokens=219
22:23:56,98 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:56,558 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:56,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.4295786199945724. input_tokens=28, output_tokens=492
22:23:58,432 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:58,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.9193405529949814. input_tokens=28, output_tokens=80
22:23:59,231 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:23:59,525 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:01,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.837917602999369. input_tokens=2893, output_tokens=4408
22:24:02,853 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:03,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.206793988996651. input_tokens=2262, output_tokens=4240
22:24:05,217 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:06,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7634053740039235. input_tokens=2110, output_tokens=332
22:24:08,507 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:08,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.514463893006905. input_tokens=28, output_tokens=898
22:24:09,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6832327309966786. input_tokens=2121, output_tokens=665
22:24:10,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6348884769977303. input_tokens=1958, output_tokens=639
22:24:12,183 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:14,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:15,186 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:16,461 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:16,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.776347192004323. input_tokens=28, output_tokens=1220
22:24:18,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.74372843799938. input_tokens=28, output_tokens=587
22:24:18,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:19,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8594120759953512. input_tokens=2142, output_tokens=336
22:24:20,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.9262635829945793. input_tokens=28, output_tokens=276
22:24:22,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.430933481999091. input_tokens=28, output_tokens=571
22:24:27,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:27,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:27,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5793232490104856. input_tokens=28, output_tokens=805
22:24:29,525 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:29,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:30,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2247768039960647. input_tokens=2210, output_tokens=318
22:24:32,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.5682314400037285. input_tokens=1715, output_tokens=40
22:24:33,789 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:34,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.30490125399956. input_tokens=2209, output_tokens=768
22:24:36,921 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:37,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.02775475999806. input_tokens=1979, output_tokens=4126
22:24:37,755 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:39,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.1419898780004587. input_tokens=28, output_tokens=721
22:24:40,886 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:40,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2825773079966893. input_tokens=28, output_tokens=485
22:24:43,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.56383738500881. input_tokens=28, output_tokens=139
22:24:46,725 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:46,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:48,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.356979221003712. input_tokens=2199, output_tokens=4560
22:24:48,863 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:49,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0972894419974182. input_tokens=1756, output_tokens=130
22:24:51,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.7729417590016965. input_tokens=28, output_tokens=1697
22:24:52,673 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:53,831 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:54,705 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:55,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.754806879995158. input_tokens=2136, output_tokens=1554
22:24:55,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:56,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:24:56,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.328351246003876. input_tokens=2222, output_tokens=2448
22:24:57,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7605963829992106. input_tokens=28, output_tokens=400
22:25:00,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6171263780124718. input_tokens=2052, output_tokens=351
22:25:01,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.730092465993948. input_tokens=28, output_tokens=1411
22:25:03,881 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:05,431 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:06,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:06,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:07,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.912802727994858. input_tokens=28, output_tokens=1322
22:25:08,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.8499481009930605. input_tokens=28, output_tokens=466
22:25:09,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7239252690051217. input_tokens=2266, output_tokens=521
22:25:11,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8643071160040563. input_tokens=28, output_tokens=400
22:25:12,69 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:12,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.8631910719996085. input_tokens=2212, output_tokens=1571
22:25:16,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:19,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.256604759997572. input_tokens=2292, output_tokens=767
22:25:19,481 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:20,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:20,838 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:20,926 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:21,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.5832721979968483. input_tokens=28, output_tokens=596
22:25:22,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.111846551008057. input_tokens=2313, output_tokens=1364
22:25:24,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.147992668004008. input_tokens=28, output_tokens=1111
22:25:25,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8197088269953383. input_tokens=2433, output_tokens=599
22:25:26,526 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:26,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.002988120002556. input_tokens=28, output_tokens=1540
22:25:33,663 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:33,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.931197826008429. input_tokens=2655, output_tokens=1509
22:25:34,236 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:36,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:36,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.113278294011252. input_tokens=28, output_tokens=1656
22:25:36,209 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:37,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6717995450017042. input_tokens=2230, output_tokens=336
22:25:39,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.062955109999166. input_tokens=2219, output_tokens=1541
22:25:39,683 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:42,652 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:43,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.2251771459996235. input_tokens=2330, output_tokens=191
22:25:44,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7829180760018062. input_tokens=28, output_tokens=382
22:25:46,672 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:46,925 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:47,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:48,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.317320891990676. input_tokens=28, output_tokens=4276
22:25:49,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.84605332800129. input_tokens=28, output_tokens=1437
22:25:49,728 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:50,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.401786843998707. input_tokens=28, output_tokens=269
22:25:52,859 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:52,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.1398391940019792. input_tokens=1787, output_tokens=206
22:25:53,102 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:55,88 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:55,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.848687898003845. input_tokens=28, output_tokens=4085
22:25:57,828 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:58,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:25:58,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.2015588720096275. input_tokens=2231, output_tokens=1898
22:26:00,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0254946570057655. input_tokens=2894, output_tokens=132
22:26:02,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3543833129951963. input_tokens=28, output_tokens=236
22:26:03,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:06,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.8692436450073728. input_tokens=1988, output_tokens=101
22:26:07,132 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:08,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.803277265993529. input_tokens=1794, output_tokens=352
22:26:08,562 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:10,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.22164152401092. input_tokens=28, output_tokens=378
22:26:11,329 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:12,918 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:13,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.247834530993714. input_tokens=2340, output_tokens=212
22:26:13,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:15,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.62736368199694. input_tokens=28, output_tokens=1818
22:26:16,4 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:18,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.19585431899759. input_tokens=28, output_tokens=613
22:26:18,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:19,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3960648329957621. input_tokens=28, output_tokens=249
22:26:21,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4603326409996953. input_tokens=2759, output_tokens=281
22:26:24,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6531719009944936. input_tokens=28, output_tokens=279
22:26:24,902 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:25,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:27,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:29,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.9197030599898426. input_tokens=1991, output_tokens=284
22:26:30,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.24519745100406. input_tokens=2423, output_tokens=1631
22:26:31,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.1414321000047494. input_tokens=28, output_tokens=501
22:26:31,638 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:32,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:34,250 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:36,220 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:36,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.3888988149992656. input_tokens=28, output_tokens=506
22:26:37,425 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:37,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3855252110079164. input_tokens=2050, output_tokens=626
22:26:38,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.0388064150029095. input_tokens=1944, output_tokens=1446
22:26:39,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.026845155007322. input_tokens=2029, output_tokens=1389
22:26:41,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.6278076370072085. input_tokens=28, output_tokens=286
22:26:45,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:45,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.582485667997389. input_tokens=28, output_tokens=948
22:26:48,701 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:50,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.253258056996856. input_tokens=2199, output_tokens=1263
22:26:50,948 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:51,301 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:52,600 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:53,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5058379340043757. input_tokens=2473, output_tokens=282
22:26:53,267 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:54,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.644350825998117. input_tokens=28, output_tokens=1839
22:26:55,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.3653274410025915. input_tokens=2315, output_tokens=1076
22:26:56,955 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:26:57,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.241753996000625. input_tokens=28, output_tokens=1614
22:27:01,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.099280171998544. input_tokens=28, output_tokens=1323
22:27:01,888 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:02,274 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:03,783 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:03,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.077641630996368. input_tokens=1735, output_tokens=170
22:27:04,404 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:05,846 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:06,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5924348219996318. input_tokens=2231, output_tokens=304
22:27:08,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:08,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1615331770008197. input_tokens=28, output_tokens=200
22:27:09,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.1864219950075494. input_tokens=28, output_tokens=680
22:27:10,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.729858045000583. input_tokens=28, output_tokens=2048
22:27:12,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.860013842000626. input_tokens=1753, output_tokens=102
22:27:16,208 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:17,629 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:17,840 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:19,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6298618880100548. input_tokens=1737, output_tokens=181
22:27:19,790 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:20,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8423321239970392. input_tokens=28, output_tokens=267
22:27:21,427 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:21,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.8452250459959032. input_tokens=1752, output_tokens=102
22:27:24,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.587770735990489. input_tokens=1736, output_tokens=170
22:27:24,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:26,721 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:28,218 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:28,876 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:28,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.0313429749949137. input_tokens=28, output_tokens=202
22:27:30,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.059088708992931. input_tokens=28, output_tokens=2458
22:27:31,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.2938390749914106. input_tokens=28, output_tokens=226
22:27:32,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.2929640530055622. input_tokens=28, output_tokens=249
22:27:33,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5833661090000533. input_tokens=1757, output_tokens=159
22:27:37,119 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:39,161 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:39,468 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:40,80 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:40,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2281159259873675. input_tokens=1741, output_tokens=377
22:27:41,469 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:42,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8549244559981162. input_tokens=28, output_tokens=422
22:27:43,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3701731600012863. input_tokens=2230, output_tokens=764
22:27:44,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.5653961599891772. input_tokens=1756, output_tokens=159
22:27:46,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7472265780088492. input_tokens=2128, output_tokens=324
22:27:47,216 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:51,765 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:51,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.4026420340087498. input_tokens=28, output_tokens=422
22:27:53,238 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:54,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.475694199994905. input_tokens=28, output_tokens=263
22:27:54,499 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:56,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.083829991010134. input_tokens=28, output_tokens=1254
22:27:57,195 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:27:59,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.9268717569939326. input_tokens=2210, output_tokens=755
22:28:00,761 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:01,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8125477529974887. input_tokens=2086, output_tokens=377
22:28:02,307 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:03,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.9684675210010028. input_tokens=2208, output_tokens=746
22:28:06,53 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:06,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.850533523000195. input_tokens=2513, output_tokens=1153
22:28:07,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.336085652001202. input_tokens=28, output_tokens=2477
22:28:08,117 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:09,511 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:12,57 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:12,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.597287338998285. input_tokens=28, output_tokens=913
22:28:13,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.0921202699973946. input_tokens=28, output_tokens=674
22:28:13,781 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:14,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.8980610170110594. input_tokens=28, output_tokens=1790
22:28:17,957 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:18,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.906346774994745. input_tokens=2110, output_tokens=796
22:28:19,119 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:20,190 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:20,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.289624855999136. input_tokens=28, output_tokens=2139
22:28:21,756 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:22,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2458378070004983. input_tokens=2145, output_tokens=457
22:28:24,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.9073545450082747. input_tokens=1702, output_tokens=74
22:28:26,48 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:26,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.089627192995977. input_tokens=2302, output_tokens=1585
22:28:29,158 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:30,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:31,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.355374417005805. input_tokens=28, output_tokens=998
22:28:32,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.851611396006774. input_tokens=2534, output_tokens=939
22:28:33,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.875293065997539. input_tokens=28, output_tokens=305
22:28:34,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:35,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:37,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:38,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.815801645992906. input_tokens=28, output_tokens=1808
22:28:39,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.128166701993905. input_tokens=28, output_tokens=1339
22:28:40,438 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:40,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.943929171000491. input_tokens=1857, output_tokens=716
22:28:41,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:43,816 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:45,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0701675200107275. input_tokens=2183, output_tokens=593
22:28:46,88 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:47,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.36983549100114. input_tokens=28, output_tokens=1506
22:28:48,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.42467538799974136. input_tokens=1699, output_tokens=6
22:28:50,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4888620559941046. input_tokens=28, output_tokens=270
22:28:51,167 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:52,655 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:55,168 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:28:55,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7520631740044337. input_tokens=28, output_tokens=370
22:28:56,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 0.8290023779991316. input_tokens=28, output_tokens=78
22:28:57,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.983180384006118. input_tokens=2799, output_tokens=4253
22:29:01,728 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:02,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6720629170013126. input_tokens=2261, output_tokens=517
22:29:05,379 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:05,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.115100772003643. input_tokens=2195, output_tokens=1183
22:29:06,747 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:07,729 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:07,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.48861633600609. input_tokens=2273, output_tokens=4385
22:29:08,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.71352303099411. input_tokens=2791, output_tokens=4103
22:29:09,495 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:11,208 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:11,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.070326365996152. input_tokens=28, output_tokens=1859
22:29:13,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.907101781005622. input_tokens=28, output_tokens=490
22:29:16,92 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:16,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:17,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.6776099569979124. input_tokens=28, output_tokens=764
22:29:17,884 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:18,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.728814627000247. input_tokens=28, output_tokens=4477
22:29:20,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.0583249969931785. input_tokens=2107, output_tokens=567
22:29:23,98 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:23,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.96036530600395. input_tokens=28, output_tokens=4380
22:29:23,548 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:24,35 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:24,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 4.336213265996776. input_tokens=2579, output_tokens=1066
22:29:26,736 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:26,886 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:28,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4380320729978848. input_tokens=1969, output_tokens=307
22:29:30,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.702613514004042. input_tokens=2465, output_tokens=2384
22:29:31,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.647340034993249. input_tokens=28, output_tokens=283
22:29:31,844 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:35,245 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:36,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.3945426889986265. input_tokens=2394, output_tokens=1209
22:29:37,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.7706238829996437. input_tokens=28, output_tokens=551
22:29:37,798 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:38,235 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:40,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:40,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.14120874099899. input_tokens=28, output_tokens=2961
22:29:41,890 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:42,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.551174449006794. input_tokens=28, output_tokens=906
22:29:42,730 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:43,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.670381774994894. input_tokens=2440, output_tokens=1279
22:29:45,738 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.176012656011153. input_tokens=2219, output_tokens=429
22:29:48,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:50,375 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:50,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.225667183010955. input_tokens=28, output_tokens=1150
22:29:51,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 0.819969795003999. input_tokens=1746, output_tokens=109
22:29:52,63 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:52,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.847226132987998. input_tokens=2120, output_tokens=1641
22:29:56,541 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:56,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.1637741080048727. input_tokens=28, output_tokens=178
22:29:56,629 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:29:57,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.123194829997374. input_tokens=28, output_tokens=1230
22:30:01,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.275567525008228. input_tokens=28, output_tokens=1661
22:30:02,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:03,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:03,564 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:03,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.01934580100351. input_tokens=2075, output_tokens=2508
22:30:04,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.507388730999082. input_tokens=28, output_tokens=1286
22:30:06,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.4072724400029983. input_tokens=1791, output_tokens=833
22:30:11,933 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:11,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.341587722010445. input_tokens=28, output_tokens=656
22:30:13,864 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:13,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.482357231987407. input_tokens=28, output_tokens=1819
22:30:14,645 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:14,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.07833300200582. input_tokens=1940, output_tokens=3840
22:30:26,947 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:26,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.292915062003885. input_tokens=28, output_tokens=4003
22:30:28,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:28,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1834639139997307. input_tokens=147, output_tokens=22
22:30:28,269 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:28,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2542028780007968. input_tokens=156, output_tokens=33
22:30:28,510 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:28,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5021194569999352. input_tokens=754, output_tokens=199
22:30:28,811 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:28,824 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:28,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.8116849810030544. input_tokens=410, output_tokens=160
22:30:28,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:29,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:29,449 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:30,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.53880243000458. input_tokens=161, output_tokens=28
22:30:31,831 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:32,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7448653700121213. input_tokens=222, output_tokens=84
22:30:33,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6224436810007319. input_tokens=189, output_tokens=67
22:30:34,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4376982050016522. input_tokens=4579, output_tokens=250
22:30:36,610 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:37,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6015146429999731. input_tokens=192, output_tokens=31
22:30:39,117 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:40,156 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:42,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5663134780043038. input_tokens=172, output_tokens=47
22:30:42,330 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:43,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:44,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6625302390020806. input_tokens=319, output_tokens=76
22:30:46,557 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:46,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49351412898977287. input_tokens=148, output_tokens=22
22:30:48,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4590495369920973. input_tokens=575, output_tokens=343
22:30:49,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46949696700903587. input_tokens=151, output_tokens=25
22:30:51,256 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:51,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8745345220086165. input_tokens=273, output_tokens=111
22:30:53,424 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:54,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:56,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:56,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7611923470103648. input_tokens=204, output_tokens=57
22:30:58,228 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:30:58,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5176675149996299. input_tokens=166, output_tokens=38
22:31:00,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7500927869987208. input_tokens=251, output_tokens=90
22:31:01,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.764620872985688. input_tokens=229, output_tokens=73
22:31:03,348 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:03,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4961627540033078. input_tokens=208, output_tokens=27
22:31:05,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:06,819 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:08,742 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:09,737 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:09,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.157586193003226. input_tokens=205, output_tokens=60
22:31:10,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8016316209977958. input_tokens=241, output_tokens=118
22:31:12,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9539690819947282. input_tokens=170, output_tokens=95
22:31:13,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6537028229940915. input_tokens=186, output_tokens=58
22:31:14,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3700482580024982. input_tokens=648, output_tokens=295
22:31:16,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:17,554 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:18,631 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:20,53 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:21,409 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:21,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9888381119963014. input_tokens=139, output_tokens=14
22:31:22,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5902345640060958. input_tokens=209, output_tokens=55
22:31:24,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4605972860008478. input_tokens=162, output_tokens=26
22:31:25,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6760062359971926. input_tokens=227, output_tokens=80
22:31:26,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8233976459887344. input_tokens=230, output_tokens=91
22:31:28,963 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:28,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.159783535011229. input_tokens=163, output_tokens=41
22:31:31,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:31,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.06195195800683. input_tokens=165, output_tokens=41
22:31:32,969 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:34,172 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:35,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:36,665 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:38,100 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:38,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5369346539955586. input_tokens=164, output_tokens=40
22:31:39,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5319055549916811. input_tokens=166, output_tokens=38
22:31:40,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7087979670031928. input_tokens=221, output_tokens=90
22:31:42,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6102459450048627. input_tokens=179, output_tokens=45
22:31:43,281 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8369103590084706. input_tokens=228, output_tokens=102
22:31:45,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:47,491 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:47,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:49,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:50,439 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:50,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1251165150024462. input_tokens=330, output_tokens=169
22:31:51,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4239385319961002. input_tokens=374, output_tokens=140
22:31:52,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.799566681991564. input_tokens=432, output_tokens=228
22:31:54,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6318699839903275. input_tokens=197, output_tokens=68
22:31:55,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9318294050026452. input_tokens=305, output_tokens=131
22:31:57,492 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:58,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:31:59,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:00,812 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:01,848 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:02,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0413215120061068. input_tokens=287, output_tokens=142
22:32:03,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5299797670013504. input_tokens=182, output_tokens=36
22:32:04,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7098324219987262. input_tokens=232, output_tokens=77
22:32:06,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.738940791998175. input_tokens=206, output_tokens=73
22:32:07,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5689897509873845. input_tokens=156, output_tokens=32
22:32:09,899 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:11,62 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:11,409 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:12,770 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:13,948 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:14,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3961589359969366. input_tokens=238, output_tokens=95
22:32:15,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3518075479951221. input_tokens=335, output_tokens=106
22:32:16,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4857460759958485. input_tokens=158, output_tokens=31
22:32:18,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6414065800054232. input_tokens=185, output_tokens=61
22:32:19,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6127304190013092. input_tokens=167, output_tokens=46
22:32:21,708 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:21,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1497354520106455. input_tokens=204, output_tokens=56
22:32:23,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:25,130 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:25,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:27,562 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:28,295 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:28,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7664673760009464. input_tokens=265, output_tokens=108
22:32:30,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.009635996990255. input_tokens=294, output_tokens=156
22:32:31,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6290165250102291. input_tokens=200, output_tokens=45
22:32:32,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0270403210015502. input_tokens=361, output_tokens=156
22:32:33,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5531414919969393. input_tokens=162, output_tokens=38
22:32:36,647 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:36,726 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:37,884 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:39,492 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:40,395 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:41,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6875090490066214. input_tokens=313, output_tokens=163
22:32:42,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5577663579897489. input_tokens=165, output_tokens=42
22:32:43,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5091362669918453. input_tokens=152, output_tokens=30
22:32:44,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9091376670112368. input_tokens=364, output_tokens=127
22:32:45,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.603844166995259. input_tokens=192, output_tokens=56
22:32:47,534 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:48,768 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:50,13 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:51,370 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:52,453 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:32:53,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5234583169949474. input_tokens=167, output_tokens=32
22:32:54,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5491511470027035. input_tokens=142, output_tokens=39
22:32:55,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5869981730065774. input_tokens=159, output_tokens=48
22:32:56,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7358941740094451. input_tokens=175, output_tokens=76
22:32:57,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6121600640035467. input_tokens=190, output_tokens=59
22:32:59,605 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:00,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:01,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:03,205 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:04,322 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:05,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5415772269916488. input_tokens=148, output_tokens=32
22:33:06,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48589107500447426. input_tokens=151, output_tokens=21
22:33:07,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49906395700236317. input_tokens=146, output_tokens=24
22:33:08,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5186334549944149. input_tokens=150, output_tokens=36
22:33:09,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42773567300173454. input_tokens=163, output_tokens=15
22:33:12,210 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:12,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0912749460112536. input_tokens=156, output_tokens=28
22:33:13,842 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:15,123 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:16,434 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:17,642 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:19,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:19,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0314303689956432. input_tokens=232, output_tokens=109
22:33:20,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42915952700423077. input_tokens=149, output_tokens=17
22:33:21,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5027988939982606. input_tokens=166, output_tokens=36
22:33:22,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6054037109861383. input_tokens=186, output_tokens=50
22:33:24,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6053951999929268. input_tokens=178, output_tokens=50
22:33:26,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:26,964 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:28,321 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:29,425 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:30,578 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:31,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0539405680028722. input_tokens=152, output_tokens=31
22:33:32,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4673526279948419. input_tokens=137, output_tokens=20
22:33:33,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.617415593995247. input_tokens=209, output_tokens=65
22:33:34,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5138926210056525. input_tokens=143, output_tokens=20
22:33:36,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45984952700382564. input_tokens=156, output_tokens=18
22:33:38,602 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:39,771 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:40,365 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:41,644 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:42,816 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:43,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2620961670036195. input_tokens=155, output_tokens=57
22:33:44,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.22429350099992. input_tokens=211, output_tokens=102
22:33:45,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6098932970053283. input_tokens=186, output_tokens=69
22:33:46,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.681663341994863. input_tokens=209, output_tokens=95
22:33:48,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6461879050038988. input_tokens=190, output_tokens=86
22:33:50,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:50,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1597354249970522. input_tokens=184, output_tokens=61
22:33:52,380 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:53,469 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:54,927 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:56,74 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:57,325 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:33:57,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6266668549942551. input_tokens=176, output_tokens=46
22:33:58,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5090334410051582. input_tokens=171, output_tokens=38
22:34:00,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7588749080023263. input_tokens=189, output_tokens=81
22:34:01,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6990708869998343. input_tokens=185, output_tokens=71
22:34:02,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7428615270036971. input_tokens=186, output_tokens=64
22:34:04,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:04,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1827990879974095. input_tokens=186, output_tokens=64
22:34:06,907 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:07,866 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:09,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:10,479 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:11,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:12,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7216737419948913. input_tokens=183, output_tokens=63
22:34:13,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4724356110091321. input_tokens=157, output_tokens=18
22:34:14,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4143338239955483. input_tokens=141, output_tokens=12
22:34:15,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6713440739986254. input_tokens=163, output_tokens=51
22:34:17,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5455882429960184. input_tokens=147, output_tokens=28
22:34:19,360 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:19,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1250402400037274. input_tokens=157, output_tokens=53
22:34:21,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:22,273 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:23,566 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:24,776 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:25,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:26,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.62597811500018. input_tokens=178, output_tokens=32
22:34:27,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.502369217996602. input_tokens=156, output_tokens=36
22:34:29,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5953912130062236. input_tokens=144, output_tokens=33
22:34:30,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5910419689898845. input_tokens=159, output_tokens=30
22:34:31,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44230160300503485. input_tokens=151, output_tokens=20
22:34:33,85 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:34,493 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:35,500 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:36,681 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:37,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:38,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4736033260123804. input_tokens=144, output_tokens=20
22:34:39,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6747970810101833. input_tokens=150, output_tokens=55
22:34:41,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47414465800102334. input_tokens=137, output_tokens=14
22:34:42,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44686576999083627. input_tokens=138, output_tokens=16
22:34:43,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49900629799230956. input_tokens=138, output_tokens=25
22:34:45,727 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:45,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0645165299938526. input_tokens=143, output_tokens=34
22:34:47,462 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:48,642 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:49,802 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:50,993 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:52,274 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:34:52,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5313215500063961. input_tokens=159, output_tokens=41
22:34:54,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5036147060018266. input_tokens=147, output_tokens=25
22:34:55,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4562573870061897. input_tokens=135, output_tokens=9
22:34:56,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4392171310028061. input_tokens=144, output_tokens=15
22:34:57,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.511361206008587. input_tokens=147, output_tokens=25
22:35:00,203 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:00,890 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:02,166 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:03,312 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:03,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2152836120076245. input_tokens=157, output_tokens=55
22:35:05,768 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:06,228 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6949217299988959. input_tokens=156, output_tokens=65
22:35:07,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7628942159935832. input_tokens=250, output_tokens=91
22:35:08,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7071158949984238. input_tokens=202, output_tokens=82
22:35:10,331 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:11,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7503785169974435. input_tokens=161, output_tokens=42
22:35:12,802 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:14,115 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:15,267 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:16,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:17,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.498014968005009. input_tokens=182, output_tokens=32
22:35:18,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5596444880065974. input_tokens=160, output_tokens=43
22:35:19,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.663875939004356. input_tokens=196, output_tokens=71
22:35:20,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.608934322008281. input_tokens=200, output_tokens=67
22:35:21,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6096982469898649. input_tokens=205, output_tokens=72
22:35:24,370 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:25,402 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:25,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1066383680008585. input_tokens=192, output_tokens=60
22:35:27,570 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:28,522 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:29,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:30,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:31,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.282032307004556. input_tokens=211, output_tokens=81
22:35:32,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9654602719965624. input_tokens=226, output_tokens=105
22:35:33,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7094495019991882. input_tokens=223, output_tokens=90
22:35:35,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.574903639004333. input_tokens=188, output_tokens=50
22:35:36,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6459196149953641. input_tokens=194, output_tokens=77
22:35:38,628 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:38,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1797867929999484. input_tokens=191, output_tokens=76
22:35:40,583 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:41,599 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:42,740 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:43,930 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:45,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:45,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7521332150063245. input_tokens=210, output_tokens=81
22:35:47,74 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5598347120103426. input_tokens=155, output_tokens=34
22:35:48,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4935779950028518. input_tokens=155, output_tokens=20
22:35:49,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4765608150046319. input_tokens=161, output_tokens=30
22:35:50,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46381617001316044. input_tokens=150, output_tokens=23
22:35:52,985 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:52,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1021301629953086. input_tokens=165, output_tokens=39
22:35:54,886 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:55,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:57,204 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:58,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:35:59,492 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:00,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6986238039971795. input_tokens=167, output_tokens=63
22:36:01,429 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5537812890106579. input_tokens=144, output_tokens=17
22:36:02,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6000656449905364. input_tokens=176, output_tokens=48
22:36:03,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44179942799382843. input_tokens=163, output_tokens=20
22:36:05,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4750708770006895. input_tokens=146, output_tokens=13
22:36:07,224 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:07,897 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:09,179 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:10,306 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:11,554 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:12,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9879593410005327. input_tokens=148, output_tokens=26
22:36:13,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45860606800124515. input_tokens=148, output_tokens=23
22:36:14,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5344666629971471. input_tokens=163, output_tokens=45
22:36:15,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4543855100055225. input_tokens=146, output_tokens=14
22:36:17,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49473046499770135. input_tokens=169, output_tokens=32
22:36:19,353 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:19,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0749140409898246. input_tokens=137, output_tokens=16
22:36:21,146 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:22,445 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:23,499 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:24,937 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:26,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:26,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5900901620043442. input_tokens=170, output_tokens=49
22:36:27,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6826293070043903. input_tokens=193, output_tokens=69
22:36:29,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5286304239998572. input_tokens=167, output_tokens=37
22:36:30,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.759372007989441. input_tokens=312, output_tokens=71
22:36:31,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6916170990007231. input_tokens=318, output_tokens=98
22:36:33,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:33,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1279123249987606. input_tokens=183, output_tokens=47
22:36:35,462 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:36,962 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:37,876 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:39,124 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:40,360 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:40,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5260623409994878. input_tokens=185, output_tokens=38
22:36:42,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.818276092002634. input_tokens=203, output_tokens=86
22:36:43,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5251800280093448. input_tokens=144, output_tokens=25
22:36:44,581 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5661809000011999. input_tokens=174, output_tokens=42
22:36:45,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5951459189964226. input_tokens=162, output_tokens=43
22:36:47,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:48,780 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:49,959 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:51,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:52,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:36:53,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7565800219890662. input_tokens=183, output_tokens=50
22:36:54,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5876186030072859. input_tokens=183, output_tokens=50
22:36:55,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5585919929872034. input_tokens=158, output_tokens=43
22:36:56,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5891720959916711. input_tokens=165, output_tokens=30
22:36:57,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7178714640031103. input_tokens=163, output_tokens=73
22:37:00,305 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:01,421 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:01,423 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1796789700019872. input_tokens=161, output_tokens=45
22:37:03,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:04,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:05,622 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:06,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2703137310018064. input_tokens=164, output_tokens=44
22:37:08,6 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:08,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5764284809993114. input_tokens=159, output_tokens=41
22:37:09,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5888230910059065. input_tokens=162, output_tokens=43
22:37:11,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.589034921998973. input_tokens=165, output_tokens=46
22:37:12,848 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:13,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5621857450023526. input_tokens=160, output_tokens=46
22:37:15,289 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:16,404 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:17,523 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:18,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:18,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46503962100541685. input_tokens=149, output_tokens=0
22:37:19,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5895999479980674. input_tokens=159, output_tokens=45
22:37:20,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6196887959958985. input_tokens=150, output_tokens=38
22:37:21,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5270446299982723. input_tokens=168, output_tokens=36
22:37:23,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4387812500062864. input_tokens=138, output_tokens=15
22:37:25,365 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:25,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0554965350020211. input_tokens=160, output_tokens=24
22:37:26,980 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:28,260 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:29,472 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:30,665 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:31,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:32,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4132190649979748. input_tokens=145, output_tokens=12
22:37:33,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48527073199511506. input_tokens=145, output_tokens=25
22:37:35,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4884502810018603. input_tokens=150, output_tokens=14
22:37:36,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47308602900011465. input_tokens=137, output_tokens=13
22:37:37,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4417414240015205. input_tokens=132, output_tokens=9
22:37:39,118 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:40,313 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:41,452 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:42,652 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:43,849 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:44,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49828286199772265. input_tokens=134, output_tokens=11
22:37:45,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4852362439996796. input_tokens=139, output_tokens=14
22:37:47,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4162039569928311. input_tokens=135, output_tokens=11
22:37:48,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4089002560067456. input_tokens=132, output_tokens=8
22:37:49,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3980636579944985. input_tokens=131, output_tokens=8
22:37:51,696 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:51,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0260367750015575. input_tokens=136, output_tokens=9
22:37:53,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:54,611 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:55,752 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:56,964 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:58,268 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:37:58,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.534452355990652. input_tokens=143, output_tokens=28
22:38:00,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5022248659952311. input_tokens=152, output_tokens=27
22:38:01,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43515710800420493. input_tokens=144, output_tokens=13
22:38:02,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4404317250009626. input_tokens=161, output_tokens=31
22:38:03,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5370825069985585. input_tokens=176, output_tokens=37
22:38:06,216 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:07,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:07,900 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:09,646 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:09,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0698144630005118. input_tokens=150, output_tokens=35
22:38:11,369 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:12,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2628192889969796. input_tokens=170, output_tokens=51
22:38:13,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0160322909941897. input_tokens=146, output_tokens=17
22:38:14,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5316639179945923. input_tokens=160, output_tokens=30
22:38:16,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:16,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.521782464988064. input_tokens=169, output_tokens=30
22:38:18,597 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:19,764 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:21,99 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:22,333 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:22,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.647458707986516. input_tokens=159, output_tokens=39
22:38:24,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5247836679918692. input_tokens=138, output_tokens=14
22:38:25,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48320637199503835. input_tokens=139, output_tokens=18
22:38:26,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6094414830004098. input_tokens=158, output_tokens=46
22:38:27,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6349238249968039. input_tokens=154, output_tokens=45
22:38:30,77 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:30,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1560744480084395. input_tokens=164, output_tokens=50
22:38:31,839 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:33,116 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:34,150 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:35,510 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:36,670 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:37,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.558054417007952. input_tokens=157, output_tokens=45
22:38:38,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6321495110023534. input_tokens=161, output_tokens=45
22:38:39,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45656570899882354. input_tokens=152, output_tokens=24
22:38:40,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6086609600024531. input_tokens=165, output_tokens=48
22:38:42,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5609336249908665. input_tokens=170, output_tokens=40
22:38:44,408 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:45,4 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:46,441 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:47,454 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:48,827 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:49,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0801094700000249. input_tokens=170, output_tokens=43
22:38:50,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46960955500253476. input_tokens=142, output_tokens=19
22:38:51,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6991304559924174. input_tokens=181, output_tokens=55
22:38:52,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.504835906001972. input_tokens=173, output_tokens=43
22:38:54,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.669804852004745. input_tokens=216, output_tokens=87
22:38:56,402 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:57,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:58,355 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:38:59,488 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:01,11 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:01,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0246152059989981. input_tokens=147, output_tokens=24
22:39:02,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5630667029909091. input_tokens=190, output_tokens=53
22:39:03,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5634683340031188. input_tokens=143, output_tokens=22
22:39:05,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4891691520024324. input_tokens=148, output_tokens=30
22:39:06,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8051534859987441. input_tokens=243, output_tokens=77
22:39:08,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:08,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1069700839871075. input_tokens=146, output_tokens=21
22:39:10,710 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:11,436 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:12,751 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:13,822 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:15,193 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:15,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9736291750014061. input_tokens=147, output_tokens=22
22:39:16,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4924664459977066. input_tokens=142, output_tokens=26
22:39:18,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5991676550038392. input_tokens=150, output_tokens=45
22:39:19,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46291840699268505. input_tokens=141, output_tokens=20
22:39:20,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6284793990053004. input_tokens=163, output_tokens=56
22:39:22,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:23,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:24,617 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:25,922 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:27,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:27,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0413458090042695. input_tokens=165, output_tokens=42
22:39:29,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5517969700013055. input_tokens=150, output_tokens=31
22:39:30,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4191062570025679. input_tokens=139, output_tokens=13
22:39:31,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5166319450072479. input_tokens=173, output_tokens=40
22:39:32,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5357029540027725. input_tokens=170, output_tokens=42
22:39:34,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:34,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0463354060048005. input_tokens=150, output_tokens=33
22:39:36,953 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:37,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:39,10 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:40,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:41,427 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:42,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8733902749954723. input_tokens=179, output_tokens=98
22:39:43,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5290003170084674. input_tokens=176, output_tokens=42
22:39:44,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5148254779924173. input_tokens=175, output_tokens=43
22:39:45,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4770870929933153. input_tokens=149, output_tokens=26
22:39:46,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5160199649981223. input_tokens=165, output_tokens=37
22:39:49,159 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:49,859 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:51,324 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:52,246 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:53,503 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:39:54,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.032824363996042. input_tokens=144, output_tokens=26
22:39:55,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.523867864991189. input_tokens=170, output_tokens=35
22:39:56,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7805284439964453. input_tokens=214, output_tokens=98
22:39:57,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4958838860038668. input_tokens=156, output_tokens=33
22:39:58,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5463341199938441. input_tokens=156, output_tokens=42
22:40:01,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:01,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1166002099926118. input_tokens=142, output_tokens=26
22:40:03,160 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:04,198 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:05,372 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:06,682 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:07,875 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:08,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6638339399942197. input_tokens=202, output_tokens=79
22:40:09,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49475796999468. input_tokens=157, output_tokens=25
22:40:10,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4613865330029512. input_tokens=161, output_tokens=23
22:40:12,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.564376683003502. input_tokens=158, output_tokens=23
22:40:13,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5495526340091601. input_tokens=158, output_tokens=24
22:40:15,225 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:16,238 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:17,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:18,635 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:19,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:20,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6794982089922996. input_tokens=162, output_tokens=22
22:40:21,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48421055600920226. input_tokens=164, output_tokens=35
22:40:22,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6287784059968544. input_tokens=171, output_tokens=41
22:40:24,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46434090500406455. input_tokens=151, output_tokens=23
22:40:25,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4429501690028701. input_tokens=155, output_tokens=27
22:40:27,185 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:28,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:29,539 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:30,658 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:31,881 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:32,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5874788649962284. input_tokens=150, output_tokens=41
22:40:33,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4926402529963525. input_tokens=155, output_tokens=32
22:40:35,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5252005700021982. input_tokens=147, output_tokens=24
22:40:36,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4379503459931584. input_tokens=149, output_tokens=22
22:40:37,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4529634240025189. input_tokens=145, output_tokens=20
22:40:39,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:39,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1904207720072009. input_tokens=208, output_tokens=59
22:40:41,867 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:42,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:44,24 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:45,299 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:46,349 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:47,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8228624280018266. input_tokens=188, output_tokens=68
22:40:48,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5689581210026518. input_tokens=180, output_tokens=37
22:40:49,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5639921700058039. input_tokens=174, output_tokens=41
22:40:50,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6314689889986767. input_tokens=153, output_tokens=43
22:40:51,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4750719990115613. input_tokens=130, output_tokens=8
22:40:54,359 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:55,502 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:55,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1970306419971166. input_tokens=200, output_tokens=69
22:40:57,170 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:58,366 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:40:59,590 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:00,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2614586430136114. input_tokens=220, output_tokens=68
22:41:02,116 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:02,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4645691930054454. input_tokens=144, output_tokens=19
22:41:03,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.453092032999848. input_tokens=136, output_tokens=13
22:41:05,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4697472870029742. input_tokens=150, output_tokens=25
22:41:06,875 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:07,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5856781129987212. input_tokens=149, output_tokens=33
22:41:09,366 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:10,553 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:11,687 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:12,869 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:13,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5307055180019233. input_tokens=172, output_tokens=40
22:41:14,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6116365259949816. input_tokens=179, output_tokens=57
22:41:15,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5917251009959728. input_tokens=177, output_tokens=42
22:41:17,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5169706419983413. input_tokens=162, output_tokens=46
22:41:18,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49686017898784485. input_tokens=148, output_tokens=28
22:41:20,776 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:20,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1902336460043443. input_tokens=143, output_tokens=23
22:41:22,622 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:24,68 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:24,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8819410889991559. input_tokens=225, output_tokens=99
22:41:25,744 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:26,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:28,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:28,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6433443519927096. input_tokens=201, output_tokens=63
22:41:30,614 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:31,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4742384800047148. input_tokens=182, output_tokens=33
22:41:32,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5064743349939818. input_tokens=154, output_tokens=28
22:41:33,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.505339744995581. input_tokens=147, output_tokens=28
22:41:35,473 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:36,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5186391609895509. input_tokens=151, output_tokens=25
22:41:38,68 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:38,949 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:40,171 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:40,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5604805890034186. input_tokens=156, output_tokens=42
22:41:42,613 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:43,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.74510358000407. input_tokens=178, output_tokens=66
22:41:44,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4180937179917237. input_tokens=140, output_tokens=13
22:41:45,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43366204301128164. input_tokens=140, output_tokens=17
22:41:47,480 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:48,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46307371600414626. input_tokens=152, output_tokens=21
22:41:50,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:51,441 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:52,399 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:53,576 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:41:54,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5147811690112576. input_tokens=149, output_tokens=22
22:41:55,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.650110863003647. input_tokens=182, output_tokens=51
22:41:56,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8595903980021831. input_tokens=197, output_tokens=91
22:41:57,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6090319420036394. input_tokens=209, output_tokens=68
22:41:59,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5806959780020406. input_tokens=159, output_tokens=40
22:42:01,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:02,459 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:02,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0372106730064843. input_tokens=262, output_tokens=153
22:42:04,486 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:05,385 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:06,538 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:07,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9776115730055608. input_tokens=129, output_tokens=10
22:42:08,929 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:09,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.824882223008899. input_tokens=255, output_tokens=93
22:42:10,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5154142730025342. input_tokens=149, output_tokens=33
22:42:12,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4619562260049861. input_tokens=138, output_tokens=13
22:42:13,806 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:14,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44331381599477027. input_tokens=148, output_tokens=22
22:42:16,221 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:17,413 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:18,719 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:19,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5055251410085475. input_tokens=145, output_tokens=30
22:42:21,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:21,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5066473519982537. input_tokens=162, output_tokens=30
22:42:22,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48991069800104015. input_tokens=146, output_tokens=23
22:42:24,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:25,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5881865010014735. input_tokens=158, output_tokens=37
22:42:26,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6417433970054844. input_tokens=159, output_tokens=15
22:42:28,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:29,392 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:30,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42770269500033464. input_tokens=140, output_tokens=12
22:42:31,806 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:33,37 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:33,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4612698300043121. input_tokens=158, output_tokens=20
22:42:35,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:36,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4148048190108966. input_tokens=130, output_tokens=11
22:42:37,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42019417000119574. input_tokens=142, output_tokens=20
22:42:38,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44334269799583126. input_tokens=134, output_tokens=10
22:42:40,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:41,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4315292350074742. input_tokens=140, output_tokens=17
22:42:42,732 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:43,913 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:45,151 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:46,266 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:47,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44088091200683266. input_tokens=132, output_tokens=9
22:42:48,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5044268639903748. input_tokens=140, output_tokens=13
22:42:49,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47535410799901. input_tokens=139, output_tokens=15
22:42:50,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5068030540132895. input_tokens=139, output_tokens=19
22:42:51,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4118409249931574. input_tokens=129, output_tokens=6
22:42:54,18 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:54,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:55,995 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:57,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:58,530 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:42:59,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9438742970087333. input_tokens=137, output_tokens=10
22:43:00,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5145388620003359. input_tokens=149, output_tokens=20
22:43:01,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5061550579994218. input_tokens=164, output_tokens=37
22:43:02,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5779966239933856. input_tokens=152, output_tokens=26
22:43:03,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6254237300017849. input_tokens=197, output_tokens=67
22:43:06,183 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:06,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.058021919001476. input_tokens=153, output_tokens=31
22:43:07,850 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:09,146 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:10,486 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:11,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:12,698 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:13,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46398108900757506. input_tokens=144, output_tokens=12
22:43:14,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5513650389912073. input_tokens=192, output_tokens=50
22:43:15,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6834200140001485. input_tokens=176, output_tokens=67
22:43:17,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.682313643002999. input_tokens=206, output_tokens=81
22:43:18,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48441785799514037. input_tokens=149, output_tokens=27
22:43:20,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:21,191 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:22,827 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:23,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:25,116 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:25,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9598014059884008. input_tokens=135, output_tokens=13
22:43:26,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.552681401008158. input_tokens=154, output_tokens=45
22:43:27,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9813907700008713. input_tokens=141, output_tokens=22
22:43:29,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5393829309905414. input_tokens=156, output_tokens=37
22:43:30,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8560792330099503. input_tokens=197, output_tokens=93
22:43:32,528 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:32,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0477972889930243. input_tokens=178, output_tokens=43
22:43:34,383 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:35,511 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:38,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:39,198 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:39,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6529804549936671. input_tokens=192, output_tokens=67
22:43:40,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:40,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.7181240749923745. input_tokens=150, output_tokens=32
22:43:42,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5723182200017618. input_tokens=163, output_tokens=50
22:43:43,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0564410880033392. input_tokens=159, output_tokens=34
22:43:44,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6359874589979881. input_tokens=196, output_tokens=61
22:43:46,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:47,374 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:48,508 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:49,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:51,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:51,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6004486290039495. input_tokens=179, output_tokens=45
22:43:52,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4920256600016728. input_tokens=171, output_tokens=30
22:43:54,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4185746439907234. input_tokens=153, output_tokens=16
22:43:55,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6067592009931104. input_tokens=169, output_tokens=41
22:43:56,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6939327709987992. input_tokens=243, output_tokens=103
22:43:58,785 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:43:59,522 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:00,690 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:01,858 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:03,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0605100150132785. input_tokens=143, output_tokens=24
22:44:03,922 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:04,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5956817569967825. input_tokens=178, output_tokens=52
22:44:06,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5572792199964169. input_tokens=166, output_tokens=49
22:44:07,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5173601300048176. input_tokens=156, output_tokens=35
22:44:09,24 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:09,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.373673420996056. input_tokens=278, output_tokens=195
22:44:11,616 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:12,713 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:13,859 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:15,352 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:15,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45587419500225224. input_tokens=150, output_tokens=27
22:44:17,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6373674500064226. input_tokens=162, output_tokens=49
22:44:18,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5259229250077624. input_tokens=145, output_tokens=19
22:44:19,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46335495999665. input_tokens=144, output_tokens=21
22:44:20,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.749351619000663. input_tokens=264, output_tokens=111
22:44:22,805 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:24,228 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:24,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1993116490048124. input_tokens=206, output_tokens=71
22:44:25,924 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:27,336 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:28,313 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:29,490 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:30,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9843030740012182. input_tokens=166, output_tokens=21
22:44:31,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49532468400138896. input_tokens=161, output_tokens=36
22:44:32,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7001464560016757. input_tokens=170, output_tokens=67
22:44:33,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4692948829906527. input_tokens=147, output_tokens=23
22:44:35,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43866156600415707. input_tokens=145, output_tokens=12
22:44:37,335 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:37,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0656405519985128. input_tokens=162, output_tokens=40
22:44:39,57 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:40,209 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:41,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:42,714 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:43,992 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:44,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5167861429945333. input_tokens=159, output_tokens=27
22:44:45,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.462551849996089. input_tokens=164, output_tokens=31
22:44:46,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47015166499477345. input_tokens=153, output_tokens=24
22:44:48,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5508003890136024. input_tokens=165, output_tokens=40
22:44:49,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6216644700034522. input_tokens=152, output_tokens=32
22:44:51,568 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:52,414 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:53,534 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:54,731 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:56,147 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:44:56,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9783873830019729. input_tokens=137, output_tokens=17
22:44:57,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6166176609985996. input_tokens=161, output_tokens=33
22:44:59,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5294506679929327. input_tokens=158, output_tokens=37
22:45:00,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5184131739952136. input_tokens=159, output_tokens=40
22:45:01,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7272301179909846. input_tokens=210, output_tokens=76
22:45:03,156 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:04,365 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:05,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:06,838 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:07,964 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:08,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5162462959997356. input_tokens=174, output_tokens=25
22:45:09,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5161355450109113. input_tokens=173, output_tokens=34
22:45:11,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6770988899952499. input_tokens=181, output_tokens=59
22:45:12,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5745006430079229. input_tokens=183, output_tokens=44
22:45:13,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4923945130140055. input_tokens=143, output_tokens=25
22:45:15,798 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:15,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1066269139992073. input_tokens=158, output_tokens=31
22:45:17,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:18,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:20,26 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:21,68 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:22,333 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:23,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4349303049966693. input_tokens=144, output_tokens=17
22:45:24,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5994221400032984. input_tokens=213, output_tokens=71
22:45:25,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6092696890118532. input_tokens=176, output_tokens=45
22:45:26,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4445394590002252. input_tokens=147, output_tokens=15
22:45:27,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5050844580109697. input_tokens=155, output_tokens=20
22:45:30,384 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:31,373 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:31,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1183353539963719. input_tokens=153, output_tokens=41
22:45:33,86 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:34,494 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:35,659 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:36,894 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:37,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3370047740027076. input_tokens=166, output_tokens=49
22:45:38,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5102093300083652. input_tokens=145, output_tokens=24
22:45:39,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.708494790000259. input_tokens=204, output_tokens=77
22:45:41,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6655109989951598. input_tokens=202, output_tokens=62
22:45:42,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6921317690139404. input_tokens=211, output_tokens=92
22:45:44,464 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:44,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0425519139971584. input_tokens=152, output_tokens=32
22:45:46,103 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:48,306 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:49,236 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:49,858 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:51,89 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:45:51,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1538102520134998. input_tokens=191, output_tokens=64
22:45:52,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43680139699426945. input_tokens=138, output_tokens=17
22:45:54,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4316977389971726. input_tokens=546, output_tokens=306
22:45:55,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5684883420035476. input_tokens=189, output_tokens=63
22:45:56,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5936118910030928. input_tokens=189, output_tokens=63
22:45:58,926 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:00,240 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:00,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:02,313 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:03,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:03,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2729483000002801. input_tokens=187, output_tokens=71
22:46:04,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3810466029972304. input_tokens=244, output_tokens=92
22:46:06,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5278874899959192. input_tokens=163, output_tokens=31
22:46:07,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0375114329945063. input_tokens=290, output_tokens=166
22:46:08,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6630890820088098. input_tokens=188, output_tokens=66
22:46:10,725 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:11,508 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:12,641 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:14,237 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:15,269 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:15,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0298627629963448. input_tokens=153, output_tokens=26
22:46:16,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.606377899996005. input_tokens=176, output_tokens=49
22:46:18,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.531531164990156. input_tokens=180, output_tokens=46
22:46:19,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9190280699986033. input_tokens=293, output_tokens=166
22:46:20,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7442668369913008. input_tokens=239, output_tokens=75
22:46:23,128 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:24,478 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:25,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:26,368 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:27,273 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:27,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3836622850067215. input_tokens=259, output_tokens=103
22:46:28,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5285932629922172. input_tokens=380, output_tokens=152
22:46:30,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9392471619939897. input_tokens=302, output_tokens=149
22:46:31,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.008086268993793. input_tokens=384, output_tokens=176
22:46:32,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7057692169910297. input_tokens=234, output_tokens=96
22:46:35,83 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:36,266 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:36,977 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:38,134 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:39,172 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:39,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2955847620032728. input_tokens=307, output_tokens=100
22:46:41,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.269370968992007. input_tokens=222, output_tokens=82
22:46:42,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.774470221993397. input_tokens=222, output_tokens=84
22:46:43,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7250237970001763. input_tokens=225, output_tokens=79
22:46:44,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5551579420134658. input_tokens=179, output_tokens=51
22:46:47,394 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:48,212 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:48,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1675435109937098. input_tokens=161, output_tokens=44
22:46:50,46 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:51,522 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:52,401 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:53,826 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:46:54,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.557368972004042. input_tokens=288, output_tokens=176
22:46:55,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6300715290126391. input_tokens=160, output_tokens=42
22:46:56,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8974950770061696. input_tokens=275, output_tokens=133
22:46:57,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5690758239943534. input_tokens=166, output_tokens=45
22:46:59,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7850779739965219. input_tokens=188, output_tokens=80
22:47:01,347 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:01,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0863173160032602. input_tokens=180, output_tokens=42
22:47:03,120 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:04,513 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:05,662 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:06,909 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:08,92 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:08,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5702110910060583. input_tokens=204, output_tokens=53
22:47:09,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7557853659964167. input_tokens=221, output_tokens=97
22:47:10,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6984451099997386. input_tokens=222, output_tokens=97
22:47:12,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7385521149990382. input_tokens=216, output_tokens=94
22:47:13,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.713910651000333. input_tokens=221, output_tokens=97
22:47:15,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:16,890 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:16,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0833210540004075. input_tokens=188, output_tokens=46
22:47:18,795 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:20,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:20,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:21,713 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.272726082999725. input_tokens=222, output_tokens=97
22:47:23,483 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:24,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7009355449990835. input_tokens=182, output_tokens=64
22:47:25,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7854845539986854. input_tokens=181, output_tokens=70
22:47:26,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4422020970087033. input_tokens=160, output_tokens=27
22:47:28,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:28,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5695029009948485. input_tokens=194, output_tokens=61
22:47:31,263 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:31,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1257025980012259. input_tokens=4023, output_tokens=159
22:47:33,27 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:34,138 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:35,996 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:36,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5708220340020489. input_tokens=183, output_tokens=44
22:47:37,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:38,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5605780510086333. input_tokens=152, output_tokens=38
22:47:39,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.465711110009579. input_tokens=155, output_tokens=19
22:47:40,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.11547851699288. input_tokens=298, output_tokens=166
22:47:42,663 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:43,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4625125710008433. input_tokens=155, output_tokens=22
22:47:45,96 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:46,367 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:47,421 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:48,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:49,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.558962209004676. input_tokens=158, output_tokens=40
22:47:50,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5812840769940522. input_tokens=186, output_tokens=45
22:47:51,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6445594779943349. input_tokens=181, output_tokens=50
22:47:52,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4906612749909982. input_tokens=145, output_tokens=23
22:47:54,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6621015390119283. input_tokens=248, output_tokens=66
22:47:56,533 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:56,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1744675519876182. input_tokens=194, output_tokens=65
22:47:58,563 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:47:59,767 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:00,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:02,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:03,108 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:03,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8286706770013552. input_tokens=289, output_tokens=156
22:48:04,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8250365109997801. input_tokens=286, output_tokens=130
22:48:06,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6580816470086575. input_tokens=180, output_tokens=68
22:48:07,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6797921349934768. input_tokens=214, output_tokens=86
22:48:08,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5427690579963382. input_tokens=193, output_tokens=57
22:48:11,52 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:12,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:12,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0802554500114638. input_tokens=169, output_tokens=48
22:48:13,914 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:14,916 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:16,340 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:17,527 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:18,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2631334780016914. input_tokens=234, output_tokens=63
22:48:19,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6348826890025521. input_tokens=159, output_tokens=40
22:48:20,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42843120999168605. input_tokens=150, output_tokens=22
22:48:21,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6445187979988987. input_tokens=183, output_tokens=65
22:48:22,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6234238519973587. input_tokens=180, output_tokens=66
22:48:24,744 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:25,923 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:27,60 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:28,394 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:29,594 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:30,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6184530120081035. input_tokens=200, output_tokens=62
22:48:31,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5899324619967956. input_tokens=146, output_tokens=37
22:48:32,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5191997540096054. input_tokens=165, output_tokens=46
22:48:33,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6451875920029124. input_tokens=173, output_tokens=45
22:48:34,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6373225059942342. input_tokens=205, output_tokens=73
22:48:37,484 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:38,419 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:39,82 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:40,608 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:41,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:42,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3025230920029571. input_tokens=156, output_tokens=36
22:48:43,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0304185150016565. input_tokens=148, output_tokens=26
22:48:44,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4865178859909065. input_tokens=154, output_tokens=30
22:48:45,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8049275810044492. input_tokens=156, output_tokens=62
22:48:47,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.464414604997728. input_tokens=153, output_tokens=20
22:48:49,346 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:49,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1120465910062194. input_tokens=173, output_tokens=46
22:48:51,212 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:52,320 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:53,520 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:54,704 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:55,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:48:56,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6630108670069603. input_tokens=166, output_tokens=55
22:48:57,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5634500549931545. input_tokens=179, output_tokens=44
22:48:58,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5560334629990393. input_tokens=170, output_tokens=41
22:49:00,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5322255099890754. input_tokens=149, output_tokens=33
22:49:01,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5869116040121298. input_tokens=158, output_tokens=32
22:49:03,306 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:04,329 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:05,558 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:06,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:08,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:08,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7082287819939665. input_tokens=162, output_tokens=36
22:49:09,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5240518469945528. input_tokens=161, output_tokens=32
22:49:11,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.545390129002044. input_tokens=159, output_tokens=37
22:49:12,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6046757309959503. input_tokens=159, output_tokens=39
22:49:13,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7729655379953329. input_tokens=163, output_tokens=71
22:49:15,763 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:15,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1139685719972476. input_tokens=166, output_tokens=48
22:49:17,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:18,732 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:19,914 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:21,29 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:22,346 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:23,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5304010239924537. input_tokens=150, output_tokens=27
22:49:24,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.559787600999698. input_tokens=152, output_tokens=29
22:49:25,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5332132170005934. input_tokens=203, output_tokens=53
22:49:26,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4397940710041439. input_tokens=153, output_tokens=14
22:49:27,816 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5496123150078347. input_tokens=160, output_tokens=39
22:49:29,690 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:30,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:31,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:33,230 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:34,267 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:35,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6738929160055704. input_tokens=257, output_tokens=82
22:49:36,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5709656609978992. input_tokens=176, output_tokens=52
22:49:37,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5497557939961553. input_tokens=168, output_tokens=33
22:49:38,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5907762089918833. input_tokens=152, output_tokens=49
22:49:39,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4201484109944431. input_tokens=148, output_tokens=15
22:49:42,285 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:43,66 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:43,997 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:45,318 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:45,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2165250190009829. input_tokens=143, output_tokens=20
22:49:47,568 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:48,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7905619000084698. input_tokens=167, output_tokens=71
22:49:49,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5123233379999874. input_tokens=166, output_tokens=33
22:49:50,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6261222169996472. input_tokens=215, output_tokens=67
22:49:52,474 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:53,126 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46712674900481943. input_tokens=150, output_tokens=24
22:49:54,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:56,4 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:57,404 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:49:57,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5569267209939426. input_tokens=164, output_tokens=32
22:49:59,754 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:00,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5347050540003693. input_tokens=162, output_tokens=35
22:50:01,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.469665566997719. input_tokens=166, output_tokens=24
22:50:02,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6613483739929507. input_tokens=192, output_tokens=58
22:50:04,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:05,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.601899767993018. input_tokens=186, output_tokens=40
22:50:06,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:08,88 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:09,285 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:10,541 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:11,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5832898220105562. input_tokens=184, output_tokens=60
22:50:12,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5178227319993312. input_tokens=146, output_tokens=33
22:50:13,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5032930700108409. input_tokens=157, output_tokens=22
22:50:14,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4921166109998012. input_tokens=144, output_tokens=12
22:50:16,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5391702079941751. input_tokens=146, output_tokens=24
22:50:18,255 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:18,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0337178319896339. input_tokens=146, output_tokens=15
22:50:19,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:21,228 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:22,626 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:23,509 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:24,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:25,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45933323400095105. input_tokens=149, output_tokens=23
22:50:26,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5625197540066438. input_tokens=152, output_tokens=25
22:50:27,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7521202439966146. input_tokens=141, output_tokens=58
22:50:29,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42778035800438374. input_tokens=151, output_tokens=13
22:50:30,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5519965510029579. input_tokens=184, output_tokens=55
22:50:32,175 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:33,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:34,596 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:35,719 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:36,951 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:37,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6669702299986966. input_tokens=176, output_tokens=43
22:50:38,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48340282498975284. input_tokens=167, output_tokens=25
22:50:39,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6724231049884111. input_tokens=207, output_tokens=79
22:50:41,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5886252319905907. input_tokens=177, output_tokens=43
22:50:42,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6131025869981386. input_tokens=213, output_tokens=61
22:50:45,185 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:45,958 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:45,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.189829856986762. input_tokens=159, output_tokens=44
22:50:47,789 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:48,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:50,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:51,330 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:51,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6254845180083066. input_tokens=193, output_tokens=91
22:50:53,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6283630669931881. input_tokens=243, output_tokens=80
22:50:54,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5811400520033203. input_tokens=198, output_tokens=67
22:50:55,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5712288340000669. input_tokens=157, output_tokens=52
22:50:56,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5426823390007485. input_tokens=166, output_tokens=37
22:50:58,567 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:50:59,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:01,27 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:02,79 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:03,492 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:04,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.562687037003343. input_tokens=157, output_tokens=36
22:51:05,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5816789620002965. input_tokens=174, output_tokens=52
22:51:06,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6113022630015621. input_tokens=155, output_tokens=33
22:51:07,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45638371899258345. input_tokens=166, output_tokens=29
22:51:08,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6608211959974142. input_tokens=213, output_tokens=81
22:51:11,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:11,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0398475719994167. input_tokens=167, output_tokens=21
22:51:12,780 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:13,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:15,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:16,490 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:17,793 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:18,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48393173501244746. input_tokens=167, output_tokens=30
22:51:19,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4769728120008949. input_tokens=171, output_tokens=22
22:51:20,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47730338499241043. input_tokens=151, output_tokens=26
22:51:21,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5703798320028. input_tokens=163, output_tokens=47
22:51:23,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6657526429917198. input_tokens=152, output_tokens=41
22:51:25,494 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:25,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1483150819985894. input_tokens=152, output_tokens=40
22:51:27,238 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:28,336 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
22:51:29,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5401605219958583. input_tokens=167, output_tokens=45
22:51:30,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4314680349925766. input_tokens=157, output_tokens=12
22:51:30,979 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
22:51:31,175 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
22:51:31,176 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
22:51:31,224 datashaper.workflow.workflow INFO executing verb create_final_entities
22:51:31,692 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
22:51:31,727 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
22:51:31,727 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
22:51:31,791 graphrag.index.operations.embed_text.strategies.openai INFO embedding 2727 inputs via 2673 snippets using 168 batches. max_batch_size=16, max_tokens=8191
22:51:32,801 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:32,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0294522550102556. input_tokens=1450, output_tokens=0
22:51:32,925 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:32,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1556427350005833. input_tokens=1049, output_tokens=0
22:51:33,320 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:33,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5488100400107214. input_tokens=611, output_tokens=0
22:51:33,346 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:33,346 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:33,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5767510199948447. input_tokens=750, output_tokens=0
22:51:33,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6028933109919308. input_tokens=1074, output_tokens=0
22:51:34,88 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:34,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2755211259936914. input_tokens=837, output_tokens=0
22:51:34,113 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:34,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1775468950072536. input_tokens=930, output_tokens=0
22:51:34,148 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:34,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7651059439958772. input_tokens=388, output_tokens=0
22:51:34,782 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:34,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3993128430010984. input_tokens=367, output_tokens=0
22:51:34,808 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:34,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4279891479964135. input_tokens=459, output_tokens=0
22:51:41,296 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:41,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.166239055004553. input_tokens=490, output_tokens=0
22:51:46,795 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:46,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6563194009941071. input_tokens=486, output_tokens=0
22:51:53,407 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:53,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2592314779903973. input_tokens=842, output_tokens=0
22:51:59,336 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:51:59,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1784425410005497. input_tokens=707, output_tokens=0
22:52:04,826 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:04,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6593496270070318. input_tokens=513, output_tokens=0
22:52:11,346 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:11,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.17361202299071. input_tokens=625, output_tokens=0
22:52:17,453 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:17,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.268848268999136. input_tokens=648, output_tokens=0
22:52:23,359 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:23,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1684552509977948. input_tokens=371, output_tokens=0
22:52:29,353 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:29,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1515895379998256. input_tokens=430, output_tokens=0
22:52:35,476 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:35,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2697740839939797. input_tokens=225, output_tokens=0
22:52:40,949 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:40,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7312275379954372. input_tokens=170, output_tokens=0
22:52:47,472 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:47,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2457973540003877. input_tokens=194, output_tokens=0
22:52:53,378 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:53,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.143677710002521. input_tokens=252, output_tokens=0
22:52:59,416 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:52:59,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1726512699970044. input_tokens=202, output_tokens=0
22:53:04,889 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:04,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6417276930005755. input_tokens=182, output_tokens=0
22:53:10,906 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:10,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6471977179899113. input_tokens=268, output_tokens=0
22:53:17,424 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:17,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1564668549981434. input_tokens=455, output_tokens=0
22:53:23,541 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:23,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2677184720087098. input_tokens=504, output_tokens=0
22:53:29,541 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:29,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.255133996004588. input_tokens=630, output_tokens=0
22:53:34,951 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:34,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6558538660028717. input_tokens=654, output_tokens=0
22:53:41,557 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:41,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2529523859993787. input_tokens=732, output_tokens=0
22:53:47,460 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:47,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1479614349955227. input_tokens=651, output_tokens=0
22:53:53,478 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:53,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1609947709948756. input_tokens=840, output_tokens=0
22:53:59,683 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:53:59,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3519465620047413. input_tokens=769, output_tokens=0
22:54:05,599 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:05,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2606295959994895. input_tokens=351, output_tokens=0
22:54:11,522 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:11,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1740133019920904. input_tokens=267, output_tokens=0
22:54:17,110 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:17,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7550055269966833. input_tokens=265, output_tokens=0
22:54:23,535 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:23,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1698880059993826. input_tokens=565, output_tokens=0
22:54:29,41 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:29,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6669006129959598. input_tokens=737, output_tokens=0
22:54:35,559 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:35,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1758027960022446. input_tokens=419, output_tokens=0
22:54:41,753 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:41,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.362136786992778. input_tokens=460, output_tokens=0
22:54:47,567 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:47,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1677482719969703. input_tokens=427, output_tokens=0
22:54:53,654 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:53,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2468782890064176. input_tokens=489, output_tokens=0
22:54:59,674 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:54:59,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2604644300008658. input_tokens=491, output_tokens=0
22:55:05,612 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:05,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1859455550002167. input_tokens=555, output_tokens=0
22:55:11,611 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:11,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1797349929984193. input_tokens=484, output_tokens=0
22:55:17,708 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:17,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.266019076007069. input_tokens=284, output_tokens=0
22:55:23,629 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:23,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1776797820057254. input_tokens=305, output_tokens=0
22:55:29,629 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:29,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1693047640001168. input_tokens=441, output_tokens=0
22:55:35,629 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:35,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1646873740101. input_tokens=475, output_tokens=0
22:55:41,326 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:41,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.847717170996475. input_tokens=385, output_tokens=0
22:55:47,751 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:47,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2686503200093284. input_tokens=380, output_tokens=0
22:55:53,822 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:53,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3271608139912132. input_tokens=470, output_tokens=0
22:55:59,673 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:55:59,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1680783760093618. input_tokens=494, output_tokens=0
22:56:05,752 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:05,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2433723550057039. input_tokens=474, output_tokens=0
22:56:11,792 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:11,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2758459920005407. input_tokens=482, output_tokens=0
22:56:17,664 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:17,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1378297470073448. input_tokens=296, output_tokens=0
22:56:23,666 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:23,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1346915819885908. input_tokens=276, output_tokens=0
22:56:29,677 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:29,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1377868349954952. input_tokens=200, output_tokens=0
22:56:35,698 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:35,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1511785769980634. input_tokens=201, output_tokens=0
22:56:41,815 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:41,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2575742659973912. input_tokens=204, output_tokens=0
22:56:47,724 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:47,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1619866410037503. input_tokens=344, output_tokens=0
22:56:53,830 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:53,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2547550590097671. input_tokens=263, output_tokens=0
22:56:59,749 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:56:59,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1661249980097637. input_tokens=236, output_tokens=0
22:57:05,440 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:05,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8474485420010751. input_tokens=366, output_tokens=0
22:57:11,754 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:11,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1532633750030072. input_tokens=564, output_tokens=0
22:57:17,360 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:17,388 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7543108710087836. input_tokens=608, output_tokens=0
22:57:23,778 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:23,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.163737641996704. input_tokens=598, output_tokens=0
22:57:29,880 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:29,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2525785340112634. input_tokens=644, output_tokens=0
22:57:35,897 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:35,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2599959979997948. input_tokens=403, output_tokens=0
22:57:41,914 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:41,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2680389449960785. input_tokens=631, output_tokens=0
22:57:47,974 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:47,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3191212840029038. input_tokens=490, output_tokens=0
22:57:53,500 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:53,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8400768990104552. input_tokens=445, output_tokens=0
22:57:59,933 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:57:59,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2605218120006612. input_tokens=408, output_tokens=0
22:58:05,844 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:05,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.170130494996556. input_tokens=391, output_tokens=0
22:58:11,876 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:11,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1891933829901973. input_tokens=407, output_tokens=0
22:58:17,960 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:17,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.264913285995135. input_tokens=342, output_tokens=0
22:58:23,353 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:23,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6470354210032383. input_tokens=352, output_tokens=0
22:58:29,884 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:29,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1702661300078034. input_tokens=355, output_tokens=0
22:58:35,555 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:35,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8347693950054236. input_tokens=348, output_tokens=0
22:58:41,897 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:41,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.171624205991975. input_tokens=465, output_tokens=0
22:58:48,16 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:48,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2769692529982422. input_tokens=449, output_tokens=0
22:58:53,915 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:53,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1672628840024117. input_tokens=674, output_tokens=0
22:58:59,589 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:58:59,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8335547919996316. input_tokens=653, output_tokens=0
22:59:06,19 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:06,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2546120479964884. input_tokens=667, output_tokens=0
22:59:11,934 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:11,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.160528948996216. input_tokens=516, output_tokens=0
22:59:17,937 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:17,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1548639700049534. input_tokens=407, output_tokens=0
22:59:23,957 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:23,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.164873087007436. input_tokens=591, output_tokens=0
22:59:29,959 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:29,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1585477789922152. input_tokens=369, output_tokens=0
22:59:35,982 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:36,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.172221871005604. input_tokens=575, output_tokens=0
22:59:41,982 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:42,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1645057440036908. input_tokens=377, output_tokens=0
22:59:48,103 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:48,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2754822170099942. input_tokens=332, output_tokens=0
22:59:54,87 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:54,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2499135339894565. input_tokens=715, output_tokens=0
22:59:59,498 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
22:59:59,525 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6546939499967266. input_tokens=283, output_tokens=0
23:00:06,30 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:06,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1736306699895067. input_tokens=294, output_tokens=0
23:00:12,119 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:12,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2541801140032476. input_tokens=270, output_tokens=0
23:00:18,35 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:18,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1609092989965575. input_tokens=287, output_tokens=0
23:00:23,723 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:23,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8405390690022614. input_tokens=270, output_tokens=0
23:00:29,726 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:29,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.834846693003783. input_tokens=297, output_tokens=0
23:00:36,164 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:36,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2675540810014354. input_tokens=263, output_tokens=0
23:00:42,166 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:42,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2568623010010924. input_tokens=209, output_tokens=0
23:00:48,139 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:48,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2208280280028703. input_tokens=273, output_tokens=0
23:00:54,105 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:00:54,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.178636999000446. input_tokens=223, output_tokens=0
23:01:00,101 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:00,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1656883870018646. input_tokens=197, output_tokens=0
23:01:05,655 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:05,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.712098251999123. input_tokens=248, output_tokens=0
23:01:12,109 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:12,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.159366787003819. input_tokens=368, output_tokens=0
23:01:18,228 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:18,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2705920160078676. input_tokens=391, output_tokens=0
23:01:23,621 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:23,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6533301659947028. input_tokens=569, output_tokens=0
23:01:30,134 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:30,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1659723869961454. input_tokens=482, output_tokens=0
23:01:36,153 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:36,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1678746349934954. input_tokens=459, output_tokens=0
23:01:42,152 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:42,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1587617740005953. input_tokens=368, output_tokens=0
23:01:48,242 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:48,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2450700640038121. input_tokens=430, output_tokens=0
23:01:54,261 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:01:54,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.257871317997342. input_tokens=445, output_tokens=0
23:02:00,282 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:00,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2705511399981333. input_tokens=570, output_tokens=0
23:02:05,675 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:05,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6566994219901972. input_tokens=404, output_tokens=0
23:02:12,177 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:12,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1538732290064218. input_tokens=389, output_tokens=0
23:02:18,304 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:18,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2680569639924215. input_tokens=559, output_tokens=0
23:02:24,221 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:24,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1759135009924648. input_tokens=520, output_tokens=0
23:02:30,224 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:30,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1691690389998257. input_tokens=415, output_tokens=0
23:02:36,225 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:36,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1655127070116578. input_tokens=427, output_tokens=0
23:02:42,337 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:42,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2643697219900787. input_tokens=378, output_tokens=0
23:02:48,346 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:48,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2643182979954872. input_tokens=450, output_tokens=0
23:02:54,250 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:02:54,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.161684692997369. input_tokens=417, output_tokens=0
23:03:00,354 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:00,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2551184759940952. input_tokens=399, output_tokens=0
23:03:05,861 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:05,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7576890409982298. input_tokens=456, output_tokens=0
23:03:12,282 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:12,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1690949949988862. input_tokens=655, output_tokens=0
23:03:17,784 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:17,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6624182359955739. input_tokens=370, output_tokens=0
23:03:23,863 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:23,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7345540340029402. input_tokens=454, output_tokens=0
23:03:29,884 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:29,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.746398011004203. input_tokens=330, output_tokens=0
23:03:36,410 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:36,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2616839999973308. input_tokens=249, output_tokens=0
23:03:42,320 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:42,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.163095190000604. input_tokens=482, output_tokens=0
23:03:48,334 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:48,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1765568330010865. input_tokens=337, output_tokens=0
23:03:54,334 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:03:54,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1653794209996704. input_tokens=307, output_tokens=0
23:04:00,352 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:00,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1721857590018772. input_tokens=359, output_tokens=0
23:04:06,440 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:06,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2564744659903226. input_tokens=294, output_tokens=0
23:04:11,839 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:11,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.641584797995165. input_tokens=512, output_tokens=0
23:04:18,520 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:18,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3145338729955256. input_tokens=920, output_tokens=0
23:04:24,575 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:24,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3652014919935027. input_tokens=1110, output_tokens=0
23:04:30,486 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:30,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2638006990018766. input_tokens=1097, output_tokens=0
23:04:36,506 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:36,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2762464889965486. input_tokens=839, output_tokens=0
23:04:42,521 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:42,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2828594559978228. input_tokens=810, output_tokens=0
23:04:48,427 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:48,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.180082876002416. input_tokens=867, output_tokens=0
23:04:54,424 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:04:54,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1672567559871823. input_tokens=821, output_tokens=0
23:05:00,522 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:00,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2628923980082618. input_tokens=770, output_tokens=0
23:05:06,517 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:06,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2551168170029996. input_tokens=1016, output_tokens=0
23:05:12,542 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:12,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.263977217997308. input_tokens=567, output_tokens=0
23:05:18,562 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:18,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.274039124007686. input_tokens=683, output_tokens=0
23:05:24,479 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:24,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1822459430113668. input_tokens=684, output_tokens=0
23:05:30,457 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:30,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1497993250086438. input_tokens=677, output_tokens=0
23:05:36,476 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:36,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1593326310103294. input_tokens=813, output_tokens=0
23:05:42,492 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:42,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1713974510057596. input_tokens=807, output_tokens=0
23:05:48,494 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:48,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1639186870015692. input_tokens=630, output_tokens=0
23:05:54,955 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:05:54,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.610841942994739. input_tokens=690, output_tokens=0
23:06:00,498 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:00,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.147488003000035. input_tokens=573, output_tokens=0
23:06:06,517 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:06,541 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1565269119892037. input_tokens=591, output_tokens=0
23:06:12,537 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:12,565 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1721986340126023. input_tokens=705, output_tokens=0
23:06:18,529 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:18,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1619539950042963. input_tokens=422, output_tokens=0
23:06:24,650 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:24,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2639512900059344. input_tokens=428, output_tokens=0
23:06:30,658 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:30,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2678393660025904. input_tokens=973, output_tokens=0
23:06:36,569 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:36,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.169624593007029. input_tokens=1227, output_tokens=0
23:06:42,259 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:42,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8458801779925125. input_tokens=396, output_tokens=0
23:06:48,577 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:48,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1594006340019405. input_tokens=481, output_tokens=0
23:06:54,693 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:06:54,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2646790069993585. input_tokens=561, output_tokens=0
23:07:00,600 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:07:00,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.164957877001143. input_tokens=301, output_tokens=0
23:07:06,615 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:07:06,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.166176907994668. input_tokens=444, output_tokens=0
23:07:12,193 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:07:12,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7375461969932076. input_tokens=404, output_tokens=0
23:07:18,620 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:07:18,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.154819999996107. input_tokens=484, output_tokens=0
23:07:24,402 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
23:07:24,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9118383710010676. input_tokens=32, output_tokens=0
23:07:24,430 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
23:07:24,809 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
23:07:24,809 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
23:07:24,833 datashaper.workflow.workflow INFO executing verb create_final_nodes
23:07:24,891 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
23:07:24,912 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
23:07:25,88 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
23:07:25,88 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
23:07:25,137 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
23:07:25,137 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
23:07:25,318 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
23:07:25,318 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
23:07:26,586 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
23:07:26,650 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
23:07:26,659 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
23:07:26,664 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
23:07:26,675 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
00:52:17,742 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:52:17,743 graphrag.index.cli INFO Starting pipeline run for: 20241018-005217, dryrun=False
00:52:17,743 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:52:17,745 graphrag.index.create_pipeline_config INFO skipping workflows 
00:52:17,745 graphrag.index.run.run INFO Running pipeline
00:52:17,746 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:52:17,748 graphrag.index.input.load_input INFO loading input from root_dir=input
00:52:17,748 graphrag.index.input.load_input INFO using file storage for input
00:52:17,749 graphrag.index.input.csv INFO Loading csv files from input
00:52:17,749 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:52:17,753 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:52:17,753 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:52:17,754 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:52:17,754 graphrag.index.run.run INFO Final # of rows loaded: 100
00:52:17,864 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:52:17,867 datashaper.workflow.workflow INFO executing verb create_base_text_units
00:52:18,370 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:52:18,503 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
00:52:18,504 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:52:18,512 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
00:52:18,515 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
00:52:18,553 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
00:52:18,553 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
00:52:19,346 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7867035229864996. input_tokens=30, output_tokens=1
00:52:19,366 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8000316730031045. input_tokens=30, output_tokens=1
00:52:19,456 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8878045400051633. input_tokens=30, output_tokens=1
00:52:19,739 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.170943477001856. input_tokens=30, output_tokens=1
00:52:19,816 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.257458542007953. input_tokens=30, output_tokens=1
00:52:21,491 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:21,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.7457661349908449. input_tokens=28, output_tokens=347
00:52:21,972 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:21,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.46365183299349155. input_tokens=30, output_tokens=1
00:52:23,124 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:23,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.7558625230012694. input_tokens=28, output_tokens=936
00:52:23,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:23,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.325243308005156. input_tokens=28, output_tokens=701
00:52:23,756 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:23,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.605758646997856. input_tokens=30, output_tokens=1
00:52:23,979 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:23,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8227752439997857. input_tokens=30, output_tokens=1
00:57:11,277 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:57:11,278 graphrag.index.cli INFO Starting pipeline run for: output, dryrun=False
00:57:11,279 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:57:11,281 graphrag.index.create_pipeline_config INFO skipping workflows 
00:57:11,281 graphrag.index.run.run INFO Running pipeline
00:57:11,281 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:57:11,282 graphrag.index.input.load_input INFO loading input from root_dir=input
00:57:11,282 graphrag.index.input.load_input INFO using file storage for input
00:57:11,284 graphrag.index.input.csv INFO Loading csv files from input
00:57:11,284 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:57:11,289 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:57:11,289 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:57:11,290 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:57:11,291 graphrag.index.run.run INFO Final # of rows loaded: 100
00:57:11,401 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
00:57:11,506 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
00:57:11,611 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
00:57:11,716 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:57:11,716 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:57:11,743 datashaper.workflow.workflow INFO executing verb create_final_nodes
00:57:11,796 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:11,798 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:11,960 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:11,961 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:12,7 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:12,8 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:12,168 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:12,168 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:13,521 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:57:13,525 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
00:57:13,527 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:57:13,528 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
00:57:13,532 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
00:57:37,514 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:57:37,516 graphrag.index.cli INFO Starting pipeline run for: output, dryrun=False
00:57:37,516 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:57:37,518 graphrag.index.create_pipeline_config INFO skipping workflows 
00:57:37,518 graphrag.index.run.run INFO Running pipeline
00:57:37,518 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:57:37,519 graphrag.index.input.load_input INFO loading input from root_dir=input
00:57:37,519 graphrag.index.input.load_input INFO using file storage for input
00:57:37,520 graphrag.index.input.csv INFO Loading csv files from input
00:57:37,520 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:57:37,524 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:57:37,525 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:57:37,526 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:57:37,526 graphrag.index.run.run INFO Final # of rows loaded: 100
00:57:37,635 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
00:57:37,740 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
00:57:37,844 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
00:57:37,950 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:57:37,950 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:57:37,974 datashaper.workflow.workflow INFO executing verb create_final_nodes
00:57:38,25 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:38,28 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:38,190 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:38,190 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:38,238 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:38,238 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:38,398 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:57:38,398 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:57:39,750 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:57:39,755 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
00:57:39,757 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:57:39,759 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
00:57:39,763 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
00:58:25,706 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:58:25,707 graphrag.index.cli INFO Starting pipeline run for: /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output, dryrun=False
00:58:25,707 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": true,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:58:25,709 graphrag.index.create_pipeline_config INFO skipping workflows 
00:58:25,709 graphrag.index.run.run INFO Running pipeline
00:58:25,709 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:58:25,710 graphrag.index.input.load_input INFO loading input from root_dir=input
00:58:25,710 graphrag.index.input.load_input INFO using file storage for input
00:58:25,712 graphrag.index.input.csv INFO Loading csv files from input
00:58:25,712 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:58:25,716 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:58:25,716 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:58:25,717 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:58:25,717 graphrag.index.run.run INFO Final # of rows loaded: 100
00:58:25,827 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
00:58:25,929 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
00:58:26,34 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
00:58:26,138 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:58:26,138 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:58:26,166 datashaper.workflow.workflow INFO executing verb create_final_nodes
00:58:26,219 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:58:26,221 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:58:26,382 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:58:26,383 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:58:26,429 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:58:26,430 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:58:26,591 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:58:26,591 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:58:27,940 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:58:27,945 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
00:58:27,946 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:58:27,948 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
00:58:27,952 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
00:59:06,319 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:59:06,320 graphrag.index.cli INFO Starting pipeline run for: /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output, dryrun=False
00:59:06,320 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:59:06,322 graphrag.index.create_pipeline_config INFO skipping workflows 
00:59:06,322 graphrag.index.run.run INFO Running pipeline
00:59:06,322 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:59:06,324 graphrag.index.input.load_input INFO loading input from root_dir=input
00:59:06,324 graphrag.index.input.load_input INFO using file storage for input
00:59:06,325 graphrag.index.input.csv INFO Loading csv files from input
00:59:06,325 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:59:06,329 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:59:06,329 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:59:06,330 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:59:06,331 graphrag.index.run.run INFO Final # of rows loaded: 100
00:59:06,440 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
00:59:06,543 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
00:59:06,647 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
00:59:06,752 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
00:59:06,752 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
00:59:06,778 datashaper.workflow.workflow INFO executing verb create_final_nodes
00:59:06,828 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:59:06,832 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:59:06,993 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:59:06,993 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:59:07,40 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:59:07,40 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:59:07,199 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
00:59:07,200 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
00:59:08,541 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:59:08,546 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
00:59:08,548 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
00:59:08,549 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
00:59:08,554 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
00:59:31,197 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
00:59:31,199 graphrag.index.cli INFO Starting pipeline run for: 20241018-005931, dryrun=False
00:59:31,199 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
00:59:31,201 graphrag.index.create_pipeline_config INFO skipping workflows 
00:59:31,201 graphrag.index.run.run INFO Running pipeline
00:59:31,201 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
00:59:31,202 graphrag.index.input.load_input INFO loading input from root_dir=input
00:59:31,202 graphrag.index.input.load_input INFO using file storage for input
00:59:31,203 graphrag.index.input.csv INFO Loading csv files from input
00:59:31,203 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
00:59:31,207 graphrag.index.input.csv INFO Found 1 csv files, loading 1
00:59:31,207 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
00:59:31,208 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
00:59:31,209 graphrag.index.run.run INFO Final # of rows loaded: 100
00:59:31,317 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
00:59:31,320 datashaper.workflow.workflow INFO executing verb create_base_text_units
00:59:31,816 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
00:59:31,943 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
00:59:31,943 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
00:59:31,952 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
00:59:31,955 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
00:59:31,994 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
00:59:31,994 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
00:59:35,933 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:35,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.9226495909970254. input_tokens=28, output_tokens=729
00:59:36,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:36,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6300482310034567. input_tokens=30, output_tokens=1
00:59:38,29 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:38,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.007055121997837. input_tokens=28, output_tokens=1482
00:59:38,665 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:38,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6156734399992274. input_tokens=30, output_tokens=1
00:59:40,26 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:40,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 8.018986411989317. input_tokens=28, output_tokens=2008
00:59:40,581 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:40,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5278856299992185. input_tokens=30, output_tokens=1
00:59:40,807 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:40,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.207663826993667. input_tokens=28, output_tokens=1171
00:59:40,986 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:40,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 8.96026810100011. input_tokens=28, output_tokens=2190
00:59:41,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:41,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.5159302710089833. input_tokens=28, output_tokens=522
00:59:41,684 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:41,686 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8520569689862896. input_tokens=30, output_tokens=1
00:59:41,708 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:41,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6942976270074723. input_tokens=30, output_tokens=1
00:59:42,41 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:42,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8277627009956632. input_tokens=30, output_tokens=1
00:59:42,605 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:42,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.01718068800983. input_tokens=28, output_tokens=436
00:59:43,131 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:43,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5044221290008863. input_tokens=30, output_tokens=1
00:59:44,418 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:44,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.2790303759975359. input_tokens=28, output_tokens=203
00:59:44,929 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:44,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4847578149929177. input_tokens=30, output_tokens=1
00:59:47,740 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:47,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.7990844339947216. input_tokens=28, output_tokens=626
00:59:48,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:48,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 16.03788128499582. input_tokens=28, output_tokens=4449
00:59:48,557 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:48,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7924464440002339. input_tokens=30, output_tokens=1
00:59:48,645 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:48,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.574768591002794. input_tokens=30, output_tokens=1
00:59:54,127 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:54,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.478733199008275. input_tokens=28, output_tokens=1369
00:59:54,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:54,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 13.179726844988181. input_tokens=28, output_tokens=4496
00:59:54,921 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:54,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7679844000085723. input_tokens=30, output_tokens=1
00:59:55,603 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:55,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.70539742600522. input_tokens=30, output_tokens=1
00:59:58,361 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:58,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 16.311812833999284. input_tokens=28, output_tokens=4495
00:59:58,934 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:58,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 17.223579493002035. input_tokens=28, output_tokens=4363
00:59:59,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:59,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7383124560001306. input_tokens=30, output_tokens=1
00:59:59,410 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:59,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44817494999733754. input_tokens=30, output_tokens=1
01:00:02,157 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:02,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.7399659790098667. input_tokens=28, output_tokens=624
01:00:02,183 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:02,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 13.614989540001261. input_tokens=28, output_tokens=4394
01:00:02,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:02,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4701881789951585. input_tokens=30, output_tokens=1
01:00:02,891 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:02,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7145999149943236. input_tokens=30, output_tokens=1
01:00:03,976 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:03,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.308368418001919. input_tokens=28, output_tokens=196
01:00:04,631 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:04,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6323105490009766. input_tokens=30, output_tokens=1
01:00:05,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:05,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.931006594008068. input_tokens=28, output_tokens=1600
01:00:05,562 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:05,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.47088617300323676. input_tokens=30, output_tokens=1
01:00:07,823 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:07,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.212724793003872. input_tokens=28, output_tokens=4481
01:00:08,467 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:08,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.56870278599672. input_tokens=28, output_tokens=1413
01:00:08,489 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:08,506 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:09,303 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:09,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 14.37833710500854. input_tokens=28, output_tokens=4419
01:00:10,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.919338699997752. input_tokens=28, output_tokens=598
01:00:12,149 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:12,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6581294670031639. input_tokens=30, output_tokens=1
01:00:14,579 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:15,751 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:17,504 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:17,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4374446489964612. input_tokens=30, output_tokens=1
01:00:18,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4558541250007693. input_tokens=30, output_tokens=1
01:00:20,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.418533837000723. input_tokens=30, output_tokens=1
01:00:21,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.868383904002258. input_tokens=28, output_tokens=4245
01:00:21,688 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:25,147 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:26,258 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:26,643 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:27,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.146005413000239. input_tokens=28, output_tokens=1393
01:00:28,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.5864691419992596. input_tokens=28, output_tokens=618
01:00:29,663 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:29,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.892485731004854. input_tokens=28, output_tokens=1394
01:00:30,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.2754298700019717. input_tokens=28, output_tokens=60
01:00:32,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.45251931600796524. input_tokens=30, output_tokens=1
01:00:34,285 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:34,936 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:36,289 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:37,397 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:38,603 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:39,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.0112936420046026. input_tokens=30, output_tokens=1
01:00:40,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.45660796300217044. input_tokens=30, output_tokens=1
01:00:41,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.6014223730016965. input_tokens=28, output_tokens=39
01:00:42,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49319673600257374. input_tokens=30, output_tokens=1
01:00:46,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49062618699099403. input_tokens=30, output_tokens=1
01:00:47,294 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:47,701 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:47,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.5756818450026913. input_tokens=28, output_tokens=811
01:00:49,579 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:52,307 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:52,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.9586716850026278. input_tokens=28, output_tokens=275
01:00:54,181 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:54,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6751044739939971. input_tokens=30, output_tokens=1
01:00:56,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.1945904079911998. input_tokens=28, output_tokens=485
01:00:56,449 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:57,860 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:58,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44673253799555823. input_tokens=30, output_tokens=1
01:01:01,424 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:01,695 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:03,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.127672774993698. input_tokens=28, output_tokens=1374
01:01:04,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5058723359979922. input_tokens=30, output_tokens=1
01:01:04,891 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:05,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4469014489877736. input_tokens=30, output_tokens=1
01:01:06,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.927861550997477. input_tokens=28, output_tokens=327
01:01:08,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:11,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.7053931040136376. input_tokens=28, output_tokens=607
01:01:12,355 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:12,780 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:13,622 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:14,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7100134560023434. input_tokens=30, output_tokens=1
01:01:15,923 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:16,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.7370356689934852. input_tokens=28, output_tokens=193
01:01:19,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.3714888129907195. input_tokens=28, output_tokens=791
01:01:20,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5930758760077879. input_tokens=30, output_tokens=1
01:01:21,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4828736619965639. input_tokens=30, output_tokens=1
01:01:22,134 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:23,258 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:24,369 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:27,347 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:27,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.262783132988261. input_tokens=28, output_tokens=474
01:01:27,881 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:28,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.284311668990995. input_tokens=28, output_tokens=1049
01:01:29,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5911623499996495. input_tokens=30, output_tokens=1
01:01:30,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4935250580019783. input_tokens=30, output_tokens=1
01:01:32,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:34,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.5880821509927046. input_tokens=28, output_tokens=127
01:01:37,585 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:37,611 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:38,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44574716298666317. input_tokens=30, output_tokens=1
01:01:38,888 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:39,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:40,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.214536087005399. input_tokens=28, output_tokens=1158
01:01:43,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.609069735990488. input_tokens=30, output_tokens=1
01:01:44,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.0982455360062886. input_tokens=28, output_tokens=510
01:01:45,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5040287649899255. input_tokens=30, output_tokens=1
01:01:47,589 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:47,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:47,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.1564193880039966. input_tokens=30, output_tokens=1
01:01:50,878 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:52,697 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:53,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 5.764202400008799. input_tokens=28, output_tokens=1476
01:01:54,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:55,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6656731729890453. input_tokens=30, output_tokens=1
01:01:56,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.6920412670006044. input_tokens=28, output_tokens=890
01:01:57,195 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:58,130 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:58,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.7039109589968575. input_tokens=28, output_tokens=646
01:02:01,645 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:02,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.564530699994066. input_tokens=28, output_tokens=919
01:02:03,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6819163030013442. input_tokens=30, output_tokens=1
01:02:05,132 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:05,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5766733139898861. input_tokens=30, output_tokens=1
01:02:09,296 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:09,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.9911099040036788. input_tokens=30, output_tokens=1
01:02:10,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44833293500414584. input_tokens=30, output_tokens=1
01:02:12,163 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:14,312 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:14,787 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:15,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.304001279000659. input_tokens=28, output_tokens=4276
01:02:16,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.609447780007031. input_tokens=28, output_tokens=601
01:02:17,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.8756745200080331. input_tokens=28, output_tokens=336
01:02:18,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:19,524 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:20,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:20,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:21,820 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:22,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.916939625996747. input_tokens=28, output_tokens=983
01:02:23,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5848083919991041. input_tokens=30, output_tokens=1
01:02:24,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 13.198074747007922. input_tokens=28, output_tokens=4112
01:02:26,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.46106903899635654. input_tokens=30, output_tokens=1
01:02:27,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.46273624199966434. input_tokens=30, output_tokens=1
01:02:29,134 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:32,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:33,881 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:33,990 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:34,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5527431079972303. input_tokens=30, output_tokens=1
01:02:35,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.8701862299931236. input_tokens=28, output_tokens=207
01:02:37,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.45982460099912714. input_tokens=30, output_tokens=1
01:02:37,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:38,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.7785698269872228. input_tokens=28, output_tokens=357
01:02:41,731 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:41,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.0820867119909963. input_tokens=30, output_tokens=1
01:02:43,467 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:44,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:44,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.1680269009957556. input_tokens=28, output_tokens=191
01:02:45,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 7.954575334006222. input_tokens=28, output_tokens=2213
01:02:47,8 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:48,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.0279987569956575. input_tokens=28, output_tokens=758
01:02:51,174 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:51,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:52,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5013228080060799. input_tokens=30, output_tokens=1
01:02:54,376 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:54,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.046421075006947. input_tokens=30, output_tokens=1
01:02:57,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4722642540000379. input_tokens=30, output_tokens=1
01:02:59,219 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:59,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6286691330024041. input_tokens=30, output_tokens=1
01:03:01,446 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:02,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.060127413002192. input_tokens=28, output_tokens=763
01:03:02,975 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:02,985 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:04,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.875446510995971. input_tokens=28, output_tokens=645
01:03:06,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:07,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.9918097269983264. input_tokens=28, output_tokens=393
01:03:08,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 15.2704704439966. input_tokens=28, output_tokens=4010
01:03:09,932 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:10,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6279111600015312. input_tokens=30, output_tokens=1
01:03:11,702 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:11,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 8.308326041995315. input_tokens=28, output_tokens=2468
01:03:13,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:14,619 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:15,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5111937119945651. input_tokens=30, output_tokens=1
01:03:17,67 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:19,567 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:20,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6048078849999001. input_tokens=30, output_tokens=1
01:03:21,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5021311339951353. input_tokens=30, output_tokens=1
01:03:22,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5355356380023295. input_tokens=30, output_tokens=1
01:03:23,23 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:23,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.8275136549927993. input_tokens=28, output_tokens=293
01:03:27,816 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:28,69 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:28,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.066983212993364. input_tokens=28, output_tokens=1030
01:03:30,399 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:31,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.6258667270012666. input_tokens=28, output_tokens=277
01:03:32,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.094899132003775. input_tokens=28, output_tokens=677
01:03:34,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5878005559934536. input_tokens=30, output_tokens=1
01:03:36,843 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:36,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.9984619259921601. input_tokens=30, output_tokens=1
01:03:38,112 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:38,542 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:41,346 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:41,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 10.712851969001349. input_tokens=28, output_tokens=2778
01:03:42,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:42,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4964088559936499. input_tokens=30, output_tokens=1
01:03:44,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.0917615399957867. input_tokens=28, output_tokens=358
01:03:45,796 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:46,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.5552161160012474. input_tokens=28, output_tokens=174
01:03:48,987 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:49,389 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:50,8 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:50,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.0978380580054363. input_tokens=30, output_tokens=1
01:03:51,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5100099820119794. input_tokens=30, output_tokens=1
01:03:52,901 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:53,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 15.554442251013825. input_tokens=30, output_tokens=1
01:03:55,870 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:57,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.689611302994308. input_tokens=28, output_tokens=258
01:03:57,291 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:59,334 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:59,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4878257179952925. input_tokens=30, output_tokens=1
01:04:00,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.0452274290000787. input_tokens=28, output_tokens=137
01:04:02,505 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:03,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.2563576719985576. input_tokens=28, output_tokens=190
01:04:05,590 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:05,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.888455107997288. input_tokens=28, output_tokens=114
01:04:07,421 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:08,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44602493300044443. input_tokens=30, output_tokens=1
01:04:09,783 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:10,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.1144837449974148. input_tokens=28, output_tokens=172
01:04:12,929 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:13,479 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:14,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5352502519963309. input_tokens=30, output_tokens=1
01:04:15,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.2185604099940974. input_tokens=28, output_tokens=204
01:04:17,74 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:17,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4862888220086461. input_tokens=30, output_tokens=1
01:04:18,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5585793999925954. input_tokens=30, output_tokens=1
01:04:22,347 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:23,593 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:24,259 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:24,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5390548690047581. input_tokens=30, output_tokens=1
01:04:26,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.988689752004575. input_tokens=30, output_tokens=1
01:04:27,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:27,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.02675603699754. input_tokens=28, output_tokens=156
01:04:28,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.110583949004649. input_tokens=28, output_tokens=1002
01:04:29,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.4256356980040437. input_tokens=28, output_tokens=803
01:04:33,14 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:33,706 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:34,195 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:35,104 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:36,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:36,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.2051480039954185. input_tokens=28, output_tokens=445
01:04:38,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4786839059961494. input_tokens=30, output_tokens=1
01:04:39,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.1762340370041784. input_tokens=28, output_tokens=432
01:04:40,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6637892559956526. input_tokens=30, output_tokens=1
01:04:41,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5238436419895152. input_tokens=30, output_tokens=1
01:04:44,331 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:47,326 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:48,388 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:48,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.4553879480081378. input_tokens=30, output_tokens=1
01:04:49,992 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:50,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.239906898990739. input_tokens=28, output_tokens=655
01:04:51,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6592793419986265. input_tokens=30, output_tokens=1
01:04:53,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.4720852460013703. input_tokens=28, output_tokens=794
01:04:55,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:55,990 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:56,397 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:57,985 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:58,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 10.37953973900585. input_tokens=28, output_tokens=2606
01:04:59,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.031675654987339. input_tokens=30, output_tokens=1
01:05:00,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.852218746993458. input_tokens=28, output_tokens=900
01:05:02,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6074725110083818. input_tokens=30, output_tokens=1
01:05:03,894 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:05,619 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:05,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.0127176270034397. input_tokens=28, output_tokens=158
01:05:07,286 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:08,680 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:09,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49709481600439176. input_tokens=30, output_tokens=1
01:05:11,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:11,288 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:11,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4647290540015092. input_tokens=30, output_tokens=1
01:05:14,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.511044737999327. input_tokens=28, output_tokens=4379
01:05:14,299 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:16,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.063065435999306. input_tokens=28, output_tokens=734
01:05:17,259 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:17,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8474103630142054. input_tokens=30, output_tokens=1
01:05:19,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:20,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.4475192530080676. input_tokens=28, output_tokens=276
01:05:23,210 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:23,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.9975292210001498. input_tokens=28, output_tokens=543
01:05:24,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.39566970500163734. input_tokens=30, output_tokens=1
01:05:26,939 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:27,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7097645239991834. input_tokens=30, output_tokens=1
01:05:29,264 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:30,344 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:32,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.8250573730038013. input_tokens=30, output_tokens=1
01:05:33,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.7392351220041746. input_tokens=28, output_tokens=81
01:05:34,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6105131700023776. input_tokens=30, output_tokens=1
01:05:34,870 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:35,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:37,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:39,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.9269002840010216. input_tokens=28, output_tokens=779
01:05:39,713 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:40,585 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 14.12344113799918. input_tokens=28, output_tokens=4486
01:05:41,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4521697930031223. input_tokens=30, output_tokens=1
01:05:43,448 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:44,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.5373952999943867. input_tokens=28, output_tokens=265
01:05:47,780 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:47,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.1689243969885865. input_tokens=30, output_tokens=1
01:05:48,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:48,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4576415600022301. input_tokens=30, output_tokens=1
01:05:50,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:51,763 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:53,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.419166722000227. input_tokens=28, output_tokens=4380
01:05:54,827 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:54,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.22356120099721. input_tokens=28, output_tokens=458
01:05:56,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6319423940003617. input_tokens=30, output_tokens=1
01:05:57,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.361203666005167. input_tokens=28, output_tokens=1568
01:05:58,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:58,882 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:00,164 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:03,184 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:03,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.62179728699266. input_tokens=28, output_tokens=1822
01:06:04,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.44513312600611243. input_tokens=30, output_tokens=1
01:06:05,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.518142747998354. input_tokens=30, output_tokens=1
01:06:06,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.1194664620124968. input_tokens=30, output_tokens=1
01:06:08,715 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:10,696 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:12,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.6305025230103638. input_tokens=30, output_tokens=1
01:06:13,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:13,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:14,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:14,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.3690351840050425. input_tokens=28, output_tokens=528
01:06:15,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.3975600919948192. input_tokens=28, output_tokens=269
01:06:17,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 2.5922681280062534. input_tokens=28, output_tokens=418
01:06:18,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.75211907600169. input_tokens=28, output_tokens=4273
01:06:20,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:21,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:21,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 1.1343890980060678. input_tokens=30, output_tokens=1
01:06:22,887 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:24,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:26,573 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:27,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 4.306946255994262. input_tokens=28, output_tokens=979
01:06:28,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:28,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.4400388809881406. input_tokens=30, output_tokens=1
01:06:29,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5744870800117496. input_tokens=30, output_tokens=1
01:06:30,899 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49096585300867446. input_tokens=30, output_tokens=1
01:06:32,808 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:33,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.3648572899983265. input_tokens=28, output_tokens=859
01:06:35,337 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:38,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.7083199899934698. input_tokens=30, output_tokens=1
01:06:38,778 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:38,852 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:39,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 0.8260572230065009. input_tokens=28, output_tokens=84
01:06:41,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.848841547005577. input_tokens=28, output_tokens=262
01:06:42,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.1316211660014233. input_tokens=28, output_tokens=822
01:06:52,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:52,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 12.204589447006583. input_tokens=28, output_tokens=3913
01:06:53,543 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:53,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7144215619919123. input_tokens=172, output_tokens=45
01:06:54,24 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:54,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46462562499800697. input_tokens=152, output_tokens=24
01:06:54,187 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:54,219 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:54,359 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:54,389 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:56,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3558588460000465. input_tokens=174, output_tokens=63
01:06:56,562 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:57,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3856158559938194. input_tokens=155, output_tokens=16
01:06:58,861 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5272792049945565. input_tokens=143, output_tokens=30
01:07:00,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5602763170027174. input_tokens=183, output_tokens=47
01:07:02,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:02,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8010049690055894. input_tokens=258, output_tokens=72
01:07:03,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3173865069984458. input_tokens=769, output_tokens=193
01:07:05,273 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:06,525 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:07,443 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:09,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:10,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:10,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8034911670110887. input_tokens=238, output_tokens=89
01:07:11,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8485722019977402. input_tokens=425, output_tokens=107
01:07:12,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5581450940080686. input_tokens=175, output_tokens=45
01:07:14,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9717398389911978. input_tokens=355, output_tokens=149
01:07:15,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7195729389932239. input_tokens=291, output_tokens=62
01:07:17,613 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:17,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.094851613001083. input_tokens=224, output_tokens=51
01:07:19,653 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:20,924 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:21,951 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:23,102 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:24,169 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:24,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.837113710003905. input_tokens=243, output_tokens=97
01:07:26,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9020368939964101. input_tokens=512, output_tokens=129
01:07:27,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7215148950053845. input_tokens=180, output_tokens=74
01:07:28,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6648175190057373. input_tokens=175, output_tokens=67
01:07:29,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5241478009993443. input_tokens=162, output_tokens=38
01:07:31,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:32,677 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:33,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:34,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:36,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9696781450038543. input_tokens=334, output_tokens=105
01:07:36,979 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:38,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6037122080015251. input_tokens=223, output_tokens=51
01:07:39,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46868557400011923. input_tokens=142, output_tokens=18
01:07:40,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46336772599897813. input_tokens=131, output_tokens=13
01:07:42,174 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:42,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.282275148012559. input_tokens=650, output_tokens=278
01:07:44,625 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:46,42 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:47,317 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:47,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45886097400216386. input_tokens=141, output_tokens=14
01:07:49,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:50,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4991738909884589. input_tokens=150, output_tokens=25
01:07:51,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7092677970067598. input_tokens=181, output_tokens=91
01:07:52,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7767186890123412. input_tokens=260, output_tokens=92
01:07:54,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9914805419975892. input_tokens=439, output_tokens=167
01:07:55,45 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:57,542 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:58,718 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:59,391 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:00,344 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:01,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2816241550026461. input_tokens=332, output_tokens=205
01:08:02,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.368213106004987. input_tokens=331, output_tokens=172
01:08:03,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3361133980070008. input_tokens=198, output_tokens=72
01:08:04,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8003310220083222. input_tokens=212, output_tokens=96
01:08:05,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5507264210027643. input_tokens=165, output_tokens=29
01:08:07,848 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:08,677 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:10,46 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:11,208 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:12,331 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:13,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.831641184995533. input_tokens=284, output_tokens=96
01:08:14,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45381595799699426. input_tokens=148, output_tokens=13
01:08:15,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6148144300095737. input_tokens=162, output_tokens=42
01:08:16,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5708744670118904. input_tokens=158, output_tokens=40
01:08:17,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4872093690064503. input_tokens=138, output_tokens=23
01:08:20,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:20,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1172625769977458. input_tokens=148, output_tokens=38
01:08:21,994 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:23,146 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:24,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:25,600 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:27,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:27,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1262263160024304. input_tokens=372, output_tokens=205
01:08:28,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6092085319978651. input_tokens=191, output_tokens=64
01:08:29,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5538395089970436. input_tokens=145, output_tokens=36
01:08:30,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5870318070083158. input_tokens=174, output_tokens=48
01:08:32,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5919607510004425. input_tokens=142, output_tokens=27
01:08:34,634 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:35,591 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:36,256 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:37,521 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:38,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:39,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2781313419982325. input_tokens=267, output_tokens=104
01:08:40,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.028896899006213. input_tokens=149, output_tokens=28
01:08:41,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4870577800029423. input_tokens=141, output_tokens=23
01:08:42,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5452518239908386. input_tokens=157, output_tokens=40
01:08:44,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7217514859949006. input_tokens=202, output_tokens=83
01:08:46,714 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:47,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:48,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:50,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:50,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0992642040073406. input_tokens=341, output_tokens=193
01:08:51,883 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:52,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3110799159912858. input_tokens=188, output_tokens=76
01:08:53,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2987795209919568. input_tokens=174, output_tokens=71
01:08:54,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6165817330038408. input_tokens=159, output_tokens=41
01:08:56,793 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:57,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5552159479993861. input_tokens=169, output_tokens=32
01:08:59,965 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:00,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:01,654 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:02,749 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:03,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6486342550051631. input_tokens=164, output_tokens=42
01:09:04,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4097805570054334. input_tokens=323, output_tokens=113
01:09:05,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9715756790101295. input_tokens=150, output_tokens=15
01:09:06,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6848889579996467. input_tokens=249, output_tokens=102
01:09:08,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5726064540067455. input_tokens=210, output_tokens=52
01:09:11,110 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:11,975 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:12,732 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:13,711 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:14,975 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:15,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7144033860095078. input_tokens=278, output_tokens=152
01:09:16,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.371211563993711. input_tokens=268, output_tokens=108
01:09:17,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9172975450055674. input_tokens=294, output_tokens=127
01:09:19,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6908680990018183. input_tokens=183, output_tokens=66
01:09:20,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7471484770067036. input_tokens=201, output_tokens=79
01:09:22,534 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:22,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.087846656999318. input_tokens=172, output_tokens=38
01:09:24,451 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:25,499 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:26,914 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:27,953 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:29,400 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:29,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7150051779899513. input_tokens=176, output_tokens=67
01:09:30,977 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5550701260071946. input_tokens=181, output_tokens=52
01:09:32,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7621484769915696. input_tokens=207, output_tokens=77
01:09:33,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5939934289926896. input_tokens=151, output_tokens=38
01:09:34,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8328280559944687. input_tokens=232, output_tokens=87
01:09:37,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:37,547 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:38,847 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:39,995 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:41,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:41,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6463223659957293. input_tokens=285, output_tokens=144
01:09:43,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5503243720013415. input_tokens=154, output_tokens=31
01:09:44,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6422102549986448. input_tokens=208, output_tokens=88
01:09:45,436 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5823512069910066. input_tokens=161, output_tokens=46
01:09:46,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5555779790011002. input_tokens=157, output_tokens=29
01:09:49,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:50,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:50,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1231601960025728. input_tokens=228, output_tokens=54
01:09:51,897 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:53,442 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:54,264 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:55,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:56,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3072872219927376. input_tokens=239, output_tokens=110
01:09:57,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5224882829934359. input_tokens=150, output_tokens=12
01:09:58,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8590444949950324. input_tokens=192, output_tokens=68
01:09:59,812 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47319053100363817. input_tokens=147, output_tokens=17
01:10:01,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6110623360000318. input_tokens=215, output_tokens=52
01:10:03,349 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:03,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1330630790034775. input_tokens=171, output_tokens=53
01:10:05,659 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:05,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1081532440002775. input_tokens=223, output_tokens=57
01:10:07,349 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:08,678 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:09,863 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:10,998 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:12,173 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:12,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4858011339965742. input_tokens=164, output_tokens=31
01:10:14,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6073371399979806. input_tokens=186, output_tokens=51
01:10:15,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5842252959992038. input_tokens=156, output_tokens=37
01:10:16,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5121079979871865. input_tokens=157, output_tokens=31
01:10:17,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.479921980004292. input_tokens=164, output_tokens=26
01:10:19,879 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:20,711 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:21,804 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:23,26 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:24,481 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:24,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9659564269968541. input_tokens=149, output_tokens=18
01:10:26,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5912017730006482. input_tokens=181, output_tokens=47
01:10:27,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4772916060028365. input_tokens=143, output_tokens=18
01:10:28,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49147760900086723. input_tokens=137, output_tokens=17
01:10:29,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7381508399994345. input_tokens=162, output_tokens=60
01:10:31,639 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:32,693 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:33,930 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:35,102 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:36,341 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:36,999 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.680935974989552. input_tokens=170, output_tokens=69
01:10:38,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5274007940024603. input_tokens=138, output_tokens=25
01:10:39,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5561737499956507. input_tokens=138, output_tokens=35
01:10:40,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5194130500021856. input_tokens=144, output_tokens=26
01:10:41,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5511647609964712. input_tokens=151, output_tokens=29
01:10:44,118 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:44,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1109397880063625. input_tokens=168, output_tokens=50
01:10:45,779 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:46,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:48,229 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:49,558 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:50,758 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:51,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4568636900075944. input_tokens=144, output_tokens=16
01:10:52,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4548027529963292. input_tokens=130, output_tokens=8
01:10:53,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49204736000683624. input_tokens=152, output_tokens=30
01:10:54,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6113829040114069. input_tokens=150, output_tokens=47
01:10:56,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6040112729970133. input_tokens=154, output_tokens=36
01:10:58,353 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:59,0 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:00,185 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:01,643 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:03,91 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:03,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9834869839978637. input_tokens=147, output_tokens=15
01:11:04,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42347392901137937. input_tokens=137, output_tokens=12
01:11:05,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40033965598559007. input_tokens=137, output_tokens=8
01:11:07,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6515344360086601. input_tokens=152, output_tokens=36
01:11:08,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8934788280021166. input_tokens=186, output_tokens=90
01:11:10,399 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:11,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:11,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0589421829936327. input_tokens=154, output_tokens=34
01:11:13,336 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:14,507 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:15,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:16,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.986372229992412. input_tokens=141, output_tokens=21
01:11:18,370 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:18,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4552523019956425. input_tokens=137, output_tokens=14
01:11:20,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.42051673799869604. input_tokens=155, output_tokens=14
01:11:21,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47450786500121467. input_tokens=153, output_tokens=24
01:11:23,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:23,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6651034310052637. input_tokens=154, output_tokens=55
01:11:25,697 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:26,725 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:27,822 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:29,319 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:29,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6771690979949199. input_tokens=188, output_tokens=68
01:11:30,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7686690720001934. input_tokens=215, output_tokens=75
01:11:32,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.587297934005619. input_tokens=210, output_tokens=66
01:11:33,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47867964500619564. input_tokens=186, output_tokens=31
01:11:34,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7685105590062449. input_tokens=230, output_tokens=84
01:11:36,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:36,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0522882049990585. input_tokens=182, output_tokens=45
01:11:38,990 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:39,748 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:41,54 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:42,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:43,766 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:44,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9642996180045884. input_tokens=302, output_tokens=111
01:11:45,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5154782000026898. input_tokens=158, output_tokens=39
01:11:46,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6139752949966351. input_tokens=168, output_tokens=59
01:11:47,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.695077216005302. input_tokens=206, output_tokens=79
01:11:48,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9115174449980259. input_tokens=268, output_tokens=125
01:11:51,166 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:51,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:53,97 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:54,161 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:55,399 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:56,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0932132580055622. input_tokens=159, output_tokens=33
01:11:57,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41892721099429764. input_tokens=147, output_tokens=18
01:11:58,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6093834010098362. input_tokens=168, output_tokens=56
01:11:59,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4661093430040637. input_tokens=167, output_tokens=30
01:12:00,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4971361589996377. input_tokens=167, output_tokens=33
01:12:03,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:03,813 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:05,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:06,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:07,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:08,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0696655319916317. input_tokens=176, output_tokens=37
01:12:09,364 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4830124779982725. input_tokens=155, output_tokens=34
01:12:10,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6574903610016918. input_tokens=183, output_tokens=45
01:12:11,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5293884470011108. input_tokens=167, output_tokens=34
01:12:12,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5220541570015484. input_tokens=160, output_tokens=43
01:12:15,452 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:16,403 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:17,230 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:18,373 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:19,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:20,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.277484887992614. input_tokens=170, output_tokens=45
01:12:21,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.020655722997617. input_tokens=156, output_tokens=30
01:12:22,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.640575302997604. input_tokens=181, output_tokens=50
01:12:23,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5761523940018378. input_tokens=177, output_tokens=35
01:12:25,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5773636529920623. input_tokens=159, output_tokens=34
01:12:27,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:27,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0409916430071462. input_tokens=155, output_tokens=27
01:12:28,883 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:30,300 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:31,330 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:32,519 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:33,878 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:34,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4146989430009853. input_tokens=149, output_tokens=15
01:12:35,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6244445479969727. input_tokens=210, output_tokens=59
01:12:36,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44709699699888006. input_tokens=185, output_tokens=22
01:12:38,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4280398279952351. input_tokens=143, output_tokens=15
01:12:39,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.577125119001721. input_tokens=163, output_tokens=42
01:12:41,539 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:42,358 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:43,611 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:44,551 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:45,835 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:46,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0205343999987235. input_tokens=229, output_tokens=113
01:12:47,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.63116509700194. input_tokens=182, output_tokens=60
01:12:48,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6771795389940962. input_tokens=307, output_tokens=85
01:12:50,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40947460698953364. input_tokens=134, output_tokens=13
01:12:51,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.485347694993834. input_tokens=137, output_tokens=17
01:12:53,616 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:53,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0476462579972576. input_tokens=166, output_tokens=31
01:12:55,338 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:56,444 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:57,827 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:58,856 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:00,119 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:00,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5193694119952852. input_tokens=139, output_tokens=26
01:13:02,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.420472288999008. input_tokens=147, output_tokens=17
01:13:03,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5960012759896927. input_tokens=151, output_tokens=41
01:13:04,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41760366600647103. input_tokens=142, output_tokens=13
01:13:05,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4743190449953545. input_tokens=148, output_tokens=19
01:13:07,438 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:08,519 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:09,782 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:11,111 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:12,405 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:12,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5731335599994054. input_tokens=159, output_tokens=43
01:13:14,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44484724799986. input_tokens=139, output_tokens=14
01:13:15,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5009741089888848. input_tokens=140, output_tokens=22
01:13:16,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6223538000049302. input_tokens=169, output_tokens=43
01:13:17,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7083251760050189. input_tokens=197, output_tokens=66
01:13:19,951 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:20,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:21,818 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:23,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:24,378 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:24,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0363644140015822. input_tokens=148, output_tokens=28
01:13:26,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4723581909929635. input_tokens=149, output_tokens=23
01:13:27,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4883337679930264. input_tokens=139, output_tokens=23
01:13:28,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5408185609994689. input_tokens=153, output_tokens=41
01:13:29,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6341745579993585. input_tokens=146, output_tokens=43
01:13:32,57 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:32,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0926139329967555. input_tokens=140, output_tokens=31
01:13:33,851 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:34,909 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:36,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:37,532 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:38,622 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:39,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5879593400022713. input_tokens=142, output_tokens=29
01:13:40,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43863426700409036. input_tokens=138, output_tokens=20
01:13:41,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5652367500006221. input_tokens=165, output_tokens=42
01:13:42,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6459908440010622. input_tokens=149, output_tokens=42
01:13:44,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5283153900090838. input_tokens=178, output_tokens=30
01:13:46,349 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:47,262 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:48,212 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:49,459 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:50,801 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:51,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0360993279900867. input_tokens=155, output_tokens=40
01:13:52,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.74181913198845. input_tokens=174, output_tokens=75
01:13:53,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48488248699868564. input_tokens=141, output_tokens=24
01:13:54,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5245145319931908. input_tokens=157, output_tokens=28
01:13:56,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6584086369985016. input_tokens=148, output_tokens=55
01:13:58,326 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:59,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:00,359 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:01,470 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:02,651 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:03,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9638732829916989. input_tokens=138, output_tokens=14
01:14:04,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5021707690029871. input_tokens=143, output_tokens=16
01:14:05,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5819081119989278. input_tokens=176, output_tokens=45
01:14:07,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4862787820020458. input_tokens=183, output_tokens=19
01:14:08,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4610401060053846. input_tokens=143, output_tokens=17
01:14:10,352 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:11,197 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:12,323 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:13,485 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:14,772 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:15,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9445034079981269. input_tokens=144, output_tokens=16
01:14:16,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5821728829905624. input_tokens=153, output_tokens=43
01:14:17,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4991870969970478. input_tokens=151, output_tokens=20
01:14:19,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45412697599385865. input_tokens=139, output_tokens=13
01:14:20,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5343392210052116. input_tokens=148, output_tokens=31
01:14:22,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:22,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0170220080035506. input_tokens=137, output_tokens=14
01:14:24,162 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:25,632 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:26,628 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:27,838 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:28,989 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:29,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48306908100494184. input_tokens=147, output_tokens=17
01:14:30,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.744906815001741. input_tokens=189, output_tokens=83
01:14:32,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5370571679959539. input_tokens=156, output_tokens=34
01:14:33,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.539432877994841. input_tokens=139, output_tokens=26
01:14:34,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48391134999110363. input_tokens=134, output_tokens=19
01:14:36,720 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:37,382 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:38,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:39,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:41,21 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:41,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9934821409988217. input_tokens=138, output_tokens=14
01:14:42,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44872300200222526. input_tokens=135, output_tokens=19
01:14:44,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4458710369945038. input_tokens=142, output_tokens=25
01:14:45,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45644507199176587. input_tokens=133, output_tokens=15
01:14:46,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47124374999839347. input_tokens=141, output_tokens=17
01:14:48,916 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:48,918 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.146325565990992. input_tokens=183, output_tokens=62
01:14:50,880 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:51,949 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:53,417 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:54,227 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:55,413 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:56,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7607505360065261. input_tokens=232, output_tokens=86
01:14:57,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.622041981012444. input_tokens=185, output_tokens=49
01:14:58,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8836753770010546. input_tokens=326, output_tokens=91
01:14:59,765 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4858286430098815. input_tokens=160, output_tokens=42
01:15:00,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4638758590008365. input_tokens=144, output_tokens=26
01:15:02,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:03,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:05,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:06,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:07,470 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:07,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.476517580988002. input_tokens=149, output_tokens=0
01:15:08,203 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6029033460072242. input_tokens=142, output_tokens=19
01:15:09,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5802343800023664. input_tokens=173, output_tokens=59
01:15:10,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4922248380025849. input_tokens=145, output_tokens=19
01:15:11,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7097329429961974. input_tokens=241, output_tokens=80
01:15:13,498 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:14,795 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:16,171 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:17,213 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:18,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:19,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48656468201079406. input_tokens=155, output_tokens=22
01:15:20,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5763825759931933. input_tokens=174, output_tokens=41
01:15:21,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7450972640071996. input_tokens=186, output_tokens=63
01:15:22,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5780371870059753. input_tokens=160, output_tokens=45
01:15:23,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6136658540053759. input_tokens=156, output_tokens=39
01:15:26,228 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:26,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1642902759922436. input_tokens=206, output_tokens=60
01:15:28,82 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:29,111 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:30,587 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:31,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:32,749 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:33,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6506127680040663. input_tokens=204, output_tokens=43
01:15:34,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4706160530040506. input_tokens=142, output_tokens=14
01:15:35,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7391524849954294. input_tokens=239, output_tokens=93
01:15:37,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5262800450000213. input_tokens=168, output_tokens=38
01:15:38,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48459935499704443. input_tokens=165, output_tokens=35
01:15:40,837 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:41,270 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:42,508 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:43,612 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:45,13 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:45,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3549662239965983. input_tokens=238, output_tokens=91
01:15:46,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5795528180024121. input_tokens=163, output_tokens=44
01:15:47,928 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6106468369980576. input_tokens=153, output_tokens=45
01:15:49,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5050754620024236. input_tokens=139, output_tokens=18
01:15:50,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6995589540019864. input_tokens=236, output_tokens=90
01:15:52,679 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:52,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1461575369903585. input_tokens=174, output_tokens=50
01:15:54,429 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:55,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:56,824 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:57,982 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:59,138 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:59,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5472284919960657. input_tokens=150, output_tokens=25
01:16:01,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4936492690030718. input_tokens=151, output_tokens=26
01:16:02,327 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5270678770029917. input_tokens=151, output_tokens=23
01:16:03,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4776938900031382. input_tokens=150, output_tokens=31
01:16:04,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4267822950059781. input_tokens=135, output_tokens=16
01:16:07,138 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:07,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.201842125999974. input_tokens=157, output_tokens=57
01:16:08,807 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:10,108 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:11,323 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:12,474 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:13,688 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:14,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4652325409988407. input_tokens=148, output_tokens=25
01:16:15,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5591924650070723. input_tokens=173, output_tokens=43
01:16:16,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5661116840055911. input_tokens=181, output_tokens=50
01:16:17,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5084856350003975. input_tokens=153, output_tokens=33
01:16:19,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5144255779887317. input_tokens=171, output_tokens=45
01:16:20,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:22,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:23,286 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:24,612 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:25,772 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:26,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5592062959913164. input_tokens=169, output_tokens=41
01:16:27,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5966764429904288. input_tokens=153, output_tokens=44
01:16:28,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47328340700187255. input_tokens=155, output_tokens=23
01:16:30,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5908221139980014. input_tokens=145, output_tokens=37
01:16:31,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5424831980053568. input_tokens=161, output_tokens=32
01:16:33,525 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:33,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0760916209983407. input_tokens=164, output_tokens=20
01:16:35,172 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:36,370 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:37,681 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:38,845 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:40,34 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:40,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.445018306010752. input_tokens=148, output_tokens=15
01:16:41,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.43562211499374826. input_tokens=149, output_tokens=14
01:16:43,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5382400840026094. input_tokens=184, output_tokens=28
01:16:44,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4952963140094653. input_tokens=165, output_tokens=20
01:16:45,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4775808629929088. input_tokens=164, output_tokens=21
01:16:47,832 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:47,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0553329620015575. input_tokens=185, output_tokens=57
01:16:49,538 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:50,751 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:51,945 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:53,519 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:54,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:55,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5033740430080798. input_tokens=176, output_tokens=43
01:16:56,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5069140560080996. input_tokens=182, output_tokens=46
01:16:57,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49281636100204196. input_tokens=151, output_tokens=27
01:16:58,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8593776199995773. input_tokens=267, output_tokens=108
01:16:59,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5243734489922645. input_tokens=153, output_tokens=30
01:17:02,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:02,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.11182559900044. input_tokens=175, output_tokens=53
01:17:04,1 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:05,599 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:06,450 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:07,577 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:09,369 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:09,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.135590460995445. input_tokens=302, output_tokens=149
01:17:10,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6008076200087089. input_tokens=154, output_tokens=38
01:17:11,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9870396540063666. input_tokens=212, output_tokens=118
01:17:12,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6315695869998308. input_tokens=168, output_tokens=53
01:17:14,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5506862869951874. input_tokens=148, output_tokens=36
01:17:16,490 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:16,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1082490389962913. input_tokens=151, output_tokens=45
01:17:18,268 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:19,507 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:20,643 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:21,737 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:23,340 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:23,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5755150059994776. input_tokens=147, output_tokens=32
01:17:24,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6068549920018995. input_tokens=149, output_tokens=44
01:17:26,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5356226059957407. input_tokens=145, output_tokens=34
01:17:27,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4198900089977542. input_tokens=144, output_tokens=16
01:17:28,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8164517629920738. input_tokens=260, output_tokens=87
01:17:30,728 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:31,765 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:32,820 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:34,638 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:35,601 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:35,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9855693179997616. input_tokens=156, output_tokens=27
01:17:36,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8156694899953436. input_tokens=312, output_tokens=147
01:17:38,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6631454460002715. input_tokens=195, output_tokens=65
01:17:39,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2736529590038117. input_tokens=223, output_tokens=94
01:17:40,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0307085419917712. input_tokens=159, output_tokens=30
01:17:42,393 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:43,664 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:44,739 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:45,998 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:47,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:47,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6028242719912669. input_tokens=146, output_tokens=24
01:17:49,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6655974419991253. input_tokens=211, output_tokens=58
01:17:50,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5324616949947085. input_tokens=159, output_tokens=34
01:17:51,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5837293799995678. input_tokens=158, output_tokens=43
01:17:52,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6385561210045125. input_tokens=210, output_tokens=62
01:17:54,506 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:55,633 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:56,845 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:58,174 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:59,287 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:59,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6599409009941155. input_tokens=169, output_tokens=42
01:18:01,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5782141109957593. input_tokens=172, output_tokens=47
01:18:02,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5821126990049379. input_tokens=147, output_tokens=28
01:18:03,492 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7033011919993442. input_tokens=176, output_tokens=60
01:18:04,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6098261209990596. input_tokens=183, output_tokens=60
01:18:06,393 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:07,542 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:08,804 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:10,7 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:11,506 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:11,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4977709819941083. input_tokens=151, output_tokens=31
01:18:13,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4392772610008251. input_tokens=143, output_tokens=14
01:18:14,338 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49327271700894926. input_tokens=187, output_tokens=30
01:18:15,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48881927799084224. input_tokens=151, output_tokens=28
01:18:16,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7804122050001752. input_tokens=215, output_tokens=87
01:18:19,333 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:20,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:20,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0232865139987553. input_tokens=153, output_tokens=29
01:18:21,829 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:23,53 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:24,196 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:25,395 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:26,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3891595220047748. input_tokens=302, output_tokens=173
01:18:27,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4487603419984225. input_tokens=132, output_tokens=8
01:18:28,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4661237069958588. input_tokens=134, output_tokens=12
01:18:29,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.40011829999275506. input_tokens=132, output_tokens=10
01:18:31,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.3926460479997331. input_tokens=132, output_tokens=10
01:18:33,214 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:33,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:34,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9922706920042401. input_tokens=138, output_tokens=16
01:18:36,557 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:37,324 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:38,501 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:39,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41423612300422974. input_tokens=134, output_tokens=11
01:18:40,908 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:41,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9375526190124219. input_tokens=138, output_tokens=16
01:18:42,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49745855199580546. input_tokens=145, output_tokens=16
01:18:44,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46732047900150064. input_tokens=163, output_tokens=15
01:18:45,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:46,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46559315800550394. input_tokens=142, output_tokens=20
01:18:48,223 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:49,287 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:50,527 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:51,819 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:52,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5775364159926539. input_tokens=153, output_tokens=38
01:18:53,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5607387740019476. input_tokens=164, output_tokens=41
01:18:54,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41528611799003556. input_tokens=150, output_tokens=16
01:18:56,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4478528089966858. input_tokens=147, output_tokens=19
01:18:57,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5318729050050024. input_tokens=171, output_tokens=38
01:18:59,129 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:00,209 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:01,606 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:02,601 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:03,905 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:04,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6213171380077256. input_tokens=181, output_tokens=62
01:19:05,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4932943679887103. input_tokens=156, output_tokens=31
01:19:06,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6823107829986839. input_tokens=211, output_tokens=60
01:19:08,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46966276899911463. input_tokens=159, output_tokens=32
01:19:09,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5650811550003709. input_tokens=152, output_tokens=35
01:19:11,204 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:12,233 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:13,638 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:14,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:15,836 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:16,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6443559180042939. input_tokens=168, output_tokens=51
01:19:17,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4658580140094273. input_tokens=138, output_tokens=22
01:19:19,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6620191170077305. input_tokens=177, output_tokens=65
01:19:20,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5171758710057475. input_tokens=153, output_tokens=31
01:19:21,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44499574799556285. input_tokens=139, output_tokens=17
01:19:23,147 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:24,517 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:25,504 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:26,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:27,966 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:28,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5356008410017239. input_tokens=181, output_tokens=35
01:19:29,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6975740829948336. input_tokens=177, output_tokens=68
01:19:31,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4779532459942857. input_tokens=143, output_tokens=24
01:19:32,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5839917440025602. input_tokens=162, output_tokens=53
01:19:33,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5245287339930655. input_tokens=169, output_tokens=38
01:19:35,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:37,8 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:37,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.140987976003089. input_tokens=157, output_tokens=53
01:19:38,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:40,82 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:41,24 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:41,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6061102889943868. input_tokens=150, output_tokens=31
01:19:43,750 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:44,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.586191089998465. input_tokens=166, output_tokens=53
01:19:45,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6635114319942659. input_tokens=202, output_tokens=75
01:19:46,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.39760234400455374. input_tokens=134, output_tokens=12
01:19:48,465 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:49,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7142228320008144. input_tokens=184, output_tokens=61
01:19:50,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:51,975 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:53,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:54,556 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:55,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6153921590012033. input_tokens=169, output_tokens=42
01:19:56,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6559124250052264. input_tokens=155, output_tokens=49
01:19:57,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5117283720028354. input_tokens=148, output_tokens=29
01:19:58,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6078435740055284. input_tokens=175, output_tokens=53
01:19:59,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6782322569924872. input_tokens=164, output_tokens=54
01:20:02,190 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:02,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0928245959948981. input_tokens=174, output_tokens=40
01:20:04,35 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:05,110 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:06,306 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:07,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:08,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:09,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6428511999984039. input_tokens=158, output_tokens=48
01:20:10,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5098736859945348. input_tokens=160, output_tokens=26
01:20:11,832 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49792083200009074. input_tokens=158, output_tokens=31
01:20:13,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7167263320006896. input_tokens=168, output_tokens=58
01:20:14,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6418024399899878. input_tokens=171, output_tokens=54
01:20:16,542 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:16,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1030471060075797. input_tokens=208, output_tokens=41
01:20:18,365 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:19,506 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:20,885 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:21,967 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:23,89 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:23,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6186824449978303. input_tokens=152, output_tokens=45
01:20:24,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5523175699927378. input_tokens=139, output_tokens=35
01:20:26,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7229426769918064. input_tokens=175, output_tokens=52
01:20:27,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5980417009996017. input_tokens=190, output_tokens=54
01:20:28,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5115672380052274. input_tokens=199, output_tokens=38
01:20:30,792 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:31,447 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:32,690 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:33,869 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:35,272 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:35,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9950574959948426. input_tokens=181, output_tokens=33
01:20:37,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44276968599297106. input_tokens=142, output_tokens=14
01:20:38,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.478274932000204. input_tokens=149, output_tokens=29
01:20:39,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45044754398986697. input_tokens=140, output_tokens=18
01:20:40,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6458870210044552. input_tokens=174, output_tokens=68
01:20:42,992 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:42,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1491397400095593. input_tokens=135, output_tokens=14
01:20:44,634 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:45,980 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:47,165 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:48,535 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:49,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:50,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4395727470109705. input_tokens=146, output_tokens=12
01:20:51,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5773665500018978. input_tokens=167, output_tokens=36
01:20:52,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5547252809919883. input_tokens=165, output_tokens=48
01:20:53,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7170096689951606. input_tokens=217, output_tokens=70
01:20:55,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8834623689908767. input_tokens=278, output_tokens=103
01:20:57,630 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:58,544 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:58,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0909711629938101. input_tokens=178, output_tokens=43
01:21:00,319 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:01,424 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:03,133 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:03,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.385705473003327. input_tokens=294, output_tokens=95
01:21:05,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:05,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5718450190033764. input_tokens=172, output_tokens=32
01:21:06,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47018083500734065. input_tokens=147, output_tokens=24
01:21:08,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9718228130077478. input_tokens=3877, output_tokens=105
01:21:09,849 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:10,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5013019870093558. input_tokens=165, output_tokens=36
01:21:12,779 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:13,537 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:14,781 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:15,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46446874500543345. input_tokens=157, output_tokens=32
01:21:17,212 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:17,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9837973429966951. input_tokens=304, output_tokens=183
01:21:19,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5345180089934729. input_tokens=159, output_tokens=41
01:21:20,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5718430650013033. input_tokens=183, output_tokens=53
01:21:22,473 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:22,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0418468660063809. input_tokens=291, output_tokens=151
01:21:23,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5942716160061536. input_tokens=182, output_tokens=53
01:21:25,452 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:26,602 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:27,962 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:29,43 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:30,344 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:30,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5739074279990746. input_tokens=192, output_tokens=59
01:21:32,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5150410900096176. input_tokens=187, output_tokens=30
01:21:33,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6673882469913224. input_tokens=168, output_tokens=64
01:21:34,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5398761309916154. input_tokens=159, output_tokens=32
01:21:35,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6340674609964481. input_tokens=170, output_tokens=53
01:21:38,84 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:38,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1531779920042027. input_tokens=192, output_tokens=71
01:21:39,760 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:41,32 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:42,292 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:44,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:44,766 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:45,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47365131400874816. input_tokens=146, output_tokens=22
01:21:46,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5371148470003391. input_tokens=180, output_tokens=27
01:21:47,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5880588470026851. input_tokens=184, output_tokens=62
01:21:48,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.215391224992345. input_tokens=213, output_tokens=90
01:21:50,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6493990879971534. input_tokens=201, output_tokens=69
01:21:52,515 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:52,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1790711179928621. input_tokens=210, output_tokens=75
01:21:54,280 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:55,515 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:56,746 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:57,916 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:59,246 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:59,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.560600444994634. input_tokens=162, output_tokens=43
01:22:00,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5914487459958764. input_tokens=157, output_tokens=39
01:22:02,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.613529546011705. input_tokens=153, output_tokens=38
01:22:03,364 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5751090419944376. input_tokens=190, output_tokens=46
01:22:04,566 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6981310979899717. input_tokens=220, output_tokens=88
01:22:06,242 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:07,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:08,784 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:10,41 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:11,406 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:11,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47434711300593335. input_tokens=159, output_tokens=15
01:22:13,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4398754860012559. input_tokens=161, output_tokens=16
01:22:14,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6015729600039776. input_tokens=221, output_tokens=46
01:22:15,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6512486280116718. input_tokens=181, output_tokens=48
01:22:16,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8085347029991681. input_tokens=254, output_tokens=128
01:22:18,429 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:19,645 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:20,748 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:22,136 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:23,566 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:23,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6117456189967925. input_tokens=180, output_tokens=49
01:22:25,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6194079699926078. input_tokens=202, output_tokens=78
01:22:26,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5142821230110712. input_tokens=166, output_tokens=32
01:22:27,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.694525729995803. input_tokens=167, output_tokens=69
01:22:28,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9161794090032345. input_tokens=251, output_tokens=132
01:22:30,868 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:30,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0021864110021852. input_tokens=155, output_tokens=17
01:22:32,601 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:33,829 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:34,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:36,186 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:37,596 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:38,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5316634190094192. input_tokens=147, output_tokens=25
01:22:39,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5519912309973733. input_tokens=147, output_tokens=29
01:22:40,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47322854299272876. input_tokens=147, output_tokens=28
01:22:41,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49975295999320224. input_tokens=153, output_tokens=30
01:22:42,913 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.70191563401022. input_tokens=194, output_tokens=68
01:22:45,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:45,842 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:47,56 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:48,270 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:49,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:50,154 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2713678430009168. input_tokens=181, output_tokens=42
01:22:51,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.520361798000522. input_tokens=151, output_tokens=29
01:22:52,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5263880859856727. input_tokens=169, output_tokens=38
01:22:53,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5334202780068154. input_tokens=176, output_tokens=41
01:22:54,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5156296530039981. input_tokens=156, output_tokens=31
01:22:57,284 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:57,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1205478470074013. input_tokens=140, output_tokens=38
01:22:59,49 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:00,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:01,734 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:02,847 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:03,956 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:04,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5612024220026797. input_tokens=158, output_tokens=36
01:23:05,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5052196229953552. input_tokens=162, output_tokens=33
01:23:06,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8303256840008544. input_tokens=341, output_tokens=124
01:23:08,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7361373290041229. input_tokens=192, output_tokens=75
01:23:09,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.637547435995657. input_tokens=179, output_tokens=42
01:23:11,591 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:12,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:13,612 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:14,714 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:16,187 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:16,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0526937579998048. input_tokens=185, output_tokens=42
01:23:17,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5676076169911539. input_tokens=189, output_tokens=45
01:23:18,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6624296020017937. input_tokens=184, output_tokens=39
01:23:20,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5559828940022271. input_tokens=158, output_tokens=38
01:23:21,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8216810599988094. input_tokens=192, output_tokens=99
01:23:23,66 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:24,249 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:25,560 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:26,767 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:27,914 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:28,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48037404799833894. input_tokens=152, output_tokens=25
01:23:29,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.45553969200409483. input_tokens=151, output_tokens=27
01:23:31,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5577235090022441. input_tokens=152, output_tokens=22
01:23:32,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5564417230052641. input_tokens=150, output_tokens=26
01:23:33,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.49734639200323727. input_tokens=149, output_tokens=27
01:23:35,640 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:35,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0041882679943228. input_tokens=157, output_tokens=32
01:23:37,346 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:38,535 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:39,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:40,933 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:42,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5015596489974996. input_tokens=148, output_tokens=35
01:23:43,151 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:44,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48414607299491763. input_tokens=153, output_tokens=26
01:23:45,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4810513760021422. input_tokens=140, output_tokens=17
01:23:46,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46486227499553934. input_tokens=144, output_tokens=21
01:23:48,624 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:48,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4748698430048535. input_tokens=157, output_tokens=14
01:23:50,529 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:51,729 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:53,117 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:54,325 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:54,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9294217650021892. input_tokens=159, output_tokens=17
01:23:56,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4253724189911736. input_tokens=156, output_tokens=13
01:23:57,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.41796698000689503. input_tokens=156, output_tokens=17
01:23:58,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5986786629946437. input_tokens=163, output_tokens=39
01:23:59,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.599409371992806. input_tokens=173, output_tokens=48
01:24:01,481 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:02,772 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:03,777 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:05,129 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:06,402 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:06,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5344382809998933. input_tokens=169, output_tokens=31
01:24:08,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6184896229970036. input_tokens=156, output_tokens=30
01:24:09,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4155006079963641. input_tokens=144, output_tokens=10
01:24:10,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.559289285010891. input_tokens=172, output_tokens=31
01:24:11,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6238132409926038. input_tokens=171, output_tokens=40
01:24:13,436 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:14,843 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:15,967 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:17,86 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:18,291 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:19,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.44025000899273437. input_tokens=153, output_tokens=13
01:24:20,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6394782349962043. input_tokens=166, output_tokens=46
01:24:21,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5551107119972585. input_tokens=166, output_tokens=36
01:24:22,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4653062540019164. input_tokens=158, output_tokens=21
01:24:23,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.46138340000470635. input_tokens=159, output_tokens=20
01:24:25,532 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:26,762 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:27,938 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:29,313 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:30,383 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:31,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.48248373600654304. input_tokens=156, output_tokens=28
01:24:32,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5051525050075725. input_tokens=171, output_tokens=41
01:24:33,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.47199627199734095. input_tokens=163, output_tokens=26
01:24:34,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6403678430069704. input_tokens=164, output_tokens=37
01:24:35,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5027000270056305. input_tokens=169, output_tokens=34
01:24:38,147 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:38,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0437900199904107. input_tokens=153, output_tokens=33
01:24:39,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:41,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:42,476 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:43,538 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:45,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5270142699882854. input_tokens=154, output_tokens=27
01:24:46,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7577519469923573. input_tokens=193, output_tokens=80
01:24:47,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.7113235369906761. input_tokens=163, output_tokens=67
01:24:48,2 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:48,3 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:48,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5662743220018456. input_tokens=167, output_tokens=43
01:24:50,392 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:50,393 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:51,422 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:51,423 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:52,632 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:52,632 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:53,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:53,841 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:55,32 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:55,32 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:56,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:56,253 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:57,458 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:57,459 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:58,610 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:58,611 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:24:59,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:24:59,818 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:01,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:01,18 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:02,227 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:02,228 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:03,326 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:03,327 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:04,527 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:04,527 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:05,740 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:05,741 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:06,947 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:06,948 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:08,164 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:08,165 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:09,380 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:09,381 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:10,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:10,575 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:11,869 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:11,870 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:15,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:15,342 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:16,646 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:16,647 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:18,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:18,146 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:19,108 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:19,109 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:20,160 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:20,161 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:25,366 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:25,366 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:26,673 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:26,675 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:28,176 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:28,177 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:29,133 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:29,134 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:30,189 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:30,190 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:35,398 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:35,399 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:36,699 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:36,700 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:38,185 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:38,185 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:39,157 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:39,158 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:40,199 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:40,199 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:48,217 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:48,218 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:49,182 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:49,183 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:49,984 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:49,985 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:50,224 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:50,225 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:51,431 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:51,431 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:58,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:58,982 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:25:59,206 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:25:59,207 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:26:00,8 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:26:00,9 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:26:00,243 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:26:00,243 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:26:01,443 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:26:01,444 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:26:09,18 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:26:09,19 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:26:09,28 datashaper.workflow.workflow ERROR Error executing verb "create_base_entity_graph" in create_base_entity_graph: Error code: 429 - {'error': {'message': "No deployments available for selected model, Try again in 60 seconds. Passed model=gpt-4o. pre-call-checks=False, cooldown_list=['55df53c914eca9ce79231d49888f27df7d8ec153d21e77af85c0cfa3b01cecfc', '825042fb151c40c3689b9d78758950a38b6933eda6b2f46dd1af0efe4a82341c', 'c5dddc586b3b118a9afc2eedfe3e827243d3b99b347cda697e0dbaa1fa210b55', '327ec1698949fcd572c6001126a5924dfd57fd48568af355f1e45a63050c464a', '532475aa42fd2225d317f85c7a66451c1544597a952043d94f06e718e561ac67']", 'type': 'None', 'param': 'None', 'code': '429'}}
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 70, in create_base_entity_graph
    summarized = await summarize_descriptions(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 136, in summarize_descriptions
    return await get_resolved_entities(input, semaphore)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 106, in get_resolved_entities
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 124, in do_summarize_descriptions
    results = await strategy_exec(
              ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/strategies.py", line 30, in run_graph_intelligence
    return await run_summarize_descriptions(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/strategies.py", line 63, in run_summarize_descriptions
    result = await extractor(items=items, descriptions=descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 73, in __call__
    result = await self._summarize_descriptions(items, descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 110, in _summarize_descriptions
    result = await self._summarize_descriptions_with_llm(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 129, in _summarize_descriptions_with_llm
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 1490, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1532, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': "No deployments available for selected model, Try again in 60 seconds. Passed model=gpt-4o. pre-call-checks=False, cooldown_list=['55df53c914eca9ce79231d49888f27df7d8ec153d21e77af85c0cfa3b01cecfc', '825042fb151c40c3689b9d78758950a38b6933eda6b2f46dd1af0efe4a82341c', 'c5dddc586b3b118a9afc2eedfe3e827243d3b99b347cda697e0dbaa1fa210b55', '327ec1698949fcd572c6001126a5924dfd57fd48568af355f1e45a63050c464a', '532475aa42fd2225d317f85c7a66451c1544597a952043d94f06e718e561ac67']", 'type': 'None', 'param': 'None', 'code': '429'}}
01:26:09,54 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_base_entity_graph" in create_base_entity_graph: Error code: 429 - {'error': {'message': "No deployments available for selected model, Try again in 60 seconds. Passed model=gpt-4o. pre-call-checks=False, cooldown_list=['55df53c914eca9ce79231d49888f27df7d8ec153d21e77af85c0cfa3b01cecfc', '825042fb151c40c3689b9d78758950a38b6933eda6b2f46dd1af0efe4a82341c', 'c5dddc586b3b118a9afc2eedfe3e827243d3b99b347cda697e0dbaa1fa210b55', '327ec1698949fcd572c6001126a5924dfd57fd48568af355f1e45a63050c464a', '532475aa42fd2225d317f85c7a66451c1544597a952043d94f06e718e561ac67']", 'type': 'None', 'param': 'None', 'code': '429'}} details=None
01:26:09,54 graphrag.index.run.run ERROR error running workflow create_base_entity_graph
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_base_entity_graph.py", line 53, in create_base_entity_graph
    output = await create_base_entity_graph_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_base_entity_graph.py", line 70, in create_base_entity_graph
    summarized = await summarize_descriptions(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 136, in summarize_descriptions
    return await get_resolved_entities(input, semaphore)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 106, in get_resolved_entities
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/summarize_descriptions.py", line 124, in do_summarize_descriptions
    results = await strategy_exec(
              ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/strategies.py", line 30, in run_graph_intelligence
    return await run_summarize_descriptions(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/summarize_descriptions/strategies.py", line 63, in run_summarize_descriptions
    result = await extractor(items=items, descriptions=descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 73, in __call__
    result = await self._summarize_descriptions(items, descriptions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 110, in _summarize_descriptions
    result = await self._summarize_descriptions_with_llm(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/summarize/description_summary_extractor.py", line 129, in _summarize_descriptions_with_llm
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 1490, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1838, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1532, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/openai/_base_client.py", line 1633, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': "No deployments available for selected model, Try again in 60 seconds. Passed model=gpt-4o. pre-call-checks=False, cooldown_list=['55df53c914eca9ce79231d49888f27df7d8ec153d21e77af85c0cfa3b01cecfc', '825042fb151c40c3689b9d78758950a38b6933eda6b2f46dd1af0efe4a82341c', 'c5dddc586b3b118a9afc2eedfe3e827243d3b99b347cda697e0dbaa1fa210b55', '327ec1698949fcd572c6001126a5924dfd57fd48568af355f1e45a63050c464a', '532475aa42fd2225d317f85c7a66451c1544597a952043d94f06e718e561ac67']", 'type': 'None', 'param': 'None', 'code': '429'}}
01:26:09,61 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
01:26:09,74 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
01:27:59,947 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
01:27:59,949 graphrag.index.cli INFO Starting pipeline run for: 20241018-012759, dryrun=False
01:27:59,949 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 8191,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 10000000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 1000000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:27:59,952 graphrag.index.create_pipeline_config INFO skipping workflows 
01:27:59,952 graphrag.index.run.run INFO Running pipeline
01:27:59,952 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
01:27:59,953 graphrag.index.input.load_input INFO loading input from root_dir=input
01:27:59,963 graphrag.index.input.load_input INFO using file storage for input
01:27:59,966 graphrag.index.input.csv INFO Loading csv files from input
01:27:59,966 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
01:27:59,981 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:27:59,982 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
01:27:59,990 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:27:59,997 graphrag.index.run.run INFO Final # of rows loaded: 100
01:28:00,121 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:28:00,124 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:28:00,632 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:28:00,781 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:28:00,782 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:28:00,790 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:28:00,793 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:28:00,831 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=10000000, RPM=50
01:28:00,831 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:28:02,945 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:02,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.108124146994669. input_tokens=2593, output_tokens=253
01:28:03,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:03,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.1756277380045503. input_tokens=2119, output_tokens=283
01:28:04,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:04,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.2045191469951533. input_tokens=2350, output_tokens=604
01:28:04,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:04,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.5334270949970232. input_tokens=1803, output_tokens=571
01:28:04,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:04,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.6085608300054446. input_tokens=2342, output_tokens=876
01:28:05,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:05,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.8545441850001225. input_tokens=28, output_tokens=687
01:28:07,9 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:07,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.623313592004706. input_tokens=28, output_tokens=615
01:28:07,305 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:07,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.35012315500353. input_tokens=28, output_tokens=1163
01:28:07,812 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:07,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.49865024199243635. input_tokens=30, output_tokens=1
01:28:08,391 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:08,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.933279221004341. input_tokens=28, output_tokens=974
01:28:08,480 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:08,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.433097187007661. input_tokens=28, output_tokens=1097
01:28:08,785 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:08,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 1.7597461249970365. input_tokens=28, output_tokens=347
01:28:08,919 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:08,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-loopcheck-0" with 0 retries took 0.5203849819954485. input_tokens=30, output_tokens=1
01:28:08,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:08,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 3.0511764639959438. input_tokens=28, output_tokens=636
01:28:12,547 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:12,548 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:12,676 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:12,676 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-1 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:12,870 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:12,871 graphrag.llm.base.rate_limiting_llm WARNING extract-loopcheck-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:13,235 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:13,236 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:14,286 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:14,287 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-1 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:14,378 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:14,379 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:14,389 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:14,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-1" with 0 retries took 6.5702232500043465. input_tokens=28, output_tokens=1812
01:28:14,423 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:14,423 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:14,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:14,456 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:14,748 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:14,749 graphrag.llm.base.rate_limiting_llm WARNING extract-loopcheck-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:16,263 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:16,264 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:16,891 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:16,892 graphrag.llm.base.rate_limiting_llm WARNING extract-loopcheck-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:17,54 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:17,55 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:17,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:17,126 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-1 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:17,143 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:17,143 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:19,237 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:19,238 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:21,356 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:21,357 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:21,494 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:21,495 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:21,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:21,758 graphrag.llm.base.rate_limiting_llm WARNING extract-loopcheck-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:21,854 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:21,854 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-1 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:24,167 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:28:24,168 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:28:40,599 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
01:28:40,600 graphrag.index.cli INFO Starting pipeline run for: /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output, dryrun=False
01:28:40,600 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 8191,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 10000000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 1000000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:28:40,602 graphrag.index.create_pipeline_config INFO skipping workflows 
01:28:40,602 graphrag.index.run.run INFO Running pipeline
01:28:40,602 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
01:28:40,604 graphrag.index.input.load_input INFO loading input from root_dir=input
01:28:40,604 graphrag.index.input.load_input INFO using file storage for input
01:28:40,605 graphrag.index.input.csv INFO Loading csv files from input
01:28:40,605 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
01:28:40,609 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:28:40,609 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
01:28:40,610 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:28:40,610 graphrag.index.run.run INFO Final # of rows loaded: 100
01:28:40,719 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
01:28:40,822 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
01:28:40,926 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
01:28:41,30 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:28:41,31 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:28:41,56 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:28:41,108 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:28:41,110 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:28:41,273 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:28:41,273 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:28:41,320 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:28:41,321 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:28:41,480 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:28:41,481 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:28:42,830 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
01:28:42,836 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
01:28:42,838 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
01:28:42,839 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
01:28:42,844 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
01:30:00,397 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
01:30:00,398 graphrag.index.cli INFO Starting pipeline run for: output, dryrun=False
01:30:00,398 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 8191,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 10000000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 1000000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 8191,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 10000000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:30:00,400 graphrag.index.create_pipeline_config INFO skipping workflows 
01:30:00,400 graphrag.index.run.run INFO Running pipeline
01:30:00,401 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
01:30:00,402 graphrag.index.input.load_input INFO loading input from root_dir=input
01:30:00,402 graphrag.index.input.load_input INFO using file storage for input
01:30:00,403 graphrag.index.input.csv INFO Loading csv files from input
01:30:00,403 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
01:30:00,407 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:30:00,407 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
01:30:00,408 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:30:00,409 graphrag.index.run.run INFO Final # of rows loaded: 100
01:30:00,520 graphrag.index.run.workflow INFO Skipping create_base_text_units because it already exists
01:30:00,625 graphrag.index.run.workflow INFO Skipping create_base_entity_graph because it already exists
01:30:00,730 graphrag.index.run.workflow INFO Skipping create_final_entities because it already exists
01:30:00,835 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:30:00,835 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:30:00,860 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:30:00,912 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:30:00,915 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:30:01,77 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:30:01,77 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:30:01,123 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:30:01,123 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:30:01,283 graphrag.index.operations.layout_graph.methods.umap ERROR Error running UMAP
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/operations/layout_graph/methods/umap.py", line 56, in run
    return compute_umap_positions(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/visualization/compute_umap_positions.py", line 71, in compute_umap_positions
    ).fit_transform(embedding_vectors)
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2891, in fit_transform
    self.fit(X, y, force_all_finite)
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/umap/umap_.py", line 2358, in fit
    X = check_array(X, dtype=np.float32, accept_sparse="csr", order="C", force_all_finite=force_all_finite)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/sklearn/utils/validation.py", line 1050, in check_array
    raise ValueError(msg)
ValueError: Expected 2D array, got 1D array instead:
array=[].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
01:30:01,283 graphrag.callbacks.file_workflow_callbacks INFO Error in Umap details=None
01:30:02,629 datashaper.workflow.workflow ERROR Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis"
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
01:30:02,634 graphrag.callbacks.file_workflow_callbacks INFO Error executing verb "create_final_nodes" in create_final_nodes: "['x', 'y'] not found in axis" details=None
01:30:02,636 graphrag.index.run.run ERROR error running workflow create_final_nodes
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/run.py", line 251, in run_pipeline
    result = await _process_workflow(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/run/workflow.py", line 89, in _process_workflow
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/workflows/v1/subflows/create_final_nodes.py", line 36, in create_final_nodes
    output = await create_final_nodes_flow(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/flows/create_final_nodes.py", line 48, in create_final_nodes
    nodes_without_positions = nodes.drop(columns=["x", "y"])
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 5581, in drop
    return super().drop(
           ^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4788, in drop
    obj = obj._drop_axis(labels, axis, level=level, errors=errors)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/generic.py", line 4830, in _drop_axis
    new_axis = axis.drop(labels, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/indexes/base.py", line 7070, in drop
    raise KeyError(f"{labels[mask].tolist()} not found in axis")
KeyError: "['x', 'y'] not found in axis"
01:30:02,637 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
01:30:02,641 graphrag.index.cli ERROR Errors occurred during the pipeline run, see logs for more details.
01:30:54,447 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output/indexing-engine.log
01:30:54,448 graphrag.index.cli INFO Starting pipeline run for: 20241018-013054, dryrun=False
01:30:54,448 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 50,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 5
    },
    "async_mode": "asyncio",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "csv",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.csv$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "THINGS"
        ],
        "max_gleanings": 2,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 50,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 5
        },
        "async_mode": "asyncio",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": [
        ""
    ]
}
01:30:54,451 graphrag.index.create_pipeline_config INFO skipping workflows 
01:30:54,451 graphrag.index.run.run INFO Running pipeline
01:30:54,451 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/output
01:30:54,452 graphrag.index.input.load_input INFO loading input from root_dir=input
01:30:54,452 graphrag.index.input.load_input INFO using file storage for input
01:30:54,453 graphrag.index.input.csv INFO Loading csv files from input
01:30:54,453 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/elec_graph/input for files matching .*\.csv$
01:30:54,457 graphrag.index.input.csv INFO Found 1 csv files, loading 1
01:30:54,457 graphrag.index.input.csv INFO Total number of unfiltered csv rows: 100
01:30:54,458 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents']
01:30:54,459 graphrag.index.run.run INFO Final # of rows loaded: 100
01:30:54,567 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
01:30:54,570 datashaper.workflow.workflow INFO executing verb create_base_text_units
01:30:55,63 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
01:30:55,193 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
01:30:55,194 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:30:55,202 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
01:30:55,206 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:30:55,244 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=50
01:30:55,244 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
01:30:56,593 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:56,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5220552919927286. input_tokens=149, output_tokens=0
01:30:58,666 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:58,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6191438359965105. input_tokens=175, output_tokens=44
01:30:59,28 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9165157969982829. input_tokens=165, output_tokens=40
01:30:59,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5552393499965547. input_tokens=157, output_tokens=26
01:30:59,305 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1975456229993142. input_tokens=157, output_tokens=38
01:30:59,504 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.4245904520066688. input_tokens=161, output_tokens=30
01:30:59,592 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5451616649952484. input_tokens=173, output_tokens=48
01:30:59,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5671150910056895. input_tokens=171, output_tokens=55
01:30:59,835 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.4993174779956462. input_tokens=167, output_tokens=26
01:30:59,862 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:59,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5006654820026597. input_tokens=165, output_tokens=37
01:31:01,8 root INFO Starting preprocessing of transition probabilities on graph with 1814 nodes and 2958 edges
01:31:01,8 root INFO Starting at time 1729207861.0083385
01:31:01,8 root INFO Beginning preprocessing of transition probabilities for 1814 vertices
01:31:01,8 root INFO Completed 1 / 1814 vertices
01:31:01,9 root INFO Completed 182 / 1814 vertices
01:31:01,10 root INFO Completed 363 / 1814 vertices
01:31:01,11 root INFO Completed 544 / 1814 vertices
01:31:01,12 root INFO Completed 725 / 1814 vertices
01:31:01,13 root INFO Completed 906 / 1814 vertices
01:31:01,14 root INFO Completed 1087 / 1814 vertices
01:31:01,15 root INFO Completed 1268 / 1814 vertices
01:31:01,16 root INFO Completed 1449 / 1814 vertices
01:31:01,16 root INFO Completed 1630 / 1814 vertices
01:31:01,17 root INFO Completed 1811 / 1814 vertices
01:31:01,18 root INFO Completed preprocessing of transition probabilities for vertices
01:31:01,18 root INFO Beginning preprocessing of transition probabilities for 2958 edges
01:31:01,19 root INFO Completed 1 / 2958 edges
01:31:01,26 root INFO Completed 296 / 2958 edges
01:31:01,39 root INFO Completed 591 / 2958 edges
01:31:01,50 root INFO Completed 886 / 2958 edges
01:31:01,69 root INFO Completed 1181 / 2958 edges
01:31:01,75 root INFO Completed 1476 / 2958 edges
01:31:01,79 root INFO Completed 1771 / 2958 edges
01:31:01,84 root INFO Completed 2066 / 2958 edges
01:31:01,88 root INFO Completed 2361 / 2958 edges
01:31:01,91 root INFO Completed 2656 / 2958 edges
01:31:01,94 root INFO Completed 2951 / 2958 edges
01:31:01,94 root INFO Completed preprocessing of transition probabilities for edges
01:31:01,94 root INFO Simulating walks on graph at time 1729207861.0946255
01:31:01,94 root INFO Walk iteration: 1/10
01:31:01,147 root INFO Walk iteration: 2/10
01:31:01,198 root INFO Walk iteration: 3/10
01:31:01,249 root INFO Walk iteration: 4/10
01:31:01,299 root INFO Walk iteration: 5/10
01:31:01,350 root INFO Walk iteration: 6/10
01:31:01,400 root INFO Walk iteration: 7/10
01:31:01,452 root INFO Walk iteration: 8/10
01:31:01,504 root INFO Walk iteration: 9/10
01:31:01,555 root INFO Walk iteration: 10/10
01:31:01,611 root INFO Learning embeddings at time 1729207861.6114292
01:31:01,621 gensim.models.word2vec INFO collecting all words and their counts
01:31:01,621 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:31:01,628 gensim.models.word2vec INFO PROGRESS: at sentence #10000, processed 188088 words, keeping 1814 word types
01:31:01,634 gensim.models.word2vec INFO collected 1814 word types from a corpus of 341520 raw words and 18140 sentences
01:31:01,634 gensim.models.word2vec INFO Creating a fresh vocabulary
01:31:01,636 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 1814 unique words (100.00% of original 1814, drops 0)', 'datetime': '2024-10-18T01:31:01.636188', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:01,636 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 341520 word corpus (100.00% of original 341520, drops 0)', 'datetime': '2024-10-18T01:31:01.636234', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:01,638 gensim.models.word2vec INFO deleting the raw counts dictionary of 1814 items
01:31:01,639 gensim.models.word2vec INFO sample=0.001 downsamples 51 most-common words
01:31:01,639 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 305540.993634476 word corpus (89.5%% of prior 341520)', 'datetime': '2024-10-18T01:31:01.639142', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:01,643 gensim.models.word2vec INFO estimated required memory for 1814 words and 1536 dimensions: 23197432 bytes
01:31:01,643 gensim.models.word2vec INFO resetting layer weights
01:31:01,650 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-18T01:31:01.650708', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:31:01,650 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 1814 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-18T01:31:01.650766', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:01,997 gensim.models.word2vec INFO EPOCH 0: training on 341520 raw words (305718 effective words) took 0.3s, 890043 effective words/s
01:31:02,323 gensim.models.word2vec INFO EPOCH 1: training on 341520 raw words (305470 effective words) took 0.3s, 945629 effective words/s
01:31:02,637 gensim.models.word2vec INFO EPOCH 2: training on 341520 raw words (305401 effective words) took 0.3s, 985418 effective words/s
01:31:02,637 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1024560 raw words (916589 effective words) took 1.0s, 928620 effective words/s', 'datetime': '2024-10-18T01:31:02.637837', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:02,638 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1814, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-18T01:31:02.638860', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:31:02,639 root INFO Completed. Ending time is 1729207862.6394453 Elapsed time is -1.6311068534851074
01:31:02,929 root INFO Starting preprocessing of transition probabilities on graph with 1814 nodes and 2958 edges
01:31:02,929 root INFO Starting at time 1729207862.9293516
01:31:02,929 root INFO Beginning preprocessing of transition probabilities for 1814 vertices
01:31:02,929 root INFO Completed 1 / 1814 vertices
01:31:02,930 root INFO Completed 182 / 1814 vertices
01:31:02,931 root INFO Completed 363 / 1814 vertices
01:31:02,932 root INFO Completed 544 / 1814 vertices
01:31:02,933 root INFO Completed 725 / 1814 vertices
01:31:02,934 root INFO Completed 906 / 1814 vertices
01:31:02,935 root INFO Completed 1087 / 1814 vertices
01:31:02,936 root INFO Completed 1268 / 1814 vertices
01:31:02,936 root INFO Completed 1449 / 1814 vertices
01:31:02,937 root INFO Completed 1630 / 1814 vertices
01:31:02,938 root INFO Completed 1811 / 1814 vertices
01:31:02,938 root INFO Completed preprocessing of transition probabilities for vertices
01:31:02,939 root INFO Beginning preprocessing of transition probabilities for 2958 edges
01:31:02,939 root INFO Completed 1 / 2958 edges
01:31:02,947 root INFO Completed 296 / 2958 edges
01:31:02,959 root INFO Completed 591 / 2958 edges
01:31:02,971 root INFO Completed 886 / 2958 edges
01:31:02,991 root INFO Completed 1181 / 2958 edges
01:31:02,997 root INFO Completed 1476 / 2958 edges
01:31:03,1 root INFO Completed 1771 / 2958 edges
01:31:03,6 root INFO Completed 2066 / 2958 edges
01:31:03,10 root INFO Completed 2361 / 2958 edges
01:31:03,13 root INFO Completed 2656 / 2958 edges
01:31:03,16 root INFO Completed 2951 / 2958 edges
01:31:03,16 root INFO Completed preprocessing of transition probabilities for edges
01:31:03,16 root INFO Simulating walks on graph at time 1729207863.0167754
01:31:03,17 root INFO Walk iteration: 1/10
01:31:03,69 root INFO Walk iteration: 2/10
01:31:03,119 root INFO Walk iteration: 3/10
01:31:03,170 root INFO Walk iteration: 4/10
01:31:03,222 root INFO Walk iteration: 5/10
01:31:03,272 root INFO Walk iteration: 6/10
01:31:03,322 root INFO Walk iteration: 7/10
01:31:03,372 root INFO Walk iteration: 8/10
01:31:03,423 root INFO Walk iteration: 9/10
01:31:03,474 root INFO Walk iteration: 10/10
01:31:03,535 root INFO Learning embeddings at time 1729207863.5357733
01:31:03,544 gensim.models.word2vec INFO collecting all words and their counts
01:31:03,544 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:31:03,551 gensim.models.word2vec INFO PROGRESS: at sentence #10000, processed 188088 words, keeping 1814 word types
01:31:03,557 gensim.models.word2vec INFO collected 1814 word types from a corpus of 341520 raw words and 18140 sentences
01:31:03,557 gensim.models.word2vec INFO Creating a fresh vocabulary
01:31:03,558 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 1814 unique words (100.00% of original 1814, drops 0)', 'datetime': '2024-10-18T01:31:03.558445', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:03,559 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 341520 word corpus (100.00% of original 341520, drops 0)', 'datetime': '2024-10-18T01:31:03.559842', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:03,562 gensim.models.word2vec INFO deleting the raw counts dictionary of 1814 items
01:31:03,562 gensim.models.word2vec INFO sample=0.001 downsamples 51 most-common words
01:31:03,562 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 305540.993634476 word corpus (89.5%% of prior 341520)', 'datetime': '2024-10-18T01:31:03.562479', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:03,566 gensim.models.word2vec INFO estimated required memory for 1814 words and 1536 dimensions: 23197432 bytes
01:31:03,567 gensim.models.word2vec INFO resetting layer weights
01:31:03,573 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-18T01:31:03.573537', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:31:03,573 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 1814 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-18T01:31:03.573586', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:03,921 gensim.models.word2vec INFO EPOCH 0: training on 341520 raw words (305551 effective words) took 0.3s, 886346 effective words/s
01:31:04,233 gensim.models.word2vec INFO EPOCH 1: training on 341520 raw words (305445 effective words) took 0.3s, 993782 effective words/s
01:31:04,531 gensim.models.word2vec INFO EPOCH 2: training on 341520 raw words (305326 effective words) took 0.3s, 1036120 effective words/s
01:31:04,531 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1024560 raw words (916322 effective words) took 1.0s, 956571 effective words/s', 'datetime': '2024-10-18T01:31:04.531560', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:04,532 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1814, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-18T01:31:04.532665', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:31:04,534 root INFO Completed. Ending time is 1729207864.5345428 Elapsed time is -1.6051912307739258
01:31:04,830 root INFO Starting preprocessing of transition probabilities on graph with 1814 nodes and 2958 edges
01:31:04,830 root INFO Starting at time 1729207864.8308337
01:31:04,830 root INFO Beginning preprocessing of transition probabilities for 1814 vertices
01:31:04,830 root INFO Completed 1 / 1814 vertices
01:31:04,832 root INFO Completed 182 / 1814 vertices
01:31:04,833 root INFO Completed 363 / 1814 vertices
01:31:04,834 root INFO Completed 544 / 1814 vertices
01:31:04,835 root INFO Completed 725 / 1814 vertices
01:31:04,836 root INFO Completed 906 / 1814 vertices
01:31:04,836 root INFO Completed 1087 / 1814 vertices
01:31:04,837 root INFO Completed 1268 / 1814 vertices
01:31:04,838 root INFO Completed 1449 / 1814 vertices
01:31:04,838 root INFO Completed 1630 / 1814 vertices
01:31:04,839 root INFO Completed 1811 / 1814 vertices
01:31:04,839 root INFO Completed preprocessing of transition probabilities for vertices
01:31:04,844 root INFO Beginning preprocessing of transition probabilities for 2958 edges
01:31:04,845 root INFO Completed 1 / 2958 edges
01:31:04,852 root INFO Completed 296 / 2958 edges
01:31:04,865 root INFO Completed 591 / 2958 edges
01:31:04,877 root INFO Completed 886 / 2958 edges
01:31:04,896 root INFO Completed 1181 / 2958 edges
01:31:04,902 root INFO Completed 1476 / 2958 edges
01:31:04,906 root INFO Completed 1771 / 2958 edges
01:31:04,911 root INFO Completed 2066 / 2958 edges
01:31:04,915 root INFO Completed 2361 / 2958 edges
01:31:04,918 root INFO Completed 2656 / 2958 edges
01:31:04,921 root INFO Completed 2951 / 2958 edges
01:31:04,921 root INFO Completed preprocessing of transition probabilities for edges
01:31:04,921 root INFO Simulating walks on graph at time 1729207864.9216475
01:31:04,922 root INFO Walk iteration: 1/10
01:31:04,975 root INFO Walk iteration: 2/10
01:31:05,26 root INFO Walk iteration: 3/10
01:31:05,77 root INFO Walk iteration: 4/10
01:31:05,127 root INFO Walk iteration: 5/10
01:31:05,177 root INFO Walk iteration: 6/10
01:31:05,228 root INFO Walk iteration: 7/10
01:31:05,278 root INFO Walk iteration: 8/10
01:31:05,330 root INFO Walk iteration: 9/10
01:31:05,380 root INFO Walk iteration: 10/10
01:31:05,430 root INFO Learning embeddings at time 1729207865.4308074
01:31:05,439 gensim.models.word2vec INFO collecting all words and their counts
01:31:05,439 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:31:05,446 gensim.models.word2vec INFO PROGRESS: at sentence #10000, processed 188088 words, keeping 1814 word types
01:31:05,451 gensim.models.word2vec INFO collected 1814 word types from a corpus of 341520 raw words and 18140 sentences
01:31:05,452 gensim.models.word2vec INFO Creating a fresh vocabulary
01:31:05,453 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 1814 unique words (100.00% of original 1814, drops 0)', 'datetime': '2024-10-18T01:31:05.453412', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:05,453 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 341520 word corpus (100.00% of original 341520, drops 0)', 'datetime': '2024-10-18T01:31:05.453443', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:05,456 gensim.models.word2vec INFO deleting the raw counts dictionary of 1814 items
01:31:05,456 gensim.models.word2vec INFO sample=0.001 downsamples 51 most-common words
01:31:05,456 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 305540.993634476 word corpus (89.5%% of prior 341520)', 'datetime': '2024-10-18T01:31:05.456215', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:05,460 gensim.models.word2vec INFO estimated required memory for 1814 words and 1536 dimensions: 23197432 bytes
01:31:05,460 gensim.models.word2vec INFO resetting layer weights
01:31:05,465 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-18T01:31:05.465303', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:31:05,465 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 1814 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-18T01:31:05.465359', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:05,807 gensim.models.word2vec INFO EPOCH 0: training on 341520 raw words (305389 effective words) took 0.3s, 902270 effective words/s
01:31:06,117 gensim.models.word2vec INFO EPOCH 1: training on 341520 raw words (305597 effective words) took 0.3s, 998875 effective words/s
01:31:06,415 gensim.models.word2vec INFO EPOCH 2: training on 341520 raw words (305646 effective words) took 0.3s, 1038779 effective words/s
01:31:06,415 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1024560 raw words (916632 effective words) took 1.0s, 964799 effective words/s', 'datetime': '2024-10-18T01:31:06.415462', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:06,415 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1814, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-18T01:31:06.415531', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:31:06,417 root INFO Completed. Ending time is 1729207866.4174974 Elapsed time is -1.5866637229919434
01:31:06,727 root INFO Starting preprocessing of transition probabilities on graph with 1814 nodes and 2958 edges
01:31:06,727 root INFO Starting at time 1729207866.727288
01:31:06,727 root INFO Beginning preprocessing of transition probabilities for 1814 vertices
01:31:06,727 root INFO Completed 1 / 1814 vertices
01:31:06,728 root INFO Completed 182 / 1814 vertices
01:31:06,729 root INFO Completed 363 / 1814 vertices
01:31:06,730 root INFO Completed 544 / 1814 vertices
01:31:06,731 root INFO Completed 725 / 1814 vertices
01:31:06,732 root INFO Completed 906 / 1814 vertices
01:31:06,737 root INFO Completed 1087 / 1814 vertices
01:31:06,738 root INFO Completed 1268 / 1814 vertices
01:31:06,739 root INFO Completed 1449 / 1814 vertices
01:31:06,739 root INFO Completed 1630 / 1814 vertices
01:31:06,748 root INFO Completed 1811 / 1814 vertices
01:31:06,748 root INFO Completed preprocessing of transition probabilities for vertices
01:31:06,749 root INFO Beginning preprocessing of transition probabilities for 2958 edges
01:31:06,750 root INFO Completed 1 / 2958 edges
01:31:06,757 root INFO Completed 296 / 2958 edges
01:31:06,770 root INFO Completed 591 / 2958 edges
01:31:06,782 root INFO Completed 886 / 2958 edges
01:31:06,800 root INFO Completed 1181 / 2958 edges
01:31:06,807 root INFO Completed 1476 / 2958 edges
01:31:06,811 root INFO Completed 1771 / 2958 edges
01:31:06,815 root INFO Completed 2066 / 2958 edges
01:31:06,819 root INFO Completed 2361 / 2958 edges
01:31:06,823 root INFO Completed 2656 / 2958 edges
01:31:06,825 root INFO Completed 2951 / 2958 edges
01:31:06,826 root INFO Completed preprocessing of transition probabilities for edges
01:31:06,826 root INFO Simulating walks on graph at time 1729207866.826064
01:31:06,826 root INFO Walk iteration: 1/10
01:31:06,878 root INFO Walk iteration: 2/10
01:31:06,929 root INFO Walk iteration: 3/10
01:31:06,979 root INFO Walk iteration: 4/10
01:31:07,30 root INFO Walk iteration: 5/10
01:31:07,81 root INFO Walk iteration: 6/10
01:31:07,131 root INFO Walk iteration: 7/10
01:31:07,182 root INFO Walk iteration: 8/10
01:31:07,232 root INFO Walk iteration: 9/10
01:31:07,283 root INFO Walk iteration: 10/10
01:31:07,333 root INFO Learning embeddings at time 1729207867.3338718
01:31:07,342 gensim.models.word2vec INFO collecting all words and their counts
01:31:07,342 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
01:31:07,348 gensim.models.word2vec INFO PROGRESS: at sentence #10000, processed 188088 words, keeping 1814 word types
01:31:07,354 gensim.models.word2vec INFO collected 1814 word types from a corpus of 341520 raw words and 18140 sentences
01:31:07,354 gensim.models.word2vec INFO Creating a fresh vocabulary
01:31:07,355 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 1814 unique words (100.00% of original 1814, drops 0)', 'datetime': '2024-10-18T01:31:07.355662', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:07,355 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 341520 word corpus (100.00% of original 341520, drops 0)', 'datetime': '2024-10-18T01:31:07.355695', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:07,358 gensim.models.word2vec INFO deleting the raw counts dictionary of 1814 items
01:31:07,358 gensim.models.word2vec INFO sample=0.001 downsamples 51 most-common words
01:31:07,358 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 305540.993634476 word corpus (89.5%% of prior 341520)', 'datetime': '2024-10-18T01:31:07.358269', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
01:31:07,362 gensim.models.word2vec INFO estimated required memory for 1814 words and 1536 dimensions: 23197432 bytes
01:31:07,362 gensim.models.word2vec INFO resetting layer weights
01:31:07,367 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-10-18T01:31:07.367348', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
01:31:07,367 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 1814 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-10-18T01:31:07.367408', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:07,716 gensim.models.word2vec INFO EPOCH 0: training on 341520 raw words (305404 effective words) took 0.3s, 884537 effective words/s
01:31:08,31 gensim.models.word2vec INFO EPOCH 1: training on 341520 raw words (305404 effective words) took 0.3s, 979237 effective words/s
01:31:08,333 gensim.models.word2vec INFO EPOCH 2: training on 341520 raw words (305423 effective words) took 0.3s, 1025830 effective words/s
01:31:08,333 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 1024560 raw words (916231 effective words) took 1.0s, 948674 effective words/s', 'datetime': '2024-10-18T01:31:08.333231', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
01:31:08,333 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1814, vector_size=1536, alpha=0.025>', 'datetime': '2024-10-18T01:31:08.333276', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.112-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
01:31:08,333 root INFO Completed. Ending time is 1729207868.3339179 Elapsed time is -1.6066298484802246
01:31:09,298 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:31:11,204 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:31:11,204 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:31:11,440 datashaper.workflow.workflow INFO executing verb create_final_entities
01:31:12,81 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:31:12,117 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
01:31:12,117 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:31:12,198 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3467 inputs via 3391 snippets using 212 batches. max_batch_size=16, max_tokens=8191
01:31:13,244 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:13,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0671852209925419. input_tokens=601, output_tokens=0
01:31:13,658 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:13,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.479285916997469. input_tokens=640, output_tokens=0
01:31:13,684 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:13,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5064759860106278. input_tokens=1115, output_tokens=0
01:31:13,742 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:13,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5650975630123867. input_tokens=462, output_tokens=0
01:31:13,768 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:13,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5901930410036584. input_tokens=894, output_tokens=0
01:31:14,571 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:14,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3167548310011625. input_tokens=1099, output_tokens=0
01:31:15,90 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:15,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3832996920100413. input_tokens=659, output_tokens=0
01:31:15,116 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:15,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3386608190048719. input_tokens=591, output_tokens=0
01:31:15,158 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:15,183 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3810346059908625. input_tokens=781, output_tokens=0
01:31:15,185 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:15,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4803879660030361. input_tokens=1607, output_tokens=0
01:31:21,749 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:21,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1668922640092205. input_tokens=1190, output_tokens=0
01:31:27,857 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:27,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2639881459908793. input_tokens=606, output_tokens=0
01:31:33,327 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:33,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7218834189989138. input_tokens=413, output_tokens=0
01:31:39,870 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:39,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2599827369995182. input_tokens=394, output_tokens=0
01:31:45,369 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:45,394 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7466406169987749. input_tokens=608, output_tokens=0
01:31:51,888 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:51,915 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2631060120038455. input_tokens=402, output_tokens=0
01:31:57,384 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:31:57,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7475619800097775. input_tokens=447, output_tokens=0
01:32:03,826 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:03,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1816143759933766. input_tokens=819, output_tokens=0
01:32:09,909 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:09,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2600116899993736. input_tokens=821, output_tokens=0
01:32:15,821 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:15,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1627328940085135. input_tokens=490, output_tokens=0
01:32:21,824 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:21,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.153519072002382. input_tokens=583, output_tokens=0
01:32:27,936 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:27,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2600456719956128. input_tokens=679, output_tokens=0
01:32:33,413 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:33,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7260755070019513. input_tokens=450, output_tokens=0
01:32:39,862 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:39,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1668013260059524. input_tokens=490, output_tokens=0
01:32:45,959 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:45,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2640775599866174. input_tokens=336, output_tokens=0
01:32:51,361 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:51,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.64836569900217. input_tokens=242, output_tokens=0
01:32:57,880 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:32:57,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1603363739995984. input_tokens=293, output_tokens=0
01:33:03,895 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:03,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1699008449941175. input_tokens=310, output_tokens=0
01:33:09,381 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:09,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6433721170033095. input_tokens=212, output_tokens=0
01:33:15,898 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:15,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1525316560000647. input_tokens=248, output_tokens=0
01:33:21,911 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:21,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1568429510080023. input_tokens=320, output_tokens=0
01:33:28,17 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:28,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2545883120037615. input_tokens=231, output_tokens=0
01:33:33,935 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:33,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1639400209969608. input_tokens=331, output_tokens=0
01:33:39,961 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:39,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1808066490048077. input_tokens=450, output_tokens=0
01:33:45,955 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:45,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.165337835001992. input_tokens=569, output_tokens=0
01:33:51,650 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:51,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8518026100064162. input_tokens=746, output_tokens=0
01:33:57,584 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:33:57,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7780301099992357. input_tokens=658, output_tokens=0
01:34:04,1 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:04,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1881282340036705. input_tokens=776, output_tokens=0
01:34:09,997 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:10,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1731312769989017. input_tokens=518, output_tokens=0
01:34:15,583 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:15,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7494497570005478. input_tokens=502, output_tokens=0
01:34:22,113 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:22,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2714738479990046. input_tokens=490, output_tokens=0
01:34:28,19 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:28,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1729044730018359. input_tokens=749, output_tokens=0
01:34:33,701 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:33,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8431772600015393. input_tokens=618, output_tokens=0
01:34:40,135 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:40,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2693873429961968. input_tokens=705, output_tokens=0
01:34:46,36 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:46,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.161789603997022. input_tokens=889, output_tokens=0
01:34:52,29 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:52,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1470786619902356. input_tokens=622, output_tokens=0
01:34:58,150 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:34:58,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2604092920082621. input_tokens=290, output_tokens=0
01:35:04,155 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:04,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2574561479996191. input_tokens=419, output_tokens=0
01:35:09,652 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:09,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7469723539979896. input_tokens=332, output_tokens=0
01:35:16,170 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:16,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2553062469960423. input_tokens=324, output_tokens=0
01:35:22,166 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:22,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2436468789965147. input_tokens=331, output_tokens=0
01:35:28,108 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:28,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1745438190118875. input_tokens=308, output_tokens=0
01:35:34,192 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:34,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2496867769950768. input_tokens=402, output_tokens=0
01:35:39,800 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:39,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8530703919968801. input_tokens=327, output_tokens=0
01:35:46,228 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:46,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.267804449002142. input_tokens=308, output_tokens=0
01:35:52,135 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:52,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1642686259874608. input_tokens=212, output_tokens=0
01:35:58,133 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:58,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.158415199999581. input_tokens=604, output_tokens=0
01:36:04,239 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:04,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.250879580999026. input_tokens=753, output_tokens=0
01:36:10,153 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:10,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1560422040056437. input_tokens=394, output_tokens=0
01:36:16,162 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:16,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1564614029921358. input_tokens=502, output_tokens=0
01:36:22,177 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:22,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1628389889956452. input_tokens=375, output_tokens=0
01:36:28,282 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:28,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.258924529000069. input_tokens=529, output_tokens=0
01:36:34,196 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:34,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1646258639957523. input_tokens=416, output_tokens=0
01:36:40,204 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:40,228 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.164444119000109. input_tokens=538, output_tokens=0
01:36:46,216 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:46,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.16621420699812. input_tokens=591, output_tokens=0
01:36:51,794 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:51,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7362188620027155. input_tokens=411, output_tokens=0
01:36:57,717 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:57,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.650004829003592. input_tokens=311, output_tokens=0
01:37:04,243 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:04,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1661769869970158. input_tokens=512, output_tokens=0
01:37:10,343 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:10,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2588108979980461. input_tokens=460, output_tokens=0
01:37:15,849 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:15,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7545748499978799. input_tokens=458, output_tokens=0
01:37:22,286 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:22,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1863385940087028. input_tokens=464, output_tokens=0
01:37:28,270 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:28,297 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.161660704005044. input_tokens=366, output_tokens=0
01:37:33,862 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:33,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7417029049975099. input_tokens=478, output_tokens=0
01:37:39,885 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:39,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7557724899961613. input_tokens=476, output_tokens=0
01:37:46,296 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:46,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1565916749968892. input_tokens=338, output_tokens=0
01:37:52,300 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:52,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1526269690075424. input_tokens=357, output_tokens=0
01:37:57,792 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:57,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6377859579952201. input_tokens=267, output_tokens=0
01:38:04,317 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:04,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1512515089998487. input_tokens=294, output_tokens=0
01:38:09,915 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:09,939 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7405211059958674. input_tokens=181, output_tokens=0
01:38:15,901 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:15,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7215240630030166. input_tokens=263, output_tokens=0
01:38:22,445 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:22,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2517714349960443. input_tokens=241, output_tokens=0
01:38:28,454 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:28,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2520922420080751. input_tokens=311, output_tokens=0
01:38:34,359 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:34,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.15274491198943. input_tokens=452, output_tokens=0
01:38:40,484 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:40,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2643392419995507. input_tokens=612, output_tokens=0
01:38:46,488 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:46,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2599187309970148. input_tokens=575, output_tokens=0
01:38:52,501 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:52,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2627381360071013. input_tokens=650, output_tokens=0
01:38:58,514 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:58,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2673011430015322. input_tokens=542, output_tokens=0
01:39:04,521 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:04,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2682356040022569. input_tokens=479, output_tokens=0
01:39:10,425 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:10,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1593949689995497. input_tokens=664, output_tokens=0
01:39:16,516 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:16,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.24079749599332. input_tokens=434, output_tokens=0
01:39:22,550 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:22,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2650233009917429. input_tokens=511, output_tokens=0
01:39:28,636 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:28,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3473674160049995. input_tokens=397, output_tokens=0
01:39:34,564 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:34,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.265241673012497. input_tokens=358, output_tokens=0
01:39:40,580 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:40,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2705762249970576. input_tokens=445, output_tokens=0
01:39:46,588 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:46,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2672914379945723. input_tokens=371, output_tokens=0
01:39:52,584 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:52,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2554119030100992. input_tokens=309, output_tokens=0
01:39:58,493 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:58,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1593672849994618. input_tokens=352, output_tokens=0
01:40:04,603 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:04,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2613207429967588. input_tokens=352, output_tokens=0
01:40:09,995 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:10,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6477613140014. input_tokens=347, output_tokens=0
01:40:16,108 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:16,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7420024859893601. input_tokens=472, output_tokens=0
01:40:22,526 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:22,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1509038410003996. input_tokens=474, output_tokens=0
01:40:28,197 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:28,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8203166169987526. input_tokens=523, output_tokens=0
01:40:34,538 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:34,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1495217509946087. input_tokens=458, output_tokens=0
01:40:40,537 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:40,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1406351659970824. input_tokens=458, output_tokens=0
01:40:46,563 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:46,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1562226800015196. input_tokens=732, output_tokens=0
01:40:52,247 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:52,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.833118448004825. input_tokens=700, output_tokens=0
01:40:58,682 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:58,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2593947560089873. input_tokens=571, output_tokens=0
01:41:04,177 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:04,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7450393199978862. input_tokens=410, output_tokens=0
01:41:10,602 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:10,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.161490665996098. input_tokens=314, output_tokens=0
01:41:16,610 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:16,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.160367263990338. input_tokens=871, output_tokens=0
01:41:22,724 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:22,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.265453668005648. input_tokens=553, output_tokens=0
01:41:28,624 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:28,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1587932109978283. input_tokens=629, output_tokens=0
01:41:34,639 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:34,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1624099659966305. input_tokens=449, output_tokens=0
01:41:40,745 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:40,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2593876559985802. input_tokens=624, output_tokens=0
01:41:46,658 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:46,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.162568892003037. input_tokens=413, output_tokens=0
01:41:52,661 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:52,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1604815329919802. input_tokens=290, output_tokens=0
01:41:58,669 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:58,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1570130699983565. input_tokens=322, output_tokens=0
01:42:04,782 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:04,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2650060320011107. input_tokens=585, output_tokens=0
01:42:10,781 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:10,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2516131040028995. input_tokens=305, output_tokens=0
01:42:16,389 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:16,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8499420739972265. input_tokens=294, output_tokens=0
01:42:22,307 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:22,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7585380670061568. input_tokens=265, output_tokens=0
01:42:28,826 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:28,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2690490529930685. input_tokens=291, output_tokens=0
01:42:34,736 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:34,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.170197179002571. input_tokens=270, output_tokens=0
01:42:40,321 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:40,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7519420250027906. input_tokens=294, output_tokens=0
01:42:46,740 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:46,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.155976533991634. input_tokens=262, output_tokens=0
01:42:52,751 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:52,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1587016659905203. input_tokens=249, output_tokens=0
01:42:58,753 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:58,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1558910790045047. input_tokens=214, output_tokens=0
01:43:04,877 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:04,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2710590810020221. input_tokens=238, output_tokens=0
01:43:10,779 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:10,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1591507340053795. input_tokens=245, output_tokens=0
01:43:16,886 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:16,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2567312980099814. input_tokens=220, output_tokens=0
01:43:22,880 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:22,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2430694969953038. input_tokens=190, output_tokens=0
01:43:29,86 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:29,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4436701259983238. input_tokens=273, output_tokens=0
01:43:34,825 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:34,848 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1693936759984354. input_tokens=355, output_tokens=0
01:43:40,824 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:40,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1650207979982952. input_tokens=422, output_tokens=0
01:43:46,932 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:46,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2589308869937668. input_tokens=467, output_tokens=0
01:43:52,933 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:52,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.253387426986592. input_tokens=459, output_tokens=0
01:43:58,855 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:58,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1687076840025838. input_tokens=475, output_tokens=0
01:44:04,545 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:04,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8474364249996142. input_tokens=344, output_tokens=0
01:44:10,972 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:11,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2683051110070664. input_tokens=555, output_tokens=0
01:44:16,986 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:17,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2679736930003855. input_tokens=486, output_tokens=0
01:44:22,982 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:23,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.257519876002334. input_tokens=435, output_tokens=0
01:44:28,896 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:28,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1601476310024736. input_tokens=448, output_tokens=0
01:44:35,11 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:35,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2683143549948. input_tokens=323, output_tokens=0
01:44:41,10 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:41,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2557574670063332. input_tokens=462, output_tokens=0
01:44:47,13 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:47,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2502192009997088. input_tokens=469, output_tokens=0
01:44:53,28 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:53,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2567434849916026. input_tokens=610, output_tokens=0
01:44:58,950 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:58,973 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1690735360025428. input_tokens=454, output_tokens=0
01:45:05,48 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:05,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.263890469999751. input_tokens=404, output_tokens=0
01:45:11,52 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:11,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2583797260012943. input_tokens=349, output_tokens=0
01:45:17,51 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:17,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2443794110004092. input_tokens=442, output_tokens=0
01:45:23,72 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:23,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2600311360001797. input_tokens=475, output_tokens=0
01:45:29,81 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:29,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2550723440072034. input_tokens=466, output_tokens=0
01:45:34,462 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:34,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.627187029007473. input_tokens=378, output_tokens=0
01:45:41,91 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:41,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.246807125993655. input_tokens=655, output_tokens=0
01:45:47,9 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:47,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1569394520047354. input_tokens=364, output_tokens=0
01:45:53,6 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:53,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1490833020070568. input_tokens=390, output_tokens=0
01:45:59,126 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:59,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2553457329922821. input_tokens=358, output_tokens=0
01:46:05,23 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:05,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1435172670026077. input_tokens=365, output_tokens=0
01:46:11,42 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:11,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.153330922999885. input_tokens=484, output_tokens=0
01:46:17,152 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:17,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2587902109953575. input_tokens=528, output_tokens=0
01:46:23,87 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:23,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1809284839982865. input_tokens=508, output_tokens=0
01:46:29,168 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:29,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2523787609970896. input_tokens=313, output_tokens=0
01:46:35,80 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:35,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.155130895000184. input_tokens=508, output_tokens=0
01:46:41,90 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:41,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1569111729913857. input_tokens=268, output_tokens=0
01:46:47,122 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:47,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1809987000015099. input_tokens=302, output_tokens=0
01:46:53,125 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:53,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1730873479973525. input_tokens=509, output_tokens=0
01:46:59,122 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:59,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.161929562993464. input_tokens=310, output_tokens=0
01:47:05,248 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:05,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2790568990021711. input_tokens=323, output_tokens=0
01:47:11,148 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:11,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1695105699909618. input_tokens=340, output_tokens=0
01:47:17,145 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:17,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1609964599920204. input_tokens=333, output_tokens=0
01:47:22,632 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:22,656 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6370749159978004. input_tokens=275, output_tokens=0
01:47:28,643 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:28,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6394609989947639. input_tokens=500, output_tokens=0
01:47:35,186 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:35,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1745061319961678. input_tokens=826, output_tokens=0
01:47:41,304 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:41,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2840332690102514. input_tokens=852, output_tokens=0
01:47:47,294 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:47,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.262621963993297. input_tokens=943, output_tokens=0
01:47:52,775 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:52,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7326844390045153. input_tokens=648, output_tokens=0
01:47:59,317 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:59,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.266950530000031. input_tokens=1102, output_tokens=0
01:48:05,221 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:05,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1596062570024515. input_tokens=717, output_tokens=0
01:48:11,350 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:11,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2800322069961112. input_tokens=1047, output_tokens=0
01:48:16,921 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:16,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8503673049999634. input_tokens=729, output_tokens=0
01:48:23,251 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:23,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1669261890056077. input_tokens=865, output_tokens=0
01:48:28,736 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:28,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6406317809887696. input_tokens=789, output_tokens=0
01:48:34,947 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:34,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8462484839983517. input_tokens=930, output_tokens=0
01:48:41,380 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:41,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2649374340107897. input_tokens=534, output_tokens=0
01:48:47,299 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:47,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1748501590045635. input_tokens=742, output_tokens=0
01:48:53,388 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:53,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2561571639962494. input_tokens=526, output_tokens=0
01:48:59,318 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:59,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1749441370047862. input_tokens=692, output_tokens=0
01:49:05,318 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:05,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1665331889962545. input_tokens=682, output_tokens=0
01:49:11,319 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:11,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1601597929984564. input_tokens=771, output_tokens=0
01:49:17,307 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:17,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1387398609949742. input_tokens=520, output_tokens=0
01:49:23,338 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:23,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1599207289982587. input_tokens=706, output_tokens=0
01:49:29,445 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:29,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2581027790001826. input_tokens=620, output_tokens=0
01:49:35,37 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:35,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8405606110027293. input_tokens=586, output_tokens=0
01:49:41,368 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:41,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.163808457000414. input_tokens=582, output_tokens=0
01:49:47,373 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:47,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.166229327005567. input_tokens=693, output_tokens=0
01:49:52,871 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:52,894 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6512845079996623. input_tokens=425, output_tokens=0
01:49:59,489 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:59,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2635401720035588. input_tokens=540, output_tokens=0
01:50:05,611 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:05,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3732011940010125. input_tokens=887, output_tokens=0
01:50:11,11 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:11,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7648559910012409. input_tokens=1346, output_tokens=0
01:50:17,445 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:17,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1906623209943064. input_tokens=563, output_tokens=0
01:50:23,524 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:23,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.259455767009058. input_tokens=360, output_tokens=0
01:50:29,442 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:29,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1674285710032564. input_tokens=625, output_tokens=0
01:50:35,548 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:35,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2650512840045849. input_tokens=361, output_tokens=0
01:50:41,435 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:41,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1456424979987787. input_tokens=276, output_tokens=0
01:50:47,450 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:47,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1526761220011394. input_tokens=465, output_tokens=0
01:50:53,469 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:53,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.16233911299787. input_tokens=428, output_tokens=0
01:50:59,152 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:59,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8422220450011082. input_tokens=435, output_tokens=0
01:51:05,475 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:51:05,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1499524819955695. input_tokens=420, output_tokens=0
01:51:05,513 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:51:06,213 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:51:06,213 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:51:06,446 datashaper.workflow.workflow INFO executing verb create_final_nodes
01:51:20,254 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:51:21,531 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:51:21,531 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:51:21,742 datashaper.workflow.workflow INFO executing verb create_final_communities
01:51:23,358 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:51:23,544 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
01:51:23,544 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:51:23,764 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:51:24,611 datashaper.workflow.workflow INFO executing verb create_final_relationships
01:51:25,313 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:51:25,530 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_relationships', 'create_final_entities']
01:51:25,531 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:51:25,535 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:51:25,548 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:51:25,609 datashaper.workflow.workflow INFO executing verb create_final_text_units
01:51:25,643 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:51:25,837 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:51:25,837 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:51:25,852 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:51:26,70 datashaper.workflow.workflow INFO executing verb create_final_community_reports
01:51:26,355 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=3 => 1727
01:51:26,382 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 3659
01:51:26,650 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 4562
01:51:26,914 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 5732
01:51:32,970 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:32,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0118947559967637. input_tokens=2213, output_tokens=528
01:51:33,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:33,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1672731729922816. input_tokens=2115, output_tokens=461
01:51:33,886 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:33,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9257161319983425. input_tokens=2866, output_tokens=610
01:51:34,43 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:34,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.080182290999801. input_tokens=2241, output_tokens=632
01:51:36,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:36,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8860573200072395. input_tokens=2444, output_tokens=755
01:51:37,9 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:37,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.94482965899806. input_tokens=2345, output_tokens=567
01:51:37,42 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:37,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1379020040039904. input_tokens=3075, output_tokens=668
01:51:37,236 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:37,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.281760304002091. input_tokens=5611, output_tokens=1533
01:51:37,579 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:37,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.431116313993698. input_tokens=3019, output_tokens=776
01:51:39,888 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:39,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.278960317999008. input_tokens=2177, output_tokens=358
01:51:40,405 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:40,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8031925719988067. input_tokens=2591, output_tokens=470
01:51:40,983 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:40,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.375744129996747. input_tokens=2301, output_tokens=590
01:51:41,62 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:41,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.456928585001151. input_tokens=2462, output_tokens=624
01:51:41,163 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:41,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.556785282009514. input_tokens=2429, output_tokens=601
01:51:42,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:42,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8849038950138493. input_tokens=2398, output_tokens=557
01:51:42,819 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:42,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7528602160018636. input_tokens=2029, output_tokens=296
01:51:43,903 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:43,906 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4796115979988826. input_tokens=3214, output_tokens=724
01:51:44,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:44,151 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1408832390006864. input_tokens=2818, output_tokens=573
01:51:44,601 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:44,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4153157290129457. input_tokens=2913, output_tokens=687
01:51:45,60 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:45,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.239310042001307. input_tokens=2227, output_tokens=384
01:51:45,150 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:45,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.322619097001734. input_tokens=2226, output_tokens=399
01:51:46,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:46,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.101293109008111. input_tokens=2108, output_tokens=337
01:51:47,142 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:47,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2155309060035506. input_tokens=2693, output_tokens=585
01:51:48,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:48,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3998556089936756. input_tokens=2330, output_tokens=635
01:51:48,195 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:48,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.108162624994293. input_tokens=2217, output_tokens=495
01:51:48,314 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:48,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.160705042988411. input_tokens=2708, output_tokens=648
01:51:48,942 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:48,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6492762900015805. input_tokens=2749, output_tokens=470
01:51:49,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:49,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.753041518997634. input_tokens=2295, output_tokens=562
01:51:49,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:49,953 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:51,59 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:51,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0183724560047267. input_tokens=2492, output_tokens=568
01:51:51,95 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:51,96 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:51,585 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:51,586 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:51,769 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:51,769 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:51,926 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:51,927 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:52,894 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:52,895 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:53,141 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:53,141 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:53,684 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:53,685 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:53,765 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:53,766 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:53,773 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:53,773 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:54,582 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:54,582 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:55,348 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:55,349 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:56,19 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:56,20 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:56,579 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:56,580 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:58,255 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:58,256 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:51:59,636 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:51:59,637 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:00,580 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:00,581 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:01,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:01,795 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:02,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:02,873 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:04,79 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:04,80 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:07,989 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:07,990 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:09,283 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:09,284 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:10,647 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:10,648 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:10,934 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:10,935 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:12,648 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:12,649 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:18,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:18,16 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:19,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:19,294 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:20,676 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:20,677 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:20,960 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:20,961 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:22,673 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:22,674 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:28,43 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:28,44 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:29,323 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:29,324 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:30,704 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:30,705 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:30,988 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:30,988 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:32,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:32,700 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:38,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:38,315 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:39,333 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:39,334 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:40,731 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:40,732 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:41,14 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:41,15 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:42,727 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:42,728 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:48,339 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 429 Too Many Requests"
01:52:48,340 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
01:52:53,520 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:53,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 2.786264291004045. input_tokens=2090, output_tokens=386
01:52:53,989 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:53,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 4.6551770430087345. input_tokens=2789, output_tokens=833
01:52:54,727 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:54,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 3.7130010170076275. input_tokens=2389, output_tokens=562
01:52:56,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:56,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.089390604000073. input_tokens=2280, output_tokens=427
01:52:56,402 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:56,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8656034539890243. input_tokens=2362, output_tokens=540
01:52:56,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:56,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 8 retries took 3.6956419680063846. input_tokens=2743, output_tokens=562
01:52:57,288 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:57,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.544670146991848. input_tokens=2255, output_tokens=466
01:52:58,201 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:58,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0884953779896023. input_tokens=2101, output_tokens=376
01:52:58,614 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:58,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.180727531987941. input_tokens=2163, output_tokens=387
01:53:00,382 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:00,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.954535748998751. input_tokens=3602, output_tokens=888
01:53:00,848 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:00,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5375393650028855. input_tokens=2842, output_tokens=716
01:53:00,887 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:01,943 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:01,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 9 retries took 3.6024420960020507. input_tokens=2628, output_tokens=593
01:53:03,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:03,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.390572130010696. input_tokens=2734, output_tokens=638
01:53:05,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.66750985699764. input_tokens=2315, output_tokens=439
01:53:06,109 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:06,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9623790249897866. input_tokens=2123, output_tokens=588
01:53:06,717 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:06,719 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:53:10,938 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:10,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6244157909968635. input_tokens=3016, output_tokens=735
01:53:11,76 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:11,594 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:13,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.093077023004298. input_tokens=7485, output_tokens=1628
01:53:13,936 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:15,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.553839796993998. input_tokens=2867, output_tokens=480
01:53:16,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8638415399909718. input_tokens=2118, output_tokens=354
01:53:19,375 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7939516649930738. input_tokens=2061, output_tokens=332
01:53:19,398 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:22,484 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:22,724 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:24,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.844564377999632. input_tokens=4156, output_tokens=1123
01:53:24,989 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:25,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.319667588992161. input_tokens=3879, output_tokens=747
01:53:26,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1481542699912097. input_tokens=2100, output_tokens=388
01:53:27,125 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:29,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.205924727997626. input_tokens=3380, output_tokens=562
01:53:31,867 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:32,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.132938653012388. input_tokens=2743, output_tokens=720
01:53:33,957 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:35,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.058819896003115. input_tokens=2917, output_tokens=796
01:53:36,69 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:36,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.641869830011274. input_tokens=4623, output_tokens=730
01:53:36,734 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:38,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.73820565600181. input_tokens=2508, output_tokens=631
01:53:40,207 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:42,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8982360899972264. input_tokens=3169, output_tokens=552
01:53:43,804 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:44,112 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:44,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.933934571003192. input_tokens=2701, output_tokens=535
01:53:46,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.120311813996523. input_tokens=2860, output_tokens=684
01:53:47,255 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:48,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2207055859907996. input_tokens=2773, output_tokens=612
01:53:48,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:51,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9539936990040587. input_tokens=2591, output_tokens=775
01:53:53,338 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:54,108 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:54,111 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5761413059954066. input_tokens=2286, output_tokens=672
01:53:55,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0341027740068967. input_tokens=2936, output_tokens=594
01:53:57,116 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:57,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.011551773990504. input_tokens=3744, output_tokens=780
01:54:00,394 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:01,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.174907724009245. input_tokens=3147, output_tokens=698
01:54:02,662 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:03,544 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:03,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8801122720033163. input_tokens=3989, output_tokens=738
01:54:05,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:06,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.737661143008154. input_tokens=3180, output_tokens=803
01:54:07,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4108433819928905. input_tokens=2443, output_tokens=810
01:54:08,987 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:09,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.839900224003941. input_tokens=2612, output_tokens=631
01:54:10,906 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:10,908 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:54:10,909 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
01:54:13,78 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:13,112 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:13,114 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:54:13,114 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
01:54:13,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.0302973090001615. input_tokens=2569, output_tokens=713
01:54:15,198 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:15,200 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:54:15,200 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
01:54:15,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1024290710047353. input_tokens=2015, output_tokens=342
01:54:16,314 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:17,382 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:17,384 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:54:17,384 graphrag.llm.openai.utils ERROR not expected dict type. type=<class 'str'>:
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/utils.py", line 130, in try_parse_json_object
    result = json.loads(input)
             ^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/json/decoder.py", line 355, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
01:54:17,385 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/graph/extractors/community_reports/community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/local/python3.11-Anaconda3-2024.03-RSU-VI/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 90, in _invoke_json
    raise RuntimeError(error_msg)
RuntimeError: Failed to generate valid JSON output - Faulty JSON: {}
01:54:17,401 graphrag.callbacks.file_workflow_callbacks INFO Community Report Extraction Error details=None
01:54:17,401 graphrag.index.operations.summarize_communities.strategies WARNING No report found for community: 238
01:54:18,903 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:19,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.726263962991652. input_tokens=2054, output_tokens=263
01:54:19,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:20,516 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:20,518 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:54:20,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.340786405999097. input_tokens=7171, output_tokens=911
01:54:22,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9039775549899787. input_tokens=2098, output_tokens=341
01:54:25,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:25,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5533420510037104. input_tokens=3721, output_tokens=684
01:54:26,482 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.4852874950011028. input_tokens=2047, output_tokens=206
01:54:28,390 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:29,917 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:32,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.2553474939923035. input_tokens=3061, output_tokens=918
01:54:32,952 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:33,378 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:33,716 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.234100175992353. input_tokens=2084, output_tokens=367
01:54:34,314 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:36,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.061745911996695. input_tokens=2976, output_tokens=754
01:54:37,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2808115259977058. input_tokens=2440, output_tokens=605
01:54:38,20 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:39,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.007958410002175. input_tokens=2334, output_tokens=477
01:54:40,480 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:43,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.103789525004686. input_tokens=2590, output_tokens=595
01:54:44,325 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:44,368 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:44,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2177764480002224. input_tokens=2041, output_tokens=378
01:54:46,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9503182420012308. input_tokens=2044, output_tokens=310
01:54:49,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.38479732099222. input_tokens=2264, output_tokens=577
01:54:49,420 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:49,633 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:52,520 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:53,633 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:54,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.850693043001229. input_tokens=2610, output_tokens=737
01:54:55,219 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.653341659999569. input_tokens=2053, output_tokens=250
01:54:55,675 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:56,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.130256632997771. input_tokens=2088, output_tokens=359
01:54:57,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0342249120003544. input_tokens=2043, output_tokens=304
01:55:01,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.867343484002049. input_tokens=2498, output_tokens=518
01:55:01,301 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:03,958 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:04,944 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:06,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.476104016008321. input_tokens=2110, output_tokens=454
01:55:07,237 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:07,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5825992619938916. input_tokens=2545, output_tokens=652
01:55:07,869 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:08,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9244089749990962. input_tokens=2189, output_tokens=589
01:55:09,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.496395278009004. input_tokens=2575, output_tokens=435
01:55:13,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0053941380028846. input_tokens=2462, output_tokens=562
01:55:13,800 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:15,610 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:15,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.553297364996979. input_tokens=2905, output_tokens=675
01:55:16,672 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:19,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.950317309994716. input_tokens=2565, output_tokens=569
01:55:19,890 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:20,496 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:21,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.20466338400729. input_tokens=2094, output_tokens=372
01:55:22,906 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:24,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8685920880088815. input_tokens=2079, output_tokens=296
01:55:26,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.680590068004676. input_tokens=2258, output_tokens=688
01:55:27,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.474299801993766. input_tokens=2209, output_tokens=435
01:55:28,50 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:28,401 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:31,962 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:32,105 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:32,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7936605049908394. input_tokens=2347, output_tokens=505
01:55:33,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.561046434988384. input_tokens=3617, output_tokens=1015
01:55:34,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8841140170115978. input_tokens=2341, output_tokens=360
01:55:35,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:36,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.236662608003826. input_tokens=2256, output_tokens=596
01:55:40,841 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:40,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5377329120092327. input_tokens=2380, output_tokens=654
01:55:41,604 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:42,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.1892790189886. input_tokens=2401, output_tokens=658
01:55:42,579 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:45,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.091801183996722. input_tokens=2238, output_tokens=437
01:55:46,200 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:47,494 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:48,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.857383569004014. input_tokens=2736, output_tokens=493
01:55:50,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9532271689968184. input_tokens=2579, output_tokens=434
01:55:50,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:51,678 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0441763480048394. input_tokens=2414, output_tokens=599
01:55:51,723 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:55,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8747453350079013. input_tokens=2815, output_tokens=772
01:55:55,662 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:57,219 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:57,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.45703314099228. input_tokens=2122, output_tokens=424
01:55:59,253 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:00,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7836016609944636. input_tokens=2094, output_tokens=460
01:56:01,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1373660170065705. input_tokens=2610, output_tokens=573
01:56:02,232 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:03,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.75947978500335. input_tokens=2225, output_tokens=483
01:56:07,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.33160148600291. input_tokens=2786, output_tokens=606
01:56:07,811 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:09,630 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:09,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.719999577006092. input_tokens=3655, output_tokens=868
01:56:09,791 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:12,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.306142745001125. input_tokens=6434, output_tokens=1120
01:56:13,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:14,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.672165145006147. input_tokens=2749, output_tokens=558
01:56:15,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.72162397899956. input_tokens=2699, output_tokens=926
01:56:16,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:16,938 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:20,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.727555499994196. input_tokens=3640, output_tokens=1201
01:56:20,596 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:20,825 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:21,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.69167026199284. input_tokens=3779, output_tokens=746
01:56:24,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7381333490047837. input_tokens=2998, output_tokens=709
01:56:24,170 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:25,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7589526639931137. input_tokens=2081, output_tokens=489
01:56:25,545 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:28,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.895804746993235. input_tokens=3452, output_tokens=993
01:56:29,683 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:29,763 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:31,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6568996220012195. input_tokens=2126, output_tokens=491
01:56:32,223 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:33,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.180601243991987. input_tokens=3837, output_tokens=606
01:56:34,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0518560359923868. input_tokens=2006, output_tokens=325
01:56:34,965 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:37,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1001070409984095. input_tokens=2015, output_tokens=330
01:56:39,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.431143712994526. input_tokens=2091, output_tokens=337
01:56:40,181 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:41,275 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:44,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.036592424992705. input_tokens=3003, output_tokens=787
01:56:45,447 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:45,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.717951622995315. input_tokens=2062, output_tokens=396
01:56:46,785 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:46,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.8181867959938245. input_tokens=3578, output_tokens=1214
01:56:48,942 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:49,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.271887896989938. input_tokens=2534, output_tokens=630
01:56:52,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.557778361995588. input_tokens=3786, output_tokens=935
01:56:53,248 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:54,462 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:56,228 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:56,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.2578433500020765. input_tokens=3590, output_tokens=999
01:56:57,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.061610749995452. input_tokens=3834, output_tokens=720
01:56:57,885 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:58,655 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:58,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.625184536009328. input_tokens=4438, output_tokens=717
01:57:02,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.869927956999163. input_tokens=3431, output_tokens=776
01:57:03,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.432678565994138. input_tokens=3012, output_tokens=656
01:57:03,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:04,80 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:08,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.714796859989292. input_tokens=4240, output_tokens=662
01:57:08,767 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:08,821 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:09,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.834055654006079. input_tokens=2224, output_tokens=516
01:57:09,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:12,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9083226389921037. input_tokens=2491, output_tokens=737
01:57:12,683 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:13,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7534453270054655. input_tokens=2213, output_tokens=498
01:57:15,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.47816897300072. input_tokens=2090, output_tokens=330
01:57:18,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7944241679942934. input_tokens=2127, output_tokens=331
01:57:20,54 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:20,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:22,339 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:22,741 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:22,948 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.552101181005128. input_tokens=4125, output_tokens=1025
01:57:24,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6813593879924156. input_tokens=2287, output_tokens=585
01:57:24,343 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:25,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0168981089955196. input_tokens=2236, output_tokens=442
01:57:26,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.210301901999628. input_tokens=2244, output_tokens=403
01:57:30,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.604248361007194. input_tokens=2115, output_tokens=503
01:57:31,339 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:31,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5825503890082473. input_tokens=2419, output_tokens=709
01:57:32,243 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:34,733 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:37,273 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:37,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.523192778986413. input_tokens=3474, output_tokens=606
01:57:37,464 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:38,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2770205289998557. input_tokens=2344, output_tokens=603
01:57:39,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1902707280096365. input_tokens=2158, output_tokens=384
01:57:40,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:42,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5067490869987523. input_tokens=2286, output_tokens=501
01:57:43,186 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:43,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.303546909999568. input_tokens=2128, output_tokens=396
01:57:46,644 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:46,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.106593667995185. input_tokens=3046, output_tokens=666
01:57:50,383 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:50,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.376077880995581. input_tokens=2040, output_tokens=408
01:57:51,345 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:51,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2528915739967488. input_tokens=2036, output_tokens=314
01:57:51,771 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:54,478 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:55,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.746831209005904. input_tokens=2957, output_tokens=1115
01:57:57,183 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:57,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.555932158997166. input_tokens=2031, output_tokens=360
01:57:58,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.6946241289988393. input_tokens=2035, output_tokens=220
01:57:59,703 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:01,232 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1854494779981906. input_tokens=2804, output_tokens=612
01:58:02,857 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:04,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2937807940033963. input_tokens=2288, output_tokens=737
01:58:06,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:06,955 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:07,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8343385780026438. input_tokens=2491, output_tokens=434
01:58:09,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.312695700005861. input_tokens=2707, output_tokens=663
01:58:10,333 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:10,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.315142014005687. input_tokens=2711, output_tokens=555
01:58:11,739 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:14,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.282113528999616. input_tokens=2311, output_tokens=790
01:58:15,139 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:16,452 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:16,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.278254414006369. input_tokens=2294, output_tokens=626
01:58:19,68 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:19,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.068347476000781. input_tokens=2196, output_tokens=549
01:58:20,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1741247640020447. input_tokens=2288, output_tokens=587
01:58:21,471 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:22,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.379714676004369. input_tokens=2521, output_tokens=560
01:58:25,394 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:26,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3714901820058003. input_tokens=2780, output_tokens=600
01:58:26,804 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:28,406 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:28,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6791448770090938. input_tokens=2152, output_tokens=694
01:58:30,22 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:30,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2800977720035007. input_tokens=2077, output_tokens=352
01:58:32,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6800158429978183. input_tokens=2138, output_tokens=480
01:58:33,62 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:33,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0756794399931096. input_tokens=2321, output_tokens=427
01:58:37,32 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:37,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8352891310059931. input_tokens=2123, output_tokens=295
01:58:38,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1918146290117875. input_tokens=2106, output_tokens=353
01:58:39,494 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:40,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4450412050064187. input_tokens=2394, output_tokens=667
01:58:44,743 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:44,969 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:46,422 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:46,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9556695369974477. input_tokens=2595, output_tokens=734
01:58:47,659 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:48,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.976843775002635. input_tokens=3125, output_tokens=586
01:58:49,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.220703352999408. input_tokens=2636, output_tokens=524
01:58:49,593 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:51,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2500517730077263. input_tokens=2542, output_tokens=547
01:58:54,644 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:55,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9797620980098145. input_tokens=3850, output_tokens=652
01:58:57,253 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:57,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.216761087998748. input_tokens=2918, output_tokens=797
01:58:59,617 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:00,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2058074200031115. input_tokens=2188, output_tokens=498
01:59:00,102 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:00,103 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:59:01,689 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:02,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.159895618999144. input_tokens=3232, output_tokens=600
01:59:04,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.268324250006117. input_tokens=2925, output_tokens=435
01:59:04,993 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:06,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.820257927989587. input_tokens=3719, output_tokens=527
01:59:07,561 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:09,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7145430660020793. input_tokens=3339, output_tokens=681
01:59:10,82 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:11,766 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:12,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8725343480036827. input_tokens=4222, output_tokens=707
01:59:13,248 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:13,251 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3268491579947295. input_tokens=2166, output_tokens=415
01:59:15,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7803583850036375. input_tokens=2519, output_tokens=503
01:59:16,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2553713400120614. input_tokens=2854, output_tokens=627
01:59:16,933 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:20,279 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:22,278 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:22,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4813895920087816. input_tokens=2212, output_tokens=519
01:59:23,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:24,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2124049900012324. input_tokens=2062, output_tokens=385
01:59:25,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0025464720092714. input_tokens=2319, output_tokens=498
01:59:25,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:27,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5805626869987464. input_tokens=2338, output_tokens=455
01:59:28,639 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:31,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.184788103986648. input_tokens=2400, output_tokens=693
01:59:31,672 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:33,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1322342320054304. input_tokens=2051, output_tokens=391
01:59:34,513 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:34,515 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
01:59:35,972 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:35,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.44209366600262. input_tokens=2167, output_tokens=663
01:59:37,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7587038929923438. input_tokens=2173, output_tokens=494
01:59:37,578 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:39,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.397446394999861. input_tokens=3317, output_tokens=785
01:59:40,853 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:43,128 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:43,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.637690666000708. input_tokens=2394, output_tokens=526
01:59:44,550 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:45,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4730310019949684. input_tokens=2513, output_tokens=442
01:59:46,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.339176102002966. input_tokens=2471, output_tokens=459
01:59:47,684 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:49,228 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5526873339986196. input_tokens=2435, output_tokens=415
01:59:50,940 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:52,396 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:52,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.277431497001089. input_tokens=3171, output_tokens=559
01:59:54,957 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:55,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.921373003991903. input_tokens=2365, output_tokens=478
01:59:56,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.9685223720007343. input_tokens=2041, output_tokens=305
01:59:58,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3212729989900254. input_tokens=2386, output_tokens=552
02:00:02,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:02,103 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:00:03,460 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:03,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.059809996993863. input_tokens=9830, output_tokens=1799
02:00:04,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.801941199999419. input_tokens=6542, output_tokens=1084
02:00:05,21 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:06,297 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:07,971 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:08,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.744980432005832. input_tokens=2352, output_tokens=520
02:00:09,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.813803020006162. input_tokens=5322, output_tokens=673
02:00:09,747 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:10,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.903441768998164. input_tokens=3734, output_tokens=1530
02:00:12,351 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:12,353 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:00:14,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.648531400001957. input_tokens=2269, output_tokens=671
02:00:15,948 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:16,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.048714393997216. input_tokens=3582, output_tokens=1106
02:00:18,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:19,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8239889499964193. input_tokens=2473, output_tokens=687
02:00:21,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.540547874988988. input_tokens=4485, output_tokens=1082
02:00:21,799 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:23,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:23,980 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:00:23,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.24171617301181. input_tokens=7102, output_tokens=1872
02:00:26,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6404794289992424. input_tokens=3374, output_tokens=803
02:00:27,86 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:28,168 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:30,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.106901199003914. input_tokens=5646, output_tokens=776
02:00:31,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.600487881005392. input_tokens=7224, output_tokens=1559
02:00:31,472 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:31,725 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:33,583 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:33,585 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:00:33,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 8.399377093999647. input_tokens=5000, output_tokens=1522
02:00:34,670 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:36,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8714532279991545. input_tokens=2580, output_tokens=712
02:00:37,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9172193109989166. input_tokens=2063, output_tokens=602
02:00:38,250 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:39,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2459469550085487. input_tokens=2055, output_tokens=415
02:00:42,166 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:43,129 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:43,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.460171876999084. input_tokens=3597, output_tokens=603
02:00:45,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7626864060002845. input_tokens=2509, output_tokens=746
02:00:45,955 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:46,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.315634489990771. input_tokens=2198, output_tokens=401
02:00:47,586 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:50,356 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:50,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.934192714004894. input_tokens=2455, output_tokens=619
02:00:52,393 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:52,762 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.313043297996046. input_tokens=2296, output_tokens=368
02:00:53,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.156726636996609. input_tokens=2266, output_tokens=595
02:00:55,965 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:56,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.142394025999238. input_tokens=2690, output_tokens=603
02:00:59,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.309968092988129. input_tokens=3139, output_tokens=840
02:01:00,11 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:01,270 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:01,959 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:02,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.844907860999228. input_tokens=2793, output_tokens=800
02:01:04,15 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:04,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6987014550104504. input_tokens=2796, output_tokens=673
02:01:06,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1810168540105224. input_tokens=2114, output_tokens=428
02:01:07,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:08,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.824286557995947. input_tokens=3078, output_tokens=653
02:01:12,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.0093205380107975. input_tokens=2688, output_tokens=796
02:01:12,793 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:14,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:15,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.578581460998976. input_tokens=5250, output_tokens=1001
02:01:16,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.564950822998071. input_tokens=3279, output_tokens=876
02:01:17,312 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:17,576 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:17,622 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:20,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.0646437800023705. input_tokens=4469, output_tokens=729
02:01:21,539 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:21,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.740327188002993. input_tokens=4443, output_tokens=1143
02:01:22,541 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:22,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1666828130109934. input_tokens=2452, output_tokens=573
02:01:25,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4716510430007474. input_tokens=3048, output_tokens=686
02:01:27,264 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:27,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2650331210024888. input_tokens=2510, output_tokens=608
02:01:28,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:31,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1734452550008427. input_tokens=3664, output_tokens=516
02:01:32,293 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:33,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:33,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0727478490007343. input_tokens=2472, output_tokens=371
02:01:35,258 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:36,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3803147750004428. input_tokens=2194, output_tokens=697
02:01:37,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0045108400081517. input_tokens=2600, output_tokens=568
02:01:38,188 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:39,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7246793220110703. input_tokens=2938, output_tokens=489
02:01:41,41 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:43,192 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:43,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2256621650012676. input_tokens=2020, output_tokens=382
02:01:44,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.246113928005798. input_tokens=2352, output_tokens=608
02:01:46,108 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:46,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.484328803999233. input_tokens=2423, output_tokens=452
02:01:50,430 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9342165190028027. input_tokens=3281, output_tokens=691
02:01:50,773 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:51,44 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:51,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:54,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.172972642001696. input_tokens=3382, output_tokens=923
02:01:54,489 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:55,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0316854220000096. input_tokens=2615, output_tokens=574
02:01:56,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.04130163800437. input_tokens=2051, output_tokens=326
02:01:56,607 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:58,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.857985879993066. input_tokens=2380, output_tokens=556
02:02:00,810 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:02,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7680299429921433. input_tokens=3092, output_tokens=685
02:02:03,807 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:04,25 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:04,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.156093243000214. input_tokens=2433, output_tokens=566
02:02:07,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7464641270053107. input_tokens=2943, output_tokens=578
02:02:07,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:08,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.758598145999713. input_tokens=2940, output_tokens=427
02:02:12,83 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:12,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.993613158992957. input_tokens=4224, output_tokens=1325
02:02:13,150 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:13,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7339446419937303. input_tokens=3104, output_tokens=577
02:02:13,330 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:16,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2355503430007957. input_tokens=2292, output_tokens=387
02:02:18,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:19,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.626586655009305. input_tokens=3390, output_tokens=726
02:02:19,483 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:21,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.103807863997645. input_tokens=3151, output_tokens=792
02:02:22,101 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:22,103 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:02:24,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7839520669949707. input_tokens=3905, output_tokens=705
02:02:24,526 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:25,710 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:26,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9959317519969773. input_tokens=3688, output_tokens=678
02:02:28,644 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:28,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.009149637000519. input_tokens=2425, output_tokens=828
02:02:30,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7816329509951174. input_tokens=2158, output_tokens=494
02:02:31,420 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:32,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3040973960014526. input_tokens=3889, output_tokens=618
02:02:34,384 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:36,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6659808330005035. input_tokens=5428, output_tokens=668
02:02:37,133 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:37,263 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:37,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4869812799879583. input_tokens=2849, output_tokens=678
02:02:39,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0193290369934402. input_tokens=2687, output_tokens=582
02:02:41,110 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:42,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.1498286699934397. input_tokens=2080, output_tokens=379
02:02:43,397 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:45,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.642328842994175. input_tokens=2239, output_tokens=488
02:02:46,230 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:47,250 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:48,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.519765263001318. input_tokens=3494, output_tokens=511
02:02:49,832 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:50,527 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9428801589965587. input_tokens=2391, output_tokens=575
02:02:51,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.754721816003439. input_tokens=2462, output_tokens=525
02:02:54,17 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:54,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.698211626993725. input_tokens=5860, output_tokens=938
02:02:55,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.9258277339977212. input_tokens=2584, output_tokens=524
02:02:59,88 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:01,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.156299604001106. input_tokens=5170, output_tokens=1158
02:03:03,214 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:03,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:03,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.8934859020082513. input_tokens=4315, output_tokens=787
02:03:04,106 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:04,107 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:03:04,580 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:05,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.579575683994335. input_tokens=5636, output_tokens=1065
02:03:05,817 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:07,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.683416631. input_tokens=7994, output_tokens=662
02:03:08,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.737253190003685. input_tokens=5318, output_tokens=1006
02:03:10,552 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:11,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.325645043005352. input_tokens=2879, output_tokens=611
02:03:14,583 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:14,587 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.612216984009137. input_tokens=2911, output_tokens=1001
02:03:15,342 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:15,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.194896974993753. input_tokens=2876, output_tokens=823
02:03:17,306 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:19,214 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:19,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.2249073120037792. input_tokens=2109, output_tokens=403
02:03:20,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.958957683003973. input_tokens=2189, output_tokens=505
02:03:21,261 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:22,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7158330520032905. input_tokens=2411, output_tokens=591
02:03:24,488 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:26,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0626453709992347. input_tokens=2278, output_tokens=585
02:03:28,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:28,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.559221906994935. input_tokens=4879, output_tokens=613
02:03:29,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:30,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.867635135000455. input_tokens=2594, output_tokens=508
02:03:31,56 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:33,517 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:33,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.096993840998039. input_tokens=8633, output_tokens=1197
02:03:36,28 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.408376478997525. input_tokens=2239, output_tokens=561
02:03:36,637 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:36,664 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:37,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3132943939999677. input_tokens=2132, output_tokens=382
02:03:40,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.8205120700004045. input_tokens=2068, output_tokens=318
02:03:41,335 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:42,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.257535235999967. input_tokens=3791, output_tokens=790
02:03:45,660 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:45,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.406660632987041. input_tokens=2088, output_tokens=442
02:03:46,292 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:46,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.903990168008022. input_tokens=2109, output_tokens=529
02:03:47,454 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:50,486 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.651073303000885. input_tokens=3125, output_tokens=1326
02:03:51,943 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:52,323 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:52,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.994041780999396. input_tokens=2236, output_tokens=417
02:03:54,801 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:55,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.874203789004241. input_tokens=2664, output_tokens=717
02:03:56,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0469578559859656. input_tokens=3019, output_tokens=572
02:03:58,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1145626380020985. input_tokens=3372, output_tokens=565
02:04:00,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:01,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:03,740 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:03,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.617653096007416. input_tokens=2046, output_tokens=535
02:04:04,610 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:04,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.267728728009388. input_tokens=2910, output_tokens=631
02:04:05,38 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:06,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.288039587001549. input_tokens=4999, output_tokens=1468
02:04:08,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.278562237988808. input_tokens=3211, output_tokens=686
02:04:10,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4991058360028546. input_tokens=2199, output_tokens=430
02:04:11,148 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:12,922 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:13,926 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:15,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.798866267010453. input_tokens=2310, output_tokens=687
02:04:17,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1585622369893827. input_tokens=2372, output_tokens=490
02:04:17,144 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:18,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7528073229914298. input_tokens=2122, output_tokens=267
02:04:18,418 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:21,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.76252976100659. input_tokens=3857, output_tokens=697
02:04:23,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.829955830005929. input_tokens=3389, output_tokens=717
02:04:23,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:24,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7702697409986285. input_tokens=3703, output_tokens=760
02:04:26,799 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:26,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.185112391001894. input_tokens=6786, output_tokens=1664
02:04:30,991 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:33,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.979980253003305. input_tokens=7488, output_tokens=649
02:04:34,22 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:34,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.392956541007152. input_tokens=4383, output_tokens=572
02:04:34,253 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:36,501 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:36,832 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:37,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.03826645600202. input_tokens=5110, output_tokens=1217
02:04:38,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.662268671003403. input_tokens=4307, output_tokens=908
02:04:39,311 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:39,626 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:39,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1912125969975023. input_tokens=2275, output_tokens=581
02:04:40,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.410399434011197. input_tokens=6418, output_tokens=773
02:04:44,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.084664518988575. input_tokens=2484, output_tokens=812
02:04:46,639 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:49,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3973056739923777. input_tokens=2408, output_tokens=645
02:04:49,391 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:49,393 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:04:49,760 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:49,981 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:51,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.366759994998574. input_tokens=5764, output_tokens=1802
02:04:52,270 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:52,716 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:52,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.106229023003834. input_tokens=4646, output_tokens=650
02:04:54,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.11999748199014. input_tokens=4978, output_tokens=543
02:04:56,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.199826259995461. input_tokens=5561, output_tokens=616
02:04:57,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.21256917998835. input_tokens=2203, output_tokens=404
02:05:00,712 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:03,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:03,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.393226093001431. input_tokens=4318, output_tokens=1026
02:05:04,415 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:04,830 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:04,834 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.687088867009152. input_tokens=4998, output_tokens=905
02:05:06,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.441748428987921. input_tokens=3096, output_tokens=672
02:05:07,87 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:08,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0624288620019797. input_tokens=2367, output_tokens=565
02:05:10,517 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:12,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.52699176700844. input_tokens=4512, output_tokens=734
02:05:14,476 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.2804204679996474. input_tokens=5124, output_tokens=548
02:05:15,573 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:15,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.717828016990097. input_tokens=9345, output_tokens=791
02:05:16,233 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:17,834 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:20,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.585458393004956. input_tokens=7693, output_tokens=1268
02:05:20,485 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:21,138 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:21,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.569100893000723. input_tokens=5145, output_tokens=793
02:05:24,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.707206425999175. input_tokens=5625, output_tokens=692
02:05:25,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1540322830114746. input_tokens=2445, output_tokens=559
02:05:25,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:28,312 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:28,314 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:05:30,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.895766668007127. input_tokens=4252, output_tokens=510
02:05:31,75 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:31,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 9.129085616004886. input_tokens=9546, output_tokens=237
02:05:32,316 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:33,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.656389773008414. input_tokens=9538, output_tokens=1266
02:05:35,12 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:35,13 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:05:36,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4811135589989135. input_tokens=9851, output_tokens=786
02:05:37,178 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:37,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.730420179999783. input_tokens=9705, output_tokens=1169
02:05:38,947 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:39,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.396068037007353. input_tokens=9871, output_tokens=1658
02:05:40,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.088459119011532. input_tokens=9441, output_tokens=998
02:05:45,213 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:45,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.829054805988562. input_tokens=9621, output_tokens=1637
02:05:45,250 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
02:05:45,477 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
02:05:45,477 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
02:05:45,489 datashaper.workflow.workflow INFO executing verb create_final_documents
02:05:45,495 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
02:05:45,518 graphrag.index.cli INFO All workflows completed successfully.
