23:05:43,801 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/indexing-engine.log
23:05:43,803 graphrag.index.cli INFO Starting pipeline run for: 20240926-230543, dryrun=False
23:05:43,803 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 150000,
        "requests_per_minute": 2,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 150000,
            "requests_per_minute": 2,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 150000,
            "requests_per_minute": 2,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 150000,
            "requests_per_minute": 2,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 150000,
            "requests_per_minute": 2,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 150000,
            "requests_per_minute": 2,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
23:05:43,805 graphrag.index.create_pipeline_config INFO skipping workflows 
23:05:43,810 graphrag.index.run.run INFO Running pipeline
23:05:43,810 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output
23:05:43,811 graphrag.index.input.load_input INFO loading input from root_dir=input
23:05:43,811 graphrag.index.input.load_input INFO using file storage for input
23:05:43,812 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/input for files matching .*\.txt$
23:05:43,813 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
23:05:43,815 graphrag.index.input.text INFO Found 1 files, loading 1
23:05:43,816 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
23:05:43,816 graphrag.index.run.run INFO Final # of rows loaded: 1
23:05:43,927 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
23:05:43,930 datashaper.workflow.workflow INFO executing verb orderby
23:05:43,931 datashaper.workflow.workflow INFO executing verb zip
23:05:43,933 datashaper.workflow.workflow INFO executing verb aggregate_override
23:05:43,936 datashaper.workflow.workflow INFO executing verb chunk
23:05:44,45 datashaper.workflow.workflow INFO executing verb select
23:05:44,47 datashaper.workflow.workflow INFO executing verb unroll
23:05:44,50 datashaper.workflow.workflow INFO executing verb rename
23:05:44,52 datashaper.workflow.workflow INFO executing verb genid
23:05:44,56 datashaper.workflow.workflow INFO executing verb unzip
23:05:44,58 datashaper.workflow.workflow INFO executing verb copy
23:05:44,64 datashaper.workflow.workflow INFO executing verb filter
23:05:44,72 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
23:05:44,204 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
23:05:44,205 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
23:05:44,215 datashaper.workflow.workflow INFO executing verb entity_extract
23:05:44,218 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
23:05:44,256 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=150000, RPM=2
23:05:44,256 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
23:05:52,410 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:05:52,416 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.158554636873305. input_tokens=34, output_tokens=2191
23:06:25,167 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:06:55,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7036174703389406. input_tokens=2936, output_tokens=409
23:07:26,969 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:07:56,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7489677146077156. input_tokens=34, output_tokens=195
23:08:30,565 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:09:00,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.531959834508598. input_tokens=2935, output_tokens=605
23:09:33,950 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:10:03,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.3362542539834976. input_tokens=34, output_tokens=879
23:10:35,100 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:11:05,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.0730137852951884. input_tokens=2936, output_tokens=138
23:11:40,377 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:12:10,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.2325553223490715. input_tokens=34, output_tokens=1274
23:12:54,941 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:12:54,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.495626359246671. input_tokens=34, output_tokens=4193
23:13:26,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:13:56,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.6141526363790035. input_tokens=2936, output_tokens=146
23:14:28,425 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:14:58,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7682781079784036. input_tokens=34, output_tokens=256
23:15:30,765 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:16:00,782 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.240635302849114. input_tokens=2935, output_tokens=369
23:16:33,441 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:17:03,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.621674206107855. input_tokens=34, output_tokens=543
23:17:35,307 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:18:05,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.792200573720038. input_tokens=34, output_tokens=234
23:18:37,623 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:19:07,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.2440553922206163. input_tokens=2936, output_tokens=325
23:19:44,32 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:20:14,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 6.395757184363902. input_tokens=34, output_tokens=1894
23:20:46,836 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:20:46,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.7541094897314906. input_tokens=2936, output_tokens=424
23:21:19,163 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:21:49,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2932469993829727. input_tokens=34, output_tokens=369
23:22:20,712 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:22:50,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.4970218166708946. input_tokens=2936, output_tokens=117
23:23:22,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:23:52,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.793036692775786. input_tokens=34, output_tokens=225
23:24:24,946 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:24:54,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.315025085583329. input_tokens=34, output_tokens=461
23:25:27,217 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:25:57,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2250355733558536. input_tokens=34, output_tokens=413
23:26:28,626 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:26:58,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3508037850260735. input_tokens=34, output_tokens=69
23:27:32,663 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:28:02,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.9815507354214787. input_tokens=34, output_tokens=875
23:28:35,319 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:29:05,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6059530340135098. input_tokens=2936, output_tokens=456
23:29:37,864 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:07,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.5292537547647953. input_tokens=34, output_tokens=508
23:30:45,271 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:30:45,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.346336931921542. input_tokens=2935, output_tokens=1935
23:31:30,885 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:32:00,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.573738203383982. input_tokens=34, output_tokens=4442
23:32:33,651 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:33:03,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6974584562703967. input_tokens=2936, output_tokens=437
23:33:38,247 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:34:08,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.56580635625869. input_tokens=34, output_tokens=433
23:34:41,157 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:35:11,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.8455857122316957. input_tokens=2935, output_tokens=533
23:35:42,529 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:36:12,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.3023151941597462. input_tokens=34, output_tokens=76
23:36:44,943 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:36:44,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.32893871050328. input_tokens=2936, output_tokens=344
23:37:17,224 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:37:47,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.2491463692858815. input_tokens=34, output_tokens=338
23:38:19,600 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:38:49,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.3279291950166225. input_tokens=2934, output_tokens=332
23:39:21,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:39:51,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.669789814390242. input_tokens=34, output_tokens=157
23:40:23,146 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:40:53,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.7755327755585313. input_tokens=2934, output_tokens=281
23:41:24,786 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:41:54,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5695711513981223. input_tokens=34, output_tokens=145
23:42:27,503 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:42:57,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.646523827686906. input_tokens=2936, output_tokens=434
23:43:30,255 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:44:00,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.682147031649947. input_tokens=34, output_tokens=552
23:44:32,135 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:45:02,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.8233655206859112. input_tokens=2936, output_tokens=193
23:45:33,475 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:46:03,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.2709686672315001. input_tokens=34, output_tokens=197
23:46:36,424 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:47:06,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.872846834361553. input_tokens=2935, output_tokens=494
23:47:40,692 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:48:10,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 4.201566951349378. input_tokens=34, output_tokens=1179
23:48:43,363 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:49:13,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.6136055225506425. input_tokens=2935, output_tokens=443
23:49:46,39 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:49:46,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.6066239587962627. input_tokens=34, output_tokens=539
23:50:18,755 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:50:48,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.661214955151081. input_tokens=2937, output_tokens=432
23:51:20,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:51:50,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.7468367191031575. input_tokens=34, output_tokens=248
23:52:23,72 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:52:53,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.4226541984826326. input_tokens=2936, output_tokens=388
23:53:24,729 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:53:54,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.5886404365301132. input_tokens=34, output_tokens=166
23:54:27,298 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:54:57,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.525399482809007. input_tokens=2936, output_tokens=391
23:55:30,865 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:56:00,898 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.5152334682643414. input_tokens=34, output_tokens=792
23:56:33,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:57:03,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.498813359066844. input_tokens=2936, output_tokens=376
23:57:35,901 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:58:05,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.4370329873636365. input_tokens=34, output_tokens=406
23:58:37,872 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
23:59:07,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.912153448909521. input_tokens=2936, output_tokens=216
23:59:39,388 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:00:09,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.4464633921161294. input_tokens=34, output_tokens=241
00:00:41,640 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:01:11,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 2.169293290004134. input_tokens=2937, output_tokens=294
00:01:44,163 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:02:14,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 2.4561489084735513. input_tokens=34, output_tokens=444
00:02:45,595 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:02:45,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 1.3594484608620405. input_tokens=2935, output_tokens=193
00:03:17,455 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:03:47,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 1.8446071138605475. input_tokens=34, output_tokens=266
00:04:20,861 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:04:50,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 3.3298503132537007. input_tokens=2790, output_tokens=718
00:05:24,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:05:54,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 3.4943295177072287. input_tokens=34, output_tokens=877
00:05:54,476 datashaper.workflow.workflow INFO executing verb merge_graphs
00:05:54,499 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
00:05:54,618 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
00:05:54,618 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
00:05:54,629 datashaper.workflow.workflow INFO executing verb summarize_descriptions
00:06:25,813 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:06:55,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1382620660588145. input_tokens=200, output_tokens=49
00:07:27,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:07:57,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2849732236936688. input_tokens=347, output_tokens=92
00:08:28,411 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:08:58,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1595752332359552. input_tokens=175, output_tokens=33
00:09:29,882 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:09:59,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3871049797162414. input_tokens=238, output_tokens=97
00:10:31,215 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:11:01,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.254461832344532. input_tokens=247, output_tokens=73
00:11:32,442 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:12:02,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.151360997930169. input_tokens=199, output_tokens=50
00:12:33,551 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:13:03,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0493898345157504. input_tokens=156, output_tokens=26
00:13:34,611 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:14:04,644 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0053634885698557. input_tokens=150, output_tokens=16
00:14:35,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:15:05,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0142805613577366. input_tokens=142, output_tokens=20
00:15:36,262 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:16:06,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5050852568820119. input_tokens=152, output_tokens=30
00:16:37,410 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:17:07,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0907881194725633. input_tokens=216, output_tokens=40
00:17:38,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:18:08,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0286403261125088. input_tokens=158, output_tokens=19
00:18:39,315 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:18:39,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.759859268553555. input_tokens=171, output_tokens=0
00:19:10,319 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:19:40,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9654332092031837. input_tokens=137, output_tokens=9
00:20:11,389 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:20:41,422 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9955686600878835. input_tokens=179, output_tokens=24
00:21:12,537 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:21:42,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.078979049809277. input_tokens=174, output_tokens=45
00:22:13,588 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:22:43,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9815758150070906. input_tokens=145, output_tokens=21
00:23:14,871 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:23:14,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2073419662192464. input_tokens=213, output_tokens=72
00:23:45,961 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:24:15,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0364876464009285. input_tokens=148, output_tokens=19
00:24:47,26 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:25:17,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9777093483135104. input_tokens=146, output_tokens=17
00:25:48,158 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:26:18,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.052302123978734. input_tokens=206, output_tokens=33
00:26:49,977 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:27:20,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.7334010312333703. input_tokens=871, output_tokens=190
00:27:51,380 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:28:21,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3466246724128723. input_tokens=288, output_tokens=88
00:28:52,510 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:29:22,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0600916584953666. input_tokens=168, output_tokens=28
00:29:53,978 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:30:24,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.411230687983334. input_tokens=283, output_tokens=133
00:30:55,269 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:31:25,301 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2058887872844934. input_tokens=238, output_tokens=82
00:31:56,597 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:32:26,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2521096682175994. input_tokens=228, output_tokens=85
00:32:57,341 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:33:27,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6728474665433168. input_tokens=152, output_tokens=28
00:33:58,113 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:34:28,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6814831998199224. input_tokens=340, output_tokens=70
00:34:59,193 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:35:29,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0037306360900402. input_tokens=148, output_tokens=12
00:36:00,393 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:36:30,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1291326312348247. input_tokens=150, output_tokens=32
00:37:01,512 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:37:31,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0378639688715339. input_tokens=156, output_tokens=34
00:38:02,719 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:38:32,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1491205440834165. input_tokens=171, output_tokens=39
00:39:03,824 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:39:33,844 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0248010568320751. input_tokens=146, output_tokens=22
00:40:05,94 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:40:35,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2240335512906313. input_tokens=168, output_tokens=57
00:41:06,109 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:41:36,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.983795722015202. input_tokens=147, output_tokens=20
00:42:07,498 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:42:37,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.318936969153583. input_tokens=245, output_tokens=86
00:43:08,797 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:43:38,804 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2445968380197883. input_tokens=191, output_tokens=66
00:44:09,895 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:44:39,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0625989958643913. input_tokens=144, output_tokens=25
00:45:11,52 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:45:41,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0983678195625544. input_tokens=141, output_tokens=20
00:46:12,304 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:46:42,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1845213696360588. input_tokens=143, output_tokens=18
00:47:13,418 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:47:43,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0324351517483592. input_tokens=137, output_tokens=14
00:48:14,526 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:48:14,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0226199189200997. input_tokens=145, output_tokens=24
00:48:45,736 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:49:15,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1573012052103877. input_tokens=149, output_tokens=28
00:49:46,888 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:50:16,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0864513935521245. input_tokens=180, output_tokens=29
00:50:48,21 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:51:18,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0383096234872937. input_tokens=144, output_tokens=21
00:51:49,145 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:52:19,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0491949021816254. input_tokens=147, output_tokens=23
00:52:50,269 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:53:20,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0316015826538205. input_tokens=153, output_tokens=18
00:53:51,488 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:54:21,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1447418117895722. input_tokens=230, output_tokens=50
00:54:52,689 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:55:22,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1312483046203852. input_tokens=167, output_tokens=33
00:55:53,788 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:56:23,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0471687288954854. input_tokens=152, output_tokens=22
00:56:54,969 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:57:24,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0936218341812491. input_tokens=149, output_tokens=25
00:57:55,680 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:58:25,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6409643711522222. input_tokens=184, output_tokens=54
00:58:57,387 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
00:59:27,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.6343215825036168. input_tokens=137, output_tokens=13
00:59:58,481 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:00:28,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.039558632299304. input_tokens=133, output_tokens=12
01:00:59,113 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:01:29,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5479857688769698. input_tokens=151, output_tokens=30
01:02:00,463 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:02:30,474 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2881777742877603. input_tokens=228, output_tokens=82
01:03:01,609 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:03:31,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0881414264440536. input_tokens=159, output_tokens=24
01:04:02,931 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:04:32,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.260571088641882. input_tokens=206, output_tokens=66
01:05:04,297 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:05:34,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.272950162179768. input_tokens=250, output_tokens=89
01:06:05,416 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:06:35,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0332840783521533. input_tokens=150, output_tokens=11
01:07:06,456 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:07:36,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9673305042088032. input_tokens=159, output_tokens=14
01:08:07,553 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:08:37,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0188712226226926. input_tokens=163, output_tokens=23
01:09:08,731 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:09:38,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0939780408516526. input_tokens=161, output_tokens=21
01:10:09,855 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:10:39,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0654235249385238. input_tokens=151, output_tokens=23
01:11:11,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:11:41,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1888563698157668. input_tokens=151, output_tokens=25
01:12:12,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:12:42,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0516165643930435. input_tokens=185, output_tokens=28
01:13:13,401 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:13:43,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0581967579200864. input_tokens=189, output_tokens=30
01:14:14,45 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:14:44,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6164375841617584. input_tokens=165, output_tokens=16
01:15:15,138 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:15:15,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0185753805562854. input_tokens=154, output_tokens=24
01:15:46,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:16:16,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0702051557600498. input_tokens=183, output_tokens=26
01:16:47,252 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:17:17,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.9700930202379823. input_tokens=144, output_tokens=10
01:17:47,972 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:18:17,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6554284570738673. input_tokens=207, output_tokens=60
01:18:49,371 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:19:19,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3394520813599229. input_tokens=234, output_tokens=97
01:19:50,695 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:20:20,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2875745296478271. input_tokens=234, output_tokens=88
01:20:51,877 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:21:21,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.1110486648976803. input_tokens=182, output_tokens=44
01:21:53,539 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:22:23,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5849856473505497. input_tokens=220, output_tokens=109
01:22:54,636 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:23:24,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0484559694305062. input_tokens=154, output_tokens=26
01:23:55,919 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:24:25,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2061290219426155. input_tokens=321, output_tokens=78
01:24:57,36 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:25:27,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0524563966318965. input_tokens=153, output_tokens=17
01:25:58,136 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:26:28,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.020241977646947. input_tokens=150, output_tokens=22
01:26:59,407 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:27:29,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2138835927471519. input_tokens=160, output_tokens=43
01:28:00,700 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:28:30,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2319277850911021. input_tokens=218, output_tokens=67
01:29:02,44 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:29:32,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.2672837683930993. input_tokens=205, output_tokens=84
01:30:03,263 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:30:33,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.144292495213449. input_tokens=171, output_tokens=39
01:31:04,426 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:31:34,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.085755756124854. input_tokens=161, output_tokens=29
01:32:05,529 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:32:35,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.019997757859528. input_tokens=146, output_tokens=16
01:33:06,139 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:33:36,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5269556967541575. input_tokens=170, output_tokens=30
01:34:07,177 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:34:37,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.0006009303033352. input_tokens=159, output_tokens=19
01:35:08,338 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:35:38,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.08957530092448. input_tokens=162, output_tokens=30
01:35:38,517 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
01:35:38,633 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
01:35:38,633 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
01:35:38,644 datashaper.workflow.workflow INFO executing verb cluster_graph
01:35:38,795 datashaper.workflow.workflow INFO executing verb select
01:35:38,799 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
01:35:38,928 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
01:35:38,928 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:35:38,942 datashaper.workflow.workflow INFO executing verb unpack_graph
01:35:38,984 datashaper.workflow.workflow INFO executing verb rename
01:35:38,988 datashaper.workflow.workflow INFO executing verb select
01:35:38,992 datashaper.workflow.workflow INFO executing verb dedupe
01:35:38,997 datashaper.workflow.workflow INFO executing verb rename
01:35:39,1 datashaper.workflow.workflow INFO executing verb filter
01:35:39,13 datashaper.workflow.workflow INFO executing verb text_split
01:35:39,20 datashaper.workflow.workflow INFO executing verb drop
01:35:39,25 datashaper.workflow.workflow INFO executing verb merge
01:35:39,66 datashaper.workflow.workflow INFO executing verb text_embed
01:35:39,71 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
01:35:39,108 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=150000, RPM=2
01:35:39,108 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
01:35:39,118 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 482 inputs via 482 snippets using 31 batches. max_batch_size=16, max_tokens=8191
01:35:40,551 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:40,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4563495432958007. input_tokens=607, output_tokens=0
01:35:41,719 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:35:41,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1540206344798207. input_tokens=446, output_tokens=0
01:36:12,921 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:12,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.162048782221973. input_tokens=359, output_tokens=0
01:36:44,265 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:36:44,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2946768775582314. input_tokens=305, output_tokens=0
01:37:15,65 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:15,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7797942413017154. input_tokens=936, output_tokens=0
01:37:46,267 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:37:46,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1716550020501018. input_tokens=385, output_tokens=0
01:38:17,583 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:17,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.26843158621341. input_tokens=434, output_tokens=0
01:38:48,897 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:38:48,921 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.270998254418373. input_tokens=337, output_tokens=0
01:39:20,91 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:20,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.151995262131095. input_tokens=234, output_tokens=0
01:39:51,476 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:39:51,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.367327181622386. input_tokens=279, output_tokens=0
01:40:22,668 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:22,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1702897176146507. input_tokens=256, output_tokens=0
01:40:53,878 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:40:53,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1620784336701035. input_tokens=326, output_tokens=0
01:41:25,162 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:25,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2668338511139154. input_tokens=291, output_tokens=0
01:41:56,446 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:41:56,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2460362911224365. input_tokens=264, output_tokens=0
01:42:27,737 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:27,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2608782639726996. input_tokens=235, output_tokens=0
01:42:58,923 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:42:58,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1505886893719435. input_tokens=237, output_tokens=0
01:43:30,104 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:43:30,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1591720134019852. input_tokens=244, output_tokens=0
01:44:01,283 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:01,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1643525706604123. input_tokens=223, output_tokens=0
01:44:32,561 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:44:32,584 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2574750576168299. input_tokens=359, output_tokens=0
01:45:03,884 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:03,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.279512601904571. input_tokens=247, output_tokens=0
01:45:35,93 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:45:35,117 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1697278022766113. input_tokens=233, output_tokens=0
01:46:06,276 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:06,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1621328731998801. input_tokens=271, output_tokens=0
01:46:37,467 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:46:37,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1651200279593468. input_tokens=276, output_tokens=0
01:47:08,642 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:08,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1632133685052395. input_tokens=264, output_tokens=0
01:47:39,857 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:39,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1891639288514853. input_tokens=268, output_tokens=0
01:47:41,119 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:47:41,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2483581248670816. input_tokens=288, output_tokens=0
01:48:11,804 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:11,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6402527336031199. input_tokens=270, output_tokens=0
01:48:43,87 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:48:43,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2388432463631034. input_tokens=230, output_tokens=0
01:49:14,280 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:14,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.150165214203298. input_tokens=326, output_tokens=0
01:49:45,500 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:49:45,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1716586081311107. input_tokens=392, output_tokens=0
01:50:16,465 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
01:50:16,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9211223116144538. input_tokens=40, output_tokens=0
01:50:16,492 datashaper.workflow.workflow INFO executing verb drop
01:50:16,497 datashaper.workflow.workflow INFO executing verb filter
01:50:16,509 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
01:50:16,709 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
01:50:16,710 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:50:16,726 datashaper.workflow.workflow INFO executing verb layout_graph
01:50:16,995 datashaper.workflow.workflow INFO executing verb unpack_graph
01:50:17,44 datashaper.workflow.workflow INFO executing verb unpack_graph
01:50:17,206 datashaper.workflow.workflow INFO executing verb filter
01:50:17,228 datashaper.workflow.workflow INFO executing verb drop
01:50:17,233 datashaper.workflow.workflow INFO executing verb select
01:50:17,239 datashaper.workflow.workflow INFO executing verb rename
01:50:17,245 datashaper.workflow.workflow INFO executing verb join
01:50:17,260 datashaper.workflow.workflow INFO executing verb convert
01:50:17,281 datashaper.workflow.workflow INFO executing verb rename
01:50:17,285 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
01:50:17,417 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
01:50:17,417 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:50:17,436 datashaper.workflow.workflow INFO executing verb create_final_communities
01:50:17,543 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
01:50:17,688 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
01:50:17,688 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:50:17,696 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
01:50:17,713 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
01:50:17,760 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
01:50:17,767 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
01:50:17,895 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
01:50:17,895 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:50:17,901 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
01:50:17,947 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
01:50:17,976 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
01:50:17,998 datashaper.workflow.workflow INFO executing verb select
01:50:18,1 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
01:50:18,142 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
01:50:18,142 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
01:50:18,147 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
01:50:18,165 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
01:50:18,182 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
01:50:18,192 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
01:50:18,204 datashaper.workflow.workflow INFO executing verb prepare_community_reports
01:50:18,204 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 482
01:50:18,237 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 482
01:50:18,258 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 482
01:50:18,310 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 482
01:50:18,358 datashaper.workflow.workflow INFO executing verb create_community_reports
01:50:21,2 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:50:21,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.60679745208472. input_tokens=2135, output_tokens=346
01:50:55,683 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:51:25,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.638712736777961. input_tokens=7188, output_tokens=662
01:51:59,907 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:52:29,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.143904911354184. input_tokens=3283, output_tokens=708
01:53:03,698 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:53:33,723 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.708123965188861. input_tokens=2215, output_tokens=606
01:54:08,472 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:54:38,485 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.711900995112956. input_tokens=3547, output_tokens=731
01:55:11,580 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:55:41,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0277389474213123. input_tokens=2102, output_tokens=422
01:56:14,75 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:56:44,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3941188249737024. input_tokens=2078, output_tokens=297
01:57:21,178 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:57:21,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 7.044078402221203. input_tokens=7282, output_tokens=1221
01:57:53,962 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:58:23,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.741434142924845. input_tokens=2223, output_tokens=438
01:58:57,571 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
01:59:27,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.528636193834245. input_tokens=2687, output_tokens=574
02:00:00,757 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:00:30,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.086054037325084. input_tokens=2420, output_tokens=474
02:01:04,191 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:01:34,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.3548628175631166. input_tokens=2983, output_tokens=478
02:02:07,18 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:02:37,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7491366043686867. input_tokens=2077, output_tokens=335
02:03:09,736 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:03:39,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6349563570693135. input_tokens=2155, output_tokens=377
02:04:12,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:04:42,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.6926259910687804. input_tokens=2049, output_tokens=396
02:05:15,408 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:05:45,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.800984757952392. input_tokens=2095, output_tokens=398
02:06:32,754 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:06:32,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 17.24972369801253. input_tokens=3148, output_tokens=587
02:07:05,435 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:07:35,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.615975873544812. input_tokens=2065, output_tokens=365
02:08:10,79 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:08:40,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.550852597691119. input_tokens=3421, output_tokens=680
02:09:13,262 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:09:43,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.129006177186966. input_tokens=2397, output_tokens=507
02:10:16,871 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:10:46,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.50929107517004. input_tokens=2314, output_tokens=499
02:11:20,658 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:11:20,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7114752857014537. input_tokens=2988, output_tokens=543
02:11:56,951 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:12:26,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.233471527695656. input_tokens=3643, output_tokens=1138
02:13:00,123 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:13:30,142 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.0927004190161824. input_tokens=2357, output_tokens=429
02:14:03,670 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:14:33,703 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4745311746373773. input_tokens=2243, output_tokens=510
02:15:09,723 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:15:39,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.98738771956414. input_tokens=7632, output_tokens=986
02:16:13,412 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:16:13,415 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
02:16:43,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.595553088001907. input_tokens=3146, output_tokens=596
02:17:16,471 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:17:46,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.970274013467133. input_tokens=2140, output_tokens=399
02:18:20,274 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:18:20,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.7344727404415607. input_tokens=2849, output_tokens=672
02:18:52,891 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:19:22,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.545987758785486. input_tokens=2066, output_tokens=347
02:19:56,152 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:20:26,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1675451220944524. input_tokens=2339, output_tokens=555
02:20:58,230 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:21:28,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.0101849799975753. input_tokens=2009, output_tokens=245
02:22:01,97 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:22:31,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.7894201120361686. input_tokens=2316, output_tokens=377
02:23:03,802 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:23:33,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.636116993613541. input_tokens=2013, output_tokens=369
02:24:06,470 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:24:36,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5740660913288593. input_tokens=2061, output_tokens=347
02:25:09,977 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:25:39,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.4491306943818927. input_tokens=2274, output_tokens=501
02:26:14,590 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:26:44,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.5480215679854155. input_tokens=2259, output_tokens=754
02:27:17,500 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:27:47,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.815656315535307. input_tokens=2008, output_tokens=358
02:28:22,310 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:28:22,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.730375007726252. input_tokens=3257, output_tokens=757
02:28:58,713 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:29:28,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.342467844486237. input_tokens=3446, output_tokens=980
02:30:01,179 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:30:31,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.3991631157696247. input_tokens=2138, output_tokens=364
02:31:03,794 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:31:33,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.522623143158853. input_tokens=2012, output_tokens=285
02:32:06,265 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:32:36,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.4087119391188025. input_tokens=2022, output_tokens=294
02:33:08,836 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:33:38,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.5156402811408043. input_tokens=2018, output_tokens=340
02:34:12,126 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:34:42,150 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.219353257678449. input_tokens=2552, output_tokens=555
02:35:15,4 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:35:45,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8001506915315986. input_tokens=2018, output_tokens=397
02:36:18,717 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:36:18,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.6230228692293167. input_tokens=2577, output_tokens=566
02:36:51,553 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:37:21,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.778079685755074. input_tokens=2256, output_tokens=414
02:37:54,514 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:38:24,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8723897067829967. input_tokens=2074, output_tokens=399
02:38:57,699 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:39:27,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.1051998287439346. input_tokens=2149, output_tokens=475
02:40:03,575 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:40:33,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.80355872027576. input_tokens=4148, output_tokens=981
02:41:06,394 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:41:36,427 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.759184886701405. input_tokens=2130, output_tokens=343
02:42:09,468 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:42:39,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.988432025536895. input_tokens=2031, output_tokens=373
02:43:14,432 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:43:44,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.8913903292268515. input_tokens=4622, output_tokens=730
02:44:18,678 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:44:18,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.16786780115217. input_tokens=2595, output_tokens=631
02:44:54,635 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:45:24,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.897122532129288. input_tokens=9481, output_tokens=976
02:45:58,55 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:46:28,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.342125521041453. input_tokens=2176, output_tokens=497
02:46:59,911 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:47:29,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 1.7958796015009284. input_tokens=2008, output_tokens=184
02:48:04,244 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:48:34,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.2596102664247155. input_tokens=2887, output_tokens=653
02:49:07,142 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
02:49:37,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 2.8393144980072975. input_tokens=2149, output_tokens=503
02:49:37,202 datashaper.workflow.workflow INFO executing verb window
02:49:37,205 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
02:49:37,206 graphrag.index.emit.parquet_table_emitter ERROR Error while emitting parquet table
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/emit/parquet_table_emitter.py", line 40, in emit
    await self._storage.set(filename, data.to_parquet())
                                      ^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 3113, in to_parquet
    return to_parquet(
           ^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/io/parquet.py", line 480, in to_parquet
    impl.write(
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/io/parquet.py", line 190, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 3874, in pyarrow.lib.Table.from_pandas
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 611, in dataframe_to_arrays
    arrays = [convert_column(c, f)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 611, in <listcomp>
    arrays = [convert_column(c, f)
              ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 598, in convert_column
    raise e
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 592, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 339, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 85, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ('cannot mix list and non-list, non-null values', 'Conversion failed for column findings with type object')
02:49:37,222 graphrag.index.reporting.file_workflow_callbacks INFO Error emitting table details=None
02:49:37,366 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
02:49:37,367 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
02:49:37,388 datashaper.workflow.workflow INFO executing verb unroll
02:49:37,398 datashaper.workflow.workflow INFO executing verb select
02:49:37,406 datashaper.workflow.workflow INFO executing verb rename
02:49:37,415 datashaper.workflow.workflow INFO executing verb join
02:49:37,426 datashaper.workflow.workflow INFO executing verb aggregate_override
02:49:37,435 datashaper.workflow.workflow INFO executing verb join
02:49:37,448 datashaper.workflow.workflow INFO executing verb rename
02:49:37,458 datashaper.workflow.workflow INFO executing verb convert
02:49:37,471 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
02:49:37,603 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
02:49:37,604 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
02:49:37,636 datashaper.workflow.workflow INFO executing verb rename
02:49:37,640 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
02:49:37,682 graphrag.index.cli INFO All workflows completed successfully.
19:46:12,765 graphrag.index.cli INFO Logging enabled at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/indexing-engine.log
19:46:12,766 graphrag.index.cli INFO Starting pipeline run for: 20240927-194612, dryrun=False
19:46:12,766 graphrag.index.cli INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4o",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:8080/",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 350000,
        "requests_per_minute": 10,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 1
    },
    "async_mode": "threaded",
    "root_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": true,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": true,
        "raw_entities": true,
        "top_level_nodes": true
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4o",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:8080/",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 350000,
            "requests_per_minute": 10,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 1
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": true
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
19:46:12,769 graphrag.index.create_pipeline_config INFO skipping workflows 
19:46:12,769 graphrag.index.run.run INFO Running pipeline
19:46:12,769 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output
19:46:12,770 graphrag.index.input.load_input INFO loading input from root_dir=input
19:46:12,770 graphrag.index.input.load_input INFO using file storage for input
19:46:12,772 graphrag.index.storage.file_pipeline_storage INFO search /home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/input for files matching .*\.txt$
19:46:12,773 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
19:46:12,774 graphrag.index.input.text INFO Found 1 files, loading 1
19:46:12,775 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_base_documents', 'create_final_documents']
19:46:12,775 graphrag.index.run.run INFO Final # of rows loaded: 1
19:46:12,885 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
19:46:12,888 datashaper.workflow.workflow INFO executing verb orderby
19:46:12,890 datashaper.workflow.workflow INFO executing verb zip
19:46:12,891 datashaper.workflow.workflow INFO executing verb aggregate_override
19:46:12,895 datashaper.workflow.workflow INFO executing verb chunk
19:46:13,3 datashaper.workflow.workflow INFO executing verb select
19:46:13,5 datashaper.workflow.workflow INFO executing verb unroll
19:46:13,8 datashaper.workflow.workflow INFO executing verb rename
19:46:13,10 datashaper.workflow.workflow INFO executing verb genid
19:46:13,14 datashaper.workflow.workflow INFO executing verb unzip
19:46:13,16 datashaper.workflow.workflow INFO executing verb copy
19:46:13,19 datashaper.workflow.workflow INFO executing verb filter
19:46:13,27 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
19:46:13,154 graphrag.index.run.workflow INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
19:46:13,155 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:46:13,165 datashaper.workflow.workflow INFO executing verb entity_extract
19:46:13,168 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:46:13,206 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4o: TPM=350000, RPM=10
19:46:13,206 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4o: 25
19:46:13,302 datashaper.workflow.workflow INFO executing verb snapshot
19:46:13,322 datashaper.workflow.workflow INFO executing verb merge_graphs
19:46:13,347 datashaper.workflow.workflow INFO executing verb snapshot_rows
19:46:13,355 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
19:46:13,473 graphrag.index.run.workflow INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
19:46:13,473 graphrag.utils.storage INFO read table from storage: create_base_extracted_entities.parquet
19:46:13,484 datashaper.workflow.workflow INFO executing verb summarize_descriptions
19:46:14,753 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:14,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.24784573353827. input_tokens=171, output_tokens=0
19:46:15,395 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:15,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.5382621083408594. input_tokens=147, output_tokens=25
19:46:16,356 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:16,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.8095207065343857. input_tokens=206, output_tokens=78
19:46:17,64 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:17,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6285234671086073. input_tokens=207, output_tokens=60
19:46:17,768 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:17,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 0.6385406358167529. input_tokens=171, output_tokens=42
19:46:17,990 datashaper.workflow.workflow INFO executing verb snapshot_rows
19:46:18,0 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
19:46:18,136 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
19:46:18,136 graphrag.utils.storage INFO read table from storage: create_summarized_entities.parquet
19:46:18,148 datashaper.workflow.workflow INFO executing verb cluster_graph
19:46:18,254 datashaper.workflow.workflow INFO executing verb snapshot_rows
19:46:18,291 datashaper.workflow.workflow INFO executing verb embed_graph
19:46:18,324 root INFO Starting preprocessing of transition probabilities on graph with 271 nodes and 367 edges
19:46:18,324 root INFO Starting at time 1727459178.324759
19:46:18,324 root INFO Beginning preprocessing of transition probabilities for 271 vertices
19:46:18,324 root INFO Completed 1 / 271 vertices
19:46:18,324 root INFO Completed 28 / 271 vertices
19:46:18,326 root INFO Completed 55 / 271 vertices
19:46:18,326 root INFO Completed 82 / 271 vertices
19:46:18,327 root INFO Completed 109 / 271 vertices
19:46:18,327 root INFO Completed 136 / 271 vertices
19:46:18,328 root INFO Completed 163 / 271 vertices
19:46:18,328 root INFO Completed 190 / 271 vertices
19:46:18,329 root INFO Completed 217 / 271 vertices
19:46:18,329 root INFO Completed 244 / 271 vertices
19:46:18,330 root INFO Completed 271 / 271 vertices
19:46:18,330 root INFO Completed preprocessing of transition probabilities for vertices
19:46:18,330 root INFO Beginning preprocessing of transition probabilities for 367 edges
19:46:18,331 root INFO Completed 1 / 367 edges
19:46:18,333 root INFO Completed 37 / 367 edges
19:46:18,336 root INFO Completed 73 / 367 edges
19:46:18,339 root INFO Completed 109 / 367 edges
19:46:18,340 root INFO Completed 145 / 367 edges
19:46:18,341 root INFO Completed 181 / 367 edges
19:46:18,343 root INFO Completed 217 / 367 edges
19:46:18,343 root INFO Completed 253 / 367 edges
19:46:18,344 root INFO Completed 289 / 367 edges
19:46:18,345 root INFO Completed 325 / 367 edges
19:46:18,346 root INFO Completed 361 / 367 edges
19:46:18,346 root INFO Completed preprocessing of transition probabilities for edges
19:46:18,347 root INFO Simulating walks on graph at time 1727459178.3475773
19:46:18,358 root INFO Walk iteration: 1/10
19:46:18,368 root INFO Walk iteration: 2/10
19:46:18,375 root INFO Walk iteration: 3/10
19:46:18,383 root INFO Walk iteration: 4/10
19:46:18,391 root INFO Walk iteration: 5/10
19:46:18,398 root INFO Walk iteration: 6/10
19:46:18,405 root INFO Walk iteration: 7/10
19:46:18,413 root INFO Walk iteration: 8/10
19:46:18,420 root INFO Walk iteration: 9/10
19:46:18,428 root INFO Walk iteration: 10/10
19:46:18,435 root INFO Learning embeddings at time 1727459178.4357774
19:46:18,437 gensim.models.word2vec INFO collecting all words and their counts
19:46:18,437 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:46:18,439 gensim.models.word2vec INFO collected 271 word types from a corpus of 47240 raw words and 2710 sentences
19:46:18,439 gensim.models.word2vec INFO Creating a fresh vocabulary
19:46:18,439 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 271 unique words (100.00% of original 271, drops 0)', 'datetime': '2024-09-27T19:46:18.439343', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,440 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 47240 word corpus (100.00% of original 47240, drops 0)', 'datetime': '2024-09-27T19:46:18.440236', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,440 gensim.models.word2vec INFO deleting the raw counts dictionary of 271 items
19:46:18,441 gensim.models.word2vec INFO sample=0.001 downsamples 73 most-common words
19:46:18,442 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 24860.53140263153 word corpus (52.6%% of prior 47240)', 'datetime': '2024-09-27T19:46:18.442168', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,443 gensim.models.word2vec INFO estimated required memory for 271 words and 1536 dimensions: 3465548 bytes
19:46:18,443 gensim.models.word2vec INFO resetting layer weights
19:46:18,444 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-09-27T19:46:18.444646', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:46:18,444 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 271 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-09-27T19:46:18.444688', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:18,519 gensim.models.word2vec INFO EPOCH 0: training on 47240 raw words (24819 effective words) took 0.1s, 350309 effective words/s
19:46:18,565 gensim.models.word2vec INFO EPOCH 1: training on 47240 raw words (24939 effective words) took 0.0s, 556447 effective words/s
19:46:18,613 gensim.models.word2vec INFO EPOCH 2: training on 47240 raw words (24945 effective words) took 0.0s, 551737 effective words/s
19:46:18,613 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 141720 raw words (74703 effective words) took 0.2s, 445258 effective words/s', 'datetime': '2024-09-27T19:46:18.613307', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:18,613 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=271, vector_size=1536, alpha=0.025>', 'datetime': '2024-09-27T19:46:18.613365', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:46:18,613 root INFO Completed. Ending time is 1727459178.6135101 Elapsed time is -0.28875112533569336
19:46:18,645 root INFO Starting preprocessing of transition probabilities on graph with 271 nodes and 367 edges
19:46:18,645 root INFO Starting at time 1727459178.6458278
19:46:18,645 root INFO Beginning preprocessing of transition probabilities for 271 vertices
19:46:18,645 root INFO Completed 1 / 271 vertices
19:46:18,645 root INFO Completed 28 / 271 vertices
19:46:18,647 root INFO Completed 55 / 271 vertices
19:46:18,648 root INFO Completed 82 / 271 vertices
19:46:18,648 root INFO Completed 109 / 271 vertices
19:46:18,649 root INFO Completed 136 / 271 vertices
19:46:18,649 root INFO Completed 163 / 271 vertices
19:46:18,650 root INFO Completed 190 / 271 vertices
19:46:18,650 root INFO Completed 217 / 271 vertices
19:46:18,651 root INFO Completed 244 / 271 vertices
19:46:18,651 root INFO Completed 271 / 271 vertices
19:46:18,651 root INFO Completed preprocessing of transition probabilities for vertices
19:46:18,652 root INFO Beginning preprocessing of transition probabilities for 367 edges
19:46:18,652 root INFO Completed 1 / 367 edges
19:46:18,653 root INFO Completed 37 / 367 edges
19:46:18,656 root INFO Completed 73 / 367 edges
19:46:18,658 root INFO Completed 109 / 367 edges
19:46:18,660 root INFO Completed 145 / 367 edges
19:46:18,660 root INFO Completed 181 / 367 edges
19:46:18,661 root INFO Completed 217 / 367 edges
19:46:18,662 root INFO Completed 253 / 367 edges
19:46:18,662 root INFO Completed 289 / 367 edges
19:46:18,663 root INFO Completed 325 / 367 edges
19:46:18,664 root INFO Completed 361 / 367 edges
19:46:18,665 root INFO Completed preprocessing of transition probabilities for edges
19:46:18,665 root INFO Simulating walks on graph at time 1727459178.6659565
19:46:18,666 root INFO Walk iteration: 1/10
19:46:18,674 root INFO Walk iteration: 2/10
19:46:18,681 root INFO Walk iteration: 3/10
19:46:18,689 root INFO Walk iteration: 4/10
19:46:18,696 root INFO Walk iteration: 5/10
19:46:18,703 root INFO Walk iteration: 6/10
19:46:18,712 root INFO Walk iteration: 7/10
19:46:18,720 root INFO Walk iteration: 8/10
19:46:18,727 root INFO Walk iteration: 9/10
19:46:18,735 root INFO Walk iteration: 10/10
19:46:18,742 root INFO Learning embeddings at time 1727459178.7429283
19:46:18,744 gensim.models.word2vec INFO collecting all words and their counts
19:46:18,751 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:46:18,753 gensim.models.word2vec INFO collected 271 word types from a corpus of 47240 raw words and 2710 sentences
19:46:18,753 gensim.models.word2vec INFO Creating a fresh vocabulary
19:46:18,753 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 271 unique words (100.00% of original 271, drops 0)', 'datetime': '2024-09-27T19:46:18.753476', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,754 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 47240 word corpus (100.00% of original 47240, drops 0)', 'datetime': '2024-09-27T19:46:18.754298', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,754 gensim.models.word2vec INFO deleting the raw counts dictionary of 271 items
19:46:18,755 gensim.models.word2vec INFO sample=0.001 downsamples 73 most-common words
19:46:18,755 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 24860.53140263153 word corpus (52.6%% of prior 47240)', 'datetime': '2024-09-27T19:46:18.755322', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:18,756 gensim.models.word2vec INFO estimated required memory for 271 words and 1536 dimensions: 3465548 bytes
19:46:18,763 gensim.models.word2vec INFO resetting layer weights
19:46:18,764 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-09-27T19:46:18.764397', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:46:18,764 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 271 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-09-27T19:46:18.764440', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:18,815 gensim.models.word2vec INFO EPOCH 0: training on 47240 raw words (24967 effective words) took 0.0s, 509909 effective words/s
19:46:18,861 gensim.models.word2vec INFO EPOCH 1: training on 47240 raw words (24868 effective words) took 0.0s, 569369 effective words/s
19:46:18,909 gensim.models.word2vec INFO EPOCH 2: training on 47240 raw words (24870 effective words) took 0.0s, 554394 effective words/s
19:46:18,909 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 141720 raw words (74705 effective words) took 0.1s, 515786 effective words/s', 'datetime': '2024-09-27T19:46:18.909300', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:18,909 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=271, vector_size=1536, alpha=0.025>', 'datetime': '2024-09-27T19:46:18.909353', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:46:18,909 root INFO Completed. Ending time is 1727459178.9094882 Elapsed time is -0.2636604309082031
19:46:19,45 root INFO Starting preprocessing of transition probabilities on graph with 271 nodes and 367 edges
19:46:19,45 root INFO Starting at time 1727459179.0459464
19:46:19,45 root INFO Beginning preprocessing of transition probabilities for 271 vertices
19:46:19,46 root INFO Completed 1 / 271 vertices
19:46:19,46 root INFO Completed 28 / 271 vertices
19:46:19,47 root INFO Completed 55 / 271 vertices
19:46:19,48 root INFO Completed 82 / 271 vertices
19:46:19,49 root INFO Completed 109 / 271 vertices
19:46:19,49 root INFO Completed 136 / 271 vertices
19:46:19,50 root INFO Completed 163 / 271 vertices
19:46:19,50 root INFO Completed 190 / 271 vertices
19:46:19,51 root INFO Completed 217 / 271 vertices
19:46:19,51 root INFO Completed 244 / 271 vertices
19:46:19,59 root INFO Completed 271 / 271 vertices
19:46:19,59 root INFO Completed preprocessing of transition probabilities for vertices
19:46:19,59 root INFO Beginning preprocessing of transition probabilities for 367 edges
19:46:19,59 root INFO Completed 1 / 367 edges
19:46:19,61 root INFO Completed 37 / 367 edges
19:46:19,63 root INFO Completed 73 / 367 edges
19:46:19,71 root INFO Completed 109 / 367 edges
19:46:19,72 root INFO Completed 145 / 367 edges
19:46:19,79 root INFO Completed 181 / 367 edges
19:46:19,80 root INFO Completed 217 / 367 edges
19:46:19,81 root INFO Completed 253 / 367 edges
19:46:19,82 root INFO Completed 289 / 367 edges
19:46:19,83 root INFO Completed 325 / 367 edges
19:46:19,84 root INFO Completed 361 / 367 edges
19:46:19,91 root INFO Completed preprocessing of transition probabilities for edges
19:46:19,92 root INFO Simulating walks on graph at time 1727459179.0926836
19:46:19,93 root INFO Walk iteration: 1/10
19:46:19,103 root INFO Walk iteration: 2/10
19:46:19,110 root INFO Walk iteration: 3/10
19:46:19,117 root INFO Walk iteration: 4/10
19:46:19,125 root INFO Walk iteration: 5/10
19:46:19,133 root INFO Walk iteration: 6/10
19:46:19,140 root INFO Walk iteration: 7/10
19:46:19,148 root INFO Walk iteration: 8/10
19:46:19,155 root INFO Walk iteration: 9/10
19:46:19,163 root INFO Walk iteration: 10/10
19:46:19,170 root INFO Learning embeddings at time 1727459179.1709242
19:46:19,172 gensim.models.word2vec INFO collecting all words and their counts
19:46:19,178 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:46:19,180 gensim.models.word2vec INFO collected 271 word types from a corpus of 47240 raw words and 2710 sentences
19:46:19,180 gensim.models.word2vec INFO Creating a fresh vocabulary
19:46:19,180 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 271 unique words (100.00% of original 271, drops 0)', 'datetime': '2024-09-27T19:46:19.180527', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,181 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 47240 word corpus (100.00% of original 47240, drops 0)', 'datetime': '2024-09-27T19:46:19.181266', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,181 gensim.models.word2vec INFO deleting the raw counts dictionary of 271 items
19:46:19,182 gensim.models.word2vec INFO sample=0.001 downsamples 73 most-common words
19:46:19,182 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 24860.53140263153 word corpus (52.6%% of prior 47240)', 'datetime': '2024-09-27T19:46:19.182481', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,183 gensim.models.word2vec INFO estimated required memory for 271 words and 1536 dimensions: 3465548 bytes
19:46:19,189 gensim.models.word2vec INFO resetting layer weights
19:46:19,190 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-09-27T19:46:19.190698', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:46:19,190 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 271 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-09-27T19:46:19.190749', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:19,238 gensim.models.word2vec INFO EPOCH 0: training on 47240 raw words (24903 effective words) took 0.0s, 548098 effective words/s
19:46:19,290 gensim.models.word2vec INFO EPOCH 1: training on 47240 raw words (24723 effective words) took 0.1s, 488348 effective words/s
19:46:19,339 gensim.models.word2vec INFO EPOCH 2: training on 47240 raw words (24841 effective words) took 0.0s, 536416 effective words/s
19:46:19,339 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 141720 raw words (74467 effective words) took 0.1s, 499975 effective words/s', 'datetime': '2024-09-27T19:46:19.339721', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:19,339 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=271, vector_size=1536, alpha=0.025>', 'datetime': '2024-09-27T19:46:19.339783', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:46:19,341 root INFO Completed. Ending time is 1727459179.341173 Elapsed time is -0.2952265739440918
19:46:19,370 root INFO Starting preprocessing of transition probabilities on graph with 271 nodes and 367 edges
19:46:19,370 root INFO Starting at time 1727459179.3708298
19:46:19,370 root INFO Beginning preprocessing of transition probabilities for 271 vertices
19:46:19,370 root INFO Completed 1 / 271 vertices
19:46:19,370 root INFO Completed 28 / 271 vertices
19:46:19,371 root INFO Completed 55 / 271 vertices
19:46:19,372 root INFO Completed 82 / 271 vertices
19:46:19,378 root INFO Completed 109 / 271 vertices
19:46:19,387 root INFO Completed 136 / 271 vertices
19:46:19,398 root INFO Completed 163 / 271 vertices
19:46:19,410 root INFO Completed 190 / 271 vertices
19:46:19,411 root INFO Completed 217 / 271 vertices
19:46:19,413 root INFO Completed 244 / 271 vertices
19:46:19,414 root INFO Completed 271 / 271 vertices
19:46:19,416 root INFO Completed preprocessing of transition probabilities for vertices
19:46:19,416 root INFO Beginning preprocessing of transition probabilities for 367 edges
19:46:19,416 root INFO Completed 1 / 367 edges
19:46:19,421 root INFO Completed 37 / 367 edges
19:46:19,437 root INFO Completed 73 / 367 edges
19:46:19,442 root INFO Completed 109 / 367 edges
19:46:19,443 root INFO Completed 145 / 367 edges
19:46:19,444 root INFO Completed 181 / 367 edges
19:46:19,456 root INFO Completed 217 / 367 edges
19:46:19,457 root INFO Completed 253 / 367 edges
19:46:19,463 root INFO Completed 289 / 367 edges
19:46:19,464 root INFO Completed 325 / 367 edges
19:46:19,465 root INFO Completed 361 / 367 edges
19:46:19,465 root INFO Completed preprocessing of transition probabilities for edges
19:46:19,466 root INFO Simulating walks on graph at time 1727459179.4664266
19:46:19,467 root INFO Walk iteration: 1/10
19:46:19,478 root INFO Walk iteration: 2/10
19:46:19,485 root INFO Walk iteration: 3/10
19:46:19,493 root INFO Walk iteration: 4/10
19:46:19,500 root INFO Walk iteration: 5/10
19:46:19,508 root INFO Walk iteration: 6/10
19:46:19,515 root INFO Walk iteration: 7/10
19:46:19,522 root INFO Walk iteration: 8/10
19:46:19,530 root INFO Walk iteration: 9/10
19:46:19,537 root INFO Walk iteration: 10/10
19:46:19,545 root INFO Learning embeddings at time 1727459179.5451355
19:46:19,546 gensim.models.word2vec INFO collecting all words and their counts
19:46:19,556 gensim.models.word2vec INFO PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
19:46:19,559 gensim.models.word2vec INFO collected 271 word types from a corpus of 47240 raw words and 2710 sentences
19:46:19,562 gensim.models.word2vec INFO Creating a fresh vocabulary
19:46:19,562 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 271 unique words (100.00% of original 271, drops 0)', 'datetime': '2024-09-27T19:46:19.562557', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,563 gensim.utils INFO Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 47240 word corpus (100.00% of original 47240, drops 0)', 'datetime': '2024-09-27T19:46:19.563412', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,563 gensim.models.word2vec INFO deleting the raw counts dictionary of 271 items
19:46:19,564 gensim.models.word2vec INFO sample=0.001 downsamples 73 most-common words
19:46:19,564 gensim.utils INFO Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 24860.53140263153 word corpus (52.6%% of prior 47240)', 'datetime': '2024-09-27T19:46:19.564383', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'prepare_vocab'}
19:46:19,565 gensim.models.word2vec INFO estimated required memory for 271 words and 1536 dimensions: 3465548 bytes
19:46:19,565 gensim.models.word2vec INFO resetting layer weights
19:46:19,565 gensim.utils INFO Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-09-27T19:46:19.565986', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'build_vocab'}
19:46:19,566 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training model with 8 workers on 271 vocabulary and 1536 features, using sg=1 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-09-27T19:46:19.566021', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:19,614 gensim.models.word2vec INFO EPOCH 0: training on 47240 raw words (24903 effective words) took 0.0s, 537874 effective words/s
19:46:19,663 gensim.models.word2vec INFO EPOCH 1: training on 47240 raw words (24723 effective words) took 0.0s, 530171 effective words/s
19:46:19,712 gensim.models.word2vec INFO EPOCH 2: training on 47240 raw words (24837 effective words) took 0.0s, 531737 effective words/s
19:46:19,712 gensim.utils INFO Word2Vec lifecycle event {'msg': 'training on 141720 raw words (74463 effective words) took 0.1s, 509788 effective words/s', 'datetime': '2024-09-27T19:46:19.712106', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'train'}
19:46:19,712 gensim.utils INFO Word2Vec lifecycle event {'params': 'Word2Vec<vocab=271, vector_size=1536, alpha=0.025>', 'datetime': '2024-09-27T19:46:19.712161', 'gensim': '4.3.3', 'python': '3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]', 'platform': 'Linux-6.1.100-1-cip-amd64-x86_64-with-glibc2.36', 'event': 'created'}
19:46:19,712 root INFO Completed. Ending time is 1727459179.712309 Elapsed time is -0.3414790630340576
19:46:19,729 datashaper.workflow.workflow INFO executing verb snapshot_rows
19:46:19,772 datashaper.workflow.workflow INFO executing verb select
19:46:19,798 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
19:46:20,289 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
19:46:20,290 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:46:20,463 datashaper.workflow.workflow INFO executing verb unpack_graph
19:46:20,505 datashaper.workflow.workflow INFO executing verb rename
19:46:20,510 datashaper.workflow.workflow INFO executing verb select
19:46:20,515 datashaper.workflow.workflow INFO executing verb dedupe
19:46:20,520 datashaper.workflow.workflow INFO executing verb rename
19:46:20,525 datashaper.workflow.workflow INFO executing verb filter
19:46:20,538 datashaper.workflow.workflow INFO executing verb text_split
19:46:20,546 datashaper.workflow.workflow INFO executing verb drop
19:46:20,551 datashaper.workflow.workflow INFO executing verb merge
19:46:20,593 datashaper.workflow.workflow INFO executing verb text_embed
19:46:20,596 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:8080
19:46:20,635 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text: TPM=350000, RPM=10
19:46:20,635 graphrag.index.llm.load_llm INFO create concurrency limiter for text: 25
19:46:20,644 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 482 inputs via 482 snippets using 31 batches. max_batch_size=16, max_tokens=8191
19:46:22,7 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:46:22,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3719218503683805. input_tokens=284, output_tokens=0
19:46:23,233 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:46:23,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2005654405802488. input_tokens=244, output_tokens=0
19:46:24,408 httpx INFO HTTP Request: POST http://localhost:8080/embeddings "HTTP/1.1 200 OK"
19:46:24,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1526734326034784. input_tokens=276, output_tokens=0
19:46:24,464 datashaper.workflow.workflow INFO executing verb drop
19:46:24,470 datashaper.workflow.workflow INFO executing verb filter
19:46:24,483 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
19:46:24,737 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
19:46:24,738 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:46:24,785 datashaper.workflow.workflow INFO executing verb layout_graph
19:46:29,252 datashaper.workflow.workflow INFO executing verb unpack_graph
19:46:29,438 datashaper.workflow.workflow INFO executing verb unpack_graph
19:46:29,487 datashaper.workflow.workflow INFO executing verb filter
19:46:29,511 datashaper.workflow.workflow INFO executing verb drop
19:46:29,518 datashaper.workflow.workflow INFO executing verb select
19:46:29,525 datashaper.workflow.workflow INFO executing verb snapshot
19:46:29,536 datashaper.workflow.workflow INFO executing verb rename
19:46:29,543 datashaper.workflow.workflow INFO executing verb convert
19:46:29,565 datashaper.workflow.workflow INFO executing verb join
19:46:29,577 datashaper.workflow.workflow INFO executing verb rename
19:46:29,580 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
19:46:29,909 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
19:46:29,910 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:46:29,958 datashaper.workflow.workflow INFO executing verb create_final_communities
19:46:30,66 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
19:46:30,229 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
19:46:30,230 graphrag.utils.storage INFO read table from storage: create_base_entity_graph.parquet
19:46:30,265 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:46:30,310 datashaper.workflow.workflow INFO executing verb create_final_relationships_pre_embedding
19:46:30,359 datashaper.workflow.workflow INFO executing verb create_final_relationships_post_embedding
19:46:30,365 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
19:46:30,523 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships']
19:46:30,523 graphrag.utils.storage INFO read table from storage: create_base_text_units.parquet
19:46:30,528 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
19:46:30,596 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:46:30,626 datashaper.workflow.workflow INFO executing verb create_final_text_units_pre_embedding
19:46:30,642 datashaper.workflow.workflow INFO executing verb select
19:46:30,645 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
19:46:30,804 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes']
19:46:30,804 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
19:46:30,809 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
19:46:30,853 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
19:46:30,871 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
19:46:30,882 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
19:46:30,894 datashaper.workflow.workflow INFO executing verb prepare_community_reports
19:46:30,895 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=3 => 482
19:46:30,909 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 482
19:46:30,931 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 482
19:46:30,974 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 482
19:46:31,32 datashaper.workflow.workflow INFO executing verb create_community_reports
19:46:35,641 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:35,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.6012018993496895. input_tokens=7191, output_tokens=664
19:46:39,623 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:39,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.9472543876618147. input_tokens=3547, output_tokens=673
19:46:43,793 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:43,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.13389714807272. input_tokens=7285, output_tokens=688
19:46:53,379 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:46:59,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.5418446119874716. input_tokens=3153, output_tokens=514
19:47:11,42 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:47:11,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.60137992259115. input_tokens=3643, output_tokens=954
19:47:23,110 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:47:23,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.028401150368154. input_tokens=7635, output_tokens=1086
19:47:32,286 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:47:32,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.128781263716519. input_tokens=2143, output_tokens=424
19:47:41,624 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:47:47,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 3.287811922840774. input_tokens=2339, output_tokens=521
19:47:58,745 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:47:58,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 5.072788927704096. input_tokens=3451, output_tokens=821
19:48:10,887 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:48:10,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.0975132482126355. input_tokens=4622, output_tokens=1009
19:48:23,253 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:48:23,255 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
19:48:23,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 6.3252395847812295. input_tokens=9484, output_tokens=1155
19:48:33,490 httpx INFO HTTP Request: POST http://localhost:8080/chat/completions "HTTP/1.1 200 OK"
19:48:33,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 4.19746938534081. input_tokens=2887, output_tokens=683
19:48:33,535 datashaper.workflow.workflow INFO executing verb window
19:48:33,538 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
19:48:33,539 graphrag.index.emit.parquet_table_emitter ERROR Error while emitting parquet table
Traceback (most recent call last):
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/graphrag/index/emit/parquet_table_emitter.py", line 40, in emit
    await self._storage.set(filename, data.to_parquet())
                                      ^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/util/_decorators.py", line 333, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/core/frame.py", line 3113, in to_parquet
    return to_parquet(
           ^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/io/parquet.py", line 480, in to_parquet
    impl.write(
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pandas/io/parquet.py", line 190, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/table.pxi", line 3874, in pyarrow.lib.Table.from_pandas
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 611, in dataframe_to_arrays
    arrays = [convert_column(c, f)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 611, in <listcomp>
    arrays = [convert_column(c, f)
              ^^^^^^^^^^^^^^^^^^^^
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 598, in convert_column
    raise e
  File "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/pyarrow/pandas_compat.py", line 592, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 339, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 85, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ('cannot mix list and non-list, non-null values', 'Conversion failed for column findings with type object')
19:48:33,546 graphrag.index.reporting.file_workflow_callbacks INFO Error emitting table details=None
19:48:33,705 graphrag.index.run.workflow INFO dependencies for create_base_documents: ['create_final_text_units']
19:48:33,705 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
19:48:33,729 datashaper.workflow.workflow INFO executing verb unroll
19:48:33,739 datashaper.workflow.workflow INFO executing verb select
19:48:33,748 datashaper.workflow.workflow INFO executing verb rename
19:48:33,758 datashaper.workflow.workflow INFO executing verb join
19:48:33,770 datashaper.workflow.workflow INFO executing verb aggregate_override
19:48:33,780 datashaper.workflow.workflow INFO executing verb join
19:48:33,792 datashaper.workflow.workflow INFO executing verb rename
19:48:33,802 datashaper.workflow.workflow INFO executing verb convert
19:48:33,817 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
19:48:33,968 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_documents']
19:48:33,968 graphrag.utils.storage INFO read table from storage: create_base_documents.parquet
19:48:34,2 datashaper.workflow.workflow INFO executing verb rename
19:48:34,6 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
19:48:34,49 graphrag.index.cli INFO All workflows completed successfully.
