{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphrag\n",
    "#https://medium.com/@ysaurabh059/graphrag-local-setup-via-vllm-and-ollama-a-detailed-integration-guide-5d85f18f7fec\n",
    "#https://medium.com/percena/inside-graphrag-analyzing-microsofts-innovative-framework-for-knowledge-graph-processing1-6f84deec5499\n",
    "\n",
    "#python -m graphrag.index --init --root ./ragtest\n",
    "#python -m graphrag.index --root ./ragtest\n",
    "\n",
    "#litellm --host 127.0.0.1 --port 8080 --config /home/cip/ce/ix05ogym/Majid/LLM/litellm/config.yaml\n",
    "#visual\n",
    "#https://noworneverev.github.io/graphrag-visualizer/\n",
    "#https://docs.llamaindex.ai/en/stable/examples/low_level/evaluation/\n",
    "#https://platform.openai.com/docs/guides/rate-limits/error-mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"anything\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator,RunConfig\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.integrations.llama_index import evaluate \n",
    "# generator with openai models\n",
    "name = \"gpt-4o\"\n",
    "base_url = \"http://localhost:8080/\"\n",
    "max_tokens = 1024*8\n",
    "generator_llm = ChatOpenAI(model=name, base_url=base_url,max_tokens=max_tokens)\n",
    "critic_llm = ChatOpenAI(model=name, base_url=base_url,max_tokens=max_tokens)\n",
    "embeddings = OpenAIEmbeddings(model='text', base_url=base_url)\n",
    "runconfig = RunConfig(max_workers=1,max_retries=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex,Document,Settings\n",
    "from llama_index.core import VectorStoreIndex,Document,Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "unit_text_path = '/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/create_base_text_units.parquet'\n",
    "\n",
    "texts = pd.read_parquet(unit_text_path)\n",
    "\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store( vector_store )\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = texts['chunk'].tolist()\n",
    "e = embeddings.embed_documents(d)\n",
    "chroma_collection.add(ids=texts['id'].tolist(),documents=d,embeddings=e)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = chroma_collection.get(include=['documents','embeddings'])\n",
    "docs = data['documents']\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Settings.embed_model = embeddings #OpenAIEmbedding(model='text',api_base=base_url,max_retries=1)\n",
    "Settings.llm=OpenAI(model=name,api_base=base_url,max_tokens=max_tokens,max_retries=1)\n",
    "Settings.chunk_size=1200\n",
    "\n",
    "\"\"\"docs_llama = []\n",
    "for i,x in texts.iterrows():\n",
    "    c = x['chunk']\n",
    "    docs_llama.append(Document(text=c))\n",
    "    docs_llama[-1].metadata['source']=x['id']\n",
    "    \n",
    "docs_llama \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from llama_index.core.llms import ChatMessage\n",
    "\n",
    "response = Settings.llm.chat([ChatMessage(role=\"user\", content=\"Hello\")])\n",
    "print(response)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "docs_llama = list(map(lambda x:Document(text=x ),docs))\n",
    "docs_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2471ab59f7cc4fe99506d938730b2667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=docs_llama[:5],\n",
    "    num_questions_per_chunk=1,\n",
    "    #text_question_template=DEFAULT_QUESTION_GENERATION_PROMPT,\n",
    "    #question_gen_query=DEFAULT_TEXT_QA_PROMPT,\n",
    "    show_progress=True,\n",
    "    workers=1\n",
    "    \n",
    ")\n",
    "\n",
    "len(dataset_generator.nodes) # 1314\n",
    "\n",
    "rag_dataset =  dataset_generator.generate_dataset_from_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_answer_by</th>\n",
       "      <th>query_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**Question:**  What is the ISBN of the book \"A...</td>\n",
       "      <td>[﻿The Project Gutenberg eBook of A Christmas C...</td>\n",
       "      <td>The ISBN of the book \"A Christmas Carol\" as pu...</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>**Question:**  How does the narrator establish...</td>\n",
       "      <td>[and thither in\\n    restless haste and moanin...</td>\n",
       "      <td>The narrator establishes the certainty of Marl...</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>## Quiz Question:</td>\n",
       "      <td>[-fisted hand at the grindstone, Scrooge! a\\ns...</td>\n",
       "      <td>Please provide the quiz question so I can answ...</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>## Quiz Question:</td>\n",
       "      <td>['Bah!' again; and followed it up with 'Humbug...</td>\n",
       "      <td>Please provide the quiz question so I can answ...</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>**Question:**  What is Scrooge's attitude towa...</td>\n",
       "      <td>[have no doubt his liberality is well represen...</td>\n",
       "      <td>Empty Response</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "      <td>ai (gpt-4o)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  **Question:**  What is the ISBN of the book \"A...   \n",
       "1  **Question:**  How does the narrator establish...   \n",
       "2                                  ## Quiz Question:   \n",
       "3                                  ## Quiz Question:   \n",
       "4  **Question:**  What is Scrooge's attitude towa...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [﻿The Project Gutenberg eBook of A Christmas C...   \n",
       "1  [and thither in\\n    restless haste and moanin...   \n",
       "2  [-fisted hand at the grindstone, Scrooge! a\\ns...   \n",
       "3  ['Bah!' again; and followed it up with 'Humbug...   \n",
       "4  [have no doubt his liberality is well represen...   \n",
       "\n",
       "                                    reference_answer reference_answer_by  \\\n",
       "0  The ISBN of the book \"A Christmas Carol\" as pu...         ai (gpt-4o)   \n",
       "1  The narrator establishes the certainty of Marl...         ai (gpt-4o)   \n",
       "2  Please provide the quiz question so I can answ...         ai (gpt-4o)   \n",
       "3  Please provide the quiz question so I can answ...         ai (gpt-4o)   \n",
       "4                                     Empty Response         ai (gpt-4o)   \n",
       "\n",
       "      query_by  \n",
       "0  ai (gpt-4o)  \n",
       "1  ai (gpt-4o)  \n",
       "2  ai (gpt-4o)  \n",
       "3  ai (gpt-4o)  \n",
       "4  ai (gpt-4o)  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "docs = []\n",
    "for i,x in texts.iterrows():\n",
    "    c = x['chunk']\n",
    "    docs.append(Document(c))\n",
    "    docs[-1].metadata['source']=x['id']\n",
    "    \n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings,\n",
    "    run_config=runconfig,\n",
    "    chunk_size=1200,\n",
    ")\n",
    "\n",
    "\n",
    "# generate testset\n",
    "message = [{\"role\":\"user\",\"content\":\"what is 2+2?\"}]\n",
    "#res = generator_llm.invoke(message)\n",
    "#res.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import numpy as np\n",
    "x = embeddings([\"hello world!\"])\n",
    "y = np.array(x)\n",
    "y.shape\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "testset = generator.generate_with_langchain_docs(docs[:5], test_size=10,run_config=runconfig,is_async=True,with_debugging_logs=True,raise_exceptions=False)\n",
    "testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = testset.to_pandas()\n",
    "display(pt)\n",
    "for i,x in pt.iterrows():\n",
    "    print(\"Q: \",x['question'])\n",
    "    print(\"GT: \",x['ground_truth'])\n",
    "    print(\"C: \",x['contexts'])\n",
    "    print('----------------------------')\n",
    "    \n",
    "pt.to_parquet('questions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pt.drop(index=4).reset_index(drop=True)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dd = pd.read_parquet('/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/questions.parquet')\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.evaluation import EvaluationDataset\n",
    "\n",
    "ed = EvaluationDataset.from_orm(dd.to_dict('list'))\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "result = evaluate(\n",
    "    ed,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        #faithfulness,\n",
    "        #answer_relevancy,\n",
    "        #context_recall,\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load graph from .graphml file\n",
    "G = nx.read_graphml('./ragtest/output/embedded_graph.0.graphml')\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "entity = pd.read_parquet('/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/create_base_extracted_entities.parquet')\n",
    "unit = pd.read_parquet('/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/create_base_text_units.parquet')\n",
    "sum = pd.read_parquet('/home/cip/ce/ix05ogym/Majid/LLM/GraphRag/ragtest/output/create_summarized_entities.parquet')\n",
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"The Eiffel Tower is located in Paris. It was designed by Gustave Eiffel in 1887 and is one of the most visited landmarks in the world.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag import model,index,llm\n",
    "import graphrag.index\n",
    "import graphrag.llm\n",
    "import graphrag.model\n",
    "\n",
    "graphrag.model.entity.Entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set your Gemini API Key\n",
    "api_key = \"AIzaSyDcsi9U5RgrnT3BG34Q0SMbbIvBc5kyFG0\"\n",
    "\n",
    "# Define the API URL\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define the data payload\n",
    "data = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, Who are you?\"}],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Print the response from the server\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
