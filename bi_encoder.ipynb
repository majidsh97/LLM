{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://motius.breezy.hr/p/0b622aa1163e01-join-our-tech-community-as-working-student-m-f-d/apply?utm_source=pt-linkedin&utm_medium=job-posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModel,AutoTokenizer,AutoModelForSequenceClassification ,AutoConfig, Trainer,TrainingArguments,\\\n",
    "BitsAndBytesConfig,pipeline,default_data_collator,DataCollatorWithPadding,DataCollatorForLanguageModeling\n",
    "from transformers.utils import move_cache\n",
    "from chat_template_utils import get_json_schema\n",
    "from llama_cpp import Llama\n",
    "from utils import *\n",
    "from peft import *\n",
    "import datasets\n",
    "from datasets import Features, Value\n",
    "import torchmetrics\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader,Dataset\n",
    "from hqq.engine.hf import HQQModelForCausalLM\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from huggingface_hub import snapshot_download\n",
    "import deepspeed\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from var_dump import var_dump\n",
    "from bs4 import BeautifulSoup,PageElement,Comment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import langchain\n",
    "from bertopic import BERTopic\n",
    "\n",
    "cache_dir='/var/tmp/.cache/' #'/proj/ciptmp/ix05ogym/.cache/'\n",
    "output_dir = cache_dir+'outputs/'\n",
    "\n",
    "move_cache(cache_dir)\n",
    "\n",
    "def pd_full_screen():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "def pd_normal_screen():\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    pd.set_option('display.max_columns', 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'] = df['query'].apply(lambda x: str(x))\n",
    "df['element'] = df['element'].apply(lambda x: str(x))\n",
    "\n",
    "df['query'] = df['query'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "df['element'] = df['element'].apply(lambda x: x.replace('\\xa0', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers  import SentenceTransformer\n",
    "#from transformers import AutoModel\n",
    "model_name= 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "#sentence-transformers/all-MiniLM-L12-v2\n",
    "embdeding_tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "\n",
    "#embeding_model= AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2',cache_dir=cache_dir)#.to('cuda')\n",
    "embeding_model= SentenceTransformer(model_name,cache_folder=cache_dir)#.to('cuda')\n",
    "#guide_model= SentenceTransformer(model_name,cache_folder=cache_dir)#.to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "embeding_model.eval()\n",
    "embeding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df(df):\n",
    "    for i,x in df.iterrows():\n",
    "        print(i)\n",
    "        for k,v in x.items():\n",
    "            if k=='query':\n",
    "                continue\n",
    "            print(k,v)\n",
    "            \n",
    "        print('---------------------')\n",
    "    \n",
    "c = df['query'].str.lower().str.contains('search')==True\n",
    "df.loc[c,'label']='search form'\n",
    "c = df['query'].str.lower().str.contains('submit application')==True\n",
    "df.loc[c,'label']='application form'\n",
    "c = df['query'].str.lower().str.contains('login')==True\n",
    "df.loc[c,'label']='login form'\n",
    "c = df['query'].str.lower().str.contains('register')==True\n",
    "df.loc[c,'label']='register form'\n",
    "\n",
    "labeled = df.dropna().reset_index(drop=True)\n",
    "print(labeled['label'].value_counts())\n",
    "print_df(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled.loc[24,'label']='application form'\n",
    "labeled.loc[34,'label']='application form'\n",
    "labeled.loc[45,'label']='application form'\n",
    "labeled.loc[100,'label']='application form'\n",
    "labeled.loc[90,'label']='application form'\n",
    "labeled.loc[53,'label']='application form'\n",
    "\n",
    "\n",
    "labeled.loc[87,'label']='next'\n",
    "labeled.loc[86,'label']='register form'\n",
    "labeled.loc[80,'label']='***'\n",
    "\n",
    "labeled.loc[14,'label']='login form'\n",
    "labeled.loc[32,'label']='login form'\n",
    "labeled.loc[40,'label']='login form'\n",
    "\n",
    "\n",
    "labeled.loc[32,'label']='login form'\n",
    "labeled.loc[27,'label']='newsletter'\n",
    "\n",
    "\n",
    "\n",
    "c = labeled['label']=='search form'\n",
    "md = labeled.loc[[24,45,100,90,53,87,86,14,32,27,40,32,10,11,12,13,17,23,28],:].copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "print_df(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.to_excel('true_form.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labeled['label'].unique()\n",
    "index2label = dict(enumerate(labels))\n",
    "label2index= dict(zip(index2label.values(),index2label.keys()))\n",
    "\n",
    "label2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled['label'] = labeled['label'].map(lambda x:label2index[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.trainer import SentenceTransformerTrainer,SentenceTransformerTrainingArguments\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#data.loc[:,'label']=1\n",
    "features = Features({\n",
    "    'query': Value('string'),\n",
    "    'element': Value('string'),\n",
    "    'answer': Value('string'),\n",
    "    'label': Value('int64')\n",
    "})\n",
    "train_dataset = datasets.Dataset.from_pandas(data,features)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['answer'])#,'label'])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.map(lambda x:\n",
    "    {\n",
    "        'element':embdeding_tokenizer(x['element'],return_tensors='pt',truncation=True,max_length=512),\n",
    "        'label':x['label']\n",
    "        \n",
    "    }\n",
    "    ,\n",
    "    remove_columns=['query']\n",
    "\n",
    "    \n",
    "    )\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled.loc[4,'query']=\"ss\"\n",
    "#labeled.loc[4,'label']=0\n",
    "features = Features({\n",
    "    'query': Value('string'),\n",
    "    'element': Value('string'),\n",
    "    'label': Value('int64')\n",
    "})\n",
    "\"\"\"labeled.loc[0:10,'query']=\"sss\"\n",
    "labeled.loc[8,'query']=\"skmklmss\"\"\"\n",
    "\"\"\"\n",
    "labeled.loc[0:10,'label']=0\n",
    "labeled.loc[2,'label']=1\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(labeled[:128],features)\n",
    "train_dataset = train_dataset.remove_columns(['element'])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def pd_full_screen():\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    \n",
    "def pd_normal_screen():\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    pd.set_option('display.max_columns', 5)\n",
    "pd_full_screen()\n",
    "\"\"\"\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import BatchSemiHardTripletLoss as myloss\n",
    "trainer  =  SentenceTransformerTrainer(model=embeding_model,\n",
    "                           train_dataset=train_dataset,\n",
    "                           tokenizer=embdeding_tokenizer,\n",
    "                           loss= myloss(embeding_model)#,384,len(u)),#,'sentence-transformers/all-MiniLM-L12-v2',True),\n",
    "                           ,args=SentenceTransformerTrainingArguments(\n",
    "                               output_dir=output_dir,\n",
    "                               num_train_epochs=20,\n",
    "                               per_device_train_batch_size=32,\n",
    "                               logging_steps=1,\n",
    "                               #learning_rate=1e-9\n",
    "                               \n",
    "                           )\n",
    "                           \n",
    "                           )\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = test_dataset['query'].isna()\n",
    "#test_dataset.loc[c,'query']='cv'\n",
    "#test_dataset.to_excel('forms.xlsx',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_excel('forms.xlsx')\n",
    "c = test_dataset['label'].notna()\n",
    "test_dataset = test_dataset.loc[c,:].reset_index(drop=True)\n",
    "\n",
    "tp = test_dataset #train_dataset.to_pandas()\n",
    "def compute_metric():\n",
    "    embeding_model.eval()\n",
    "    acc=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        l=embeding_model.encode(LABEL_INDEX_TO_KEY)\n",
    "        l = torch.tensor(l)\n",
    "        print(l.shape)\n",
    "        \n",
    "        for i,x in tp.iterrows():\n",
    "            #print(x)\n",
    "            #for i,x in train_dataset.iterrows():\n",
    "            #te = embdeding_tokenizer([x['element']],max_length=512,truncation=True,return_tensors='pt')['input_ids']\n",
    "            #print(i)\n",
    "            #x['label']\n",
    "            o = embeding_model.encode( [x['query']] )\n",
    "            o2 = embeding_model.encode( x['element'] )\n",
    "            \n",
    "            o = torch.tensor(o)\n",
    "            o2 = torch.tensor(o2)\n",
    "            \n",
    "            sim = torch.cosine_similarity(l,o)\n",
    "            #print(o2.shape)\n",
    "            #sim = torch.cosine_similarity(o2,o,0)\n",
    "\n",
    "            m = sim.argmax()\n",
    "            tp.loc[i,'sudo_label']=LABEL_INDEX_TO_KEY[m]\n",
    "            #tp.loc[i,'sudo_label_value']=sim\n",
    "            if LABEL_INDEX_TO_KEY[m]==x['label']:\n",
    "                acc+=1\n",
    "                pass\n",
    "            #print(LABEL_INDEX_TO_KEY[sim.argmax()],x['label'])\n",
    "            \n",
    "           \n",
    "             \n",
    "    \n",
    "    acc/=len(test_dataset)\n",
    "    print(\"acc:\",acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "compute_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.loc[11,'query']='cv *'\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in tp.iterrows():\n",
    "            print(x['query'])\n",
    "            print(x['element'])\n",
    "            print(x['sudo_label'])\n",
    "            #print(x['sudo_label_value'])\n",
    "            print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
