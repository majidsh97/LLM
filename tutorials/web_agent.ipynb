{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No ROCm runtime is found, using ROCM_HOME='/usr'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-11 20:27:52,972] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/cip/ce/ix05ogym/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer , Trainer,TrainingArguments,BitsAndBytesConfig\n",
    "from peft import get_peft_model,LoraConfig\n",
    "import datasets\n",
    "import torchmetrics\n",
    "import torch\n",
    "from hqq.engine.hf import HQQModelForCausalLM\n",
    "from hqq.models.hf.base import AutoHQQHFModel\n",
    "from huggingface_hub import snapshot_download\n",
    "import deepspeed\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cache_dir='/proj/ciptmp/ix05ogym/.cache/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*<input data-testid=\"textinput\" id=\"email\" required=\"\" type=\"email\" autocomplete=\"email\" name=\"email\" class=\"sc-bsgiji jskkmm\" value=\"\">*<input data-testid=\"firstname\" id=\"firstname\" required=\"\" name=\"firstname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\">*<input data-testid=\"lastname\" id=\"lastname\" required=\"\" name=\"lastname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\">documents:.doc, .docx, .pdf, .rtf, .txt<input accept=\".doc, .docx, .pdf, .rtf, .txt\" type=\"file\" autocomplete=\"off\" tabindex=\"-1\" class=\"fileuploader___styledinput-sc-e2002db3-0 djcsgj\">cv*<input accept=\".doc, .docx, .pdf, .rtf, .txt\" type=\"file\" autocomplete=\"off\" tabindex=\"-1\" class=\"fileuploader___styledinput-sc-e2002db3-0 djcsgj\">cover letteroptionalterms & conditionsprivacy policyapply now<button type=\"submit\" class=\"sc-jotjna fnrczm\">apply now</button>\n"
     ]
    }
   ],
   "source": [
    "mydata = pd.read_pickle('cleaned.pkl')\n",
    "mydata.reset_index(inplace=True)\n",
    "mydata:pd.DataFrame\n",
    "def myf(x):\n",
    "    print(x)\n",
    "    #input()\n",
    "\n",
    "print(mydata['text'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"McGill-NLP/Llama-3-8B-Web\")\n",
    "x = tokenizer(\"[[xpath]]\")\n",
    "tokenizer.convert_ids_to_tokens(x['input_ids'])\n",
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test', 'test_iid', 'test_geo', 'test_vis', 'test_cat', 'test_web'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weblinx_dataset = datasets.load_dataset('McGill-NLP/WebLINX',cache_dir=cache_dir)\n",
    "weblinx_dataset.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport'],\n",
      "    num_rows: 75\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"(uid = 0b6ec548-30c4-4dfa) [[tag]] div [[xpath]] /html/body/div[1]/...div[2]/div/div[1] [[bbox]] x=230 y=210 width=767 height=370 [[attributes]] data-event-id='19' data... font-size: 16px;' [[children]] div div \\n(uid = fc30e488-ea91-4030) [[tag]] div [[xpath]] /html/body/div[1]/...div/div[1]/div[1] [[text]] Dear\\xa0 [[bbox]] x=230 y=378 width=751 height=18 [[attributes]] dir='ltr' data-setdir='...ea91-4030' [[children]] span \\n(uid = 27ae19c7-8f93-499e) [[tag]] span [[xpath]] /html/body/div[1]/div...div[1]/div[1]/span [[text]] Team, [[bbox]] x=230 y=360 width=95.2 height=17 [[attributes]] data-webtasks-id='27...8f93-499e' \\n(uid = 40d97e2a-ad43-48d2) [[tag]] div [[xpath]] /html/body/div[1]/div...div[1]/div/div/div [[bbox]] x=194 y=111 width=839 height=538 [[attributes]] role='main' data-test-id='... P_ZzJed p_R ' [[children]] ul div \\n(uid = 89c6ffef-8b3d-47df) [[tag]] a [[xpath]] /html/body/div[1]/div...nav/div/div[1]/a [[text]] Compose [[bbox]] x=16 y=127 width=160 height=36 [[attributes]] tabindex='20' data-iskeyn...dTOHS_28Otf4' \\n(uid = 4e40b885-e9c4-4991) [[tag]] div [[xpath]] /html/body/div[1]/div...div/div/div[2]/div [[bbox]] x=259.2 y=127 width=673.2 height=36 [[attributes]] data-webtasks-id='4e... p_s s_3v1qi' [[children]] ul \\n(uid = 8f607ff1-e1d3-465c) [[tag]] div [[xpath]] /html/body/div[1]/div...1]/div/div[2]/div [[bbox]] x=0 y=111 width=1366 height=538 [[attributes]] data-test-id='content-below... o_h s_1HCsWR' [[children]] div span \\n(uid = a8f50ff6-5452-4a71) [[tag]] ul [[xpath]] /html/body/div[1]/div...div/div[2]/div/ul [[bbox]] x=259.2 y=127 width=673.2 height=36 [[attributes]] data-webtasks-id='a8f... o_A K_3qWeB' [[children]] li li \\n(uid = a3331817-000e-4fb0) [[tag]] input [[xpath]] /html/body/div[1]/...2]/div/div/input[1] [[bbox]] x=423.5 y=131 width=508.9 height=28 [[attributes]] aria-label='' placeholder='' value='... p_a L_0 B_0' \\n(uid = 8307c2c0-ae38-43b7) [[tag]] button [[xpath]] /html/body/div[1]/div...div[2]/div/div/button [[text]] Cc / Bcc [[bbox]] x=940.3 y=137 width=52.7 height=17 [[attributes]] tabindex='-1' type='button'...S2U q_T r_P'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = weblinx_dataset['train']\n",
    "def myfind(x):\n",
    "    if isinstance(x,str):\n",
    "        if x.find('resume')!=-1:\n",
    "            #print(x)\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "job = train_dataset.filter(myfind,input_columns='utterances')\n",
    "print(job)\n",
    "job['candidates'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 176231.26it/s]\n",
      "100%|██████████| 130/130 [00:00<00:00, 2800.54it/s]\n",
      "100%|██████████| 225/225 [00:00<00:00, 19649.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"PrunaAI/McGill-NLP-Llama-3-8B-Web-HQQ-4bit-smashed\"\n",
    "#config = BitsAndBytesConfig(\n",
    "    #load_in_4bit=True,\n",
    "\n",
    "    #load_in_8bit=True,\n",
    "    \n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #)\n",
    "\n",
    "\"\"\"\n",
    "model = AutoModel.from_pretrained(model_name,\n",
    "                          #quantization_config=GPTQConfig(bits=4, disable_exllama=False),\n",
    "                          #load_in_8bit=True,\n",
    "                          cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\"\"\"\n",
    "#https://huggingface.co/docs/transformers/main/en/quantization/hqq\n",
    "try:\n",
    " model_hqq = HQQModelForCausalLM.from_quantized(model_name, device_map='auto',cache_dir=cache_dir).half()#.to(torch.bfloat16) #.half()\n",
    "except: \n",
    " model_hqq = AutoHQQHFModel.from_quantized(model_name,cache_dir=cache_dir)\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.eval()\n",
    "#input_ids = tokenizer(\"create an html apply button?\", return_tensors='pt').to('cuda')[\"input_ids\"]\n",
    "#outputs = model.generate(input_ids, max_new_tokens=216)\n",
    "#tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): HQQLinear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): HQQLinear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): HQQLinear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): HQQLinear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): HQQLinear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): HQQLinear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): HQQLinear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_hqq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{\n",
    "  \"train_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.00015\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"zero_optimization\": true\n",
    "}\n",
    "\"\"\"\n",
    "#os.environ['DS_SKIP_CUDA_CHECK'] = '1'\n",
    "\n",
    "\"\"\"\n",
    "  \"optimizer\": {\n",
    "      \"type\": \"AdamW\",\n",
    "      \"params\": {\n",
    "        \"lr\": 2e-5,\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"bias_correction\": True\n",
    "      }\n",
    "    },\n",
    "\n",
    "\"\"\"\n",
    "ds_config = {\n",
    "    \"train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "       \n",
    "  \n",
    "   \"\"\" \"quantize_training\": {\n",
    "      \"enabled\": True,\n",
    "      \"quantize_verbose\": True,\n",
    "      \"quantizer_kernel\": True,\n",
    "      \"quantize-algo\": {\n",
    "        \"q_type\": \"symmetric\"\n",
    "      },\n",
    "      \"quantize_bits\": {\n",
    "        \"start_bits\": 4,\n",
    "        \"target_bits\": 2\n",
    "      },\n",
    "      \"quantize_schedule\": {\n",
    "        \"quantize_period\": 400,\n",
    "        \"schedule_offset\": 0\n",
    "      },\n",
    "      #\"quantize_groups\": 8,\n",
    "    },\"\"\"\n",
    "  \n",
    "    \n",
    "    \"bf16\": {\n",
    "        \"enabled\": False,\n",
    "        #\"initial_scale_power\": 16,\n",
    "\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "      \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "          \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \n",
    "    \n",
    "    },\n",
    "    \"zero_force_ds_cpu_optimizer\": False,\n",
    "   \n",
    "   \n",
    "}\n",
    "\"\"\"\n",
    "\"activation_checkpointing\": {\n",
    "    \"partition_activations\": True,\n",
    "    \"cpu_checkpointing\": True\n",
    "},\n",
    "    \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "          \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "\"\"\"\n",
    "#os.environ[\"PATH\"] = '/usr/bin/g++'\n",
    "#os.environ[\"DEFAULT_TORCH_EXTENSION_PATH \"] = '/proj/ciptmp/ix05ogym/tmp'\n",
    "#os.environ[\"TMPDIR\"]=\"/proj/ciptmp/ix05ogym/tmp\"\n",
    "# Move the optimizer instance to the GPU\n",
    "#optimizer = optimizer.to(\"cuda\")\n",
    "\n",
    "#op = torch.optim.Adam\n",
    "\"\"\"\n",
    "model, optimizer, _, _ = deepspeed.initialize(\n",
    "    model=model_hqq,\n",
    "    model_parameters=model_hqq.parameters(),\n",
    "    #optimizer=torch.optim.Adam, #deepspeed.ops.adam.DeepSpeedCPUAdam\n",
    "    config=ds_config\n",
    "    \n",
    ")\n",
    "\"\"\"\n",
    "model = model_hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 1859.18it/s]\n"
     ]
    }
   ],
   "source": [
    "snapshot_download(\"McGill-NLP/WebLINX\", repo_type=\"dataset\", allow_patterns=\"templates/*\",cache_dir=cache_dir)\n",
    "valid = datasets.load_dataset(\"McGill-NLP/WebLINX\", split=\"validation[:5]\",cache_dir=cache_dir)\n",
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "{clean_html}\n",
    "You will find above the HTML elements that are available for the current webpage.\n",
    "You are an AI assistant with a deep understanding of HTML \n",
    "and you must predict actions based on a user request, which will be executed. \n",
    "Use one of the following, replacing [] with an appropriate value: change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker=\"navigator\", utterance=[str]) ; scroll(x=[int], y=[int]) ; \n",
    "submit(uid=[str]) ; text_input(text=[str], uid=[str]) ;\n",
    "The user's first and last 4 utterances are:\n",
    "{utterances} ;\n",
    "Viewport size: {viewport}\n",
    "Only the last 5 turns are provided.\n",
    "Here are the top candidates for this turn:\n",
    "{candidates}\n",
    "\n",
    "<</SYS>>[/INST]\n",
    "{action_history}\n",
    "Please select the best action using the correct format, do not provide any other information or explanation.\n",
    "[/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "{clean_html}\n",
    "You will find above the HTML elements that are available for the current webpage.\n",
    "You are an AI assistant with a deep understanding of HTML \n",
    "and you must predict actions based on a user request, which will be executed. \n",
    "Use one of the following, replacing [] with an appropriate value: change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker=\"navigator\", utterance=[str]) ; scroll(x=[int], y=[int]) ; \n",
    "submit(uid=[str]) ; text_input(text=[str], uid=[str]) ;\n",
    "The user's first and last 4 utterances are:\n",
    "{utterances} ;\n",
    "Viewport size: {viewport}\n",
    "Only the last 5 turns are provided.\n",
    "Here are the top candidates for this turn:\n",
    "{candidates}\n",
    "\n",
    "<</SYS>>[/INST]\n",
    "{action_history}\n",
    "Please select the best action using the correct format, do not provide any other information or explanation.\n",
    "[/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(uid: 0) [[tag]] input [[attribute]] data-testid=\"textinput\"id=\"email\"required=\"\"type=\"email\"autocomplete=\"email\"name=\"email\"class=\"sc-bsgijijskkmm\" (uid: 1) [[tag]] input [[attribute]] data-testid=\"firstname\"id=\"firstname\"required=\"\"name=\"firstname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 2) [[tag]] input [[attribute]] data-testid=\"lastname\"id=\"lastname\"required=\"\"name=\"lastname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 3) [[tag]] input [[attribute]] accept=\".doc,.docx,.pdf,.rtf,.txt\"type=\"file\"autocomplete=\"off\"tabindex=\"-1\"class=\"fileuploader___styledinput-sc-e2002db3-0 (uid: 4) [[tag]] button [[attribute]] type=\"submit\"class=\"sc-jotjnafnrczm\">apply '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "i=1\n",
    "\n",
    "l = mydata[mydata['text']==mydata['text'][i]]['element'].tolist()\n",
    "s=''\n",
    "for j in range(len(l)):\n",
    "    p = l[j].split()\n",
    "    pp = ''.join(p[1:-1])\n",
    "    s += f\"(uid: {j}) [[tag]] {p[0][1:]} [[attribute]] {pp} \"\n",
    "    \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "*<input data-testid=\"textinput\" id=\"email\" required=\"\" type=\"email\" autocomplete=\"email\" name=\"email\" class=\"sc-bsgiji jskkmm\" value=\"\">*<input data-testid=\"firstname\" id=\"firstname\" required=\"\" name=\"firstname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\">*<input data-testid=\"lastname\" id=\"lastname\" required=\"\" name=\"lastname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\">documents:.doc, .docx, .pdf, .rtf, .txt<input accept=\".doc, .docx, .pdf, .rtf, .txt\" type=\"file\" autocomplete=\"off\" tabindex=\"-1\" class=\"fileuploader___styledinput-sc-e2002db3-0 djcsgj\">cv*<input accept=\".doc, .docx, .pdf, .rtf, .txt\" type=\"file\" autocomplete=\"off\" tabindex=\"-1\" class=\"fileuploader___styledinput-sc-e2002db3-0 djcsgj\">cover letteroptionalterms & conditionsprivacy policyapply now<button type=\"submit\" class=\"sc-jotjna fnrczm\">apply now</button>\n",
      "You will find above the HTML elements that are available for the current webpage.\n",
      "You are an AI assistant with a deep understanding of HTML \n",
      "and you must predict actions based on a user request, which will be executed. \n",
      "Use one of the following, replacing [] with an appropriate value: change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker=\"navigator\", utterance=[str]) ; scroll(x=[int], y=[int]) ; \n",
      "submit(uid=[str]) ; text_input(text=[str], uid=[str]) ;\n",
      "The user's first and last 4 utterances are:\n",
      "here is my email: majid@gmail.com and my password: majidpassword please fill the correct input and submit ;\n",
      "Only the last 5 turns are provided.\n",
      "Here are the top candidates for this turn:\n",
      "(uid: 0) <input data-testid=\"textinput\" id=\"email\" required=\"\" type=\"email\" autocomplete=\"email\" name=\"email\" class=\"sc-bsgiji jskkmm\" value=\"\"> (uid: 1) <input data-testid=\"firstname\" id=\"firstname\" required=\"\" name=\"firstname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\"> (uid: 2) <input data-testid=\"lastname\" id=\"lastname\" required=\"\" name=\"lastname\" type=\"text\" class=\"sc-kilemz pddck\" value=\"\"> (uid: 3) <input accept=\".doc, .docx, .pdf, .rtf, .txt\" type=\"file\" autocomplete=\"off\" tabindex=\"-1\" class=\"fileuploader___styledinput-sc-e2002db3-0 djcsgj\"> (uid: 4) <button type=\"submit\" class=\"sc-jotjna fnrczm\">apply now</button> \n",
      "\n",
      "<</SYS>>[/INST]\n",
      "\n",
      "Please select the best action using the correct format, do not provide any other information or explanation.\n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "d = {\"clean_html\":mydata['text'][i],\n",
    " \"utterances\":\"here is my email: majid@gmail.com and my password: majidpassword please fill the correct input and submit\",\n",
    " \"candidates\":s,\n",
    " \"action_history\":\"\"\n",
    " }\n",
    "mytemplate = \"\"\"<s>[INST] <<SYS>>\n",
    "{clean_html}\n",
    "You will find above the HTML elements that are available for the current webpage.\n",
    "You are an AI assistant with a deep understanding of HTML \n",
    "and you must predict actions based on a user request, which will be executed. \n",
    "Use one of the following, replacing [] with an appropriate value: change(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker=\"navigator\", utterance=[str]) ; scroll(x=[int], y=[int]) ; \n",
    "submit(uid=[str]) ; text_input(text=[str], uid=[str]) ;\n",
    "The user's first and last 4 utterances are:\n",
    "{utterances} ;\n",
    "Only the last 5 turns are provided.\n",
    "Here are the top candidates for this turn:\n",
    "{candidates}\n",
    "\n",
    "<</SYS>>[/INST]\n",
    "{action_history}\n",
    "Please select the best action using the correct format, do not provide any other information or explanation.\n",
    "[/INST]\"\"\"\n",
    "\n",
    "state = mytemplate.format(**d)\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "realstate = template.format(**valid[0])\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state = template.format(**valid[0])\n",
    "#print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the third uid? (uid: 0) [[tag]] input [[attribute]] data-testid=\"textinput\"id=\"email\"required=\"\"type=\"email\"autocomplete=\"email\"name=\"email\"class=\"sc-bsgijijskkmm\" (uid: 1) [[tag]] input [[attribute]] data-testid=\"firstname\"id=\"firstname\"required=\"\"name=\"firstname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 2) [[tag]] input [[attribute]] data-testid=\"lastname\"id=\"lastname\"required=\"\"name=\"lastname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 3) [[tag]] input [[attribute]] accept=\".doc,.docx,.pdf,.rtf,.txt\"type=\"file\"autocomplete=\"off\"tabindex=\"-1\"class=\"fileuploader___styledinput-sc-e2002db3-0 (uid: 4) [[tag]] button [[attribute]] type=\"submit\"class=\"sc-jotjnafnrczm\">apply '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy =f\"what is the third uid? {s}\"\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[12840,   374,   279,  4948, 14959,    30,   320,  2480,    25,   220,\n",
      "            15,     8,  4416,  4681,  5163,  1988,  4416,  9294,  5163,   828,\n",
      "         90352,   429,  1342,  1379, 29800,   429,  2386,     1,  6413,  8573,\n",
      "          1337,   429,  2386,     1, 57614,   429,  2386, 32586,   429,  2386,\n",
      "         31508,   429,  2445, 57530,    70, 35973,  2580, 19747,  3906,     1,\n",
      "           320,  2480,    25,   220,    16,     8,  4416,  4681,  5163,  1988,\n",
      "          4416,  9294,  5163,   828, 90352,   429, 29525, 29800,   429, 29525,\n",
      "             1,  6413,  8573,   609,   429, 29525, 45570,   429,  1342, 31508,\n",
      "           429,  2445, 12934,   458,    76, 94869,   634,   377,     1,   320,\n",
      "          2480,    25,   220,    17,     8,  4416,  4681,  5163,  1988,  4416,\n",
      "          9294,  5163,   828, 90352,   429, 30306, 29800,   429, 30306,     1,\n",
      "          6413,  8573,   609,   429, 30306, 45570,   429,  1342, 31508,   429,\n",
      "          2445, 12934,   458,    76, 94869,   634,   377,     1,   320,  2480,\n",
      "            25,   220,    18,     8,  4416,  4681,  5163,  1988,  4416,  9294,\n",
      "          5163,  4287, 36326,  5349, 17974,  5349,    87, 17974, 12091, 17974,\n",
      "          3423,    69, 17974,  8754, 45570,   429,  1213,     1, 57614,   429,\n",
      "          1885,     1,  6323,  1275, 24900,    16, 31508,   429,  1213, 87890,\n",
      "          6101, 23696,  1379, 31419,  5773,  1049,    17,  2042,    18,    12,\n",
      "            15,   320,  2480,    25,   220,    19,     8,  4416,  4681,  5163,\n",
      "          3215,  4416,  9294,  5163,   955,   429,  6081, 31508,   429,  2445,\n",
      "         13636,   354, 94908,  2642,    77,  1310, 32589,   760, 10492,   220]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "torch.Size([1, 210])\n"
     ]
    }
   ],
   "source": [
    "tokenized_state = tokenizer(yy,return_tensors='pt',truncation=True,max_length=512).to('cuda')#.to(torch.bfloat16)\n",
    "print(tokenized_state)\n",
    "print(tokenized_state['input_ids'].shape)\n",
    "#model = model.eval()\n",
    "#with torch.no_grad():\n",
    "#    o=model(**tokenized_state)\n",
    "    #del tokenized_state\n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "#o\n",
    "#print(\"Action:\", out['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =o.logits.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "m = y.argmax(2)\n",
    "print(m.shape)\n",
    "pp=tokenizer.decode(m[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|> the difference of theawns? The\n"
     ]
    }
   ],
   "source": [
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the third uid? (uid: 0) [[tag]] input [[attribute]] data-testid=\"textinput\"id=\"email\"required=\"\"type=\"email\"autocomplete=\"email\"name=\"email\"class=\"sc-bsgijijskkmm\" (uid: 1) [[tag]] input [[attribute]] data-testid=\"firstname\"id=\"firstname\"required=\"\"name=\"firstname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 2) [[tag]] input [[attribute]] data-testid=\"lastname\"id=\"lastname\"required=\"\"name=\"lastname\"type=\"text\"class=\"sc-kilemzpddck\" (uid: 3) [[tag]] input [[attribute]] accept=\".doc,.docx,.pdf,.rtf,.txt\"type=\"file\"autocomplete=\"off\"tabindex=\"-1\"class=\"fileuploader___styledinput-sc-e2002db3-0 (uid: 4) [[tag]] button [[attribute]] type=\"submit\"class=\"sc-jotjnafnrczm\">apply  [[children]] span \n",
      "(uid: 5) [[tag]] div [[attribute]] data-webtasks-id=\"5\" class=\"sc-1kfofo2-0 eQYjR\" [[children]] div \n",
      "(uid: 6) [[tag]] div [[attribute]] data-webtasks-id=\"6\" class=\"sc-1kfofo2-0 eQYjR\" [[children]] div \n",
      "(uid: 7) [[tag]] div [[attribute]] data-webtasks-id=\"7\" class=\"sc-1kfofo2-0 eQYjR\" [[children]] div \n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(tokenized_state['input_ids'], max_new_tokens=128)\n",
    "o = tokenizer.decode(outputs[0])\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Input length of input_ids is 210, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "agent = pipeline(\"\",model=model,tokenizer=tokenizer,device='cuda', torch_dtype='auto')\n",
    "out = agent(yy, return_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ' [['}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
