{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPYJUV9Jc9j"
      },
      "source": [
        "# Question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "way3SxOsJc9l"
      },
      "source": [
        "Question answering tasks return an answer given a question. If you've ever asked a virtual assistant like Alexa, Siri or Google what the weather is, then you've used a question answering model before. There are two common types of question answering tasks:\n",
        "\n",
        "- Extractive: extract the answer from the given context.\n",
        "- Abstractive: generate an answer from the context that correctly answers the question.\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [SQuAD](https://huggingface.co/datasets/squad) dataset for extractive question answering.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "<Tip>\n",
        "The task illustrated in this tutorial is supported by the following model architectures:\n",
        "\n",
        "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
        "\n",
        "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [LXMERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lxmert), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [Nystr√∂mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [Splinter](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/splinter), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
        "\n",
        "\n",
        "<!--End of the generated tip-->\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate\n",
        "```\n",
        "\n",
        "We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2XGBChJc9m"
      },
      "source": [
        "## Load SQuAD dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CySLoVz8Jc9m"
      },
      "source": [
        "Start by loading a smaller subset of the SQuAD dataset from the ü§ó Datasets library. This'll give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y4zPoF5kJc9m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "cache_dir = '/proj/ciptmp/ix05ogym/.cache/'\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\",streaming=False,cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8N5xSwVJc9n"
      },
      "source": [
        "Split the dataset's `train` split into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T_enxbNAJc9n"
      },
      "outputs": [],
      "source": [
        "squad = squad.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHApoP-Jc9n"
      },
      "source": [
        "Then take a look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PVBMY8ZNJc9n",
        "outputId": "69fc988d-d8e0-4760-8a03-6030722c2d59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56ce66e4aab44d1400b8875d',\n",
              " 'title': 'Solar_energy',\n",
              " 'context': 'Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.',\n",
              " 'question': 'In all the different CSP systems, concentrated sunlight is used to heat what?',\n",
              " 'answers': {'text': ['a working fluid'], 'answer_start': [491]}}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9m9-CCJc9n"
      },
      "source": [
        "There are several important fields here:\n",
        "\n",
        "- `answers`: the starting location of the answer token and the answer text.\n",
        "- `context`: background information from which the model needs to extract the answer.\n",
        "- `question`: the question a model should answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBwSgm7VJc9o"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF3e_rh1Jc9o"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to process the `question` and `context` fields:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8ClzKBVqJc9o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, albertfor\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\",cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iqA9RnQJc9o"
      },
      "source": [
        "There are a few preprocessing steps particular to question answering tasks you should be aware of:\n",
        "\n",
        "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
        "2. Next, map the start and end positions of the answer to the original `context` by setting\n",
        "   `return_offset_mapping=True`.\n",
        "3. With the mapping in hand, now you can find the start and end tokens of the answer. Use the [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) method to\n",
        "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
        "\n",
        "Here is how you can create a function to truncate and map the start and end tokens of the `answer` to the `context`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "gTOrbxUWJc9o"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs['id'] = examples['id']\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGMl2sDJc9p"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ü§ó Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once. Remove any columns you don't need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "EoY7PK4sJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:00<00:00, 5743.97 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 5551.44 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['id', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_squad['train'][0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMgkX807Jc9p"
      },
      "source": [
        "Now create a batch of examples using [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator). Unlike other data collators in ü§ó Transformers, the [DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) does not apply any additional preprocessing such as padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "q4mzMRuuJc9p"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb5kV6LCJc9p"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from unsloth import FastLanguageModel , is_bfloat16_supported\\nmodel_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\\nmax_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\\n\\nmodel , tokenizer= FastLanguageModel.from_pretrained(model_name ,\\n\\n                                                      load_in_4bit=True,\\n                                                      \\n                                                      max_seq_length=max_seq_length,\\n                                                      cache_dir=\\'/proj/ciptmp/ix05ogym/.cache/\\',\\n                                                      \\n                                                      )\\n\\nmodel = FastLanguageModel.get_peft_model( model,\\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\\n    lora_alpha = 16,\\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\\n\\n)'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from unsloth import FastLanguageModel , is_bfloat16_supported\n",
        "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n",
        "\n",
        "model , tokenizer= FastLanguageModel.from_pretrained(model_name ,\n",
        "\n",
        "                                                      load_in_4bit=True,\n",
        "                                                      \n",
        "                                                      max_seq_length=max_seq_length,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      \n",
        "                                                      )\n",
        "\n",
        "model = FastLanguageModel.get_peft_model( model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJfDX8OBJc9p"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForQuestionAnswering](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Csc0G1vmJc9p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No ROCm runtime is found, using ROCM_HOME='/usr'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AlbertForQuestionAnswering(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (activation): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n",
            "trainable params: 99,842 || all params: 11,194,372 || trainable%: 0.8919\n",
            "44785680\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#from unsloth import FastLanguageModel , is_bfloat16_supported\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering,BitsAndBytesConfig\n",
        "from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\n",
        "print(torch.cuda.is_available())\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "#\"albert/albert-base-v2\"\n",
        "model_name = \"albert/albert-base-v2\"#\"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name ,\n",
        "                                                      #attn_implementation=\"flash_attention_2\",\n",
        "                                                      #quantization_config=config,\n",
        "                                                      #low_cpu_mem_usage=True,\n",
        "                                                      cache_dir='/proj/ciptmp/ix05ogym/.cache/',\n",
        "                                                      #device_map=\"auto\",\n",
        "                                                      \n",
        "                                                      )\n",
        "print(model)\n",
        "\n",
        "#model = prepare_model_for_kbit_training(model,use_gradient_checkpointing=False)\n",
        "loraconfig = LoraConfig(r=16,target_modules=['query','key','value','dense'],task_type='QUESTION_ANS')\n",
        "model = get_peft_model(model, loraconfig)\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "print(model.get_memory_footprint())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.lora_B.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.lora_A.default.weight\n",
            "base_model.model.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.lora_B.default.weight\n",
            "base_model.model.qa_outputs.modules_to_save.default.weight\n",
            "base_model.model.qa_outputs.modules_to_save.default.bias\n"
          ]
        }
      ],
      "source": [
        "for n,p in model.named_parameters():\n",
        "    if p.requires_grad==True:\n",
        "        print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRHUxiR_Jc9p"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. You'll push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).\n",
        "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, and data collator.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def var_dump(obj, indent=0):\n",
        "    spacing = '  ' * indent\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            print(f'{spacing}{key}:')\n",
        "            var_dump(value, indent + 1)\n",
        "    elif isinstance(obj, (list, tuple, set)):\n",
        "        for idx, item in enumerate(obj):\n",
        "            print(f'{spacing}[{idx}]:')\n",
        "            var_dump(item, indent + 1)\n",
        "    elif hasattr(obj, '__dict__'):\n",
        "        print(f'{spacing}{obj.__class__.__name__}:')\n",
        "        var_dump(vars(obj), indent + 1)\n",
        "    else:\n",
        "        print(f'{spacing}{obj}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "st =squad['test']\n",
        "d = tokenized_squad['test']\n",
        "model=model.cuda()\n",
        "o = trainer.predict(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = []\n",
        "for i in range(len(d['input_ids'])):\n",
        "    x = d['input_ids'][i][o1[i]:o2[i]]\n",
        "    x = tokenizer.decode(x)\n",
        "    pp =0.0\n",
        "    if x == '':\n",
        "        pp = 1.0\n",
        "        x = ' '\n",
        "\n",
        "    preds.append({'prediction_text':x,'id':d['id'][i],'no_answer_probability': pp})\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prediction_text': ' ',\n",
              " 'id': '573392e24776f41900660d9e',\n",
              " 'no_answer_probability': 1.0}"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56cd7ab462d2951400fa660d\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': '56cd7ab462d2951400fa660d',\n",
              " 'title': 'IPod',\n",
              " 'context': 'Besides earning a reputation as a respected entertainment device, the iPod has also been accepted as a business device. Government departments, major institutions and international organisations have turned to the iPod line as a delivery mechanism for business communication and training, such as the Royal and Western Infirmaries in Glasgow, Scotland, where iPods are used to train new staff.',\n",
              " 'question': 'Where is Royal and Western Infirmaries located?',\n",
              " 'answers': {'text': ['Glasgow, Scotland'], 'answer_start': [334]}}"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "o1 = o.predictions[0].argmax(1)\n",
        "o2 = o.predictions[1].argmax(1)\n",
        "\n",
        "\n",
        "ro1 = o.label_ids[0]\n",
        "ro2 = o.label_ids[1]\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "#em = torchmetrics.classification.MulticlassExactMatch(num_classes =384)\n",
        "#em(torch.tensor(o1),torch.tensor(ro1))\n",
        "\n",
        "#torchmetrics.text.SQuAD(preds =  ,target = )\n",
        "print(d[0]['id'])\n",
        "st[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'prediction_text': ' ', 'id': '56cfbab4234ae51400d9bf1d', 'no_answer_probability': 1.0} {'answers': {'text': ['1790'], 'answer_start': [549]}, 'id': '56cfbab4234ae51400d9bf1d'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'exact': 0.0,\n",
              " 'f1': 25.23088023088023,\n",
              " 'total': 21,\n",
              " 'HasAns_exact': 0.0,\n",
              " 'HasAns_f1': 25.23088023088023,\n",
              " 'HasAns_total': 21,\n",
              " 'best_exact': 0.0,\n",
              " 'best_exact_thresh': 0.0,\n",
              " 'best_f1': 25.23088023088023,\n",
              " 'best_f1_thresh': 0.0}"
            ]
          },
          "execution_count": 227,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mysq = torchmetrics.text.SQuAD()\n",
        "#mysq(preds,st)\n",
        "import evaluate\n",
        "mm =evaluate.load('squad_v2')\n",
        "def transform_id(example):\n",
        "    example['id'] = example['id']\n",
        "    #example['answers']['text']:str\n",
        "    #print(example['answers']['text'])\n",
        "    example['answers']['text'] = [example['answers']['text'][0].lower()]\n",
        "    \n",
        "    #print(type(example['id']))# Assuming you want to extract the first character for illustration\n",
        "    return example\n",
        "\n",
        "st2 = st.select_columns(['answers','id'])\n",
        "st2 = st2.map(transform_id)\n",
        "st2 = st2.to_list()\n",
        "j=20\n",
        "ssss = mm.compute(predictions=preds[0:j+1],references=st2[0:j+1])\n",
        "print(preds[j],st2[j])\n",
        "#print(preds)\n",
        "#print(st2)\n",
        "ssss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glasgow, ['Glasgow, Scotland']\n",
            "as ['Aspiro']\n",
            "go set a watch ['Go Set a Watchman']\n",
            "ten languages. in the years since, it has sold more than 30 million copies and been translated into more than ['more than 40']\n",
            " ['its own Great Purge']\n",
            "montana ( ['from the Spanish word monta√±a']\n",
            "adrian ['Adrian Gallagher']\n",
            "tudor ['brownstone rowhouses']\n",
            "2011 glastonbury ['the 2011 Glastonbury Festival']\n",
            " ['1862']\n",
            "political killing ['political killings']\n",
            "4 ['44']\n",
            "ordon ['Ordon Village']\n",
            "november ['2005']\n",
            " ['The word genocide is the combination of the Greek prefix geno- (meaning tribe or race) and caedere (the Latin word for to kill).']\n",
            " ['10,000']\n",
            "the giving of ['four-hour program called The Giving of Love']\n",
            "20% gramicidin and 80% tyroc ['wounds and ulcers']\n",
            "april ['2013']\n",
            "giselle knowles-car ['Beyonc√© Giselle Knowles-Carter']\n",
            " ['1790']\n",
            "$30 ['73 million']\n",
            "rondo op. ['Rondo Op. 1']\n",
            " ['Madonna']\n",
            "friederike ['Friederike M√ºller']\n",
            "pre-professional ['Department of Pre-Professional Studies']\n",
            "l-targeting (z-targeting on the wii), a system that allows the player to keep link's view focused on an enemy or important object while moving and attacking. link can walk, run, and attack, and will automatically jump when running off of or reaching for a ledge.[c] link uses a sword and shield in combat, complemented with secondary weapons and items, including a bow and arrows, a boomerang, bombs, and the claws ['Clawshot']\n",
            "3,8 ['approximately 3,000 EJ per year']\n",
            " ['DRM']\n",
            " ['women']\n",
            "30 ['30 million']\n",
            "mariah ['six']\n",
            "zelda and she ['Zelda and Sheik']\n",
            "turrell v. wy ['Morris Rossabi']\n",
            "16 ['1626']\n",
            "apple ['Apple']\n",
            " ['40%']\n",
            "stephanie sig ['Stephanie Sigman']\n",
            "new york's roseland [\"New York's Roseland Ballroom\"]\n",
            "alexander ['Alexander Hamilton']\n",
            "$80 ['250 million']\n",
            " ['his conquest of Tibet']\n",
            "chopin ‚Äì the women behind the music ( ['Chopin ‚Äì The Women Behind The Music']\n",
            " ['iPod Shuffle']\n",
            "leon ['Leon Ulrich']\n",
            "10.7 billion yuan (approximately us$1.5 billion) had been donated by the chinese public. houston rockets center yao ['Yao Ming']\n",
            "williams f ['Williams F1']\n",
            "franz ober ['Franz Oberhauser']\n",
            "seven fryderyk had begun giving public concerts, and in 18 ['7']\n",
            "jay ['Women']\n",
            "$3.2 ['US$3.2 billion']\n",
            " ['105']\n",
            " ['Majorca']\n",
            "square d'orle [\"Square d'Orl√©ans\"]\n",
            "walter  ['Walter Lett']\n",
            " ['The London Fire Brigade']\n",
            "metlife stadium, the new yankee stadium, madison square garden, and citi ['MetLife Stadium, the new Yankee Stadium, Madison Square Garden, and Citi Field']\n",
            "time prediction method\" through collecting statistics on geomagnetism with tidal gravitational potential. using this method, they were said to have predicted the time of the 2008 sichuan earthquake with an accuracy of <unk>1 ['opportunities for researchers to retrofit data in order to model future earthquake predictions']\n",
            "griffin ['Nike']\n",
            " ['a cast of his left hand.']\n",
            "april 30, ['April 30, 2008']\n",
            "paragon of ['integrity']\n",
            "september ['2006']\n",
            "a.s.e. acker ['a U.S. inventor, engineer and solar energy pioneer']\n",
            "george ['George Washington']\n",
            "millard ['Millard Sheets']\n",
            "queens borough public ['Brooklyn Public Library']\n",
            "countess wodzin ['Countess Wodzi≈Ñska']\n",
            " ['tractors']\n",
            " ['sixth']\n",
            "itunes ['the iTunes Store']\n",
            " ['seven']\n",
            "penicillin and erythromy ['penicillin and erythromycin']\n",
            "daniel craig in his fourth performance as james bond, and christoph ['Daniel Craig']\n",
            "tom robinson's ['Jem and Scout']\n",
            "lucrezia florian ['Lucrezia Floriani']\n",
            "national park ['National Park Service']\n",
            "sequenced music instead. kondo later cited the lack of interactivity that comes with orchestral music as one of the main reasons for the decision. both six- and seven-track versions of the game's soundtrack were released on november 19, ['November 19, 2006']\n",
            " ['Nunchuk']\n",
            "southern ['southern Sichuan']\n",
            " ['1918']\n",
            "four ballades and four scherzos stand supreme\", and adds that \"the barcarolle op. ['the Barcarolle Op. 60']\n",
            "temporal lobe ['temporal lobe epilepsy']\n",
            "sasha ['Sasha Fierce for Der√©on']\n",
            "sichuan ['Sichuan area']\n",
            "1880 ['1880s']\n",
            "7 ['USDA 7b']\n",
            "788,000 yuan (us$11 ['788,000 yuan']\n",
            "fr. matthew ['Fr. Matthew Walsh']\n",
            " ['two']\n",
            " ['1626']\n",
            "new york ['The New York Times']\n",
            "16 ['1664']\n",
            "several ['before the doctor knows the exact identification of microorgansim']\n",
            "vinnie chi ['Vinnie Chieco']\n",
            "5 ['40,000-plus']\n",
            "obertilliach and lake altaus ['Ice Q Restaurant']\n",
            "1up.com, computer and video games, electronic gaming monthly, game informer, gamesradar, ign and the washington ['IGN and GameSpy']\n",
            " ['four']\n",
            "marie d'ago [\"Marie d'Agoult\"]\n",
            "d ['humid continental']\n",
            "zhengtong ['the Karmapa']\n",
            " ['very destructive']\n",
            "high definition music videos. beyonce's husband jay z acquired the parent company of tidal, aspiro, in the first quarter of 2015. including beyonce and jay-z, sixteen artist stakeholders (such as kanye west, rihanna, madonna, chris martin, nicki minaj and more) co-own tidal, with the majority owning a 3% equity stake. the idea of having an all artist owned streaming service was created by those involved to adapt to the increased demand for streaming within the current music industry, and to rival other streaming services such as spot ['music streaming service']\n",
            "martha argerich, vladimir ashkenazy, emanuel ax, evgeny kissin, murray perahia, maurizio pollini and krystian zimerman. the warsaw chopin ['The Warsaw Chopin Society']\n",
            "yonten gyat ['Yonten Gyatso']\n",
            "4 ['164.1']\n",
            "$1.58 ['$1.58 billion']\n",
            "12, ['8,448']\n",
            "ernst stavro blo ['Ernst Stavro Blofeld']\n",
            " ['conspiracy to commit genocide']\n",
            "rondo op. ['Rondo Op. 1.']\n",
            "angela merkel and nkosa ['the head of the G7 in Germany']\n",
            " ['The Internet']\n",
            "eastern border of the tibetan plateau and contains several faults. this earthquake ruptured at least two imbricate structures in longmen shan fault system, i.e. the beichuan fault and the guanxian‚Äìanxian ['Longmen Shan Fault System']\n",
            "mercedes-benz, volvo, nissan, toyota, alfa romeo, ferrari, acura, audi, honda, renault, infiniti and volkswagen. s ['Scion']\n",
            "hudson ['The Bronx River']\n",
            "tom ['that which is innocent and harmless']\n",
            "hundreds of years after ocarina of time and majora's mask, in an alternate timeline from the wind wake ['The Wind Waker']\n",
            "international standards<unk>. los angeles ['up to international standards']\n",
            " ['Taiwan Taoyuan International Airport']\n",
            "america the ['America the Beautiful']\n",
            "marseille ['Marseilles']\n",
            "ma ['Maadi, Egypt']\n",
            "1970 ['1970s']\n",
            "wojciech zywn ['Wojciech ≈ªywny']\n",
            "jay ['B.I.C.']\n",
            " ['25,000']\n",
            " ['234']\n",
            " ['fall of 2015']\n",
            "over  ['over 7,000']\n",
            "an antibiotic ['antibiotic target']\n",
            "c, the head of the privately-backed joint intelligence service, consisting of the recently merged mi5 and mi ['MI5 and MI6']\n",
            "the 7 subway ['the Second Avenue Subway']\n",
            "empirical therapy, a patient has proven or suspected infection, but the responsible microorganism is not yet unidentified. while the microorgainsim is being identified the doctor will usually administer the best choice of antibiotic that will be most active against the likely cause of infection usually a broad spectrum antibiotic. empirical therapy is usually initiated before the doctor knows the exact identification of microorgansim causing the infection as the identification process make take several ['broad spectrum antibiotic']\n",
            "day of the dead festival filmed in and around the zo ['the Day of the Dead festival']\n",
            "sandy hook elementary school ['Sandy Hook Elementary School shooting.']\n",
            "almost ['almost 80%']\n",
            " ['inland areas']\n",
            " ['1898']\n",
            " ['22 February 1810']\n",
            "luria‚Äìdel ['Luria‚ÄìDelbr√ºck']\n",
            " ['L-targeting']\n",
            "kornel micha<unk>owski and jim ['Op. 58']\n",
            "zhu yuanzhang then established the ming dynasty, ruling as the hongwu ['the Red Turban Rebellion']\n",
            "shadow ['Shadow Crystal']\n",
            " ['MTV']\n",
            " ['Milan']\n",
            "tumen ['T√ºmen Khan']\n",
            "hurricane ['Hurricane Sandy']\n",
            "superbugs\", now contribute to the emergence of diseases that were for a while well controlled. for example, emergent bacterial strains causing tuberculosis (tb) that are resistant to previously effective antibacterial treatments pose many therapeutic challenges. every year, nearly half a million new cases of multidrug-resistant tuberculosis (mdr-tb) are estimated to occur worldwide. for example, ndm- ['superbugs']\n",
            "book ['a close analysis by an alleged Chinese construction engineer']\n",
            " ['six shorter pieces']\n",
            "n ['rotational user inputs']\n",
            " ['1986']\n",
            "piano technique and ['piano technique and composition.']\n",
            "robert ['publishing his works and teaching piano to affluent students']\n",
            " ['2010']\n",
            "valldemo ['Valldemossa']\n",
            "forbes ['Forbes']\n",
            "mother's advice, \"to live life, to be inspired by things again\". during the break she and her ['her mother']\n",
            " ['1928']\n",
            "fragile hard drives. a 2005 survey conducted on the macintouch website found that the ipod line had an average failure rate of 13.7% (although they note that comments from respondents indicate that \"the true ipod failure rate may be lower than it appears\"). it concluded that some models were more durable than others. in particular, failure rates for ipods employing hard drives was usually above ['short life-span and fragile hard drives']\n",
            "tzu chi ['Tzu Chi Foundation']\n",
            "25 ['approximately 25 percent']\n",
            "1970 ['1970s']\n",
            "32.6 <unk> ['0.3']\n",
            "mariah carey's singing and her song \"vision of ['Vision of Love']\n",
            "1, ['1,700']\n",
            "michael r. ['Michael R. Bloomberg']\n",
            "hotel de ['Salle Pleyel and the Paris Conservatory']\n",
            " ['wider interpretation of genocide']\n",
            " ['autobiography']\n",
            "new york's [\"New York's Finest\"]\n",
            "manhattan neighborhood ['Manhattan Neighborhood Network']\n",
            "salonik chopin ['Chopin Family Parlour']\n",
            "preludes no. 4 in e minor and no. 6 in b minor were also played. the organist at the funeral was louis lefebure-wely. the funeral procession to pere lachaise cemetery, which included chopin's sister ludwika, was led by the aged prince adam czartoryski. the pallbearers included delacroix, franchomme, and camille pleyel. at the graveside, the funeral march from chopin's piano sonata no. [\"the Funeral March from Chopin's Piano Sonata No. 2\"]\n",
            "march ['2006']\n",
            "several ['several days']\n",
            " ['1835']\n",
            "374, ['374,176']\n",
            "felix ['Hiller']\n",
            "harlem ['the Harlem Renaissance']\n",
            "a ['a march']\n",
            "louis lefebure-we ['Louis Lef√©bure-W√©ly']\n",
            "3,5 ['3,577']\n",
            " ['Spanish']\n",
            "paris in ['Paris']\n",
            " ['division of bacterial cells']\n",
            "karmapa kar ['the Karmapa Kargyu']\n",
            "revel atlantic city's ovation [\"Revel Atlantic City's Ovation Hall\"]\n",
            "colin ['Colin Powell']\n",
            " ['human dignity and respect for others']\n",
            " ['Adam Mickiewicz']\n",
            "gamepro and eg ['GamePro and EGM']\n",
            " ['the Sun']\n",
            "1 ['120,000']\n",
            "saint mary's [\"Saint Mary's College\"]\n",
            "four months, aonuma's team managed to present realistic horseback riding,[l] which nintendo later revealed to the public with a trailer at electronic entertainment expo ['four']\n",
            "roosevelt ['Roosevelt Island']\n",
            "calpurnia and her neighbor miss maudie, both of whom are strong willed, independent, and protective. mayella ewell also has an influence; scout watches her destroy an innocent man in order to hide her desire for him. the female characters who comment the most on scout's lack of willingness to adhere to a more feminine role are also those who promote the most racist and classist points of view. for example, mrs. dubose chastises scout for not wearing a dress and camisole, and indicates she is ruining the family name by not doing so, in addition to insulting atticus' intentions to defend tom robinson. by balancing the masculine influences of atticus and jem with the feminine influences of calpurnia and miss maudie, one scholar writes, \"lee gradually demonstrates that scout is becoming a ['feminist']\n",
            "julian ['Julian Fontana']\n",
            " ['1950']\n",
            " ['first 80 sec.']\n",
            " ['21%']\n",
            "adam zamoy ['Adam Zamoyski']\n",
            "peter stuyves ['Peter Stuyvesant']\n",
            "the civil ['the Civil War']\n",
            " ['freedom']\n",
            " ['House of Der√©on collection']\n",
            "one-hundred millionth ipod, making it the biggest selling digital music player of all time. in april ['2007']\n",
            " ['51']\n",
            "tytus woyciechowski, jan nepomucen bia<unk>ob<unk>ocki, jan matuszynski and julian fontana; the latter two would become part of his paris milieu. he was friendly with members of warsaw's young artistic and intellectual world, including fontana, jozef bohdan zaleski and stefan witwicki. he was also attracted to the singing student konstancja g<unk>adk ['Piano Concerto No. 1 (in E minor)']\n",
            "1.94 ['1.94 million']\n",
            "12 december 1831 he mentioned in a letter to his friend woyciechowski that \"i have met rossini, cherubini, baillot, etc.<unk>also kalkbrenner. you would not believe how curious i was about herz, liszt, hiller, etc.\" liszt was in attendance at chopin's parisian debut on 26 february ['26 February 1832']\n",
            "staten island railway rapid transit system solely serves staten island, operating 24 hours a day. the port authority trans-hudson (path train) links midtown and lower manhattan to northeastern new jersey, primarily hoboken, jersey city, and newark. like the new york city subway, the path operates 24 hours a day; meaning three of the six rapid transit systems in the world which operate on 24-hour schedules are wholly or partly in new ['Copenhagen Metro']\n",
            "the overall size of the entire group. in addition to the numeric size of the targeted portion, its prominence within the group can be a useful consideration. if a specific part of the group is emblematic of the overall group, or is essential to its survival, that may support a finding that the part qualifies as substantial within the meaning of article ['absolute terms']\n",
            "december 13, ['December 13, 2013']\n",
            "ultra-comp ['touchscreen']\n",
            "december 2005 when british publication ngc magazine claimed that when a gamecube copy of twilight princess was played on the revolution, it would give the player the option of using the revolution controller. miyamoto confirmed the revolution controller-functionality in an interview with nintendo of europe and time reported this soon after. however, support for the wii controller did not make it into the gamecube release. at e3 ['E3 2006']\n",
            "warsaw ['Warsaw Conservatory']\n",
            "manchu qing dynasty invasion during the 18th ['Chinese writers of the early 20th century']\n",
            " ['Chinese history']\n",
            "firewire connection to the host computer was used to update songs or recharge the battery. the battery could also be charged with a power adapt ['power adapter']\n",
            "shadow ['Shadow Crystal']\n",
            " ['220 million']\n",
            " ['Warsaw']\n",
            "14 ['1407']\n",
            "stage presence and ['stage presence']\n",
            " ['2005']\n",
            " ['The Beat Goes On...']\n",
            "michael ['Michael Jackson']\n",
            "dominican ['Dominican Republic']\n",
            " ['England']\n",
            "jamie foxx, and eddie murphy playing a pop singer based on diana ['Shakira']\n",
            "national library of ['the National Library of Australia']\n",
            " ['8th']\n",
            " ['65']\n",
            "zhang juzhen ['Zhang Juzheng']\n",
            "new york's braves [\"New York's Bravest\"]\n",
            " ['piano']\n",
            "the bacterial growth ['bacterial growth phase']\n",
            "subway ['Subway Series']\n",
            "11 ['a million']\n",
            "june ['June 2013']\n",
            "di ['Dill']\n",
            "washington, d. ['President Obama']\n",
            "mongol ['the Mongols']\n",
            " ['A River Runs Through It']\n",
            "december 2005 when british publication ngc magazine claimed that when a gamecube copy of twilight princess was played on the revolution, it would give the player the option of using the revolution controller. miyamoto confirmed the revolution controller-functionality in an interview with nintendo of europe and time reported this soon after. however, support for the wii controller did not make it into the gamecube release. at e3 ['2006']\n",
            "polish ['Polish']\n",
            " ['five']\n",
            " ['Todt Hill']\n",
            "album ['the Album Era']\n",
            "20 musical selections from the game was available as a gamestop preorder bonus in the united states; it is included in all bundles in japan, europe, and ['Japan, Europe, and Australia']\n",
            " ['three']\n",
            "jon rubin ['Toshiba']\n",
            "69,180 known deaths including 68,636 in sichuan province; 18,498 people are listed as missing, and 374,176 injured, but these figures may further increase as more reports come in.[dated info] this estimate includes  ['tried to repair roads']\n",
            " ['1882']\n",
            "mazur ['intuitive']\n",
            "august 24, 16 ['August 24, 1673']\n",
            "god ['Godiva']\n",
            "penicillin and erythromy ['survive high doses of antibiotics']\n",
            " ['Bozeman Yellowstone International Airport']\n",
            "auguste cle ['radical political pursuits']\n",
            " ['a global issue']\n",
            " ['2006']\n",
            "los angeles ['Los Angeles Times']\n",
            "only ['their only child']\n",
            "3,8 ['approximately 3,850,000 exajoules (EJ) per year']\n",
            "zelazowa wol ['≈ªelazowa Wola']\n",
            "new years ceremonies and ['New Years ceremonies and prayers']\n",
            "john glen directed the living daylights and licence to kill in 1987 and ['Skyfall and Spectre']\n",
            " ['Scottish']\n",
            " ['April 2013']\n",
            " ['the ACC']\n",
            " ['200']\n",
            " ['the Stonewall Inn']\n",
            "dreamgirls (2006), and starring roles in the pink panther (2006) and obsessed (2009). her marriage to rapper jay ['Cadillac Records']\n",
            "$3 ['$300']\n",
            " ['Paris']\n",
            " ['imperial edicts']\n",
            " ['Miyamoto']\n",
            "september ['September 2015']\n",
            " ['Pyramid stage']\n",
            "revolutionary etude (op. 10, no. ['the Revolutionary √âtude']\n",
            "kelly rowland met latavia roberson while in an audition for an all-girl entertainment group. they were placed into a group with three other girls as girl's tyme, and rapped and danced on the talent show circuit in houston. after seeing the group, r&b producer arne fra ['Arne Frager']\n",
            " ['Baseball']\n",
            " ['New York City Health and Hospitals Corporation']\n",
            "11 acute care hospitals, ['five']\n",
            "felicien mallef ['F√©licien Mallefille']\n",
            " ['biological-physical']\n",
            "ernst stavro blo ['Ernst Stavro Blofeld']\n",
            "degs ['Degsi']\n",
            " ['30']\n",
            "uk, norway, and ['UK, Norway, and Belgium']\n",
            "raphael lem ['Raphael Lemkin']\n",
            "two grammy awards and is one of the best-selling singles of all time at around 8 million copies. the music video for \"single ladies (put a ring on it)\", which achieved fame for its intricate choreography and its deployment of jazz hands, was credited by the toronto star as having started the \"first major dance craze of both the new millennium and the internet\", triggering a number of parodies of the dance choreography and a legion of amateur imitators on youtube. in 2013, drake released a single titled \"girls love beyonce\", which featured an interpolation from destiny child's \"say my name\" and discussed his relationship with women. in january 2012, research scientist bryan lessard named scaptia beyonceae, a species of horse fly found in northern queensland, australia after beyonce due to the fly's unique golden hairs on its abdomen. in july ['July 2014']\n",
            " ['helmet']\n",
            "ganden phodrang regime by the 5th dalai ['the Ganden Phodrang']\n",
            "15 ['1565']\n",
            " ['Asians']\n",
            "baz luhr ['Baz Luhrmann']\n",
            " ['Z√≥calo and the Centro Hist√≥rico district']\n",
            "ponderosa pine, was selected by montana schoolchildren as the preferred state tree by an overwhelming majority in a referendum held in 1908. however, the legislature did not designate a state tree until 1949, when the montana federation of garden clubs, with the support of the state forester, lobbied for formal recognition. schoolchildren also chose the western meadowlar ['grizzly bear']\n",
            "franz ['Franz Liszt']\n",
            "photodermat ['fever and nausea']\n",
            " ['1972']\n",
            "tom ford's spring/summer 2011 fashion show. she was named \"world's most beautiful woman\" by people and the \"hottest female singer of all time\" by ['Complex']\n",
            " ['wooden roof-mounted water towers']\n",
            " ['five largest cities']\n",
            "4. ['4.6%']\n",
            "november 13 ['November 1378']\n",
            "karma tseten who styled himself as the tsangpa, \"the one of tsang\", and established his base of power at shigatse. the second successor of this first tsang king, karma phuntsok namgy ['The fourth Dalai Lama']\n",
            "pauline viardo ['Pauline Viardot']\n",
            "5 percent of the city's private sector jobs, 8.5 percent (us$3.8 billion) of its tax revenue, and 22 percent of the city's total wages, including an average salary of us$360, ['22']\n",
            " ['1881']\n",
            " ['Health care']\n",
            "danja ['Danjaq']\n",
            " ['25 times more']\n",
            "bergen ['Bergen']\n",
            " ['bacterial infection']\n",
            "16 ['1614']\n",
            " ['CEO']\n",
            "24 hours a ['8.4']\n",
            "3,850,000 exajoules (ej) per year. in 2002, this was more energy in one hour than the world used in one ['one year']\n",
            "10 ['megacity']\n",
            " ['Promise']\n",
            "microsoft ['Apple Macintosh and Microsoft Windows']\n",
            "kublai ['Kublai']\n",
            "level ['most serious']\n",
            "men in black. the following year, the group released their self-titled debut album, scoring their first major hit \"no, no, no\". the album established the group as a viable act in the music industry, with moderate sales and winning the group three soul train lady of soul awards for best r&b/soul album of the year, best r&b/soul or rap new artist, and best r&b/soul single for \"no, no, no\". the group released their multi-platinum second album the writing's on the wall in 1999. the record features some of the group's most widely known songs such as \"bills, bills, bills\", the group's first number-one single, \"jumpin' jumpin'\" and \"say my ['\"Say My Name\"']\n",
            "february ['February 6, 2016']\n",
            " ['29']\n",
            "dangerously in love ( ['2003']\n",
            "truman capo ['Truman Capote']\n",
            " ['Spectre']\n",
            " ['4th']\n",
            "farrah franklin and michelle williams. beyonce experienced ['depression']\n",
            "aac files rather than w ['WMA']\n",
            " ['Second Anglo-Dutch War']\n",
            "baruch ['Baruch College']\n",
            "the fifth karma ['fifth Karmapa']\n",
            "tang ['by foot or air']\n",
            "lifeandtimes. ['Lifeandtimes.com']\n",
            " ['Fifth generation']\n",
            "us$ ['$99']\n",
            "12.21 ['12.21 million']\n",
            "15 ['1524']\n",
            "destiny ['Destiny Fulfilled']\n",
            "cornel ['Cornel Wilde']\n",
            "paris salon ['Paris salons']\n",
            "the ['Prosecutor of the International Criminal Court']\n",
            "fanciful ['opera']\n",
            "departure and ['departure and return']\n",
            " ['83']\n",
            "15 ['1524']\n",
            "tytus woyciech ['Tytus Woyciechowski']\n",
            "march ['March 2006']\n",
            " ['New Yawk']\n",
            "sasha fierce, conceived during the making of her 2003 single \"crazy in love\", selling 482,000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number-one album in the us. the album featured the number-one song \"single ladies (put a ring on it)\" and the top-five songs \"if i were a boy\" and \"halo\". achieving the accomplishment of becoming her longest-running hot 100 single in her career, \"halo\"'s success in the us helped beyonce attain more top-ten singles on the list than any other woman during the 2000 ['2000s']\n",
            "december 31, ['2006']\n",
            "1311‚Äì13 ['1311‚Äì1320']\n",
            " ['Nine Eyes']\n",
            "147,0 ['147,040 square miles']\n",
            " ['his association with Sand began in earnest']\n",
            "30,000 tibetan prisoners and  ['200,000']\n",
            "12.4 ['12.4 million']\n",
            " ['Say My Name']\n",
            " ['Tonja Carter']\n",
            "nine dungeons<unk>large, contained areas where link battles enemies, collects items, and solves puzzles. link navigates these dungeons and fights a ['enemies']\n",
            " ['60 million']\n",
            "12 people from the state seismological bureau, ['12']\n",
            "oprah winfre ['Oprah Winfrey']\n",
            "greenwich ['Greenwich Village']\n",
            "ordon ['Ordon Village']\n",
            " ['120']\n",
            "60 out of ['60 out of 100']\n",
            "maurice schlesinger and camille pley ['popular 19th-century piano anthologies.']\n",
            "14 ['1434']\n",
            "raphael lem ['Winston Churchill']\n",
            "houston, ['Houston']\n",
            "dream ['Dreamgirls']\n",
            " ['67']\n",
            " ['the Wisconsinan glaciation']\n",
            "foxcon ['Foxconn']\n",
            "liu shao ['Liu Shaokun']\n",
            " ['Rome']\n",
            "38. ['38.4']\n",
            "silicon ['Silicon Alley']\n",
            "prince antoni radziwi<unk>, governor of the grand duchy of pose ['Prince Antoni Radziwi≈Ç≈Ç']\n",
            "carls ['the Lower Rhenish Music Festival']\n",
            " ['parabolic dish, trough and Scheffler reflectors']\n",
            "john lennon's \" ['Imagine']\n",
            "kane ['Kane Kramer']\n",
            "diamonds are ['Diamonds Are Forever']\n",
            "hearst ['Hearst Tower']\n",
            "demand a ['Catapult']\n",
            " ['1971']\n",
            " ['CEA']\n",
            " ['1983']\n",
            "agnez dereon, a respected seamstress. according to tina, the overall style of the line best reflects her and beyonce's taste and style. beyonce and her mother founded their family's company beyond ['grandmother, Agn√®z Der√©on']\n",
            " ['2014']\n",
            "the clinton bush haiti ['The Huffington Post']\n",
            "hector berlio ['Hector Berlioz, Franz Liszt, Ferdinand Hiller, Heinrich Heine, Eug√®ne Delacroix, and Alfred de Vigny']\n",
            "8 ['the borders between Tibet and China']\n",
            "the supreme court debate on california's proposition ['same sex marriage']\n",
            "$115 ['more than double her earnings']\n",
            "no, no, ['No, No, No']\n",
            "a ['a rally']\n",
            " ['24']\n",
            " ['sixth']\n",
            "giovanni da verrazzano, a florentine explorer in the service of the french crown, who sailed his ship la dauphine into new york harbor. he claimed the area for france and named it \"nouvelle angoule ['La Dauphine']\n",
            "saint bernadette soubi ['Saint Bernadette Soubirous']\n",
            " ['17']\n",
            "adverse side effects. side-effects range from mild to very serious depending on the antibiotics used, the microbial organisms targeted, and the individual patient. side effects may reflect the pharmacological or toxicological properties of the antibiotic or may involve hypersensitivity reactions or anaphylaxis. safety profiles of newer drugs are often not as well established as for those that have a long history of use. adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis. common side-effects include ['negative effects']\n",
            "political ['political insurrection']\n",
            " ['number four']\n",
            " ['March 2009']\n",
            "1 ['180,000']\n",
            "survivor foundation to provide transitional housing for victims in the houston area, to which beyonce contributed an initial $250,000. the foundation has since expanded to work with other charities in the city, and also provided relief following hurricane  ['Hurricane Ike']\n",
            "firewire connection to the host computer was used to update songs or recharge the battery. the battery could also be charged with a power adapt ['FireWire']\n",
            "poor white ['white']\n",
            " ['Knute Rockne']\n",
            "16th-century arab alchemist ['1872']\n",
            " ['Brooklyn']\n",
            " ['during filming.']\n",
            " ['fourth']\n",
            "7 brackets, with rates ranging from 1 percent to 6.9 ['7']\n",
            "nyse euronext, the operator of the new york stock exchange, took over the administration of the london interbank offered rate from the british bankers ['British Bankers Association']\n",
            "35 mm film ['Kodak 35 mm']\n",
            "irish and german ['Irish']\n",
            "milk ['Milk Famous']\n",
            "civil rights ['civil rights movement']\n",
            "ernst stavro blofeld. dave bautista was cast as mr. hinx after producers sought an actor with a background in contact sports. after casting berenice lim marlohe, a relative newcomer, as severine in skyfall, mendes consciously sought out a more experienced actor for the role of madeleine swann, ultimately casting lea seydoux in the role. monica bellucci joined the cast as lucia sciarra, becoming, at the age of ['fifty']\n",
            "150 to 300 ['150 to 300 watts per square meter or 3.5 to 7.0 kWh/m2 per day']\n",
            "oxen, horses, camels, sheep, fur products, medical herbs, tibetan incenses, thang ['Tibetan Buddhist']\n",
            "7 brackets, with rates ranging from 1 percent to 6.9 ['no']\n",
            "eunuch liu ['the Tibetan lamas']\n",
            "fall of ['2015']\n",
            "massachusetts ['Massachusetts v. Environmental Protection Agency']\n",
            "cold ['Coldplay']\n",
            " ['Chopin']\n",
            "13th century is a very recent construction.\" he writes that chinese writers of the early 20th century were of the view that tibet was not annexed by china until the manchu qing dynasty invasion during the 18th century. he also states that chinese writers of the early 20th century described tibet as a feudal dependency of china, not an integral part of it. sperling states that this is because \"tibet was ruled as such, within the empires of the mongols and the manchus\" and also that \"china's intervening ming [\"China's intervening Ming dynasty\"]\n",
            "alexander glaz ['Michel Fokine']\n",
            "30-pin dock connector, allowing for firewire or usb connectivity. this provided better compatibility with non-apple machines, as most of them did not have firewire ports at the time. eventually apple began shipping ipods with usb cables instead of firewire, although the latter was available separately. as of the first-generation ipod nano and the fifth-generation ipod ['Shuffle']\n",
            " ['44%']\n",
            "antimicrobial and antibiotic ['antimicrobial and antibiotic resistance']\n",
            "darlette ['Darlette Johnson']\n",
            "long ['Battle of Long Island']\n",
            " ['passive or active']\n",
            "the clinton bush haiti ['The Huffington Post']\n",
            "one million emergency room visits and five million clinic visits to new yorkers. hhc facilities treat nearly one- ['one third']\n",
            " ['The College of Arts and Letters']\n",
            "13 petrol tanks derailed in hui ['venues']\n",
            " ['2002']\n",
            "3,9 ['374']\n",
            "mcdonald' [\"McDonald's\"]\n",
            "foley square near city ['Foley Square']\n",
            "new york's braves ['New York City Fire Department']\n",
            " ['the 1970s']\n",
            "120 ['120 sec']\n",
            "spider-man ['Spider-Man 3']\n",
            "karol szyman ['Karol Szymanowski']\n",
            "the ['The Lincoln Tunnel']\n",
            "mobile phones, such as phones from sony ericsson and ['Sony Ericsson and Nokia']\n",
            "phagmodrupa ['Phagmodrupa Dynasty']\n",
            " ['French']\n",
            " ['Lady Gaga']\n",
            "ballades and scher ['ballades and scherzi']\n",
            "wny ['WNYC']\n",
            "love ['romance']\n",
            " ['antibacterial resistance genes']\n",
            "nyc ['NYCTV']\n",
            " ['1908']\n",
            "prince adam czartory ['Prince Adam Czartoryski']\n",
            "china uni ['China Unicom and China Mobile']\n",
            "june ['June 1849']\n",
            "$457 million in donated money and goods for rescue efforts so far, including $83 million from 19 countries and four international organizations. saudi arabia was the largest aid donor to china, providing close to ‚Ç¨40 ['‚Ç¨40,000,000']\n",
            "star ['Starpower: Beyonc√©']\n",
            " ['Queens']\n",
            "raoul koczal ['Raoul Koczalski']\n",
            "steve ['Steve Jobs']\n",
            "the cello sonata op. ['Cello']\n",
            "coordinated strategy to destroy a group of people, a process that could be accomplished through total annihilation as well as strategies that eliminate key elements of the group's basic existence, including language, culture, and economic infrastructure.<unk> he created a concept of mobilizing much of the international relations and community, to working together and preventing the occurrence of such events happening within history and the international society. australian anthropologist peg levine coined the term \"ritualc ['anthropologist']\n",
            "$70‚Äì75 ['$70.4 million']\n",
            " ['1976']\n",
            " ['1505‚Äì1521']\n",
            "mtv, fox ['Fox News']\n",
            "independent women part ['Survivor']\n",
            "fat ['HFS+']\n",
            "shoddy tofu-dreg ['in a media interview']\n",
            "nintendo power promotion and bundled with replicas of the master sword and the hylian ['Nintendo Power']\n",
            "1430 ['1430s']\n",
            "one ['Spectre']\n",
            "creative ['Creative Technology']\n",
            "angelo bozzolini and roberto pross ['Angelo Bozzolini and Roberto Prosseda']\n",
            "episo ['drone bass']\n",
            "shadow crystal. now able to use it to switch between both forms at will, link is led by midna to the mirror of twilight located deep within the gerudo desert, the only known gateway between the twilight realm and hyrule. however, they discover that the mirror is broken. the sages there explain that zant tried to destroy it, but he was only able to shatter it into fragments; only the true ruler of the twili can completely destroy the mirror of twilight. they also reveal that they used it a century ago to banish ganondorf, the gerudo leader who attempted to steal the triforce, to the twilight realm when executing him failed. assisted by an underground resistance group they meet in castle town, link and midna set out to retrieve the missing shards of the mirror, defeating those they infected. once the portal has been restored, midna is revealed to be the true ruler of the twilight realm, usurped by zant when he cursed her into her current form. confronting zant, link and midna learn that zant's coup was made possible when he forged a pact with ganondorf, who asked for zant's assistance in conquering hyrule. after link defeats zant, midna recovers the fused shadows, but destroys zant after learning that only ganondorf's death can release her from her curse. returning to hyrule, link and midna find ganondorf in hyrule castle, with a lifeless [\"Zelda's\"]\n",
            "peter bradshaw gave the film a full five stars, calling it \"inventive, intelligent and complex\", and singling out craig's performance as the film's highlight. in another five star review, the daily telegraph's robbie ['Robbie Collin']\n",
            " ['six']\n",
            "wilhelm wur ['Wilhelm W√ºrfel']\n",
            " ['500']\n",
            "missoula and great ['Missoula and Great Falls']\n",
            " ['Napoleon']\n",
            "july 11, ['July 11, 1960']\n",
            " ['2003']\n",
            "may ['May 2005']\n",
            " ['Brooklyn, Queens, Manhattan, the Bronx, and Staten Island']\n",
            "solar thermal collectors, ['increase the supply of energy']\n",
            "13 million bison in montana in 1870. in 1875, general philip sheridan pleaded to a joint session of congress to authorize the slaughtering of herds in order to deprive the indians of their source of food. by 1884, commercial hunting had brought bison to the verge of extinction; only about  ['about 325']\n",
            "shoddy ['shoddy construction']\n",
            " ['thirteen']\n",
            "february ['February 2013']\n",
            "school construction ['response to the quake']\n",
            "one out of ten private sector jobs in the city is with a foreign ['One out of ten']\n",
            " ['playing and discussing music']\n",
            " ['convert solar light to heat']\n",
            "september 4, ['September 4, 1981']\n",
            "rev. ['balcony']\n",
            "5 rue tronchet was close to sand's rented accommodation at the rue pigall ['5 rue Tronchet']\n",
            " ['eight']\n",
            " ['Run']\n",
            "poe ['Poes']\n",
            "white rabbit ['White Rabbits']\n",
            "staten island, which, at 409.8 feet (124.9 m) above sea level, is the highest point on the eastern seaboard south of ['Staten Island']\n",
            " ['tuberculosis']\n",
            "nohan ['Nohant']\n",
            "three current versions of the ipod: the ultra-compact ipod ['three']\n",
            " ['1883']\n",
            "$400 ['400 million']\n",
            " ['New York Fashion Week']\n",
            "democrat barack ['Republican']\n",
            "veron ['a national treasure']\n",
            " ['seven']\n",
            " ['the Notre Dame Victory March and the Notre Dame Alma Mater']\n",
            " ['Hildebrand Rarities and Antiques']\n",
            "the beyontour ['Beyontourage']\n",
            "manhattan. under such influential united states founders as alexander hamilton and john jay, the new york manumission society worked for abolition and established the african free school to educate black children. it was not until 1827 that slavery was completely abolished in the state, and free blacks struggled afterward with discrimination. new york interracial abolitionist activism continued; among its leaders were graduates of the african free school. the city's black population reached more than ['Manhattan']\n",
            "deliberate ['officials in power of a state or area']\n",
            "1 suffolk ['1 Suffolk Street in Trafalgar Square']\n",
            "60 were to be provided by the plaaf, and ['30']\n",
            "oprah winfre ['lyrical and raw']\n",
            "2,5 ['2,535']\n",
            "john f. kennedy international airport, newark liberty international airport, and laguardia ['John F. Kennedy International Airport, Newark Liberty International Airport, and LaGuardia Airport']\n",
            "calpurnia and her neighbor miss maud ['Calpurnia and Miss Maudie']\n",
            "three doctors have visited me... the first said i was dead; the second said i was dying; and the third said i was about to die.\" he also had problems having his pleyel piano sent to him. it finally arrived from paris in december. chopin wrote to pleyel in january 1839: \"i am sending you my preludes [(op. 28)]. i finished them on your little piano, which arrived in the best possible condition in spite of the ['best possible condition']\n",
            "eric f. wieschau ['Olympic gold']\n",
            "proactive action that spared the lives of all 2, ['An County']\n",
            "red nose [\"Comic Relief's Red Nose Day\"]\n",
            "17 ['1754']\n",
            " ['2006']\n",
            " ['C']\n",
            " ['2014']\n",
            "strict timing is disregarded, what is it called?[SEP] chopin's music is frequently played with rub ['rubato']\n",
            " ['International Peace studies']\n",
            "scout, atticus, and ['Scout, Atticus, and Boo']\n",
            "tytus woyciech ['Jan Matuszy≈Ñski and Julian Fontana']\n",
            "the propaganda ['propaganda bureau']\n",
            "18 ['1817']\n",
            "music\" and \"video ['\"Music\" and \"Videos\"']\n",
            "same sex ['same sex marriage']\n",
            "state or its agents?[SEP] barbara harff and ted gurr defined genocide as \"the promotion and execution of policies by a state or its ['policies']\n",
            " ['10']\n",
            " ['poor']\n",
            " ['1919']\n",
            "2 ['25']\n",
            "macy's and what other store?[SEP] in 2005, beyonce teamed up with house of brands, a shoe company, to produce a range of footwear for house of dereon. in january 2008, starwave mobile launched beyonce fashion diva, a \"high-style\" mobile game with a social networking component, featuring the house of dereon collection. in july 2009, beyonce and her mother launched a new junior apparel label, sasha fierce for dereon, for back-to-school selling. the collection included sportswear, outerwear, handbags, footwear, eyewear, lingerie and jewelry. it was available at department stores including macy's and dillard' [\"Dillard's\"]\n",
            " ['thriller']\n",
            "josephine ['Dreamgirls']\n",
            " ['the Congregation of the Holy Cross']\n",
            "tom robinson is the chief example among several innocents destroyed carelessly or deliberately throughout the novel. however, scholar christopher metress connects the mockingbird to boo rad ['real nice']\n",
            "adam mic ['Adam Mickiewicz']\n",
            "the ['the Times']\n",
            "hurricane  ['Ike']\n",
            " ['2005']\n",
            "polish lyrics. his keyboard style is highly individual and often technically demanding; his own performances were noted for their nuance and sensitivity. chopin invented the concept of instrumental ballad ['Polish']\n",
            " ['delayed arrival']\n",
            " ['concentration']\n",
            "kalkbren ['Liszt']\n",
            "clarification of clinical trial ['economic incentives']\n",
            " ['race relations']\n",
            "jorgic v. ['Jorgic v. Germany']\n",
            "children's day, june 1, 2008, many parents went to the rubble of schools to mourn for their children. the surviving children, who were mostly living in relief centres, performed ceremonies marking the special day, but also acknowledging the ['performed ceremonies']\n",
            "clapperboard ['clapperboards']\n",
            " ['historian Tsepon W. D. Shakabpa']\n",
            " ['1847']\n",
            "10 ['calm']\n",
            "headphone amplifier. the first-generation ipod ['external headphone amplifier']\n",
            "60 to  ['70']\n",
            "twilight ['GameCube']\n",
            "26 february ['26 February 1832']\n",
            "new ['New Orange']\n",
            " ['April 1994']\n",
            "missy elliott and alicia ['Missy Elliott and Alicia Keys']\n",
            "10.7 billion yuan (approximately us$1.5 billion) had been donated by the chinese public. houston rockets center yao ming, one of the country's most popular sports icons, gave $214,000 and $71,000 to the red cross society of china. the association has also collected a total of $26 ['26 million']\n",
            " ['will']\n",
            "washington ['Washington Hall']\n",
            "battery park ['Battery Park City']\n",
            "tyra ['Tyra Banks']\n",
            "59 to ['59 to 41.']\n",
            " ['2000 atomic mass units']\n",
            "blurry textures and low-resolution ['IGN and GameSpy']\n",
            "i <unk> ny) is both a logo and a ['I Love New York']\n",
            " ['Vogue']\n",
            "miss a ['Miss a Meal']\n",
            "$40 ['$40 billion']\n",
            " ['fungi']\n",
            "boo rad ['Boo Radley']\n",
            "1.3 ['1.3 million']\n",
            " ['melodies and ideas']\n",
            "268,000 tweets per minute. at the 55th annual grammy awards, beyonce won for best traditional r&b performance for \"love on top\". her feature-length documentary film, life is but a ['Life Is But a Dream']\n",
            "hewlett-pack ['Hewlett-Packard']\n",
            "medicinal folklore. mixtures with antimicrobial properties that were used in treatments of infections were described over 2000 years ago. many ancient cultures, including the ancient egyptians and ancient greeks, used specially selected mold and plant materials and extracts to treat infections. more recent observations made in the laboratory of antibiosis between microorganisms led to the discovery of natural antibacterials produced by microorganisms. louis pasteur observed, \"if we could intervene in the antagonism observed between some bacteria, it would offer perhaps the greatest hopes for therapeutics\". the term 'antibiosis', meaning \"against life\", was introduced by the french bacteriologist jean paul vuillemin as a descriptive name of the phenomenon exhibited by these early antibacterial drugs. antibiosis was first described in 1877 in bacteria when louis pasteur and robert koch observed that an airborne bacillus could inhibit the growth of bacillus anthracis. these drugs were later renamed antibiotics by selman waksman, an american microbiologist, in 1942. synthetic antibiotic chemotherapy as a science and development of antibacterials began in germany with paul ehrlich in the late 1880s. ehrlich noted certain dyes would color human, animal, or bacterial cells, whereas others did not. he then proposed the idea that it might be possible to create chemicals that would act as a selective drug that would bind to and kill bacteria without harming the human host. after screening hundreds of dyes against various organisms, in 1907, he discovered a medicinally useful drug, the synthetic antibacterial salvarsan now called arsphen ['against life']\n",
            "duchy of ['Duchy of Warsaw']\n",
            " ['B.I.C.']\n",
            " ['zoologist']\n",
            "the wall street ['The Wall Street Journal']\n",
            "george washington bridge is the world's busiest motor vehicle bridge, connecting manhattan to bergen county, new jersey. the verrazano-narrows ['The Verrazano-Narrows Bridge']\n",
            "shamanism and blood ['the native Mongol practices of shamanism and blood sacrifice']\n",
            "democrat barack ['Barack Obama']\n",
            " ['seventeen']\n",
            " ['16,000']\n",
            "de ['Decca Records']\n",
            "julian ['Julian Fontana']\n",
            "short-lived outbursts by mob ['pogroms']\n",
            "4.8 ['20%']\n",
            " ['73']\n",
            " ['From 1842 onwards']\n",
            "the giving of ['The Giving of Love']\n",
            "blue ivy ['Blue Ivy Carter']\n",
            " ['1887']\n",
            "sandy hook elementary school ['Sandy Hook Elementary School shooting']\n",
            "world health ['livestock raising']\n",
            "100  ['100 dB']\n",
            " ['northern']\n",
            "the ming ['the Ming dynasty']\n",
            "the revolutionary etude (op. 10, no. 12), and the minute waltz (op. 64, no. 1). however, with the exception of his funeral march, the composer never named an instrumental work beyond genre and number, leaving all potential extramusical associations to the listener; the names by which many of his pieces are known were invented by others. there is no evidence to suggest that the revolutionary etude was written with the failed polish uprising against russia in mind; it merely appeared at that time. the funeral march, the third movement of his sonata no. ['Sonata No. 2']\n",
            "master ['chest']\n",
            "mongol ['the Ming']\n",
            " ['climates']\n",
            " ['Over 230']\n",
            "all in the ['All in the Family']\n",
            "cornel ['Cornel Wilde']\n",
            " ['Hyrule Castle']\n",
            " ['2013']\n",
            "mrs. dub ['Mrs. Dubose']\n",
            "j ['JVC, Pioneer, Kenwood, Alpine, Sony, and Harman Kardon']\n",
            " ['1925']\n",
            "equal ['clipping']\n",
            " ['1,500']\n",
            "sky ['Skyfall']\n",
            "july ['July 2014']\n",
            "three-year period that ended in ['three-year period']\n",
            "the minute ['Funeral March']\n",
            " ['not']\n",
            "16 ['1664']\n",
            "november ['November 2014']\n",
            "empirical ['empirical therapy']\n",
            "2006, beyonce introduced her all-female tour band suga ['the 2006 BET Awards']\n",
            " ['piano']\n",
            "governors ['Governors Island']\n",
            "irreemplaz [\"re-release of B'Day\"]\n",
            " ['variety of reasons']\n",
            "bootyli ['Bootylicious']\n",
            " ['England']\n",
            "demand a ['Demand A Plan']\n",
            " ['absorption of estrogens']\n",
            "mtv, fox news, hbo, showtime, bravo, food network, amc, and comedy ['Comedy Central']\n",
            "westminster ['the old MI6 building']\n",
            "norman heat ['Norman Heatley']\n",
            "gwyneth palt ['Gwyneth Paltrow']\n",
            " ['United Nations General Assembly']\n",
            " ['soldering tools']\n",
            "april ['April 2014.']\n",
            "1279, so tibet was a component of the early mongol empire before it was combined into one of its descendant empires with the whole of china under the yuan dynasty (1271‚Äì13 ['1271‚Äì1368']\n",
            "largely undefined definitions of group destruction, despite what factor?[SEP] in the same judgement the echr reviewed the judgements of several international and municipal courts judgements. it noted that international criminal tribunal for the former yugoslavia and the international court of justice had agreed with the narrow interpretation, that biological-physical ['Convention States municipal laws']\n",
            "decca ['23 October 2015']\n",
            " ['Under Armour']\n",
            "wenchuan ['Wenchuan County']\n",
            " ['1179th Transportation Brigade']\n",
            "walk score named new york city the most walkable large city in the united states. citi ['Walk Score']\n",
            " ['a grand piano.']\n",
            "tommy hilfi ['Heat']\n",
            "eolomelodi ['eolomelodicon']\n",
            " ['many families']\n",
            " ['one out of four']\n",
            "september ['1831']\n",
            "one ['one thousand']\n",
            " ['Christianity']\n",
            "main floor, so by invitation of rev. ['the colored balcony']\n",
            "albert grzyma<unk> ['Julian Fontana']\n",
            " ['alternative definitions']\n",
            "four-year terms. the city council is a unicameral body consisting of 51 council members whose districts are defined by geographic population boundaries. each term for the mayor and council members lasts four ['four']\n",
            "jf ['John F. Kennedy International Airport']\n",
            "agnez der ['her mother']\n",
            "general grant national memorial, is also called what?[SEP] the statue of liberty national monument and ellis island immigration museum are managed by the national park service and are in both the states of new york and new jersey. they are joined in the harbor by governors island national monument, in new york. historic sites under federal management on manhattan island include castle clinton national monument; federal hall national memorial; theodore roosevelt birthplace national historic site; general grant national memorial (\"grant's [\"Grant's Tomb\"]\n",
            " ['the Gelugpas']\n",
            "bey hive is the name given to beyonce's fan base. fans were previously titled \"the beyontour ['The Bey Hive']\n",
            "george w. ['George W. Bush']\n",
            "the iphone. differences include a lack of a phone application. both devices use  ['iPhone']\n",
            "cold ['Coldplay']\n",
            " ['64']\n",
            " ['north']\n",
            "30-pin dock ['third generation']\n",
            "rush ['rush hour']\n",
            " ['lenses or mirrors and tracking systems']\n",
            "south china until 1279, so tibet was a component of the early mongol empire before it was combined into one of its descendant empires with the whole of china under the yuan dynasty (1271‚Äì1368). van praag writes that this conquest \"marked the end of independent china,\" which was then incorporated into the yuan dynasty that ruled china, tibet, mongolia, ['China']\n",
            "$772 ['blood']\n",
            "hfs+ file system format, which allows it to serve as a boot disk for a mac computer. if it is formatted on windows, the fat32 format is used. with the release of the windows-compatible ipod, the default file system used on the ipod line switched from hfs+ to fat ['FAT32']\n",
            "french ['France']\n",
            "bellevue ['Bellevue Hospital']\n",
            "l'offici [\"L'Officiel\"]\n",
            "hi- ['iPod Hi-Fi']\n",
            " ['√â-L√¨-Sƒ´ Army-Civilian Marshal Office']\n",
            "fertility clinic ['fertility clinics']\n",
            "san ['the California Academy of Sciences in San Francisco']\n",
            " ['64 certifications']\n",
            "knute rock ['its Fighting Irish football team']\n",
            "frederick ['Houston']\n",
            "4 ['469']\n",
            "april 10, 14 ['1403']\n",
            "seismological and geological ['access to seismological and geological data']\n",
            "beta-lactam antibiotics, which include the penicillin ['beta-lactam antibiotics']\n",
            "wenchuan ['Armed Police General Hospital']\n",
            "blue ivy ['Blue Ivy Carter']\n",
            "dangerously in ['Dangerously in Love']\n",
            "8.0 ['8.0 Ms and 7.9 Mw']\n",
            "nicolas chopin, was a frenchman from lorraine who had emigrated to poland in 1787 at the age of sixteen. nicolas tutored children of the polish aristocracy, and in 1806 married justyna krzyzanowska, a poor relative of the skarbeks, one of the families for whom he worked. fryderyk was baptized on easter sunday, 23 april 1810, in the same church where his parents had married, in brochow. his eighteen-year-old godfather, for whom he was named, was fryderyk skar ['Nicolas']\n",
            " ['Asians']\n",
            "staten island and fort totten in ['Queens']\n",
            " ['individual worth']\n",
            " ['1928']\n",
            "bow ['chime']\n",
            "niccolo paganini play the violin, and composed a set of variations, souvenir de paganini. it may have been this experience which encouraged him to commence writing his first etudes, (1829‚Äì32), exploring the capacities of his own instrument. on 11 august, three weeks after completing his studies at the warsaw ['Vienna']\n",
            " ['a specific set of violent crimes that are committed against a certain group with the attempt to remove the entire group from existence or to destroy them']\n",
            "september 12, ['2006']\n",
            "jane ['Jane Stirling']\n",
            "14th ['the 14th century']\n",
            "flushing meadows-corona ['Queens']\n",
            "15 ['1578']\n",
            "medical ['medical treatment']\n",
            " ['1977']\n",
            "diarrhea, resulting from disruption of the species composition in the intestinal flora, resulting, for example, in overgrowth of pathogenic bacteria, such as clostridium difficile. antibacterials can also affect the vaginal flora, and may lead to overgrowth of yeast species of the genus candida in the vulvo-vaginal area. additional side-effects can result from interaction with other drugs, such as elevated risk of tendon ['Additional side-effects']\n",
            "great white ['The Great White Way']\n",
            "level ii emergency contingency plan\", which covers the most serious class of natural disasters. the plan rose to level ['Level II emergency contingency plan']\n",
            "3,577 from a pool of 18, ['19.7%']\n",
            " ['the ICTY']\n",
            "tofu-dregs schoolhouse ['legal replacements']\n",
            "state earthquake relic ['the terrible disaster']\n",
            " ['120']\n",
            "co-writing credits for most of the songs recorded with destiny's child and her solo efforts. her early songs were personally driven and female-empowerment themed compositions like \"independent women\" and \"surviv ['co-producing credits']\n",
            "provo, ['Provo, Utah']\n",
            "ten languages. in the years since, it has sold more than 30 million copies and been translated into more than ['ten']\n",
            "sony pictures ['Sony Pictures Entertainment']\n",
            "the itunes ['iTunes Store']\n",
            " ['Coldplay']\n",
            " ['a working fluid']\n",
            "11 ['11 million']\n",
            " ['28']\n",
            "us$1.27 ['US$1.27 billion']\n",
            "jarett wies ['Jarett Wieselman']\n",
            "destiny's [\"Destiny's Child\"]\n",
            "8-pin dock connector, named ['Lightning']\n",
            "xin ['to find the blind spots of disaster recovery']\n",
            " ['21']\n",
            "cpp ['the Convention on the Prevention and Punishment of the Crime of Genocide']\n",
            "independent women\" and \"surviv ['Cater 2 U']\n",
            " ['indirect']\n",
            "2 ['10,521.83']\n",
            "red bull arena in nearby harrison, new jersey. historically, the city is known for the new york cosmos, the highly successful former professional soccer team which was the american home of pele, one of the world's most famous soccer players. a new version of the new york cosmos was formed in 2010, and began play in the second division north american soccer league in 2013. the cosmos play their home games at james m. shuart stadium on the campus of hofstra ['Hofstra University']\n",
            "it-98- ['entire human groups']\n",
            "louis lefebure-we ['Louis Lef√©bure-W√©ly']\n",
            "urban heat islands (uhi) are metropolitan areas with higher temperatures than that of the surrounding environment. the higher temperatures are a result of increased absorption of the solar light by urban materials such as asphalt and ['Urban heat islands']\n",
            "bootyli ['wide-ranging']\n",
            " ['First Lady Michelle Obama']\n",
            "north ['North Dakota']\n",
            "altan ['the Ming court']\n",
            " ['2013']\n",
            "16th-century arab alchemist ['single-slope']\n",
            "the center for science in the public interest (cspine ['Center for Science in the Public Interest']\n",
            "vajrad ['Master of Vajradhara']\n",
            "polish ['Polish']\n",
            "medical research and technology, non-profit institutions, and ['Brooklyn']\n",
            " ['1952']\n",
            "rui ['Beijing Olympic torch relay']\n",
            "4 ['469']\n",
            " ['no prediction notification']\n",
            " ['Copper']\n",
            " ['1958']\n",
            "mid-frac ['Yingxiu-Beichuan fracture']\n",
            "15 ['60 million']\n",
            " ['seventeen Qianhu offices']\n",
            " ['Drake']\n",
            "robert ['keyboard technique']\n",
            "parkwood topshop athletic ['Parkwood Topshop Athletic Ltd']\n",
            "1611‚Äì16 ['1611‚Äì1621']\n",
            "september 8, 1883, at gold ['1883']\n",
            "christoph ['Christoph Waltz.']\n",
            "william ['William Faulkner']\n",
            "june 3, ['June 3, 2008']\n",
            " ['to seek out the Karmapa']\n",
            "tommy hilfi ['Tommy Hilfiger']\n",
            " ['over 2000 years ago']\n",
            "boo rad ['Boo Radley']\n",
            " ['black and white']\n",
            " ['to take control of her own career']\n",
            "l'offici [\"L'Officiel\"]\n",
            "george gi ['George Gipp']\n",
            "september ['September 2015']\n",
            "april 4, ['April 4, 2008']\n",
            "polymyxin ['polymyxins']\n",
            "l-target ['Ocarina of Time']\n",
            "philip ['General Philip Sheridan']\n",
            "sasha ['Single Ladies']\n",
            "parabolic trough ['parabolic troughs']\n",
            "mexico ['Mexico City']\n",
            "mr. ['Franz Oberhauser']\n",
            "adam mickiewicz, principal of the polish literary ['principal of the Polish Literary Society']\n",
            " ['2010']\n",
            "randalls ['Randalls Island']\n",
            "great fire of ['the Great Fire of 1835']\n",
            "$80 ['250 million']\n",
            " ['music industry']\n",
            "16 ['King William III']\n",
            "sonam chop ['G√ºshi Khan']\n",
            "$11 ['$11 billion']\n",
            " ['Alexander Scriabin']\n",
            "polish and french (by citizenship and birth of father) composer and a virtuoso pianist of the romantic era, who wrote primarily for the solo piano. he gained and has maintained renown worldwide as one of the leading musicians of his era, whose \"poetic genius was based on a professional technique that was without equal in his generation.\" chopin was born in what was then the duchy of ['Polish and French']\n",
            " ['Director-General']\n",
            "5, ['5,335']\n",
            "july ['2009']\n",
            " ['Hockey East']\n",
            "bmw released the first ipod automobile interface, allowing drivers of newer bmw vehicles to control an ipod using either the built-in steering wheel controls or the radio head-unit buttons. apple announced in 2005 that similar systems would be available for other vehicle brands, including mercedes-benz, volvo, nissan, toyota, alfa romeo, ferrari, acura, audi, honda, renault, infiniti and ['BMW']\n",
            "jamyang shakya gyalt ['Jamyang Shakya Gyaltsen']\n",
            " ['sword']\n",
            "whitney ['Whitney Houston']\n",
            "salma hayek and frida gianni ['Salma Hayek and Frida Giannini']\n",
            "november 26, ['November 26, 1842']\n",
            "a ['a hiatus']\n",
            " ['Electronic Industry Code of Conduct Implementation Group']\n",
            "tommy hilfi ['Beyonc√©']\n",
            "us$ ['$99']\n",
            "1, ['1,700']\n",
            "ambrozy mieros ['Ambro≈ºy Mieroszewski']\n",
            "millrose games is an annual track and field meet whose featured event is the wanamaker ['Millrose Games']\n",
            " ['host computer']\n",
            "family and ['her family and neighbors']\n",
            "a ['a month']\n",
            "sonam gyatso was granted the title dorjichang or vajradhara dalai lama in 1587 [sic!], but china daily does not mention who granted him the title. without mentioning the role of the mongols, china daily states that it was the successive qing ['the third Dalai Lama']\n",
            " ['1827']\n",
            "90% of the market for hard drive-based players and over  ['90%']\n",
            "the central ['the central government of China']\n",
            " ['12']\n",
            "13.7% (although they note that comments from respondents indicate that \"the true ipod failure rate may be lower than it appears\"). it concluded that some models were more durable than others. in particular, failure rates for ipods employing hard drives was usually above ['screen']\n",
            " ['R&B']\n",
            " ['openness']\n",
            "hudson ['Harlem River']\n",
            " ['2006']\n",
            " ['two']\n",
            "stickball ['Stickball']\n",
            " ['1842']\n",
            "3.5 ['3.5 metres']\n",
            "power generation or energy ['a heat source for a conventional power plant']\n",
            "174,000 terawatts (tw) of incoming solar radiation (insolation) at the upper atmosphere. approximately  ['30%']\n",
            "ver ['Verit√©']\n",
            "german catholic journals. it quickly emerged as part of an international catholic intellectual revival, offering an alternative vision to positivist ['German Catholic journals']\n",
            "new york city or the city of new york to distinguish it from the state of new york, of which it is a part<unk>is the most populous city in the united states and the center of the new york metropolitan ['New York City']\n",
            "rapper jay ['Jay Z']\n",
            "yao guangxia ['the Buddhist monk Yao Guangxiao']\n",
            "l-target ['L-targeting']\n",
            "1521‚Äì15 ['1521‚Äì1567']\n",
            "neo- ['Neo-Confucian establishment']\n",
            "delacroix and the mezzo-soprano pauline viardo ['Delacroix and the mezzo-soprano Pauline Viardot']\n",
            "equal ['Bass']\n",
            "almost ['80%']\n",
            " ['alcohol']\n",
            "adolphe nour ['Adolphe Nourrit']\n",
            "sky ['Spectre.']\n",
            "a ['rabbit']\n",
            "darlette ['Darlette Johnson']\n",
            " ['were threatened']\n",
            "poor white trash'... lee demonstrates how issues of gender and [\"people's motives and behavior\"]\n",
            " ['35 percent']\n",
            " ['failure of medical professionals to prescribe the correct dosage']\n",
            "5,937 high-rise buildings, of which  ['550']\n",
            "staggered heights between rows and the mixing of plant varieties can improve crop yields. while sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. during the short growing seasons of the little ice age, french and english farmers employed fruit walls to maximize the collection of solar energy. these walls acted as thermal masses and accelerated ripening by keeping plants warm. early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. in 1699, nicolas fatio de duillier even suggested using a tracking mechanism which could pivot to follow the sun. applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. more recently the technology has been embraced by vinters, who use the energy generated by solar panels to power grape ['power grape presses']\n",
            " ['$250,000.']\n",
            "long ['Longhua, Shenzhen']\n",
            "john j. cavanaugh, c.s.c. served as president from 1946 to 1952. cavanaugh's legacy at notre dame in the post-war years was devoted to raising academic standards and reshaping the university administration to suit it to an enlarged educational mission and an expanded student body and stressing advanced studies and research at a time when notre dame quadrupled in student census, undergraduate enrollment increased by more than half, and graduate student enrollment grew fivefold. cavanaugh also established the lobund institute for animal studies and notre dame's medieval institute. cavanaugh also presided over the construction of the nieuwland science hall, fisher hall, and the morris inn, as well as the hall of liberal arts (now o'shaughnessy ['Hall of Liberal Arts']\n",
            " ['Largest film stunt explosion']\n",
            "mingshi ['the Mingshi or the Mingshi Lu']\n",
            "greenwich ['Stonewall Inn in the Greenwich Village neighborhood of Lower Manhattan']\n",
            "wenchuan county, ['Wenchuan County, Sichuan']\n",
            " ['About 30']\n",
            "moreau ['Moreau Seminary']\n",
            " ['make a detailed preliminary survey of damaged buildings']\n",
            " ['France']\n",
            "jozef els ['J√≥zef Elsner']\n",
            "carolyne zu sayn-wittg ['Carolyne zu Sayn-Wittgenstein']\n",
            "4 ['304.8']\n",
            "dorothy crowfoot hodg ['Dorothy Crowfoot Hodgkin']\n",
            "1895 performance by paul pabst of the nocturne in e major op. 62 no. ['Nocturne in E major Op. 62 No. 2']\n",
            " ['Jonassohn and Bj√∂rnson']\n",
            "drummers' [\"the Drummers' Circle\"]\n",
            "kobylanska ['the Kobyla≈Ñska Catalogue']\n",
            "november 14, ['2003']\n",
            "nouvelle angoule ['Nouvelle Angoul√™me']\n",
            "house of ['C&A']\n",
            " ['seven']\n",
            "missoula and great ['Billings']\n",
            "22,000 litres (4,800 imp gal; 5,800 us gal) of water per minute from the nile ['Nile River']\n",
            "adverse side effects. side-effects range from mild to very serious depending on the antibiotics used, the microbial organisms targeted, and the individual patient. side effects may reflect the pharmacological or toxicological properties of the antibiotic or may involve hypersensitivity reactions or anaphylaxis. safety profiles of newer drugs are often not as well established as for those that have a long history of use. adverse effects range from fever and nausea to major allergic reactions, including photodermatitis and anaphylaxis. common side-effects include ['clinical use']\n",
            "beyonce fashion ['Beyonc√© Fashion Diva']\n",
            "muammar gadda ['documents obtained by WikiLeaks']\n",
            "the ['The Mail on Sunday']\n",
            "the washington ['Jeff Gerstmann']\n",
            "livestock raising, prompting bacteria to develop resistance. this has led to widespread problems with antimicrobial and antibiotic ['antibiotic resistance']\n",
            " ['University of Notre Dame du']\n",
            " ['19 countries']\n",
            "henri her ['Liszt and Henri Herz']\n",
            "12. ['12.2']\n",
            " ['800']\n",
            "adam mic ['Adam Mickiewicz']\n",
            "motion ['motion']\n",
            " ['Pelham Bay Park']\n",
            "adolphe nour ['Adolphe Nourrit']\n",
            "metlife ['MetLife Stadium']\n",
            "0. ['0.5']\n",
            "1368, a han chinese revolt known as the red turban rebellion toppled the mongol yuan dynasty in china. zhu yuanzhang then established the ming dynasty, ruling as the hongwu emperor (r. 1368‚Äì13 ['1368‚Äì1398']\n",
            "11 ['at least 5 million']\n",
            "divide-and- ['divide-and-rule']\n",
            "king james ['James II']\n",
            "house of der ['House of Der√©on']\n",
            " ['1904']\n",
            "january ['January 2013']\n",
            "the stonewall in ['Stonewall Inn']\n",
            " ['flashback']\n",
            "1854‚Äì ['1854']\n",
            "times square, iconic as the world's \"heart\" and its \"crossroads\", is the brightly illuminated hub of the broadway theater district, one of the world's busiest pedestrian intersections, and a major center of the world's entertainment industry. the names of many of the city's bridges, skyscrapers, and parks are known around the world. anchored by wall street in the financial district of lower manhattan, new york city has been called both the most economically powerful city and the leading financial center of the world, and the city is home to the world's two largest stock exchanges by total market capitalization, the new york stock exchange and nasd ['New York Stock Exchange and NASDAQ']\n",
            "dutch east india ['Dutch East India Company']\n",
            "manhattan ['Manhattan Island']\n",
            "5th dalai ['the Phagmodrupa']\n",
            "1870 to ['1930']\n",
            "the solar total energy project ( ['Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA']\n",
            "sasha ['119.5 million']\n",
            "3,715 hybrid taxis and other clean diesel vehicles, representing around 2 ['28%']\n",
            "field (whom chopin met in ['nocturnes']\n",
            " ['In 2002']\n",
            "stage presence and ['stage presence and voice']\n",
            " ['nine']\n",
            "marcel proust and andre gide; and he has also featured in works of gottfried benn and boris pasternak. there are numerous biographies of chopin in ['English']\n",
            " ['Notre Dame']\n",
            " ['1938']\n",
            "october 12, ['2003']\n",
            " ['1877']\n",
            "run the world (girls)\" and \"best thing i never had\", which both attained moderate success. the fourth single \"love on ['Love on Top']\n",
            " ['four']\n",
            " ['The majority of studies indicate antibiotics do interfere with contraceptive pills']\n",
            " ['20 km deep']\n",
            "taipei fire ['the Taipei Fire Department']\n",
            "ziping ['Hydropower Plant']\n",
            " ['Port Authority Trans-Hudson']\n",
            "mount tang ['Mount Tangjia in Beichuan County, Sichuan']\n",
            "beethoven seventh symphony arrangement at erard's on 1 march 1843. late in 1844, charles halle visited chopin and found him \"hardly able to move, bent like a half-opened penknife and evidently in great pain\", although his spirits returned when he started to play the ['piano']\n",
            " ['piano']\n",
            "69, ['69,197']\n",
            "field (whom chopin met in ['Field']\n",
            " ['three mazurkas']\n",
            "9 ['9 meters']\n",
            "harlem ['Harlem Renaissance']\n",
            "itunes ['the iTunes Store']\n",
            "roger deakin ['Roger Deakins']\n",
            " ['in two military transport planes']\n",
            " ['France']\n",
            "1.1 ['1.1 million']\n",
            " ['25']\n",
            " ['photosynthesis']\n",
            " ['1783']\n",
            "100  ['EU']\n",
            "8,491,079 residents as of 2014, incorporating more immigration into the city than outmigration since the 2010 united states census. more than twice as many people live in new york city as in the second-most populous u.s. city (los ['Los Angeles']\n",
            "julian ['elder brother']\n",
            "jane ['Jane Stirling']\n",
            "noh-t<unk>r-daym) is a catholic research university located adjacent to south bend, indiana, in the united states. in french, notre dame du lac means \"our lady of the ['Our Lady of the Lake']\n",
            "farrah franklin and michelle ['her mother']\n",
            "truman capo ['old Underwood typewriter']\n",
            " ['four']\n",
            " ['London']\n",
            "jane ['Jane Stirling']\n",
            "$350 million to more than $3 ['$350 million']\n",
            " ['2006']\n",
            "33 ['video']\n",
            "tom for [\"Tom Ford's Spring/Summer 2011 fashion show\"]\n",
            "3, ['at least 3,223']\n",
            " ['bass']\n",
            "chengdu shuangliu international airport was shut down, and the control tower and regional radar control evacuated. one silkair flight was diverted and landed in kunming as a result. cathay ['Chengdu Shuangliu International Airport']\n",
            "the new york ['The New York Times']\n",
            " ['2006 Fashion Rocks concert']\n",
            "condoleezza ['Condoleezza Rice']\n",
            "35 major league baseball world series and ['35']\n"
          ]
        }
      ],
      "source": [
        "for p,a in zip(preds,st2):\n",
        "    print(p['prediction_text'],a['answers']['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prediction_text': '',\n",
              " 'id': '573392e24776f41900660d9e',\n",
              " 'no_answer_probability': 0.0}"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "VpWeLFttJc9q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [2, 113, 25, 612, 17, 650, 19, 20590, 11301, 335, 60, 3, 3410, 6555, 21, 4530, 28, 21, 10861, 2302, 3646, 15, 14, 31, 10670, 63, 67, 74, 2217, 28, 21, 508, 3646, 9, 283, 8627, 15, 394, 3449, 17, 294, 8626, 57, 412, 20, 14, 31, 10670, 293, 28, 21, 6010, 6534, 26, 508, 3291, 17, 838, 15, 145, 28, 14, 612, 17, 650, 19, 20590, 11301, 19, 6005, 15, 2207, 15, 113, 31, 16894, 50, 147, 20, 1528, 78, 1138, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'start_positions': 69, 'end_positions': 71}\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.484375,\n",
              " 'eval_runtime': 0.0223,\n",
              " 'eval_samples_per_second': 89.555,\n",
              " 'eval_steps_per_second': 44.777}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "#['adamw_hf', 'adamw_torch', 'adamw_torch_fused', 'adamw_torch_xla', 'adamw_torch_npu_fused', 'adamw_apex_fused', 'adafactor', 'adamw_anyprecision', 'sgd', 'adagrad', 'adamw_bnb_8bit', 'adamw_8bit', 'lion_8bit', 'lion_32bit', 'paged_adamw_32bit', 'paged_adamw_8bit', 'paged_lion_32bit', 'paged_lion_8bit', 'rmsprop', 'rmsprop_bnb', 'rmsprop_bnb_8bit', 'rmsprop_bnb_32bit', 'galore_adamw', 'galore_adamw_8bit', 'galore_adafactor', 'galore_adamw_layerwise', 'galore_adamw_8bit_layerwise', 'galore_adafactor_layerwise']\n",
        "#import torch._dynamo\n",
        "#torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    outputs,labels = eval_preds\n",
        "    o1 , o2 = outputs\n",
        "    s , e = labels\n",
        "    \n",
        "    #var_dump(eval_preds)\n",
        "    print(o1.shape)\n",
        "    os = o1.argmax(1)\n",
        "    oe = o2.argmax(1)\n",
        "    preds =[]\n",
        "    ref =[]\n",
        "    print(os,oe,s,e)\n",
        "    for i in range(len(s)):\n",
        "        preds.append({\"id\": i, \"prediction_text\": \" \".join(map(str, np.arange(os[i], oe[i])))})\n",
        "        ref.append({\"id\": i, \"answers\": {\"text\": \" \".join(map(str, np.arange(s[i], e[i])))}})\n",
        "    \n",
        "    print(preds)   \n",
        "    print(preds)    \n",
        "     \n",
        "    \n",
        "    return metric.compute(predictions= preds,references= ref)\n",
        " \n",
        "import evaluate\n",
        "metric = evaluate.load('squad')\n",
        "from transformers import TrainingArguments, Trainer\n",
        "t = tokenized_squad[\"test\"].select(range(2))\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=t,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics,\n",
        "    args=TrainingArguments(\n",
        "            output_dir=\"my_awesome_qa_model\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            num_train_epochs=10,\n",
        "            #weight_decay=0.01,\n",
        "            #bf16 =True, #amper series\n",
        "            tf32=True,\n",
        "            #fp16 = False,#not is_bfloat16_supported(),\n",
        "            bf16 =True, #is_bfloat16_supported(),\n",
        "            #optim='adamw_hf',\n",
        "            #weight_decay=0.01,\n",
        "            #dataloader_pin_memory=True,\n",
        "            #dataloader_num_workers=0,\n",
        "            #torch_compile=True,  #seems not good error :(\n",
        "            \n",
        "            #push_to_hub=True,\n",
        "            report_to='tensorboard'\n",
        "            \n",
        "            )\n",
        ")\n",
        "\n",
        "#trainer.train()\n",
        "print(t[0])\n",
        "\n",
        "trainer.evaluate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " [5000/5000 15:22, Epoch 10/10]\n",
        "Epoch\tTraining Loss\tValidation Loss\n",
        "1\t4.859800\t3.931242\n",
        "2\t3.372500\t2.843172\n",
        "3\t2.442000\t2.213433\n",
        "4\t2.050300\t1.954712\n",
        "5\t1.865000\t1.826765\n",
        "6\t1.750300\t1.764793\n",
        "7\t1.677600\t1.692042\n",
        "8\t1.626800\t1.656364\n",
        "9\t1.599700\t1.637798\n",
        "10\t1.583800\t1.631518"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('my_awesome_qa_model/tokenizer_config.json',\n",
              " 'my_awesome_qa_model/special_tokens_map.json',\n",
              " 'my_awesome_qa_model/spiece.model',\n",
              " 'my_awesome_qa_model/added_tokens.json',\n",
              " 'my_awesome_qa_model/tokenizer.json')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(model.save_pretrained(\"my_awesome_qa_model\"))\n",
        "print(tokenizer.save_pretrained(\"my_awesome_qa_model\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRSp4sdgJc9q"
      },
      "source": [
        "Once training is completed, share your model to the Hub with the [push_to_hub()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method so everyone can use your model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TScr8NIhJc9q"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for question answering, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdYllCwdJc9q"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRXM4rCmJc9q"
      },
      "source": [
        "Evaluation for question answering requires a significant amount of postprocessing. To avoid taking up too much of your time, this guide skips the evaluation step. The [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) still calculates the evaluation loss during training so you're not completely in the dark about your model's performance.\n",
        "\n",
        "If have more time and you're interested in how to evaluate your model for question answering, take a look at the [Question answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) chapter from the ü§ó Hugging Face Course!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AWXIEhJc9r"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58smYqZJc9r"
      },
      "source": [
        "Great, now that you've finetuned a model, you can use it for inference!\n",
        "\n",
        "Come up with a question and some context you'd like the model to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3ZgS3YinJc9r"
      },
      "outputs": [],
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bColCDK3Jc9r"
      },
      "source": [
        "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for question answering with your model, and pass your text to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8s4QdRuuJc9r",
        "outputId": "67b311da-e4b7-4996-8ddc-14f283c03aa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.012835928238928318,\n",
              " 'start': 89,\n",
              " 'end': 118,\n",
              " 'answer': 'and 13 programming languages.'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n",
        "question_answerer(question=question, context=context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lJo7j-VJc9r"
      },
      "source": [
        "You can also manually replicate the results of the `pipeline` if you'd like:\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CMQHxqnxJc9w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,   184,   151,  3143,  2556,   630,  8064,   555,    60,     3,\n",
              "          8064,    63,    13, 11633,  2786, 12905,    17,    92,  7920,  1854,\n",
              "            19,  5084,  2556,  1112,  2556,    17,   539,  3143,  2556,     9,\n",
              "             3]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "inputs['input_ids']=inputs['input_ids'].cuda()\n",
        "inputs['token_type_ids']=inputs['token_type_ids'].cuda()\n",
        "inputs['attention_mask']=inputs['attention_mask'].cuda()\n",
        "inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkbdwXWhJc9x"
      },
      "source": [
        "Pass your inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lDfvfZvmJc9x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/proj/ciptmp/ix05ogym/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No ROCm runtime is found, using ROCM_HOME='/usr'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\").cuda()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.1825,  1.3876,  2.3054,  1.4554,  0.0895, -1.3429,  0.3068, -1.0149,\n",
              "         -3.1631, -0.2974,  1.2329,  0.2215,  3.8604,  6.7246,  1.5241,  0.1145,\n",
              "         -0.4441, -0.1687,  0.0218,  1.4461,  1.5058,  8.8211,  1.8876,  4.5003,\n",
              "          1.6900,  1.5468,  9.3359,  3.3335,  1.8808, -2.0084, -0.2974]],\n",
              "       device='cuda:0'), end_logits=tensor([[ 0.5415, -0.3170,  3.1900, -0.0726,  4.1455, -0.4629,  0.5939, -0.1192,\n",
              "         -1.2102, -0.0291,  0.9579,  0.2125,  1.6114,  6.1145,  5.2858,  2.9438,\n",
              "          0.3411, -0.6261, -1.0451,  1.2660,  0.4267,  7.5396,  4.6675,  3.5092,\n",
              "          5.8924,  1.7175,  8.0168,  1.6303,  5.9388, -0.1727, -0.0291]],\n",
              "       device='cuda:0'), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.start_logits.shape\n",
        "#inputs['input_ids'].shape\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQpzG1hZJc9x"
      },
      "source": [
        "Get the highest probability from the model output for the start and end positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0pyrLCnKJc9x"
      },
      "outputs": [],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjCRwwjZJc9x"
      },
      "source": [
        "Decode the predicted tokens to get the answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nJNQfEXMJc9y",
        "outputId": "df50798f-2eea-4407-a834-d97b44d302ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'13'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mstart_logits\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#labels = batch[\"labels\"].cpu().numpy()\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     compute_metrics((predictions, \u001b[43mlabels\u001b[49m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ],
      "source": [
        "#trainer.evaluate()\n",
        "def compute_metrics(eval_preds):\n",
        "    outputs,labels = eval_preds\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    return metric.compute(answer_start_index,labels)\n",
        "    \n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = trainer.model(**batch)\n",
        "    \n",
        "        print(batch.keys())\n",
        "        predictions = outputs.start_logits.cpu().numpy()\n",
        "        #labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        compute_metrics((predictions, labels))\n",
        "    break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/unslothai/unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a comparison table of the listed optimizers based on various criteria such as usage, precision, hardware support, and typical applications. Note that specific details such as supported hardware (GPU, TPU, NPU) and implementation differences can influence performance and suitability for different training scenarios.\n",
        "\n",
        "| Optimizer Name                 | Implementation      | Precision        | Hardware Support            | Key Features                                   | Typical Applications                       |\n",
        "|--------------------------------|---------------------|------------------|-----------------------------|------------------------------------------------|--------------------------------------------|\n",
        "| adamw_hf                       | Hugging Face        | 32-bit           | CPU, GPU                    | Decoupled weight decay, Transformer training  | NLP, Transformers                          |\n",
        "| adamw_torch                    | PyTorch             | 32-bit           | CPU, GPU                    | Decoupled weight decay, flexible             | General deep learning                      |\n",
        "| adamw_torch_fused              | PyTorch             | 32-bit           | CPU, GPU                    | Fused operations for efficiency              | General deep learning                      |\n",
        "| adamw_torch_xla                | PyTorch             | 32-bit           | TPU                         | TPU optimized                                 | High-performance training on TPUs          |\n",
        "| adamw_torch_npu_fused          | PyTorch             | 32-bit           | NPU                         | Fused operations for NPUs                    | Training on NPU hardware                   |\n",
        "| adamw_apex_fused               | NVIDIA Apex         | 32-bit, mixed    | GPU                         | Fused operations, mixed precision            | High-performance training on GPUs          |\n",
        "| adafactor                      | TensorFlow, HF      | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| adamw_anyprecision             | Custom              | Variable         | CPU, GPU                    | Flexible precision handling                  | Custom precision requirements              |\n",
        "| sgd                            | Standard            | 32-bit           | CPU, GPU                    | Simple, effective                            | Classic machine learning, simple models    |\n",
        "| adagrad                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | Sparse data                                |\n",
        "| adamw_bnb_8bit                 | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| adamw_8bit                     | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Large models with memory constraints       |\n",
        "| lion_8bit                      | Custom              | 8-bit            | CPU, GPU                    | Memory efficient, reduced precision          | Memory constrained environments            |\n",
        "| lion_32bit                     | Custom              | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| paged_adamw_32bit              | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_adamw_8bit               | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| paged_lion_32bit               | Custom              | 32-bit           | CPU, GPU                    | Paged optimizer for memory management        | Large datasets                             |\n",
        "| paged_lion_8bit                | Custom              | 8-bit            | CPU, GPU                    | Paged optimizer, memory efficient            | Large models with memory constraints       |\n",
        "| rmsprop                        | Standard            | 32-bit           | CPU, GPU                    | Adaptive learning rate                       | RNNs, general deep learning                |\n",
        "| rmsprop_bnb                    | BitsAndBytes        | 32-bit           | CPU, GPU                    | BitsAndBytes optimization                    | Memory efficient training                  |\n",
        "| rmsprop_bnb_8bit               | BitsAndBytes        | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Large-scale training on limited hardware   |\n",
        "| rmsprop_bnb_32bit              | BitsAndBytes        | 32-bit           | CPU, GPU                    | Higher precision                              | General deep learning                      |\n",
        "| galore_adamw                   | Galore              | 32-bit           | CPU, GPU                    | Enhanced AdamW                               | Advanced training scenarios                |\n",
        "| galore_adamw_8bit              | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, fast                       | Memory constrained environments            |\n",
        "| galore_adafactor               | Galore              | 32-bit, mixed    | CPU, GPU                    | Memory efficient, scalable                   | NLP, large models                          |\n",
        "| galore_adamw_layerwise         | Galore              | 32-bit           | CPU, GPU                    | Layerwise optimization                       | Advanced training scenarios                |\n",
        "| galore_adamw_8bit_layerwise    | Galore              | 8-bit            | CPU, GPU                    | Memory efficient, layerwise optimization     | Memory constrained environments            |\n",
        "| galore_adafactor_layerwise     | Galore              | 32-bit, mixed    | CPU, GPU                    | Layerwise optimization, memory efficient     | NLP, large models                          |\n",
        "\n",
        "### Notes:\n",
        "- **Precision**: Indicates whether the optimizer supports standard 32-bit precision or has options for mixed/8-bit precision for memory efficiency.\n",
        "- **Hardware Support**: Identifies the primary hardware the optimizer is designed to run on efficiently, e.g., CPU, GPU, TPU, NPU.\n",
        "- **Key Features**: Highlights unique aspects or enhancements that distinguish each optimizer.\n",
        "- **Typical Applications**: Common use cases or scenarios where the optimizer is particularly effective.\n",
        "\n",
        "Choosing the right optimizer depends on your specific training needs, hardware availability, and whether you need to manage large models or datasets within memory constraints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The convergence speed of an optimizer depends on various factors such as the type of model, the dataset, the specific problem being solved, and the tuning of hyperparameters. However, here are some general insights into the convergence speed of the listed optimizers:\n",
        "\n",
        "1. **AdamW variants**:\n",
        "    - `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, `adamw_torch_xla`, `adamw_torch_npu_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adamw_bnb_8bit`, `adamw_8bit`, `galore_adamw`, `galore_adamw_8bit`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`:\n",
        "      - AdamW is known for fast convergence due to its adaptive learning rate and decoupled weight decay. The fused versions (`fused`, `apex_fused`, `torch_fused`) can provide additional speedup due to more efficient computations.\n",
        "      - `adamw_xla` and `adamw_npu_fused` are optimized for specific hardware (TPU and NPU, respectively), which can lead to faster convergence on those platforms.\n",
        "\n",
        "2. **Adafactor**:\n",
        "    - `adafactor`, `galore_adafactor`, `galore_adafactor_layerwise`:\n",
        "      - Adafactor is memory-efficient and suitable for training very large models. It can converge quickly in large-scale NLP tasks, particularly when memory constraints are an issue.\n",
        "\n",
        "3. **Lion**:\n",
        "    - `lion_8bit`, `lion_32bit`, `paged_lion_32bit`, `paged_lion_8bit`:\n",
        "      - Lion optimizers are less commonly used but can offer faster convergence in some scenarios due to their specific optimization strategies.\n",
        "\n",
        "4. **SGD**:\n",
        "    - `sgd`:\n",
        "      - SGD with momentum can converge quickly in some scenarios but generally requires more careful tuning of learning rates and momentum parameters. It may not converge as fast as AdamW in many deep learning tasks.\n",
        "\n",
        "5. **Adagrad**:\n",
        "    - `adagrad`:\n",
        "      - Adagrad adapts the learning rate based on the historical gradient, which can be beneficial for sparse data but may lead to slower convergence in dense data scenarios.\n",
        "\n",
        "6. **Paged AdamW**:\n",
        "    - `paged_adamw_32bit`, `paged_adamw_8bit`:\n",
        "      - These optimizers are designed to handle large datasets with better memory management. Convergence speed can be good, especially for large-scale training.\n",
        "\n",
        "7. **RMSprop**:\n",
        "    - `rmsprop`, `rmsprop_bnb`, `rmsprop_bnb_8bit`, `rmsprop_bnb_32bit`:\n",
        "      - RMSprop is designed for fast convergence in non-stationary settings. It can converge faster than SGD in many cases.\n",
        "\n",
        "8. **Galore Optimizers**:\n",
        "    - `galore_adamw`, `galore_adamw_8bit`, `galore_adafactor`, `galore_adamw_layerwise`, `galore_adamw_8bit_layerwise`, `galore_adafactor_layerwise`:\n",
        "      - These optimizers are designed for advanced training scenarios and can offer fast convergence, especially when specific memory constraints or layer-wise optimizations are needed.\n",
        "\n",
        "### General Recommendations:\n",
        "- For most standard deep learning tasks, **AdamW variants** are likely to offer the fastest convergence due to their adaptive learning rates and weight decay.\n",
        "- For large-scale NLP tasks, **Adafactor** can be very efficient and fast.\n",
        "- For training on specific hardware (TPU, NPU), use the optimizers optimized for those platforms like `adamw_torch_xla` or `adamw_torch_npu_fused`.\n",
        "- For memory-constrained environments, **8-bit variants** and **Paged optimizers** can offer good convergence speed while managing memory efficiently.\n",
        "- If using large datasets, **paged variants** of AdamW or Lion can be particularly effective.\n",
        "\n",
        "Ultimately, the best way to determine which optimizer converges the fastest for your specific use case is to experiment with a few of them on your dataset and model. Hyperparameter tuning (such as learning rate adjustments) also plays a significant role in convergence speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a complete table summarizing the evaluation metrics mentioned, including their pros, cons, use cases, formulas, and code usage:\n",
        "\n",
        "| **Metric**          | **Pros**                                                    | **Cons**                                                      | **Use Case**                                           | **Formula**                                                                              | **Code Usage**                                                         |\n",
        "|---------------------|-------------------------------------------------------------|---------------------------------------------------------------|--------------------------------------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------|\n",
        "| **MSE**             | Penalizes larger errors more                                | Sensitive to outliers                                         | Regression                                              | \\[MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]                                   | `import evaluate \\nmetric = evaluate.load(\"mse\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Mean IoU**        | Standard in segmentation                                    | Can be misleading if classes are imbalanced                   | Image Segmentation                                     | \\[Mean IoU = \\frac{1}{k} \\sum_{i=1}^k \\frac{TP_i}{TP_i + FP_i + FN_i}\\]                   | `import evaluate \\nmetric = evaluate.load(\"mean_iou\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Pearson Correlation Coefficient** | Measures linear correlation                        | Only captures linear relationships                            | Regression                                              | \\[r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\\] | `import evaluate \\nmetric = evaluate.load(\"pearsonr\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **GLUE**            | Comprehensive benchmark                                     | Complex to interpret                                          | NLP                                                     | Various component metrics                                                               | `import evaluate \\nmetric = evaluate.load(\"glue\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Confusion Matrix**| Comprehensive error analysis                                | Can be difficult to interpret for many classes                | Classification                                          | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"confusion_matrix\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **SQuAD**           | Standard for QA                                             | Limited to QA tasks                                           | Question Answering                                      | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"squad\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Code Eval**       | Specialized for code generation                             | Limited to code generation                                    | Code Generation                                         | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"code_eval\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **TER**             | Measures post-edit distance                                 | Less commonly used                                            | Machine Translation                                     | \\[TER = \\frac{\\text{# of edits}}{\\text{average # of reference words}}\\]                  | `import evaluate \\nmetric = evaluate.load(\"ter\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Mahalanobis Distance** | Accounts for data distribution                             | Requires covariance matrix, less intuitive                    | Anomaly Detection, Clustering                           | \\[D_M = \\sqrt{(x - \\mu)^T S^{-1} (x - \\mu)}\\]                                            | `import evaluate \\nmetric = evaluate.load(\"mahalanobis\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **CUAD**            | Legal document understanding                                | Specialized for legal documents                               | Legal Document Analysis                                 | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"cuad\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Spearman Correlation Coefficient** | Measures rank correlation                            | Only captures monotonic relationships                         | Regression                                              | \\[œÅ = 1 - \\frac{6 \\sum d_i^2}{n (n^2 - 1)}\\]                                              | `import evaluate \\nmetric = evaluate.load(\"spearmanr\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Brier Score**     | Measures probability prediction accuracy                    | Limited to binary outcomes                                    | Probability Forecasting                                 | \\[BS = \\frac{1}{n} \\sum_{i=1}^n (f_i - o_i)^2\\]                                          | `import evaluate \\nmetric = evaluate.load(\"brier\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **BERT Score**      | Leverages contextual embeddings, correlates with human judgment | Requires large computational resources, newer and less widespread | Text Summarization, Translation                          | \\[BERTScore = cosine\\_similarity(pred_embeddings, ref_embeddings)\\]                     | `import evaluate \\nmetric = evaluate.load(\"bertscore\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **IndicGLUE**       | Benchmark for Indian languages                              | Limited to specific languages                                 | NLP for Indian Languages                                | Various component metrics                                                               | `import evaluate \\nmetric = evaluate.load(\"indic_glue\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **F1**              | Balances precision and recall                               | Not informative alone, can be misleading if data is imbalanced | Classification tasks                                    | \\[F1 = 2 * (precision \\* recall) / (precision + recall)\\]                              | `import evaluate \\nmetric = evaluate.load(\"f1\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **SQuAD v2**        | Includes unanswerable questions                             | Limited to QA tasks                                           | Question Answering                                      | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"squad_v2\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **chrF**            | Measures character n-gram F-score                           | Can be less interpretable                                     | Machine Translation                                     | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"chrf\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **WikiSplit**       | Measures sentence splitting and rephrasing                  | Specialized for text simplification                           | Text Simplification                                     | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"wikisplit\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **XTREME-S**        | Cross-lingual benchmark                                      | Complex and multifaceted                                      | Multilingual NLP                                        | Various component metrics                                                               | `import evaluate \\nmetric = evaluate.load(\"xtreme_s\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **NIST_MT**         | Measures precision with information weight                  | Less commonly used                                            | Machine Translation                                     | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"nist_mt\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **ROC AUC**         | Measures the ability of a classifier to distinguish classes | Can be over-optimistic with imbalanced datasets               | Binary Classification                                   | Area under the ROC curve                                                                 | `import evaluate \\nmetric = evaluate.load(\"roc_auc\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **CharacTER**       | Character-level translation error rate                      | Can be overly harsh on minor errors                           | Machine Translation                                     | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"character\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **CER**             | Measures character-level errors                             | Can be overly harsh on minor errors                           | Speech Recognition                                      | \\[CER = (Substitutions + Deletions + Insertions) / Number of characters in reference\\]  | `import evaluate \\nmetric = evaluate.load(\"cer\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Precision**       | Measures exactness of positive predictions                  | Does not account for false negatives                          | Classification tasks                                    | \\[Precision = TP / (TP + FP)\\]                                                           | `import evaluate \\nmetric = evaluate.load(\"precision\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **BLEURT**          | Context-aware evaluation metric for text                    | Requires pretrained model                                     | Text Generation, Translation                            | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"bleurt\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **SacreBLEU**       | Standardized BLEU metric                                    | Ignores synonyms, sensitive to exact matches                  | Machine Translation                                     | \\[P_n = (BP * exp(\\sum_{n=1}^N w_n \\* log(p_n))\\]                                      | `import evaluate \\nmetric = evaluate.load(\"sacrebleu\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Matthews Correlation Coefficient** | Measures quality of binary classifications           | Can be less intuitive                                         | Binary Classification                                   | \\[MCC = \\frac{TP \\* TN - FP \\* FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\\]       | `import evaluate \\nmetric = evaluate.load(\"mcc\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **CharCut**         | Measures character-level translation cut error              | Can be overly harsh on minor errors                           | Machine Translation                                     | N/A                                                                                      | `import evaluate \\nmetric = evaluate.load(\"charcut\") \\nresults = metric.compute(predictions, references)` |\n",
        "| **Competition MATH**| Standardized math benchmark                                 | Specialized for mathematical problem-solving                  | Math Problem Solving                                    | N/A                                                                                      | `import evaluate \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hugging Face provides a variety of evaluation metrics for assessing the performance of machine learning models, especially in natural language processing (NLP) tasks. Here‚Äôs a table summarizing the pros, cons, use cases, and formulas (where applicable) for some common evaluation metrics:\n",
        "\n",
        "| **Metric**      | **Pros**                                                                                     | **Cons**                                                                                    | **Use Cases**                                                                                     | **Formula**                                                                 |\n",
        "|-----------------|----------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
        "| **Accuracy**    | - Easy to understand and interpret.                                                          | - Not useful for imbalanced datasets.                                                      | - Classification tasks with balanced classes.                                                    | \\(\\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\\)                         |\n",
        "| **Precision**   | - Useful for understanding false positive rates.                                             | - Does not account for false negatives.                                                    | - When the cost of false positives is high (e.g., spam detection).                                | \\(\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\\)                           |\n",
        "| **Recall**      | - Useful for understanding false negative rates.                                             | - Does not account for false positives.                                                    | - When the cost of false negatives is high (e.g., medical diagnosis).                             | \\(\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\\)                           |\n",
        "| **F1 Score**    | - Balances precision and recall.                                                             | - Can be misleading if precision and recall are very different.                            | - General purpose, especially for imbalanced datasets.                                           | \\(2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)                  |\n",
        "| **ROC-AUC**     | - Measures the ability of the classifier to distinguish between classes.                     | - Can be over-optimistic with imbalanced datasets.                                         | - Binary classification tasks.                                                                    | Area under the ROC curve.                                                |\n",
        "| **BLEU**        | - Standard for evaluating machine translation.                                               | - Sensitive to exact matches, which may not capture semantic similarity.                   | - Machine translation, text generation.                                                           | \\(BLEU = BP \\cdot \\exp \\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\\)                                        |\n",
        "| **ROUGE**       | - Measures overlap of n-grams, useful for evaluating summaries.                              | - Can be biased towards longer summaries.                                                  | - Text summarization.                                                                             | Based on n-gram overlap and longest common subsequence.                          |\n",
        "| **METEOR**      | - Considers synonymy and stemming, often correlates better with human judgment.              | - More complex and computationally expensive than BLEU.                                    | - Machine translation, text generation.                                                           | Based on precision, recall, and fragmentation of matched segments.        |\n",
        "| **Perplexity**  | - Measures how well a probability model predicts a sample.                                   | - Not directly interpretable.                                                              | - Language modeling.                                                                              | \\(2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(x_i)}\\)                                                          |\n",
        "| **WER (Word Error Rate)** | - Measures the rate of errors in speech recognition.                                                 | - Can be unfairly harsh for minor errors.                                                   | - Speech recognition.                                                                             | \\(\\frac{\\text{Substitutions} + \\text{Insertions} + \\text{Deletions}}{\\text{Number of words in reference}}\\) |\n",
        "| **CIDEr**       | - Measures consensus in image captioning, based on cosine similarity of TF-IDF vectors.      | - May not capture all nuances of human judgment.                                           | - Image captioning.                                                                               | \\(\\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{TF-IDF}(g_i) \\cdot \\text{TF-IDF}(c_i)}{||\\text{TF-IDF}(g_i)|| \\cdot ||\\text{TF-IDF}(c_i)||}\\) |\n",
        "\n",
        "### Explanation of Metrics:\n",
        "\n",
        "1. **Accuracy**: Measures the proportion of correct predictions among the total predictions.\n",
        "2. **Precision**: Measures the proportion of true positive results among all positive results predicted by the classifier.\n",
        "3. **Recall**: Measures the proportion of true positive results among all actual positive instances.\n",
        "4. **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "5. **ROC-AUC**: Area under the receiver operating characteristic curve, evaluating the classifier's performance across all classification thresholds.\n",
        "6. **BLEU**: Bilingual Evaluation Understudy, measures the overlap between n-grams of the generated and reference texts.\n",
        "7. **ROUGE**: Recall-Oriented Understudy for Gisting Evaluation, measures the overlap of n-grams and longest common subsequence between generated and reference summaries.\n",
        "8. **METEOR**: Metric for Evaluation of Translation with Explicit ORdering, considers precision, recall, and fragmentation with stemming and synonymy.\n",
        "9. **Perplexity**: Measures how well a probabilistic model predicts a sample, often used in language modeling.\n",
        "10. **WER (Word Error Rate)**: Measures the rate of errors in speech recognition systems.\n",
        "11. **CIDEr**: Consensus-based Image Description Evaluation, measures the similarity of generated captions to reference captions using TF-IDF weighting."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
